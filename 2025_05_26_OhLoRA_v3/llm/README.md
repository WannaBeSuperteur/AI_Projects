## ëª©ì°¨

* [1. OhLoRA-v3 LLM ì „ì²´ ë©”ì»¤ë‹ˆì¦˜](#1-ohlora-v3-llm-ì „ì²´-ë©”ì»¤ë‹ˆì¦˜)
  * [1-1. í˜„ì¬ ì‹œê°„ ì •ë³´ ì¶”ê°€](#1-1-í˜„ì¬-ì‹œê°„-ì •ë³´-ì¶”ê°€)
  * [1-2. LLM Memory (RAG-like concept)](#1-2-llm-memory-rag-like-concept)
  * [1-3. LLM Ethics (S-BERT)](#1-3-llm-ethics-s-bert)
* [2. OhLoRA-v2 LLM Fine-Tuning](#2-ohlora-v2-llm-fine-tuning)
* [3. ì½”ë“œ ì‹¤í–‰ ë°©ë²•](#3-ì½”ë“œ-ì‹¤í–‰-ë°©ë²•)
  * [3-1. LLM Fine-Tuning](#3-1-llm-fine-tuning)
  * [3-2. Memory Mechanism (RAG-like concept)](#3-2-memory-mechanism-rag-like-concept)
  * [3-3. Ethics Mechanism](#3-3-ethics-mechanism)
* [4. ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ê²½ë¡œ](#4-ëª¨ë¸-ë‹¤ìš´ë¡œë“œ-ê²½ë¡œ)
* [5. í–¥í›„ ì§„í–‰í•˜ê³  ì‹¶ì€ ê²ƒ](#5-í–¥í›„-ì§„í–‰í•˜ê³ -ì‹¶ì€-ê²ƒ)

## 1. OhLoRA-v3 LLM ì „ì²´ ë©”ì»¤ë‹ˆì¦˜

* ìš”ì•½
  * [ì˜¤ë¡œë¼ v2 LLM ì „ì²´ ë©”ì»¤ë‹ˆì¦˜](../../2025_05_02_OhLoRA_v2/llm/README.md#1-ohlora-v2-llm-ì „ì²´-ë©”ì»¤ë‹ˆì¦˜) ì„ ê¸°ë°˜ìœ¼ë¡œ ì„¤ê³„
  * ë‹¨, ì‚¬ìš©ìì˜ ìµœì´ˆ ì›ë³¸ ì§ˆë¬¸ì— **ì‹œê°„ ë‹¨ì–´ (```ì˜¤ëŠ˜``` ```ë‚´ì¼``` ë“±) í¬í•¨** ì‹œ, **í˜„ì¬ ì‹œê°„ ì •ë³´** ë¥¼ ì‚¬ìš©ì ì§ˆë¬¸ì— ì¶”ê°€ [(ìƒì„¸ ì„¤ëª…)](#1-1-í˜„ì¬-ì‹œê°„-ì •ë³´-ì¶”ê°€)

![image](../../images/250526_21.png)

* LLM ëª¨ë¸
  * [LLM ëª¨ë¸ ì‚¬ìš© ìƒì„¸ ê²°ì •ì•ˆ ì½”ë©˜íŠ¸](https://github.com/WannaBeSuperteur/AI_Projects/issues/21#issuecomment-2926149441) 
  * [LoRA Rank 64 vs. 16 ì‹¤í—˜ ê²°ê³¼ ë³´ê³ ì„œ](fine_tuning/report_LoRA_rank_64_vs_16.md)

| ëª¨ë¸                                   | ì„¤ëª…                                                                                                                                  | Base Model                                                                                                  |
|--------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------|
| ë‹µë³€ ë©”ì‹œì§€ ```output_message```          | Oh-LoRA ğŸ‘±â€â™€ï¸ (ì˜¤ë¡œë¼) ì˜ ë‹µë³€ì„ ìœ„í•œ ë©”ì¸ LLM                                                                                                 | [Kanana-1.5 2.1B (HuggingFace)](https://huggingface.co/kakaocorp/kanana-1.5-2.1b-base)<br>(by **Kakao**)    |
| ìµœê·¼ ëŒ€í™” ë‚´ìš© ìš”ì•½ ```summary```            | ì‚¬ìš©ìì˜ ì§ˆë¬¸ ë° Oh-LoRA ğŸ‘±â€â™€ï¸ (ì˜¤ë¡œë¼) ì˜ ë‹µë³€ ë‚´ìš©ì„ ìš”ì•½í•˜ì—¬, **ë‹¤ìŒ í„´ì—ì„œ ì´ ì •ë³´ë¥¼ í™œìš©í•˜ì—¬ ì˜¤ë¡œë¼ê°€ ë³´ë‹¤ ìì—°ìŠ¤ëŸ½ê²Œ ë‹µí•  ìˆ˜ ìˆê²Œ** í•¨                                          | [Kanana-1.5 2.1B (HuggingFace)](https://huggingface.co/kakaocorp/kanana-1.5-2.1b-base)<br>(by **Kakao**)    |
| ë©”ëª¨ë¦¬ (ì‚¬ìš©ìì— ëŒ€í•´ ê¸°ì–µ í•„ìš”í•œ ë‚´ìš©) ```memory``` | ì‚¬ìš©ìì˜ ì§ˆë¬¸ ë° ê´€ë ¨ ì •ë³´ë¡œë¶€í„° Oh-LoRA ğŸ‘±â€â™€ï¸ (ì˜¤ë¡œë¼) ê°€ ê¸°ì–µí•´ì•¼ í•  ë‚´ìš© ì¶”ì¶œ<br>- ì´ë¥¼ í†µí•´ [Oh-LoRA ğŸ‘±â€â™€ï¸ (ì˜¤ë¡œë¼) ì˜ ë©”ëª¨ë¦¬](#1-2-llm-memory-rag-like-concept) ì—…ë°ì´íŠ¸ | [Polyglot-Ko 1.3B (HuggingFace)](https://huggingface.co/EleutherAI/polyglot-ko-1.3b)<br>(by **EleutherAI**) |
| í‘œì • ë° ê³ ê°œ ëŒë¦¼ ì œì–´ ```eyes_mouth_pose```  | [Oh-LoRA ğŸ‘±â€â™€ï¸ (ì˜¤ë¡œë¼) ì´ë¯¸ì§€ ìƒì„±](../stylegan/README.md) ì„ ìœ„í•œ í‘œì • ì •ë³´ ì¶”ì¶œ                                                                   | [Polyglot-Ko 1.3B (HuggingFace)](https://huggingface.co/EleutherAI/polyglot-ko-1.3b)<br>(by **EleutherAI**) |

* ì¶”ê°€ ë©”ì»¤ë‹ˆì¦˜
  * [LLM Memory](#1-2-llm-memory-rag-like-concept) ë° [LLM Ethics](#1-3-llm-ethics-s-bert) ì— ì‚¬ìš©ë˜ëŠ” S-BERT ëª¨ë¸ì€ **ì„œë¡œ ë³„ê°œì˜ ëª¨ë¸ì„**

| ë©”ì»¤ë‹ˆì¦˜                                           | ì„¤ëª…                                                                                                                                                                                                                           | í•™ìŠµ ëª¨ë¸                                                                                                                                                                                      |
|------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| [LLM Memory](#1-2-llm-memory-rag-like-concept) | - [RAG (Retrieval Augmented Generation)](https://github.com/WannaBeSuperteur/AI-study/blob/main/AI%20Basics/LLM%20Basics/LLM_%EA%B8%B0%EC%B4%88_RAG.md) ê³¼ ìœ ì‚¬í•œ ì»¨ì…‰<br>- ì‚¬ìš©ì ì§ˆë¬¸ì— ëŒ€í•´ Oh-LoRA ğŸ‘±â€â™€ï¸ ì˜ ë©”ëª¨ë¦¬ì—ì„œ ê·¸ ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ì ì ˆí•œ ì •ë³´ë¥¼ ê°€ì ¸ì˜´ | [Sentence BERT (S-BERT)]((https://github.com/WannaBeSuperteur/AI-study/blob/main/Natural%20Language%20Processing/Basics_BERT%2C%20SBERT%20%EB%AA%A8%EB%8D%B8.md#sbert-%EB%AA%A8%EB%8D%B8)) |
| [LLM Ethics](#1-3-llm-ethics-s-bert)           | - ì‚¬ìš©ìì˜ ì§ˆë¬¸ì´ ë¬¸ì œê°€ ìˆëŠ” ì§ˆë¬¸ (```ë§Œë‚¨/ì‚¬ë‘ ê³ ë°±``` ```ì •ì¹˜``` ```íŒ¨ë“œë¦½``` ì¹´í…Œê³ ë¦¬ ì¤‘ í•˜ë‚˜) ì¸ì§€ë¥¼ íŒë‹¨<br>- ê° ì¹´í…Œê³ ë¦¬ì™€ì˜ ìœ ì‚¬ë„ë¥¼ íŒë‹¨                                                                                                                              | [Sentence BERT (S-BERT)]((https://github.com/WannaBeSuperteur/AI-study/blob/main/Natural%20Language%20Processing/Basics_BERT%2C%20SBERT%20%EB%AA%A8%EB%8D%B8.md#sbert-%EB%AA%A8%EB%8D%B8)) | 

### 1-1. í˜„ì¬ ì‹œê°„ ì •ë³´ ì¶”ê°€

* ì‚¬ìš©ì ì§ˆë¬¸ì— ë‹¤ìŒ ë‹¨ì–´ ì¤‘ ì ì–´ë„ 1ê°œ í¬í•¨ ì‹œ, ì‚¬ìš©ìì˜ ìµœì´ˆ ì›ë³¸ ì§ˆë¬¸ì— í˜„ì¬ ì‹œê°„ ì •ë³´ ì¶”ê°€
  * ```ì˜¤ëŠ˜``` ```ë‚´ì¼``` ```ì§€ê¸ˆ``` ```ìš”ì¼``` ```ì´ë”°``` ```íœ´ì¼```
* í˜„ì¬ ì‹œê°„ ì •ë³´

| ì»´í“¨í„° ì‹œìŠ¤í…œ ì‹œê° (ë¡œì»¬ ê¸°ì¤€)                        | í˜„ì¬ ì‹œê°„ ì •ë³´           |
|-------------------------------------------|--------------------|
| ë‹¹ì¼ AM 04:00 - AM 11:59                    | ```(ì§€ê¸ˆì€ Oìš”ì¼ ì˜¤ì „)``` |
| ë‹¹ì¼ ì •ì˜¤ - PM 05:59 (ê¸ˆìš”ì¼ì€ PM 04:59)          | ```(ì§€ê¸ˆì€ Oìš”ì¼ ì˜¤í›„)``` |
| ë‹¹ì¼ PM 06:00 (ê¸ˆìš”ì¼ì€ PM 05:00) - ìµì¼ AM 03:59 | ```(ì§€ê¸ˆì€ Oìš”ì¼ ì €ë…)``` |

* ì˜ˆì‹œ
  * ```í† ìš”ì¼ PM 02:00``` ì— ```ë¡œë¼ì•¼ ì˜¤ëŠ˜ ì•½ì† ì—†ëŠ”ë° ë­í•˜ì§€``` ë¼ê³  ì§ˆë¬¸í•˜ë©´,
  * í˜„ì¬ ì‹œê°„ ì •ë³´ì¸ ```(ì§€ê¸ˆì€ í† ìš”ì¼ ì˜¤í›„)``` ê°€ ì¶”ê°€ë˜ì–´ ì‚¬ìš©ì ì§ˆë¬¸ì´ ```(ì§€ê¸ˆì€ í† ìš”ì¼ ì˜¤í›„) ë¡œë¼ì•¼ ì˜¤ëŠ˜ ì•½ì† ì—†ëŠ”ë° ë­í•˜ì§€``` ê°€ ë¨

### 1-2. LLM Memory (RAG-like concept)

![image](../../images/250408_28.PNG)

* ë™ì‘ ì›ë¦¬
  * [ì˜¤ë¡œë¼ v1 ì˜ LLM Memory êµ¬í˜„](../../2025_04_08_OhLoRA/llm/README.md#3-llm-memory-rag-like-concept) ê³¼ ë™ì¼
  * í•™ìŠµ ë° inference ë“± **ëª¨ë“  ì•Œê³ ë¦¬ì¦˜ ë¡œì§** ì€ [ì˜¤ë¡œë¼ v2 ì˜ LLM Memory êµ¬í˜„](../../2025_05_02_OhLoRA_v2/llm/README.md#1-1-llm-memory-rag-like-concept) ê³¼ ë™ì¼ [(S-BERT í•™ìŠµ ì•Œê³ ë¦¬ì¦˜)](../../2025_05_02_OhLoRA_v2/llm/README.md#1-2-llm-memory-ë©”ì»¤ë‹ˆì¦˜-í•™ìŠµ-s-bert)
* êµ¬í˜„ ì½”ë“œ
  * [S-BERT Inference](memory_mechanism/inference_sbert.py)
  * [Entry & Best Memory Item Choice](run_memory_mechanism.py)
  * LLM ì´ ì¶œë ¥í•œ í‘œì • ì •ë³´ë¥¼ Property Score ë¡œ ë³€í™˜ (TBU)

### 1-3. LLM Ethics (S-BERT)

![image](../../images/250526_22.png)

* ë™ì‘ ì›ë¦¬
  * [Sentence BERT (S-BERT)](https://github.com/WannaBeSuperteur/AI-study/blob/main/Natural%20Language%20Processing/Basics_BERT%2C%20SBERT%20%EB%AA%A8%EB%8D%B8.md#sbert-%EB%AA%A8%EB%8D%B8) ê¸°ë°˜ 
  * ì‚¬ìš©ìì˜ ìµœì´ˆ ì›ë³¸ í”„ë¡¬í”„íŠ¸ì™€ ê° ë¶ˆëŸ‰ ì–¸ì–´ ì¹´í…Œê³ ë¦¬ (```ì‚¬ë‘ ê³ ë°±/ë§Œë‚¨``` ```ì •ì¹˜``` ```íŒ¨ë“œë¦½```) ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ë‹¨ì–´ë¥¼ S-BERT ë¡œ ë¹„êµ
  * ê° ì¹´í…Œê³ ë¦¬ ë³„ **ì¼ì • threshold** ë³´ë‹¤ ë†’ìœ¼ë©´ í•´ë‹¹ ì¹´í…Œê³ ë¦¬ì˜ ë°œì–¸ìœ¼ë¡œ íŒë‹¨
  * ì‚¬ìš©ìì˜ ê¸°ì¡´ ì œì¬ ë¡œê·¸ë¥¼ ê²€í† í•˜ì—¬ **ìµœì¢… ì œì¬ ìˆ˜ìœ„ (Oh-LoRA ğŸ‘±â€â™€ï¸ ê´€ë ¨ ì œí’ˆ "ì „ì²´ ì´ìš© ì œí•œ") ê²°ì •**
* êµ¬í˜„ ì½”ë“œ
  * [S-BERT Training](ethics_mechanism/train_sbert.py)
  * [S-BERT Inference](ethics_mechanism/inference_sbert.py)
  * ì‹¤ì œ ì‚¬ìš©ì ì œì¬ ì²˜ë¦¬ (TBU)
* ë³´ê³ ì„œ
  * [Ethics mechanism í…ŒìŠ¤íŠ¸ ê²°ê³¼ ë³´ê³ ì„œ](ethics_mechanism/test_report.md) 

## 2. OhLoRA-v2 LLM Fine-Tuning

* í•™ìŠµ ëª¨ë¸
  * **Kanana-1.5 2.1B (by Kakao, 2.09 B params) (âœ… for ```output_message``` ```summary```)** [(HuggingFace)](https://huggingface.co/kakaocorp/kanana-1.5-2.1b-base)
  * **Polyglot-Ko 1.3B (by EleutherAI, 1.43 B params) (âœ… for ```memory``` ```eyes_mouth_pose```)** [(HuggingFace)](https://huggingface.co/EleutherAI/polyglot-ko-1.3b) 
* í•™ìŠµ ë°©ë²• 
  * [SFT (Supervised Fine-Tuning)](https://github.com/WannaBeSuperteur/AI-study/blob/main/AI%20Basics/LLM%20Basics/LLM_%EA%B8%B0%EC%B4%88_Fine_Tuning_SFT.md)
  * [LoRA (Low-Rank Adaption)](https://github.com/WannaBeSuperteur/AI-study/blob/main/AI%20Basics/LLM%20Basics/LLM_%EA%B8%B0%EC%B4%88_Fine_Tuning_LoRA_QLoRA.md), LoRA Rank = 64
  * train for **5 - 30 epochs**
  * initial [learning rate](https://github.com/WannaBeSuperteur/AI-study/blob/main/AI%20Basics/Deep%20Learning%20Basics/%EB%94%A5%EB%9F%AC%EB%8B%9D_%EA%B8%B0%EC%B4%88_Learning_Rate.md) : **0.0003 (= 3e-4)**
* í•™ìŠµ ë°ì´í„°ì…‹
  * train ë°ì´í„° **572 rows ([v2](../../2025_05_02_OhLoRA_v2/llm/README.md#3-ohlora-v2-llm-fine-tuning) ëŒ€ë¹„ ğŸ”º 25.4 %)**, valid ë°ì´í„° **90 rows ([v2](../../2025_05_02_OhLoRA_v2/llm/README.md#3-ohlora-v2-llm-fine-tuning) ëŒ€ë¹„ ğŸ”º 28.6 %)**
  * train & valid data ëŠ” ëª¨ë‘ [v3 dataset](fine_tuning_dataset/OhLoRA_fine_tuning_v3.csv) ì„ ì‚¬ìš©
  * **ì˜¤ë¡œë¼ì˜ ë‹µë³€ ë©”ì‹œì§€ (```output_message```)** LLM ì˜ ê²½ìš°, ì…ë ¥ ë°ì´í„°ì— **ìœ ì˜ì–´ë¡œì˜ êµì²´** ì— ê¸°ë°˜í•œ ê°„ë‹¨í•œ data augmentation ì ìš© [(êµ¬í˜„ ì½”ë“œ)](fine_tuning/augmentation.py)
* ìƒì„¸ í•™ìŠµ ì„¤ì •

| ëª¨ë¸ (task)                            | ì‚¬ìš©í•  LLM          | LoRA rank | epochs | ë¹„ê³                                                         |
|--------------------------------------|------------------|-----------|--------|-----------------------------------------------------------|
| ë‹µë³€ ë©”ì‹œì§€ ```output_message```          | Kanana-1.5 2.1B  | 64        | 5      | LLM ì´ ì–´ëŠ ì •ë„ í•™ìŠµë˜ë©´ì„œë„ í™˜ê° í˜„ìƒì´ ì¶©ë¶„íˆ ì ì„ ê²ƒìœ¼ë¡œ ì˜ˆìƒë˜ëŠ” ìµœì ì˜ epoch count |
| ìµœê·¼ ëŒ€í™” ë‚´ìš© ìš”ì•½ ```summary```            | Kanana-1.5 2.1B  | 64        | 10     | 10 epoch ë§Œìœ¼ë¡œë„ ì¶©ë¶„íˆ ì„±ëŠ¥ ì¢‹ê²Œ Fine-Tuning ë¨                     |
| ë©”ëª¨ë¦¬ (ì‚¬ìš©ìì— ëŒ€í•´ ê¸°ì–µ í•„ìš”í•œ ë‚´ìš©) ```memory``` | Polyglot-Ko 1.3B | 64        | 20     | Kanana-1.5 2.1B ì‚¬ìš© ì‹œ ì¶”ë¡  ì‹œê°„ ì¦ê°€ & OOM ìš°ë ¤                    |
| í‘œì • ë° ê³ ê°œ ëŒë¦¼ ì œì–´ ```eyes_mouth_pose```  | Polyglot-Ko 1.3B | 64        | 30     | Kanana-1.5 2.1B ì‚¬ìš© ì‹œ ì¶”ë¡  ì‹œê°„ ì¦ê°€ & OOM ìš°ë ¤                    |

* ì°¸ê³ 
  * [ìƒì„¸ í•™ìŠµ ì„¤ì • ê²°ì • ìµœì¢…ì•ˆ ì½”ë©˜íŠ¸](https://github.com/WannaBeSuperteur/AI_Projects/issues/21#issuecomment-2926149441) 
  * [ì˜¤ë¡œë¼ v1 ('25.04.08 - 04.25) ì—ì„œì˜ LLM Fine-Tuning ë°©ë²•](../../2025_04_08_OhLoRA/llm/README.md#2-how-to-run-fine-tuning) 
  * [ì˜¤ë¡œë¼ v2 ('25.05.02 - 05.21) ì—ì„œì˜ LLM Fine-Tuning ë°©ë²•](../../2025_05_02_OhLoRA_v2/llm/README.md#3-ohlora-v2-llm-fine-tuning) 

## 3. ì½”ë“œ ì‹¤í–‰ ë°©ë²•

ëª¨ë“  ì½”ë“œëŠ” **ë¨¼ì € ì•„ë˜ [ë‹¤ìš´ë¡œë“œ ê²½ë¡œ ì•ˆë‚´](#4-ëª¨ë¸-ë‹¤ìš´ë¡œë“œ-ê²½ë¡œ) (TBU) ë° í•´ë‹¹ ê° HuggingFace ë§í¬ì— ìˆëŠ” Model Card ì— ë‚˜íƒ€ë‚œ ì €ì¥ ê²½ë¡œ (Save Path) ì •ë³´ë¥¼ ì°¸ê³ í•˜ì—¬ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ í›„,** ```2025_05_26_OhLoRA_v3``` (í”„ë¡œì íŠ¸ ë©”ì¸ ë””ë ‰í† ë¦¬) ì—ì„œ ì‹¤í–‰

### 3-1. LLM Fine-Tuning

ì§€ì •ëœ ê²½ë¡œì— í•´ë‹¹ LLM ì´ ì´ë¯¸ ì¡´ì¬í•˜ëŠ” ê²½ìš°, Fine-Tuning ëŒ€ì‹  **inference test ì‹¤í–‰ë¨**

| LLM                                  | Fine-Tuning ì½”ë“œ ì‹¤í–‰ ë°©ë²•                                                                 |
|--------------------------------------|--------------------------------------------------------------------------------------|
| ë‹µë³€ ë©”ì‹œì§€ ```output_message```          | ```python llm/run_fine_tuning.py -llm_names kanana -output_cols output_message```    |
| ìµœê·¼ ëŒ€í™” ë‚´ìš© ìš”ì•½ ```summary```            | ```python llm/run_fine_tuning.py -llm_names polyglot -output_cols summary```         |
| ë©”ëª¨ë¦¬ (ì‚¬ìš©ìì— ëŒ€í•´ ê¸°ì–µ í•„ìš”í•œ ë‚´ìš©) ```memory``` | ```python llm/run_fine_tuning.py -llm_names kanana -output_cols memory```            |
| í‘œì • ë° ê³ ê°œ ëŒë¦¼ ì œì–´ ```eyes_mouth_pose```  | ```python llm/run_fine_tuning.py -llm_names polyglot -output_cols eyes_mouth_pose``` |

### 3-2. Memory Mechanism (RAG-like concept)

* **Memory Mechanism (S-BERT)** ëª¨ë¸ ì‹¤í–‰ (í•´ë‹¹ ëª¨ë¸ ì—†ì„ ì‹œ, Training ë¨¼ì € ì‹¤í–‰)
  * ```python llm/run_memory_mechanism.py```

### 3-3. Ethics Mechanism

* **Ethics Mechanism (S-BERT)** ëª¨ë¸ ì‹¤í–‰ (í•´ë‹¹ ëª¨ë¸ ì—†ì„ ì‹œ, Training ë¨¼ì € ì‹¤í–‰)
  * ```python llm/run_ethics_mechanism.py```

## 4. ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ê²½ë¡œ

TBU

## 5. í–¥í›„ ì§„í–‰í•˜ê³  ì‹¶ì€ ê²ƒ

TBU
