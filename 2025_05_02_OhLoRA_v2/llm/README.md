## ëª©ì°¨

* [1. OhLoRA-v2 LLM ì „ì²´ ë©”ì»¤ë‹ˆì¦˜](#1-ohlora-v2-llm-ì „ì²´-ë©”ì»¤ë‹ˆì¦˜)
  * [1-1. LLM Memory (RAG-like concept)](#1-1-llm-memory-rag-like-concept)
  * [1-2. LLM Memory ë©”ì»¤ë‹ˆì¦˜ í•™ìŠµ (S-BERT)](#1-2-llm-memory-ë©”ì»¤ë‹ˆì¦˜-í•™ìŠµ-s-bert)
  * [1-3. LLM Memory ë©”ì»¤ë‹ˆì¦˜ í…ŒìŠ¤íŠ¸ ê²°ê³¼](#1-3-llm-memory-ë©”ì»¤ë‹ˆì¦˜-í…ŒìŠ¤íŠ¸-ê²°ê³¼)
* [2. OhLoRA-v2 LLM Final Selection](#2-ohlora-v2-llm-final-selection)
* [3. OhLoRA-v2 LLM Fine-Tuning](#3-ohlora-v2-llm-fine-tuning)
* [4. ì½”ë“œ ì‹¤í–‰ ë°©ë²•](#4-ì½”ë“œ-ì‹¤í–‰-ë°©ë²•)
  * [4-1. ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ê²½ë¡œ](#4-1-ëª¨ë¸-ë‹¤ìš´ë¡œë“œ-ê²½ë¡œ)

## 1. OhLoRA-v2 LLM ì „ì²´ ë©”ì»¤ë‹ˆì¦˜

![image](../../images/250502_20.PNG)

* [LLM Memory](#1-1-llm-memory-rag-like-concept) ëŠ” [RAG (Retrieval Augmented Generation)](https://github.com/WannaBeSuperteur/AI-study/blob/main/AI%20Basics/LLM%20Basics/LLM_%EA%B8%B0%EC%B4%88_RAG.md) ê³¼ ìœ ì‚¬í•œ ì»¨ì…‰

| ëª¨ë¸                                     | ì„¤ëª…                                                                                                                                  |
|----------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------|
| LLM ë‹µë³€ ```output_message```            | Oh-LoRA ğŸ‘±â€â™€ï¸ (ì˜¤ë¡œë¼) ì˜ ë‹µë³€ì„ ìœ„í•œ ë©”ì¸ LLM                                                                                                 |
| memory (RAG-like concept) ```memory``` | ì‚¬ìš©ìì˜ ì§ˆë¬¸ ë° ê´€ë ¨ ì •ë³´ë¡œë¶€í„° Oh-LoRA ğŸ‘±â€â™€ï¸ (ì˜¤ë¡œë¼) ê°€ ê¸°ì–µí•´ì•¼ í•  ë‚´ìš© ì¶”ì¶œ<br>- ì´ë¥¼ í†µí•´ [Oh-LoRA ğŸ‘±â€â™€ï¸ (ì˜¤ë¡œë¼) ì˜ ë©”ëª¨ë¦¬](#1-1-llm-memory-rag-like-concept) ì—…ë°ì´íŠ¸ |
| í‘œì •/ëª¸ì§“ ```eyes_mouth_pose```            | [Oh-LoRA ğŸ‘±â€â™€ï¸ (ì˜¤ë¡œë¼) ì´ë¯¸ì§€ ìƒì„±](../stylegan/README.md) ì„ ìœ„í•œ í‘œì • ì •ë³´ ì¶”ì¶œ                                                                   |
| summary (í•˜ê³  ìˆëŠ” ëŒ€í™” ìš”ì•½) ```summary```    | ì‚¬ìš©ìì˜ ì§ˆë¬¸ ë° Oh-LoRA ğŸ‘±â€â™€ï¸ (ì˜¤ë¡œë¼) ì˜ ë‹µë³€ ë‚´ìš©ì„ ìš”ì•½í•˜ì—¬, **ë‹¤ìŒ í„´ì—ì„œ ì´ ì •ë³´ë¥¼ í™œìš©í•˜ì—¬ ì˜¤ë¡œë¼ê°€ ë³´ë‹¤ ìì—°ìŠ¤ëŸ½ê²Œ ë‹µí•  ìˆ˜ ìˆê²Œ** í•¨                                          |

### 1-1. LLM Memory (RAG-like concept)

![image](../../images/250408_28.PNG)

* ë™ì‘ ì›ë¦¬
  * [ì˜¤ë¡œë¼ 1ì°¨ í”„ë¡œì íŠ¸ì˜ LLM Memory êµ¬í˜„](../../2025_04_08_OhLoRA/llm/README.md#3-llm-memory-rag-like-concept) ê³¼ ë™ì¼
* êµ¬í˜„ ì½”ë“œ
  * [S-BERT Training](memory_mechanism/train_sbert.py)
  * [S-BERT Inference](memory_mechanism/inference_sbert.py)
  * [Entry & Best Memory Item Choice](run_memory_mechanism.py)

### 1-2. LLM Memory ë©”ì»¤ë‹ˆì¦˜ í•™ìŠµ (S-BERT)

![image](../../images/250502_19.PNG)

* í•™ìŠµ ë° í…ŒìŠ¤íŠ¸ ë°ì´í„°
  * **ì‹¤ì œ ë°ì´í„°** ëŠ” **ë°ì´í„° ìƒì„±ìš© ì¡°í•©** ì˜ ê° line ì˜ **memory** (ì˜ˆ: ```[ì˜¤ëŠ˜ ì¼ì •: ì¹œêµ¬ë‘ ì¹´í˜ ë°©ë¬¸]```) ì™€ **message** (ë‚˜ë¨¸ì§€ ë¶€ë¶„) ì„ SQL ì˜ cartesian product ì™€ ìœ ì‚¬í•œ ë°©ë²•ìœ¼ë¡œ combination (?) í•˜ì—¬ ìƒì„±
  * [ë°ì´í„° ìƒì„± êµ¬í˜„ ì½”ë“œ](memory_mechanism/generate_dataset.py)

| ë°ì´í„°        | ë°ì´í„° ìƒì„±ìš© ì¡°í•©                                                                    | ì‹¤ì œ ë°ì´í„°<br>(í•™ìŠµ ëŒ€ìƒ column : ```memory_0``` ```user_prompt_1``` ```similarity_score```) |
|------------|-------------------------------------------------------------------------------|--------------------------------------------------------------------------------------|
| í•™ìŠµ ë° valid | [train_dataset_combs.txt](memory_mechanism/train_dataset_combs.txt) (80 rows) | [train_dataset.csv](memory_mechanism/train_dataset.csv) (6,400 rows)                 |
| í…ŒìŠ¤íŠ¸        | [test_dataset_combs.txt](memory_mechanism/test_dataset_combs.txt) (40 rows)   | [test_dataset.csv](memory_mechanism/test_dataset.csv) (1,600 rows)                   |

* Cosine Similarity ì˜ Ground Truth ê°’
  * ê¸°ë³¸ ì»¨ì…‰ 
    * 2 ê°œì˜ memory text ì˜ key (ì˜ˆ: ```[ì˜¤ëŠ˜ ì¼ì •: ì¹œêµ¬ë‘ ì¹´í˜ ë°©ë¬¸]``` â†’ ```ì˜¤ëŠ˜ ì¼ì •```) ì— ëŒ€í•´,
    * **Pre-trained [S-BERT (Sentence BERT)](https://github.com/WannaBeSuperteur/AI-study/blob/main/Natural%20Language%20Processing/Basics_BERT%2C%20SBERT%20%EB%AA%A8%EB%8D%B8.md#sbert-%EB%AA%A8%EB%8D%B8) Model** ì— ì˜í•´ ë„ì¶œëœ ìœ ì‚¬ë„ **(Cosine Similarity)** ë¥¼ Ground Truth ë¡œ í•¨
  * ì¶”ê°€ êµ¬í˜„ ì‚¬í•­
    * ```ì¢‹ì•„í•˜ëŠ” ì•„ì´ëŒ``` ê³¼ ```ì¢‹ì•„í•˜ëŠ” ê°€ìˆ˜``` ë¼ëŠ” key ëŠ” ë™ì¼í•œ key ë¡œ ê°„ì£¼ 
    * S-BERT ì— ì˜í•´ ê³„ì‚°ëœ similarity score ```x``` ì˜ ë¶„í¬ë¥¼ **0 ~ 1 ë¡œ [ì •ê·œí™” (Normalization)](https://github.com/WannaBeSuperteur/AI-study/blob/main/AI%20Basics/Data%20Science%20Basics/%EB%8D%B0%EC%9D%B4%ED%84%B0_%EC%82%AC%EC%9D%B4%EC%96%B8%EC%8A%A4_%EA%B8%B0%EC%B4%88_Normalization.md)** í•˜ê¸° ìœ„í•´ ë‹¤ìŒ ìˆ˜ì‹ ì ìš©
      * **```x``` â† max(2.6 $\times$ ```x``` - 1.6, 0)**
    * memory text ì˜ key ê°€ ```ìƒíƒœ``` ì¸ ê²½ìš°ì—ëŠ” ê·¸ ëŒ€ì‹  memory text ì˜ 'value'ë¥¼ ì´ìš©
      * ì˜ˆ: ```[ìƒíƒœ: ì˜¤ë¡œë¼ ë§Œë‚˜ê³  ì‹¶ìŒ]``` â†’ key ì¸ ```ìƒíƒœ``` ëŒ€ì‹  value ì¸ ```ì˜¤ë¡œë¼ ë§Œë‚˜ê³  ì‹¶ìŒ``` ì„ ì´ìš© 

* í•™ìŠµ ì„¤ì •
  * Base Model : ```klue/roberta-base``` [(HuggingFace Link)](https://huggingface.co/klue/roberta-base)
  * Pooling ì„¤ì • : Mean Pooling ì ìš©
  * 10 epochs

* ì°¸ê³ 
  * [ì˜¤ë¡œë¼ 1ì°¨ í”„ë¡œì íŠ¸ì˜ LLM Memory ìš© S-BERT ëª¨ë¸ í•™ìŠµ](../../2025_04_08_OhLoRA/llm/README.md#3-2-í•™ìŠµ-ë°-í…ŒìŠ¤íŠ¸-ë°ì´í„°--í•™ìŠµ-ì„¤ì •) 
  * [ë¸”ë¡œê·¸ í¬ìŠ¤íŒ…](https://velog.io/@jaehyeong/Basic-NLP-sentence-transformers-%EB%9D%BC%EC%9D%B4%EB%B8%8C%EB%9F%AC%EB%A6%AC%EB%A5%BC-%ED%99%9C%EC%9A%A9%ED%95%9C-SBERT-%ED%95%99%EC%8A%B5-%EB%B0%A9%EB%B2%95)

### 1-3. LLM Memory ë©”ì»¤ë‹ˆì¦˜ í…ŒìŠ¤íŠ¸ ê²°ê³¼

* Predicted vs. True Cosine Similarity ë¹„êµ (í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹)

![image](../../images/250502_18.PNG)

* MSE, MAE & Corr-coef (í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹)

| Fine-Tuned S-BERT ëª¨ë¸                                            | [MSE](https://github.com/WannaBeSuperteur/AI-study/blob/main/AI%20Basics/Deep%20Learning%20Basics/%EB%94%A5%EB%9F%AC%EB%8B%9D_%EA%B8%B0%EC%B4%88_Loss_function.md#2-1-mean-squared-error-mse) | [MAE](https://github.com/WannaBeSuperteur/AI-study/blob/main/AI%20Basics/Deep%20Learning%20Basics/%EB%94%A5%EB%9F%AC%EB%8B%9D_%EA%B8%B0%EC%B4%88_Loss_function.md#2-3-mean-absolute-error-mae) | Corr-coef (ìƒê´€ê³„ìˆ˜) |
|-----------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|------------------|
| í˜„ì¬ ë²„ì „                                                           | **0.0355**                                                                                                                                                                                    | **0.1280**                                                                                                                                                                                     | **0.7449**       |
| [ì˜¤ë¡œë¼ 1ì°¨ í”„ë¡œì íŠ¸](../../2025_04_08_OhLoRA/llm/README.md#3-3-í…ŒìŠ¤íŠ¸-ê²°ê³¼) | 0.0880                                                                                                                                                                                        | 0.1681                                                                                                                                                                                         | 0.6259           |
| ë¹„êµ                                                              | ğŸ”½ **59.7 %**                                                                                                                                                                                 | ğŸ”½ **23.9 %**                                                                                                                                                                                  | ğŸ”¼ **11.9 %p**   |

## 2. OhLoRA-v2 LLM Final Selection

* **Polyglot-Ko 1.3B (1.43 B params)**
  * [HuggingFace](https://huggingface.co/EleutherAI/polyglot-ko-1.3b)
* [ì˜¤ë¡œë¼ 1ì°¨ í”„ë¡œì íŠ¸](../../2025_04_08_OhLoRA/llm/README.md#1-llm-final-selection) ì™€ ì™„ì „íˆ ë™ì¼

## 3. OhLoRA-v2 LLM Fine-Tuning

* í•™ìŠµ ëª¨ë¸
  * **Polyglot-Ko 1.3B (1.43 B params) (âœ… ìµœì¢… ì±„íƒ)** [HuggingFace](https://huggingface.co/EleutherAI/polyglot-ko-1.3b) 
* í•™ìŠµ ë°©ë²• 
  * [SFT (Supervised Fine-Tuning)](https://github.com/WannaBeSuperteur/AI-study/blob/main/AI%20Basics/LLM%20Basics/LLM_%EA%B8%B0%EC%B4%88_Fine_Tuning_SFT.md)
  * [LoRA (Low-Rank Adaption)](https://github.com/WannaBeSuperteur/AI-study/blob/main/AI%20Basics/LLM%20Basics/LLM_%EA%B8%B0%EC%B4%88_Fine_Tuning_LoRA_QLoRA.md), LoRA Rank = 128
  * train for **60 epochs**
  * initial [learning rate](https://github.com/WannaBeSuperteur/AI-study/blob/main/AI%20Basics/Deep%20Learning%20Basics/%EB%94%A5%EB%9F%AC%EB%8B%9D_%EA%B8%B0%EC%B4%88_Learning_Rate.md) : **0.0003 (= 3e-4)**
* í•™ìŠµ ë°ì´í„°ì…‹
  * train ë°ì´í„° **456 rows**, valid ë°ì´í„° **70 rows** (v2, v2.1, v2.2 ëª¨ë‘ ë™ì¼)

| ëª¨ë¸                                     | í•™ìŠµ ë°ì´í„°ì…‹                                                                    |
|----------------------------------------|----------------------------------------------------------------------------|
| LLM ë‹µë³€ ```output_message```            | dataset **v2.1** [(link)](fine_tuning_dataset/OhLoRA_fine_tuning_v2_1.csv) |
| memory (RAG-like concept) ```memory``` | dataset **v2** [(link)](fine_tuning_dataset/OhLoRA_fine_tuning_v2.csv)     |
| í‘œì •/ëª¸ì§“ ```eyes_mouth_pose```            | dataset **v2** [(link)](fine_tuning_dataset/OhLoRA_fine_tuning_v2.csv)     |
| summary (í•˜ê³  ìˆëŠ” ëŒ€í™” ìš”ì•½) ```summary```    | dataset **v2.2** [(link)](fine_tuning_dataset/OhLoRA_fine_tuning_v2_2.csv) |

* ì°¸ê³ 
  * [ì˜¤ë¡œë¼ 1ì°¨ í”„ë¡œì íŠ¸ì—ì„œì˜ LLM Fine-Tuning ë°©ë²•](../../2025_04_08_OhLoRA/llm/README.md#2-how-to-run-fine-tuning) 

## 4. ì½”ë“œ ì‹¤í–‰ ë°©ë²•

ëª¨ë“  ì½”ë“œëŠ” ```2025_05_02_OhLoRA_v2``` (í”„ë¡œì íŠ¸ ë©”ì¸ ë””ë ‰í† ë¦¬) ì—ì„œ ì‹¤í–‰

* **Polyglot-Ko 1.3B** Fine-Tuned ëª¨ë¸ ì‹¤í–‰ (í•´ë‹¹ ëª¨ë¸ ì—†ì„ ì‹œ, Fine-Tuning ë¨¼ì € ì‹¤í–‰) 

| ëª¨ë¸                                      | ì‹¤í–‰ ë°©ë²• (option 1)                                                                   | ì‹¤í–‰ ë°©ë²• (option 2)                                                |
|-----------------------------------------|------------------------------------------------------------------------------------|-----------------------------------------------------------------|
| **ë©”ì‹œì§€ (LLM answer)** ì¶œë ¥ ëª¨ë¸              | ```python llm/run_fine_tuning.py -llm_name polyglot -output_col output_message```  | ```python llm/run_fine_tuning.py -output_col output_message```  |
| **LLM ë©”ëª¨ë¦¬ (RAG-like concept)** ì¶œë ¥ ëª¨ë¸    | ```python llm/run_fine_tuning.py -llm_name polyglot -output_col memory```          | ```python llm/run_fine_tuning.py -output_col memory```          |
| **LLM answer ìš”ì•½** ì¶œë ¥ ëª¨ë¸                 | ```python llm/run_fine_tuning.py -llm_name polyglot -output_col summary```         | ```python llm/run_fine_tuning.py -output_col summary```         |
| **Oh-LoRA ğŸ‘±â€â™€ï¸ (ì˜¤ë¡œë¼) ì˜ í‘œì • & ëª¸ì§“** ì¶œë ¥ ëª¨ë¸ | ```python llm/run_fine_tuning.py -llm_name polyglot -output_col eyes_mouth_pose``` | ```python llm/run_fine_tuning.py -output_col eyes_mouth_pose``` |

* **Memory Mechanism (S-BERT)** ëª¨ë¸ ì‹¤í–‰ (í•´ë‹¹ ëª¨ë¸ ì—†ì„ ì‹œ, Training ë¨¼ì € ì‹¤í–‰)
  * ```python llm/run_memory_mechanism.py```

### 4-1. ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ê²½ë¡œ

* ```S-BERT (roberta-base)``` ëª¨ë¸ì€ í•™ìŠµ ì½”ë“œ ì‹¤í–‰ ì‹œ ì›ë³¸ ëª¨ë¸ì„ ìë™ìœ¼ë¡œ ë‹¤ìš´ë¡œë“œ í›„ í•™ìŠµí•˜ë¯€ë¡œ, **ë³„ë„ ë‹¤ìš´ë¡œë“œ ë¶ˆí•„ìš”**

| ëª¨ë¸ ì´ë¦„                       | ì›ë³¸ ëª¨ë¸                                                                                | Fine-Tuned LLM<br>(for OhLoRA-v2 ğŸ‘±â€â™€ï¸)                               |
|-----------------------------|--------------------------------------------------------------------------------------|-----------------------------------------------------------------------|
| ```Polyglot-Ko 1.3B```      | [EleutherAI HuggingFace](https://huggingface.co/EleutherAI/polyglot-ko-1.3b)         | TBU                                                                   |
| ```KoreanLM 1.5B```         | [Quantum AI HuggingFace](https://huggingface.co/quantumaikr/KoreanLM-1.5b/tree/main) | âŒ í•™ìŠµ ì‹¤íŒ¨ [(ì°¸ê³ )](../issue_reported.md#2-2-koreanlm-15b-llm-í•™ìŠµ-ë¶ˆê°€-í•´ê²°-ë³´ë¥˜) |
| ```S-BERT (roberta-base)``` | [HuggingFace](https://huggingface.co/klue/roberta-base)                              | TBU                                                                   |