import torch
from transformers import StoppingCriteria, StoppingCriteriaList

import os
PROJECT_DIR_PATH = os.path.dirname(os.path.abspath(os.path.dirname(__file__)))


# stop when "LAST N TOKENS MATCHES stop_token_ids" - class code by ChatGPT-4o
class StopOnTokens(StoppingCriteria):
    def __init__(self, stop_token_ids):
        self.stop_token_ids = list(stop_token_ids.detach().cpu().numpy())
        self.current_ids = []

    def __call__(self, input_ids, scores, **kwargs):
        self.current_ids = input_ids[0].tolist()

        if len(self.current_ids) >= len(self.stop_token_ids):
            if self.current_ids[-len(self.stop_token_ids):] == self.stop_token_ids:
                return True  # stop generation

        return False


# LLM ë³„ Stop Token List ë°˜í™˜
# Create Date : 2025.09.22
# Last Update Date : -

# Arguments :
# - function_type (str) : ì‹¤í–‰í•  ê¸°ëŠ¥ìœ¼ë¡œ, 'qna', 'quiz', 'interview' ì¤‘ í•˜ë‚˜

def get_stop_token_list(function_type):
    if function_type == 'qna':
        return [109659, 104449, 99458, 64356]  # (ë‹µë³€ ì¢…ë£Œ)
    elif function_type == 'quiz':
        return [34983, 102546, 99458, 64356]  # (í•´ì„¤ ì¢…ë£Œ)
    else:  # interview
        return [102133, 57390, 99458, 64356]  # (ë°œí™” ì¢…ë£Œ)


# Oh-LoRA (ì˜¤ë¡œë¼) ì˜ ë‹µë³€ ìƒì„±
# Create Date : 2025.09.24
# Last Update Date : -

# Arguments :
# - ohlora_llm           (LLM)       : output_message LLM (Polyglot-Ko 1.3B & Kanana-1.5 2.1B Fine-Tuned)
# - ohlora_llm_tokenizer (tokenizer) : output_message LLM (Polyglot-Ko 1.3B & Kanana-1.5 2.1B Fine-Tuned) ì˜ tokenizer
# - final_ohlora_input   (str)       : ì˜¤ë¡œë¼ğŸ‘±â€â™€ï¸ ì—ê²Œ ìµœì¢…ì ìœ¼ë¡œ ì…ë ¥ë˜ëŠ” ë©”ì‹œì§€ (ê²½ìš°ì— ë”°ë¼ summary, memory text í¬í•¨)
# - function_type        (str)       : ì‹¤í–‰í•  ê¸°ëŠ¥ìœ¼ë¡œ, 'qna', 'quiz', 'interview' ì¤‘ í•˜ë‚˜

# Returns :
# - ohlora_answer (str) : ì˜¤ë¡œë¼ğŸ‘±â€â™€ï¸ ê°€ ìƒì„±í•œ ë‹µë³€

def generate_llm_answer(ohlora_llm, ohlora_llm_tokenizer, final_ohlora_input, function_type):
    trial_count = 0
    max_trials = 5

    # tokenize final Oh-LoRA input
    if function_type == 'qna':
        answer_start_mark = '(ë‹µë³€ ì‹œì‘)'
        answer_end_mark = '(ë‹µë³€ ì¢…ë£Œ)'
    elif function_type == 'quiz':
        answer_start_mark = '(í•´ì„¤ ì‹œì‘)'
        answer_end_mark = '(í•´ì„¤ ì¢…ë£Œ)'
    else:  # interview
        answer_start_mark = '(ë°œí™” ì‹œì‘)'
        answer_end_mark = '(ë°œí™” ì¢…ë£Œ)'

    final_ohlora_input_ = final_ohlora_input + ' ' + answer_start_mark

    inputs = ohlora_llm_tokenizer(final_ohlora_input_, return_tensors='pt')
    inputs = {'input_ids': inputs['input_ids'].to(ohlora_llm.device),
              'attention_mask': inputs['attention_mask'].to(ohlora_llm.device)}

    # for stopping criteria
    stop_token_ids = torch.tensor(get_stop_token_list(function_type=function_type)).to(ohlora_llm.device)
    stopping_criteria = StoppingCriteriaList([StopOnTokens(stop_token_ids)])

    while trial_count < max_trials:
        outputs = ohlora_llm.generate(**inputs,
                                      max_length=512,
                                      do_sample=True,
                                      temperature=0.6,
                                      stopping_criteria=stopping_criteria)

        llm_answer = ohlora_llm_tokenizer.decode(outputs[0], skip_special_tokens=True)
        llm_answer = llm_answer[len(final_ohlora_input_):]
        llm_answer = llm_answer.replace('\u200b', '').replace('\xa0', '')  # zwsp, nbsp (í­ ì—†ëŠ” ê³µë°±, ì¤„ ë°”ê¿ˆ ì—†ëŠ” ê³µë°±) ì œê±°

        trial_count += 1

        # check LLM answer and return or retry
        is_empty = llm_answer.replace('\n', '').replace('(ë°œí™” ì¢…ë£Œ)', '').replace(' ', '') == ''
        is_answer_end_mark = ('ë°œí™” ì¢…ë£Œ' in llm_answer.replace('(ë°œí™” ì¢…ë£Œ', '') or
                              'ë‹µë³€ì¢…ë£Œ' in llm_answer.replace('(ë‹µë³€ ì¢…ë£Œ', '') or
                              'í•´ì„¤ì¢…ë£Œ' in llm_answer.replace('(í•´ì„¤ ì¢…ë£Œ', ''))

        is_other_mark = '(ì‚¬ìš©ì' in llm_answer.replace(' ', '') or 'ìš”ì•½)' in llm_answer.replace(' ', '')
        is_low_quality = is_empty or is_answer_end_mark or is_other_mark

        if not is_low_quality and ('http' not in llm_answer):
            return llm_answer.replace(answer_end_mark, '')

    return '(ì½ì”¹)'


# Oh-LoRA (ì˜¤ë¡œë¼) ì˜ ë‹µë³€ì„ clean ì²˜ë¦¬
# Create Date : 2025.09.22
# Last Update Date : -

# Arguments :
# - ohlora_answer (str) : ì˜¤ë¡œë¼ğŸ‘±â€â™€ï¸ ê°€ ìƒì„±í•œ ë‹µë³€

# Returns :
# - llm_answer_cleaned (str) : ì˜¤ë¡œë¼ğŸ‘±â€â™€ï¸ ê°€ ìƒì„±í•œ ì›ë³¸ ë‹µë³€ì—ì„œ text clean ì„ ì‹¤ì‹œí•œ ì´í›„ì˜ ë‹µë³€

def clean_llm_answer(ohlora_answer):
    return ohlora_answer
