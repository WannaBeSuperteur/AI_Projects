input_part,next_question,predicted_similarity,ground_truth_similarity,absolute_error
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서? (남은 답변: 면접 시작 인사),BCE Loss 설명,-0.0050976807,0.0,0.005097680725157261
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서? (남은 답변: 면접 시작 인사),LLM Fine-Tuning 의 PEFT,-0.003451126,0.0,0.003451125929132104
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서? (남은 답변: 면접 시작 인사),LoRA,-0.008376832,0.0,0.008376832120120525
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서? (남은 답변: 면접 시작 인사),LoRA 와 QLoRA 의 차이,-0.0033075502,0.0,0.0033075502142310143
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서? (남은 답변: 면접 시작 인사),Loss Function 관련 실무 경험,-0.004104935,0.0,0.004104935098439455
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서? (남은 답변: 면접 시작 인사),Loss Function 예시,-0.009901322,0.0,0.009901322424411774
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서? (남은 답변: 면접 시작 인사),Loss Function 정의,-0.00046090758,0.0,0.0004609075840562582
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서? (남은 답변: 면접 시작 인사),MBTI / 좋아하는 아이돌,-0.0025098843,0.0,0.0025098843034356833
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서? (남은 답변: 면접 시작 인사),MSE Loss 설명,-0.0047671995,0.0,0.004767199512571096
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서? (남은 답변: 면접 시작 인사),MSE Loss 용도,-0.000764747,0.0,0.0007647469756193459
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서? (남은 답변: 면접 시작 인사),"Multi-Class, Multi-Label 중 BCE 가 좋은 task",-0.0009302551,0.0,0.0009302551043219864
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서? (남은 답변: 면접 시작 인사),Multi-Label 에서 CE + Softmax 적용 문제점,-0.004894076,0.0,0.0048940759152174
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서? (남은 답변: 면접 시작 인사),PEFT 방법 5가지,0.00055619434,0.0,0.0005561943398788571
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서? (남은 답변: 면접 시작 인사),거대 언어 모델 정의,-0.0039753527,0.0,0.003975352738052607
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서? (남은 답변: 면접 시작 인사),마지막 할 말,-0.0048207054,0.0,0.004820705391466618
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서? (남은 답변: 면접 시작 인사),면접 시작 인사,0.9973437,1.0,0.002656280994415283
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서? (남은 답변: 면접 시작 인사),면접 종료,-0.0053160186,0.0,0.005316018592566252
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서? (남은 답변: 면접 시작 인사),"인공지능, 머신러닝, 딥러닝 차이",-0.00266231,0.0,0.0026623099111020565
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서? (남은 답변: 면접 시작 인사),잠시 휴식,-0.0066008554,0.0,0.0066008553840219975
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서? (남은 답변: 면접 시작 인사),확률 예측에서 MSE Loss 미 사용 이유,-0.0044360803,0.0,0.00443608034402132
면접 시작 인사 -> 로라야 안녕 정말 반가워 (남은 답변: 면접 시작 인사),BCE Loss 설명,-0.004186252,0.0,0.004186252132058144
면접 시작 인사 -> 로라야 안녕 정말 반가워 (남은 답변: 면접 시작 인사),LLM Fine-Tuning 의 PEFT,-0.0018745874,0.0,0.0018745873821899295
면접 시작 인사 -> 로라야 안녕 정말 반가워 (남은 답변: 면접 시작 인사),LoRA,-0.0040310277,0.0,0.004031027667224407
면접 시작 인사 -> 로라야 안녕 정말 반가워 (남은 답변: 면접 시작 인사),LoRA 와 QLoRA 의 차이,-0.0025257235,0.0,0.0025257235392928123
면접 시작 인사 -> 로라야 안녕 정말 반가워 (남은 답변: 면접 시작 인사),Loss Function 관련 실무 경험,-0.0039612902,0.0,0.0039612902328372
면접 시작 인사 -> 로라야 안녕 정말 반가워 (남은 답변: 면접 시작 인사),Loss Function 예시,-0.011820209,0.0,0.011820209212601185
면접 시작 인사 -> 로라야 안녕 정말 반가워 (남은 답변: 면접 시작 인사),Loss Function 정의,-0.0042554657,0.0,0.0042554656974971294
면접 시작 인사 -> 로라야 안녕 정말 반가워 (남은 답변: 면접 시작 인사),MBTI / 좋아하는 아이돌,-0.0011311049,0.0,0.0011311048874631524
면접 시작 인사 -> 로라야 안녕 정말 반가워 (남은 답변: 면접 시작 인사),MSE Loss 설명,-0.004095749,0.0,0.004095748998224735
면접 시작 인사 -> 로라야 안녕 정말 반가워 (남은 답변: 면접 시작 인사),MSE Loss 용도,-0.0022493773,0.0,0.002249377314001322
면접 시작 인사 -> 로라야 안녕 정말 반가워 (남은 답변: 면접 시작 인사),"Multi-Class, Multi-Label 중 BCE 가 좋은 task",-8.1685575e-05,0.0,8.16855754237622e-05
면접 시작 인사 -> 로라야 안녕 정말 반가워 (남은 답변: 면접 시작 인사),Multi-Label 에서 CE + Softmax 적용 문제점,-0.0040803733,0.0,0.004080373328179121
면접 시작 인사 -> 로라야 안녕 정말 반가워 (남은 답변: 면접 시작 인사),PEFT 방법 5가지,-0.0010869794,0.0,0.0010869794059544802
면접 시작 인사 -> 로라야 안녕 정말 반가워 (남은 답변: 면접 시작 인사),거대 언어 모델 정의,-0.0026360296,0.0,0.0026360296178609133
면접 시작 인사 -> 로라야 안녕 정말 반가워 (남은 답변: 면접 시작 인사),마지막 할 말,-0.0063739745,0.0,0.006373974494636059
면접 시작 인사 -> 로라야 안녕 정말 반가워 (남은 답변: 면접 시작 인사),면접 시작 인사,0.9973145,1.0,0.0026854872703552246
면접 시작 인사 -> 로라야 안녕 정말 반가워 (남은 답변: 면접 시작 인사),면접 종료,-0.0015215527,0.0,0.0015215526800602674
면접 시작 인사 -> 로라야 안녕 정말 반가워 (남은 답변: 면접 시작 인사),"인공지능, 머신러닝, 딥러닝 차이",-0.0043495274,0.0,0.0043495274148881435
면접 시작 인사 -> 로라야 안녕 정말 반가워 (남은 답변: 면접 시작 인사),잠시 휴식,-0.0041528805,0.0,0.0041528805159032345
면접 시작 인사 -> 로라야 안녕 정말 반가워 (남은 답변: 면접 시작 인사),확률 예측에서 MSE Loss 미 사용 이유,-0.0062106564,0.0,0.006210656370967627
면접 시작 인사 -> 로라야 그럼 네가 면접관이야? (남은 답변: 면접 시작 인사),BCE Loss 설명,-0.0068510002,0.0,0.0068510002456605434
면접 시작 인사 -> 로라야 그럼 네가 면접관이야? (남은 답변: 면접 시작 인사),LLM Fine-Tuning 의 PEFT,-0.0038021554,0.0,0.003802155377343297
면접 시작 인사 -> 로라야 그럼 네가 면접관이야? (남은 답변: 면접 시작 인사),LoRA,-0.005202444,0.0,0.005202443804591894
면접 시작 인사 -> 로라야 그럼 네가 면접관이야? (남은 답변: 면접 시작 인사),LoRA 와 QLoRA 의 차이,-0.0046026222,0.0,0.004602622240781784
면접 시작 인사 -> 로라야 그럼 네가 면접관이야? (남은 답변: 면접 시작 인사),Loss Function 관련 실무 경험,-0.004057747,0.0,0.004057746846228838
면접 시작 인사 -> 로라야 그럼 네가 면접관이야? (남은 답변: 면접 시작 인사),Loss Function 예시,-0.007664697,0.0,0.007664696779102087
면접 시작 인사 -> 로라야 그럼 네가 면접관이야? (남은 답변: 면접 시작 인사),Loss Function 정의,-0.0011210683,0.0,0.001121068256907165
면접 시작 인사 -> 로라야 그럼 네가 면접관이야? (남은 답변: 면접 시작 인사),MBTI / 좋아하는 아이돌,-0.004798796,0.0,0.0047987960278987885
면접 시작 인사 -> 로라야 그럼 네가 면접관이야? (남은 답변: 면접 시작 인사),MSE Loss 설명,-0.006096597,0.0,0.006096596829593182
면접 시작 인사 -> 로라야 그럼 네가 면접관이야? (남은 답변: 면접 시작 인사),MSE Loss 용도,0.0002399382,0.0,0.0002399382065050304
면접 시작 인사 -> 로라야 그럼 네가 면접관이야? (남은 답변: 면접 시작 인사),"Multi-Class, Multi-Label 중 BCE 가 좋은 task",-0.0012851512,0.0,0.0012851512292400002
면접 시작 인사 -> 로라야 그럼 네가 면접관이야? (남은 답변: 면접 시작 인사),Multi-Label 에서 CE + Softmax 적용 문제점,-0.003828429,0.0,0.0038284289184957743
면접 시작 인사 -> 로라야 그럼 네가 면접관이야? (남은 답변: 면접 시작 인사),PEFT 방법 5가지,-0.00039817143,0.0,0.00039817142533138394
면접 시작 인사 -> 로라야 그럼 네가 면접관이야? (남은 답변: 면접 시작 인사),거대 언어 모델 정의,-0.0048888773,0.0,0.004888877272605896
면접 시작 인사 -> 로라야 그럼 네가 면접관이야? (남은 답변: 면접 시작 인사),마지막 할 말,-0.0072837854,0.0,0.007283785380423069
면접 시작 인사 -> 로라야 그럼 네가 면접관이야? (남은 답변: 면접 시작 인사),면접 시작 인사,0.99684405,1.0,0.003155946731567383
면접 시작 인사 -> 로라야 그럼 네가 면접관이야? (남은 답변: 면접 시작 인사),면접 종료,-0.0060174647,0.0,0.006017464678734541
면접 시작 인사 -> 로라야 그럼 네가 면접관이야? (남은 답변: 면접 시작 인사),"인공지능, 머신러닝, 딥러닝 차이",0.0027455043,0.0,0.0027455042582005262
면접 시작 인사 -> 로라야 그럼 네가 면접관이야? (남은 답변: 면접 시작 인사),잠시 휴식,-0.0062316703,0.0,0.006231670267879963
면접 시작 인사 -> 로라야 그럼 네가 면접관이야? (남은 답변: 면접 시작 인사),확률 예측에서 MSE Loss 미 사용 이유,-0.0023872722,0.0,0.0023872721940279007
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까 (남은 답변: 면접 시작 인사),BCE Loss 설명,-0.009889958,0.0,0.009889958426356316
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까 (남은 답변: 면접 시작 인사),LLM Fine-Tuning 의 PEFT,-0.007354554,0.0,0.007354553788900375
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까 (남은 답변: 면접 시작 인사),LoRA,-0.0028811994,0.0,0.0028811993543058634
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까 (남은 답변: 면접 시작 인사),LoRA 와 QLoRA 의 차이,-0.006397998,0.0,0.006397997960448265
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까 (남은 답변: 면접 시작 인사),Loss Function 관련 실무 경험,-0.005377893,0.0,0.00537789287045598
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까 (남은 답변: 면접 시작 인사),Loss Function 예시,-0.008072544,0.0,0.008072543889284134
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까 (남은 답변: 면접 시작 인사),Loss Function 정의,0.007853585,0.0,0.007853585295379162
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까 (남은 답변: 면접 시작 인사),MBTI / 좋아하는 아이돌,-0.0072001102,0.0,0.007200110238045454
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까 (남은 답변: 면접 시작 인사),MSE Loss 설명,-0.0074105635,0.0,0.00741056352853775
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까 (남은 답변: 면접 시작 인사),MSE Loss 용도,0.0014663128,0.0,0.001466312794946134
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까 (남은 답변: 면접 시작 인사),"Multi-Class, Multi-Label 중 BCE 가 좋은 task",-0.0010050132,0.0,0.0010050132405012846
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까 (남은 답변: 면접 시작 인사),Multi-Label 에서 CE + Softmax 적용 문제점,-0.008369822,0.0,0.008369822055101395
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까 (남은 답변: 면접 시작 인사),PEFT 방법 5가지,0.0010772523,0.0,0.0010772523237392306
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까 (남은 답변: 면접 시작 인사),거대 언어 모델 정의,-0.004256568,0.0,0.004256567917764187
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까 (남은 답변: 면접 시작 인사),마지막 할 말,-0.00465411,0.0,0.0046541099436581135
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까 (남은 답변: 면접 시작 인사),면접 시작 인사,0.9933529,1.0,0.0066471099853515625
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까 (남은 답변: 면접 시작 인사),면접 종료,0.0018253963,0.0,0.0018253963207826018
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까 (남은 답변: 면접 시작 인사),"인공지능, 머신러닝, 딥러닝 차이",0.023888053,0.0,0.023888053372502327
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까 (남은 답변: 면접 시작 인사),잠시 휴식,-0.006221881,0.0,0.00622188113629818
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까 (남은 답변: 면접 시작 인사),확률 예측에서 MSE Loss 미 사용 이유,-0.0046587233,0.0,0.004658723250031471
면접 시작 인사 -> 파이팅! 시작하자 (남은 답변: 면접 시작 인사),BCE Loss 설명,-0.0039228178,0.0,0.003922817762941122
면접 시작 인사 -> 파이팅! 시작하자 (남은 답변: 면접 시작 인사),LLM Fine-Tuning 의 PEFT,-0.0014720187,0.0,0.0014720186591148376
면접 시작 인사 -> 파이팅! 시작하자 (남은 답변: 면접 시작 인사),LoRA,-0.0016997135,0.0,0.0016997135244309902
면접 시작 인사 -> 파이팅! 시작하자 (남은 답변: 면접 시작 인사),LoRA 와 QLoRA 의 차이,-0.0027802123,0.0,0.0027802123222500086
면접 시작 인사 -> 파이팅! 시작하자 (남은 답변: 면접 시작 인사),Loss Function 관련 실무 경험,-0.0027438218,0.0,0.0027438218239694834
면접 시작 인사 -> 파이팅! 시작하자 (남은 답변: 면접 시작 인사),Loss Function 예시,0.005502527,0.0,0.005502527114003897
면접 시작 인사 -> 파이팅! 시작하자 (남은 답변: 면접 시작 인사),Loss Function 정의,-0.004577809,0.0,0.004577809013426304
면접 시작 인사 -> 파이팅! 시작하자 (남은 답변: 면접 시작 인사),MBTI / 좋아하는 아이돌,0.0004971096,0.0,0.0004971096059307456
면접 시작 인사 -> 파이팅! 시작하자 (남은 답변: 면접 시작 인사),MSE Loss 설명,-0.0050129658,0.0,0.005012965761125088
면접 시작 인사 -> 파이팅! 시작하자 (남은 답변: 면접 시작 인사),MSE Loss 용도,-0.00028206073,0.0,0.00028206073329783976
면접 시작 인사 -> 파이팅! 시작하자 (남은 답변: 면접 시작 인사),"Multi-Class, Multi-Label 중 BCE 가 좋은 task",-0.0018873406,0.0,0.0018873405642807484
면접 시작 인사 -> 파이팅! 시작하자 (남은 답변: 면접 시작 인사),Multi-Label 에서 CE + Softmax 적용 문제점,-0.0018840662,0.0,0.0018840661505237222
면접 시작 인사 -> 파이팅! 시작하자 (남은 답변: 면접 시작 인사),PEFT 방법 5가지,-0.0037325397,0.0,0.003732539713382721
면접 시작 인사 -> 파이팅! 시작하자 (남은 답변: 면접 시작 인사),거대 언어 모델 정의,-0.0048096552,0.0,0.004809655249118805
면접 시작 인사 -> 파이팅! 시작하자 (남은 답변: 면접 시작 인사),마지막 할 말,-0.004047341,0.0,0.004047341179102659
면접 시작 인사 -> 파이팅! 시작하자 (남은 답변: 면접 시작 인사),면접 시작 인사,-0.003202717,0.0,0.003202717052772641
면접 시작 인사 -> 파이팅! 시작하자 (남은 답변: 면접 시작 인사),면접 종료,-0.007130938,0.0,0.007130938116461039
면접 시작 인사 -> 파이팅! 시작하자 (남은 답변: 면접 시작 인사),"인공지능, 머신러닝, 딥러닝 차이",0.99717116,1.0,0.002828836441040039
면접 시작 인사 -> 파이팅! 시작하자 (남은 답변: 면접 시작 인사),잠시 휴식,-0.001552867,0.0,0.001552867004647851
면접 시작 인사 -> 파이팅! 시작하자 (남은 답변: 면접 시작 인사),확률 예측에서 MSE Loss 미 사용 이유,-0.0032639876,0.0,0.0032639876008033752
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데 (남은 답변: 인공지능, 머신러닝, 딥러닝)",BCE Loss 설명,-0.0027887495,0.0,0.002788749523460865
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데 (남은 답변: 인공지능, 머신러닝, 딥러닝)",LLM Fine-Tuning 의 PEFT,-0.00080102414,0.0,0.0008010241435840726
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데 (남은 답변: 인공지능, 머신러닝, 딥러닝)",LoRA,-0.0010021486,0.0,0.0010021486086770892
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데 (남은 답변: 인공지능, 머신러닝, 딥러닝)",LoRA 와 QLoRA 의 차이,-0.0030204575,0.0,0.0030204574577510357
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데 (남은 답변: 인공지능, 머신러닝, 딥러닝)",Loss Function 관련 실무 경험,-0.0036626915,0.0,0.0036626914516091347
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데 (남은 답변: 인공지능, 머신러닝, 딥러닝)",Loss Function 예시,-0.002888933,0.0,0.0028889330569654703
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데 (남은 답변: 인공지능, 머신러닝, 딥러닝)",Loss Function 정의,-0.002364614,0.0,0.0023646140471100807
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데 (남은 답변: 인공지능, 머신러닝, 딥러닝)",MBTI / 좋아하는 아이돌,-0.0006100511,0.0,0.0006100510945543647
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데 (남은 답변: 인공지능, 머신러닝, 딥러닝)",MSE Loss 설명,-0.0041301125,0.0,0.00413011247292161
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데 (남은 답변: 인공지능, 머신러닝, 딥러닝)",MSE Loss 용도,-0.0019786633,0.0,0.0019786632619798183
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데 (남은 답변: 인공지능, 머신러닝, 딥러닝)","Multi-Class, Multi-Label 중 BCE 가 좋은 task",-0.0015135588,0.0,0.0015135587891563773
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데 (남은 답변: 인공지능, 머신러닝, 딥러닝)",Multi-Label 에서 CE + Softmax 적용 문제점,-0.0017180059,0.0,0.0017180058639496565
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데 (남은 답변: 인공지능, 머신러닝, 딥러닝)",PEFT 방법 5가지,-0.0055580772,0.0,0.005558077245950699
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데 (남은 답변: 인공지능, 머신러닝, 딥러닝)",거대 언어 모델 정의,-0.002361263,0.0,0.0023612629156559706
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데 (남은 답변: 인공지능, 머신러닝, 딥러닝)",마지막 할 말,-0.0049376385,0.0,0.004937638528645039
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데 (남은 답변: 인공지능, 머신러닝, 딥러닝)",면접 시작 인사,-0.004080848,0.0,0.004080847837030888
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데 (남은 답변: 인공지능, 머신러닝, 딥러닝)",면접 종료,-0.004955512,0.0,0.0049555120058357716
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데 (남은 답변: 인공지능, 머신러닝, 딥러닝)","인공지능, 머신러닝, 딥러닝 차이",0.99927086,1.0,0.0007291436195373535
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데 (남은 답변: 인공지능, 머신러닝, 딥러닝)",잠시 휴식,-0.0022666468,0.0,0.0022666468285024166
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데 (남은 답변: 인공지능, 머신러닝, 딥러닝)",확률 예측에서 MSE Loss 미 사용 이유,-0.0041207187,0.0,0.004120718687772751
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거! (남은 답변: 머신러닝, 딥러닝)",BCE Loss 설명,-0.0025586907,0.0,0.0025586907286196947
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거! (남은 답변: 머신러닝, 딥러닝)",LLM Fine-Tuning 의 PEFT,-0.0019265733,0.0,0.0019265733426436782
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거! (남은 답변: 머신러닝, 딥러닝)",LoRA,-0.0009959074,0.0,0.0009959073504433036
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거! (남은 답변: 머신러닝, 딥러닝)",LoRA 와 QLoRA 의 차이,-0.0026145622,0.0,0.002614562166854739
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거! (남은 답변: 머신러닝, 딥러닝)",Loss Function 관련 실무 경험,-0.004204981,0.0,0.004204981029033661
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거! (남은 답변: 머신러닝, 딥러닝)",Loss Function 예시,-0.0014060461,0.0,0.0014060460962355137
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거! (남은 답변: 머신러닝, 딥러닝)",Loss Function 정의,-0.0022513368,0.0,0.0022513368166983128
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거! (남은 답변: 머신러닝, 딥러닝)",MBTI / 좋아하는 아이돌,-0.0012396789,0.0,0.0012396789388731122
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거! (남은 답변: 머신러닝, 딥러닝)",MSE Loss 설명,-0.0031753008,0.0,0.0031753007788211107
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거! (남은 답변: 머신러닝, 딥러닝)",MSE Loss 용도,-0.0026466341,0.0,0.0026466341223567724
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거! (남은 답변: 머신러닝, 딥러닝)","Multi-Class, Multi-Label 중 BCE 가 좋은 task",-0.0006330535,0.0,0.0006330534815788269
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거! (남은 답변: 머신러닝, 딥러닝)",Multi-Label 에서 CE + Softmax 적용 문제점,-0.00082270184,0.0,0.0008227018406614661
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거! (남은 답변: 머신러닝, 딥러닝)",PEFT 방법 5가지,-0.005929701,0.0,0.005929701030254364
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거! (남은 답변: 머신러닝, 딥러닝)",거대 언어 모델 정의,-0.0018574871,0.0,0.001857487135566771
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거! (남은 답변: 머신러닝, 딥러닝)",마지막 할 말,-0.0052459715,0.0,0.005245971493422985
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거! (남은 답변: 머신러닝, 딥러닝)",면접 시작 인사,-0.0030902429,0.0,0.0030902428552508354
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거! (남은 답변: 머신러닝, 딥러닝)",면접 종료,-0.005409404,0.0,0.005409404169768095
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거! (남은 답변: 머신러닝, 딥러닝)","인공지능, 머신러닝, 딥러닝 차이",0.9993194,1.0,0.0006806254386901855
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거! (남은 답변: 머신러닝, 딥러닝)",잠시 휴식,-0.0028391862,0.0,0.002839186228811741
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거! (남은 답변: 머신러닝, 딥러닝)",확률 예측에서 MSE Loss 미 사용 이유,-0.0049254927,0.0,0.004925492685288191
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지 (남은 답변: 딥러닝)",BCE Loss 설명,-0.002713682,0.0,0.00271368189714849
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지 (남은 답변: 딥러닝)",LLM Fine-Tuning 의 PEFT,-0.002197668,0.0,0.002197667956352234
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지 (남은 답변: 딥러닝)",LoRA,-0.00055224285,0.0,0.0005522428546100855
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지 (남은 답변: 딥러닝)",LoRA 와 QLoRA 의 차이,-0.0023519841,0.0,0.0023519841488450766
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지 (남은 답변: 딥러닝)",Loss Function 관련 실무 경험,-0.005363737,0.0,0.005363736767321825
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지 (남은 답변: 딥러닝)",Loss Function 예시,-3.0653424e-05,0.0,3.065342389163561e-05
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지 (남은 답변: 딥러닝)",Loss Function 정의,-0.0016734401,0.0,0.0016734400996938348
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지 (남은 답변: 딥러닝)",MBTI / 좋아하는 아이돌,-0.0017234621,0.0,0.001723462133668363
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지 (남은 답변: 딥러닝)",MSE Loss 설명,-0.0035001358,0.0,0.003500135848298669
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지 (남은 답변: 딥러닝)",MSE Loss 용도,-0.0021332086,0.0,0.0021332085598260164
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지 (남은 답변: 딥러닝)","Multi-Class, Multi-Label 중 BCE 가 좋은 task",-0.00046414728,0.0,0.0004641472769435495
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지 (남은 답변: 딥러닝)",Multi-Label 에서 CE + Softmax 적용 문제점,-0.0015655516,0.0,0.0015655516181141138
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지 (남은 답변: 딥러닝)",PEFT 방법 5가지,-0.0053174538,0.0,0.005317453760653734
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지 (남은 답변: 딥러닝)",거대 언어 모델 정의,-0.0028577126,0.0,0.0028577125631272793
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지 (남은 답변: 딥러닝)",마지막 할 말,-0.0047190166,0.0,0.00471901660785079
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지 (남은 답변: 딥러닝)",면접 시작 인사,-0.0024020646,0.0,0.002402064623311162
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지 (남은 답변: 딥러닝)",면접 종료,-0.005076728,0.0,0.005076727829873562
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지 (남은 답변: 딥러닝)","인공지능, 머신러닝, 딥러닝 차이",0.99927354,1.0,0.0007264614105224609
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지 (남은 답변: 딥러닝)",잠시 휴식,-0.0021897769,0.0,0.002189776860177517
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지 (남은 답변: 딥러닝)",확률 예측에서 MSE Loss 미 사용 이유,-0.004738649,0.0,0.004738648887723684
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지? (남은 답변: 인공지능, 머신러닝)",BCE Loss 설명,-0.003421218,0.0,0.0034212179016321898
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지? (남은 답변: 인공지능, 머신러닝)",LLM Fine-Tuning 의 PEFT,-0.0013053147,0.0,0.0013053147122263908
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지? (남은 답변: 인공지능, 머신러닝)",LoRA,-0.0012967903,0.0,0.0012967903167009354
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지? (남은 답변: 인공지능, 머신러닝)",LoRA 와 QLoRA 의 차이,-0.0022043562,0.0,0.002204356249421835
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지? (남은 답변: 인공지능, 머신러닝)",Loss Function 관련 실무 경험,-0.0043810396,0.0,0.004381039645522833
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지? (남은 답변: 인공지능, 머신러닝)",Loss Function 예시,-0.0016133562,0.0,0.0016133561730384827
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지? (남은 답변: 인공지능, 머신러닝)",Loss Function 정의,-0.002225827,0.0,0.00222582696005702
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지? (남은 답변: 인공지능, 머신러닝)",MBTI / 좋아하는 아이돌,-0.0016192546,0.0,0.001619254588149488
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지? (남은 답변: 인공지능, 머신러닝)",MSE Loss 설명,-0.0035672744,0.0,0.0035672744270414114
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지? (남은 답변: 인공지능, 머신러닝)",MSE Loss 용도,-0.002893776,0.0,0.002893775934353471
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지? (남은 답변: 인공지능, 머신러닝)","Multi-Class, Multi-Label 중 BCE 가 좋은 task",-0.0007860331,0.0,0.0007860331097617745
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지? (남은 답변: 인공지능, 머신러닝)",Multi-Label 에서 CE + Softmax 적용 문제점,-0.0013681124,0.0,0.0013681123964488506
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지? (남은 답변: 인공지능, 머신러닝)",PEFT 방법 5가지,-0.005605212,0.0,0.0056052119471132755
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지? (남은 답변: 인공지능, 머신러닝)",거대 언어 모델 정의,-0.00083334063,0.0,0.0008333406294696033
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지? (남은 답변: 인공지능, 머신러닝)",마지막 할 말,-0.005253776,0.0,0.005253775976598263
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지? (남은 답변: 인공지능, 머신러닝)",면접 시작 인사,-0.002806633,0.0,0.002806633012369275
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지? (남은 답변: 인공지능, 머신러닝)",면접 종료,-0.005557554,0.0,0.005557553842663765
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지? (남은 답변: 인공지능, 머신러닝)","인공지능, 머신러닝, 딥러닝 차이",0.9992771,1.0,0.0007228851318359375
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지? (남은 답변: 인공지능, 머신러닝)",잠시 휴식,-0.0030689978,0.0,0.0030689977575093508
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지? (남은 답변: 인공지능, 머신러닝)",확률 예측에서 MSE Loss 미 사용 이유,-0.0045418497,0.0,0.004541849717497826
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야 (남은 답변: 모든 질문 해결 완료)",BCE Loss 설명,-0.001340571,0.0,0.0013405709760263562
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야 (남은 답변: 모든 질문 해결 완료)",LLM Fine-Tuning 의 PEFT,-0.0017633784,0.0,0.0017633783863857388
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야 (남은 답변: 모든 질문 해결 완료)",LoRA,-0.00014281024,0.0,0.0001428102405043319
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야 (남은 답변: 모든 질문 해결 완료)",LoRA 와 QLoRA 의 차이,-0.00068036467,0.0,0.0006803646683692932
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야 (남은 답변: 모든 질문 해결 완료)",Loss Function 관련 실무 경험,-0.0039047988,0.0,0.0039047987665981054
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야 (남은 답변: 모든 질문 해결 완료)",Loss Function 예시,-0.0018545222,0.0,0.0018545221537351608
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야 (남은 답변: 모든 질문 해결 완료)",Loss Function 정의,-0.0038047947,0.0,0.0038047947455197573
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야 (남은 답변: 모든 질문 해결 완료)",MBTI / 좋아하는 아이돌,-0.005184574,0.0,0.00518457405269146
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야 (남은 답변: 모든 질문 해결 완료)",MSE Loss 설명,-0.00038679753,0.0,0.0003867975319735706
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야 (남은 답변: 모든 질문 해결 완료)",MSE Loss 용도,-0.003196578,0.0,0.0031965780071914196
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야 (남은 답변: 모든 질문 해결 완료)","Multi-Class, Multi-Label 중 BCE 가 좋은 task",0.0016595818,0.0,0.0016595817869529128
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야 (남은 답변: 모든 질문 해결 완료)",Multi-Label 에서 CE + Softmax 적용 문제점,-0.0035438023,0.0,0.0035438023041933775
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야 (남은 답변: 모든 질문 해결 완료)",PEFT 방법 5가지,-0.001210979,0.0,0.0012109789531677961
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야 (남은 답변: 모든 질문 해결 완료)",거대 언어 모델 정의,0.9981012,1.0,0.0018988251686096191
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야 (남은 답변: 모든 질문 해결 완료)",마지막 할 말,0.0016041655,0.0,0.0016041655326262116
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야 (남은 답변: 모든 질문 해결 완료)",면접 시작 인사,-0.002325558,0.0,0.002325558103621006
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야 (남은 답변: 모든 질문 해결 완료)",면접 종료,-0.00022520093,0.0,0.0002252009289804846
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야 (남은 답변: 모든 질문 해결 완료)","인공지능, 머신러닝, 딥러닝 차이",-0.003122185,0.0,0.00312218489125371
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야 (남은 답변: 모든 질문 해결 완료)",잠시 휴식,0.001607002,0.0,0.001607001991942525
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야 (남은 답변: 모든 질문 해결 완료)",확률 예측에서 MSE Loss 미 사용 이유,-0.00040402153,0.0,0.00040402152808383107
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야? (남은 답변: 머신러닝)",BCE Loss 설명,-0.002279059,0.0,0.0022790590301156044
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야? (남은 답변: 머신러닝)",LLM Fine-Tuning 의 PEFT,-0.0018458171,0.0,0.00184581708163023
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야? (남은 답변: 머신러닝)",LoRA,-0.0010829298,0.0,0.0010829297825694084
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야? (남은 답변: 머신러닝)",LoRA 와 QLoRA 의 차이,-0.0020494945,0.0,0.0020494945347309113
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야? (남은 답변: 머신러닝)",Loss Function 관련 실무 경험,-0.005201137,0.0,0.005201137159019709
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야? (남은 답변: 머신러닝)",Loss Function 예시,-0.0007784625,0.0,0.0007784625049680471
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야? (남은 답변: 머신러닝)",Loss Function 정의,-0.0017038425,0.0,0.001703842543065548
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야? (남은 답변: 머신러닝)",MBTI / 좋아하는 아이돌,-0.0011920036,0.0,0.0011920036049559712
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야? (남은 답변: 머신러닝)",MSE Loss 설명,-0.0033043758,0.0,0.0033043758012354374
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야? (남은 답변: 머신러닝)",MSE Loss 용도,-0.0027048874,0.0,0.002704887418076396
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야? (남은 답변: 머신러닝)","Multi-Class, Multi-Label 중 BCE 가 좋은 task",-0.0006148443,0.0,0.000614844320807606
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야? (남은 답변: 머신러닝)",Multi-Label 에서 CE + Softmax 적용 문제점,-0.0017186902,0.0,0.0017186901532113552
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야? (남은 답변: 머신러닝)",PEFT 방법 5가지,-0.004781302,0.0,0.004781302064657211
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야? (남은 답변: 머신러닝)",거대 언어 모델 정의,-0.0024482443,0.0,0.0024482442531734705
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야? (남은 답변: 머신러닝)",마지막 할 말,-0.005264351,0.0,0.0052643511444330215
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야? (남은 답변: 머신러닝)",면접 시작 인사,-0.0034372008,0.0,0.0034372007939964533
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야? (남은 답변: 머신러닝)",면접 종료,-0.005310821,0.0,0.005310820881277323
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야? (남은 답변: 머신러닝)","인공지능, 머신러닝, 딥러닝 차이",0.9993636,1.0,0.0006363987922668457
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야? (남은 답변: 머신러닝)",잠시 휴식,-0.0021243326,0.0,0.0021243325900286436
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야? (남은 답변: 머신러닝)",확률 예측에서 MSE Loss 미 사용 이유,-0.0042936997,0.0,0.004293699748814106
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아 (남은 답변: 인공지능, 머신러닝)",BCE Loss 설명,-0.0013804422,0.0,0.001380442176014185
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아 (남은 답변: 인공지능, 머신러닝)",LLM Fine-Tuning 의 PEFT,-0.000978607,0.0,0.0009786069858819246
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아 (남은 답변: 인공지능, 머신러닝)",LoRA,-0.00090988853,0.0,0.0009098885348066688
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아 (남은 답변: 인공지능, 머신러닝)",LoRA 와 QLoRA 의 차이,-0.002163628,0.0,0.002163628116250038
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아 (남은 답변: 인공지능, 머신러닝)",Loss Function 관련 실무 경험,-0.0039423564,0.0,0.0039423564448952675
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아 (남은 답변: 인공지능, 머신러닝)",Loss Function 예시,-0.001473906,0.0,0.001473905984312296
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아 (남은 답변: 인공지능, 머신러닝)",Loss Function 정의,-0.0022246237,0.0,0.002224623691290617
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아 (남은 답변: 인공지능, 머신러닝)",MBTI / 좋아하는 아이돌,-0.00072166265,0.0,0.0007216626545414329
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아 (남은 답변: 인공지능, 머신러닝)",MSE Loss 설명,-0.0050465637,0.0,0.005046563688665628
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아 (남은 답변: 인공지능, 머신러닝)",MSE Loss 용도,-0.002853581,0.0,0.0028535809833556414
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아 (남은 답변: 인공지능, 머신러닝)","Multi-Class, Multi-Label 중 BCE 가 좋은 task",-0.0013003473,0.0,0.0013003472704440355
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아 (남은 답변: 인공지능, 머신러닝)",Multi-Label 에서 CE + Softmax 적용 문제점,-0.0019231058,0.0,0.001923105795867741
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아 (남은 답변: 인공지능, 머신러닝)",PEFT 방법 5가지,-0.0057215556,0.0,0.005721555557101965
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아 (남은 답변: 인공지능, 머신러닝)",거대 언어 모델 정의,-0.0033185391,0.0,0.003318539122119546
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아 (남은 답변: 인공지능, 머신러닝)",마지막 할 말,-0.005125227,0.0,0.00512522691860795
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아 (남은 답변: 인공지능, 머신러닝)",면접 시작 인사,-0.004480882,0.0,0.004480882082134485
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아 (남은 답변: 인공지능, 머신러닝)",면접 종료,-0.005319696,0.0,0.005319695919752121
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아 (남은 답변: 인공지능, 머신러닝)","인공지능, 머신러닝, 딥러닝 차이",0.99932003,1.0,0.0006799697875976562
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아 (남은 답변: 인공지능, 머신러닝)",잠시 휴식,-0.0014377198,0.0,0.001437719794921577
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아 (남은 답변: 인공지능, 머신러닝)",확률 예측에서 MSE Loss 미 사용 이유,-0.004478745,0.0,0.00447874516248703
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지? (남은 답변: 머신러닝)",BCE Loss 설명,-0.0030277343,0.0,0.0030277343466877937
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지? (남은 답변: 머신러닝)",LLM Fine-Tuning 의 PEFT,-0.0021005513,0.0,0.0021005512680858374
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지? (남은 답변: 머신러닝)",LoRA,-0.0010450948,0.0,0.0010450948029756546
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지? (남은 답변: 머신러닝)",LoRA 와 QLoRA 의 차이,-0.0020046504,0.0,0.0020046504214406013
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지? (남은 답변: 머신러닝)",Loss Function 관련 실무 경험,-0.0039847894,0.0,0.003984789364039898
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지? (남은 답변: 머신러닝)",Loss Function 예시,0.00083927147,0.0,0.0008392714662477374
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지? (남은 답변: 머신러닝)",Loss Function 정의,-0.0025004982,0.0,0.002500498201698065
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지? (남은 답변: 머신러닝)",MBTI / 좋아하는 아이돌,-0.00095841306,0.0,0.0009584130602888763
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지? (남은 답변: 머신러닝)",MSE Loss 설명,-0.0042068926,0.0,0.004206892568618059
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지? (남은 답변: 머신러닝)",MSE Loss 용도,-0.0024887405,0.0,0.002488740487024188
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지? (남은 답변: 머신러닝)","Multi-Class, Multi-Label 중 BCE 가 좋은 task",-0.0011398717,0.0,0.0011398716596886516
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지? (남은 답변: 머신러닝)",Multi-Label 에서 CE + Softmax 적용 문제점,-0.0014052081,0.0,0.0014052081387490034
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지? (남은 답변: 머신러닝)",PEFT 방법 5가지,-0.0049432795,0.0,0.0049432795494794846
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지? (남은 답변: 머신러닝)",거대 언어 모델 정의,-0.002975315,0.0,0.002975315088406205
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지? (남은 답변: 머신러닝)",마지막 할 말,-0.0049655423,0.0,0.00496554234996438
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지? (남은 답변: 머신러닝)",면접 시작 인사,-0.003260058,0.0,0.0032600578851997852
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지? (남은 답변: 머신러닝)",면접 종료,-0.0058120596,0.0,0.005812059622257948
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지? (남은 답변: 머신러닝)","인공지능, 머신러닝, 딥러닝 차이",0.99903023,1.0,0.0009697675704956055
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지? (남은 답변: 머신러닝)",잠시 휴식,-0.002057564,0.0,0.002057563979178667
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지? (남은 답변: 머신러닝)",확률 예측에서 MSE Loss 미 사용 이유,-0.0037658096,0.0,0.0037658095825463533
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야! (남은 답변: 모든 질문 해결 완료)",BCE Loss 설명,-0.0021022544,0.0,0.0021022544242441654
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야! (남은 답변: 모든 질문 해결 완료)",LLM Fine-Tuning 의 PEFT,-0.0017456183,0.0,0.0017456182977184653
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야! (남은 답변: 모든 질문 해결 완료)",LoRA,0.00047089724,0.0,0.0004708972410298884
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야! (남은 답변: 모든 질문 해결 완료)",LoRA 와 QLoRA 의 차이,-0.0004146799,0.0,0.00041467990376986563
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야! (남은 답변: 모든 질문 해결 완료)",Loss Function 관련 실무 경험,-0.0052307197,0.0,0.005230719689279795
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야! (남은 답변: 모든 질문 해결 완료)",Loss Function 예시,-0.0006916271,0.0,0.0006916270940564573
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야! (남은 답변: 모든 질문 해결 완료)",Loss Function 정의,-0.0040172315,0.0,0.004017231520265341
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야! (남은 답변: 모든 질문 해결 완료)",MBTI / 좋아하는 아이돌,-0.004828536,0.0,0.0048285359516739845
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야! (남은 답변: 모든 질문 해결 완료)",MSE Loss 설명,-0.0010203536,0.0,0.0010203536367043853
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야! (남은 답변: 모든 질문 해결 완료)",MSE Loss 용도,-0.00350041,0.0,0.0035004098899662495
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야! (남은 답변: 모든 질문 해결 완료)","Multi-Class, Multi-Label 중 BCE 가 좋은 task",0.0007094152,0.0,0.0007094151806086302
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야! (남은 답변: 모든 질문 해결 완료)",Multi-Label 에서 CE + Softmax 적용 문제점,-0.0028705965,0.0,0.00287059647962451
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야! (남은 답변: 모든 질문 해결 완료)",PEFT 방법 5가지,-0.0015698912,0.0,0.0015698912320658565
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야! (남은 답변: 모든 질문 해결 완료)",거대 언어 모델 정의,0.99752164,1.0,0.002478361129760742
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야! (남은 답변: 모든 질문 해결 완료)",마지막 할 말,0.0018692253,0.0,0.0018692252924665809
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야! (남은 답변: 모든 질문 해결 완료)",면접 시작 인사,-0.002598753,0.0,0.0025987529661506414
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야! (남은 답변: 모든 질문 해결 완료)",면접 종료,-0.00078665005,0.0,0.0007866500527597964
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야! (남은 답변: 모든 질문 해결 완료)","인공지능, 머신러닝, 딥러닝 차이",-0.0028460836,0.0,0.002846083603799343
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야! (남은 답변: 모든 질문 해결 완료)",잠시 휴식,0.00076228974,0.0,0.0007622897392138839
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야! (남은 답변: 모든 질문 해결 완료)",확률 예측에서 MSE Loss 미 사용 이유,-0.00088659045,0.0,0.0008865904528647661
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지 (남은 답변: 모든 질문 해결 완료)",BCE Loss 설명,-0.0019555097,0.0,0.001955509651452303
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지 (남은 답변: 모든 질문 해결 완료)",LLM Fine-Tuning 의 PEFT,-0.001840571,0.0,0.0018405710579827428
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지 (남은 답변: 모든 질문 해결 완료)",LoRA,-0.00032527375,0.0,0.0003252737515140325
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지 (남은 답변: 모든 질문 해결 완료)",LoRA 와 QLoRA 의 차이,-0.00082990323,0.0,0.0008299032342620194
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지 (남은 답변: 모든 질문 해결 완료)",Loss Function 관련 실무 경험,-0.0040125074,0.0,0.004012507386505604
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지 (남은 답변: 모든 질문 해결 완료)",Loss Function 예시,-0.00029293122,0.0,0.000292931217700243
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지 (남은 답변: 모든 질문 해결 완료)",Loss Function 정의,-0.0036784057,0.0,0.0036784056574106216
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지 (남은 답변: 모든 질문 해결 완료)",MBTI / 좋아하는 아이돌,-0.0050704423,0.0,0.005070442333817482
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지 (남은 답변: 모든 질문 해결 완료)",MSE Loss 설명,-0.00034353827,0.0,0.0003435382677707821
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지 (남은 답변: 모든 질문 해결 완료)",MSE Loss 용도,-0.0023153673,0.0,0.00231536733917892
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지 (남은 답변: 모든 질문 해결 완료)","Multi-Class, Multi-Label 중 BCE 가 좋은 task",0.0008446596,0.0,0.0008446595747955143
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지 (남은 답변: 모든 질문 해결 완료)",Multi-Label 에서 CE + Softmax 적용 문제점,-0.003621002,0.0,0.003621001960709691
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지 (남은 답변: 모든 질문 해결 완료)",PEFT 방법 5가지,-0.0013393114,0.0,0.0013393113622441888
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지 (남은 답변: 모든 질문 해결 완료)",거대 언어 모델 정의,0.9978718,1.0,0.0021281838417053223
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지 (남은 답변: 모든 질문 해결 완료)",마지막 할 말,0.0011325533,0.0,0.0011325533268973231
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지 (남은 답변: 모든 질문 해결 완료)",면접 시작 인사,-0.0019477978,0.0,0.0019477978348731995
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지 (남은 답변: 모든 질문 해결 완료)",면접 종료,-0.00010744278,0.0,0.00010744277824414894
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지 (남은 답변: 모든 질문 해결 완료)","인공지능, 머신러닝, 딥러닝 차이",-0.0039865547,0.0,0.0039865546859800816
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지 (남은 답변: 모든 질문 해결 완료)",잠시 휴식,0.0014132629,0.0,0.001413262914866209
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지 (남은 답변: 모든 질문 해결 완료)",확률 예측에서 MSE Loss 미 사용 이유,-0.0010845498,0.0,0.001084549818187952
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야. (남은 답변: 모든 질문 해결 완료)",BCE Loss 설명,-0.0023610028,0.0,0.002361002843827009
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야. (남은 답변: 모든 질문 해결 완료)",LLM Fine-Tuning 의 PEFT,-0.0014803865,0.0,0.001480386476032436
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야. (남은 답변: 모든 질문 해결 완료)",LoRA,-0.00026755893,0.0,0.0002675589348655194
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야. (남은 답변: 모든 질문 해결 완료)",LoRA 와 QLoRA 의 차이,-0.00045363986,0.0,0.00045363986282609403
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야. (남은 답변: 모든 질문 해결 완료)",Loss Function 관련 실무 경험,-0.0036511584,0.0,0.003651158418506384
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야. (남은 답변: 모든 질문 해결 완료)",Loss Function 예시,-0.00034609705,0.0,0.0003460970474407077
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야. (남은 답변: 모든 질문 해결 완료)",Loss Function 정의,-0.0028347818,0.0,0.0028347817715257406
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야. (남은 답변: 모든 질문 해결 완료)",MBTI / 좋아하는 아이돌,-0.0043272856,0.0,0.004327285569161177
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야. (남은 답변: 모든 질문 해결 완료)",MSE Loss 설명,0.0005589453,0.0,0.0005589452921412885
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야. (남은 답변: 모든 질문 해결 완료)",MSE Loss 용도,-0.0026750474,0.0,0.0026750473771244287
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야. (남은 답변: 모든 질문 해결 완료)","Multi-Class, Multi-Label 중 BCE 가 좋은 task",0.0010650365,0.0,0.0010650365147739649
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야. (남은 답변: 모든 질문 해결 완료)",Multi-Label 에서 CE + Softmax 적용 문제점,-0.004608369,0.0,0.004608368966728449
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야. (남은 답변: 모든 질문 해결 완료)",PEFT 방법 5가지,-0.0017507892,0.0,0.0017507892334833741
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야. (남은 답변: 모든 질문 해결 완료)",거대 언어 모델 정의,0.9979078,1.0,0.0020921826362609863
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야. (남은 답변: 모든 질문 해결 완료)",마지막 할 말,0.0004112666,0.0,0.0004112666065338999
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야. (남은 답변: 모든 질문 해결 완료)",면접 시작 인사,-0.002202287,0.0,0.002202287083491683
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야. (남은 답변: 모든 질문 해결 완료)",면접 종료,0.000105608946,0.0,0.00010560894588707015
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야. (남은 답변: 모든 질문 해결 완료)","인공지능, 머신러닝, 딥러닝 차이",-0.0037499536,0.0,0.003749953582882881
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야. (남은 답변: 모든 질문 해결 완료)",잠시 휴식,0.0006721311,0.0,0.0006721310783177614
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야. (남은 답변: 모든 질문 해결 완료)",확률 예측에서 MSE Loss 미 사용 이유,-0.0016825832,0.0,0.0016825832426548004
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로 (남은 답변: 머신러닝, 딥러닝)",BCE Loss 설명,-0.0017357981,0.0,0.0017357980832457542
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로 (남은 답변: 머신러닝, 딥러닝)",LLM Fine-Tuning 의 PEFT,-0.0014442042,0.0,0.001444204244762659
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로 (남은 답변: 머신러닝, 딥러닝)",LoRA,-0.0012139152,0.0,0.001213915180414915
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로 (남은 답변: 머신러닝, 딥러닝)",LoRA 와 QLoRA 의 차이,-0.0023614664,0.0,0.002361466409638524
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로 (남은 답변: 머신러닝, 딥러닝)",Loss Function 관련 실무 경험,-0.0040581063,0.0,0.0040581063367426395
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로 (남은 답변: 머신러닝, 딥러닝)",Loss Function 예시,-0.001489495,0.0,0.001489495043642819
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로 (남은 답변: 머신러닝, 딥러닝)",Loss Function 정의,-0.0024325612,0.0,0.002432561246678233
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로 (남은 답변: 머신러닝, 딥러닝)",MBTI / 좋아하는 아이돌,-0.0011622898,0.0,0.0011622897582128644
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로 (남은 답변: 머신러닝, 딥러닝)",MSE Loss 설명,-0.0030754532,0.0,0.003075453219935298
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로 (남은 답변: 머신러닝, 딥러닝)",MSE Loss 용도,-0.002940518,0.0,0.002940518083050847
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로 (남은 답변: 머신러닝, 딥러닝)","Multi-Class, Multi-Label 중 BCE 가 좋은 task",-0.00037300473,0.0,0.0003730047319550067
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로 (남은 답변: 머신러닝, 딥러닝)",Multi-Label 에서 CE + Softmax 적용 문제점,-0.0014895728,0.0,0.0014895728090777993
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로 (남은 답변: 머신러닝, 딥러닝)",PEFT 방법 5가지,-0.005546115,0.0,0.00554611487314105
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로 (남은 답변: 머신러닝, 딥러닝)",거대 언어 모델 정의,-0.0015807451,0.0,0.001580745098181069
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로 (남은 답변: 머신러닝, 딥러닝)",마지막 할 말,-0.005426575,0.0,0.005426574964076281
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로 (남은 답변: 머신러닝, 딥러닝)",면접 시작 인사,-0.0038725277,0.0,0.003872527740895748
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로 (남은 답변: 머신러닝, 딥러닝)",면접 종료,-0.0052501895,0.0,0.005250189453363419
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로 (남은 답변: 머신러닝, 딥러닝)","인공지능, 머신러닝, 딥러닝 차이",0.9993901,1.0,0.0006098747253417969
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로 (남은 답변: 머신러닝, 딥러닝)",잠시 휴식,-0.0023454577,0.0,0.002345457673072815
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로 (남은 답변: 머신러닝, 딥러닝)",확률 예측에서 MSE Loss 미 사용 이유,-0.004946149,0.0,0.004946148954331875
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야 (남은 답변: 모든 질문 해결 완료),BCE Loss 설명,-0.0038471413,0.0,0.003847141284495592
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야 (남은 답변: 모든 질문 해결 완료),LLM Fine-Tuning 의 PEFT,-0.004179544,0.0,0.004179543815553188
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야 (남은 답변: 모든 질문 해결 완료),LoRA,-0.0015980657,0.0,0.0015980657190084457
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야 (남은 답변: 모든 질문 해결 완료),LoRA 와 QLoRA 의 차이,-0.002049224,0.0,0.0020492239855229855
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야 (남은 답변: 모든 질문 해결 완료),Loss Function 관련 실무 경험,-0.0039939154,0.0,0.003993915393948555
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야 (남은 답변: 모든 질문 해결 완료),Loss Function 예시,-0.006524857,0.0,0.006524857133626938
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야 (남은 답변: 모든 질문 해결 완료),Loss Function 정의,0.99839884,1.0,0.0016011595726013184
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야 (남은 답변: 모든 질문 해결 완료),MBTI / 좋아하는 아이돌,-0.0041131754,0.0,0.004113175440579653
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야 (남은 답변: 모든 질문 해결 완료),MSE Loss 설명,-0.004591359,0.0,0.0045913588255643845
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야 (남은 답변: 모든 질문 해결 완료),MSE Loss 용도,-0.0059976983,0.0,0.005997698288410902
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야 (남은 답변: 모든 질문 해결 완료),"Multi-Class, Multi-Label 중 BCE 가 좋은 task",0.0003744978,0.0,0.0003744977875612676
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야 (남은 답변: 모든 질문 해결 완료),Multi-Label 에서 CE + Softmax 적용 문제점,-0.00031076514,0.0,0.00031076514278538525
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야 (남은 답변: 모든 질문 해결 완료),PEFT 방법 5가지,-0.0025620458,0.0,0.002562045818194747
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야 (남은 답변: 모든 질문 해결 완료),거대 언어 모델 정의,-0.004806947,0.0,0.004806946963071823
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야 (남은 답변: 모든 질문 해결 완료),마지막 할 말,-0.004444349,0.0,0.004444349091500044
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야 (남은 답변: 모든 질문 해결 완료),면접 시작 인사,-0.0027413454,0.0,0.002741345437243581
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야 (남은 답변: 모든 질문 해결 완료),면접 종료,-0.0027812605,0.0,0.0027812605258077383
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야 (남은 답변: 모든 질문 해결 완료),"인공지능, 머신러닝, 딥러닝 차이",-0.002030089,0.0,0.002030089031904936
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야 (남은 답변: 모든 질문 해결 완료),잠시 휴식,-0.0038571735,0.0,0.00385717349126935
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야 (남은 답변: 모든 질문 해결 완료),확률 예측에서 MSE Loss 미 사용 이유,-0.0057671517,0.0,0.005767151713371277
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데 (남은 답변: 거대 언어 모델 정의),BCE Loss 설명,-0.020479353,0.0,0.0204793531447649
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데 (남은 답변: 거대 언어 모델 정의),LLM Fine-Tuning 의 PEFT,-0.009546651,0.0,0.00954665057361126
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데 (남은 답변: 거대 언어 모델 정의),LoRA,-0.0148811955,0.0,0.01488119550049305
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데 (남은 답변: 거대 언어 모델 정의),LoRA 와 QLoRA 의 차이,-0.008097668,0.0,0.008097668178379536
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데 (남은 답변: 거대 언어 모델 정의),Loss Function 관련 실무 경험,-0.01384592,0.0,0.013845919631421566
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데 (남은 답변: 거대 언어 모델 정의),Loss Function 예시,0.005088947,0.0,0.005088946782052517
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데 (남은 답변: 거대 언어 모델 정의),Loss Function 정의,0.54606825,0.0,0.5460682511329651
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데 (남은 답변: 거대 언어 모델 정의),MBTI / 좋아하는 아이돌,-0.019533202,0.0,0.019533202052116394
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데 (남은 답변: 거대 언어 모델 정의),MSE Loss 설명,-0.0091495225,0.0,0.00914952252060175
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데 (남은 답변: 거대 언어 모델 정의),MSE Loss 용도,-0.013787332,0.0,0.013787331990897655
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데 (남은 답변: 거대 언어 모델 정의),"Multi-Class, Multi-Label 중 BCE 가 좋은 task",-0.003588847,0.0,0.0035888468846678734
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데 (남은 답변: 거대 언어 모델 정의),Multi-Label 에서 CE + Softmax 적용 문제점,-0.00767927,0.0,0.00767927011474967
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데 (남은 답변: 거대 언어 모델 정의),PEFT 방법 5가지,-0.0077758105,0.0,0.007775810547173023
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데 (남은 답변: 거대 언어 모델 정의),거대 언어 모델 정의,0.7929601,1.0,0.20703989267349243
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데 (남은 답변: 거대 언어 모델 정의),마지막 할 말,-0.013365492,0.0,0.013365492224693298
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데 (남은 답변: 거대 언어 모델 정의),면접 시작 인사,-0.0068861092,0.0,0.006886109244078398
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데 (남은 답변: 거대 언어 모델 정의),면접 종료,-0.030360091,0.0,0.030360091477632523
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데 (남은 답변: 거대 언어 모델 정의),"인공지능, 머신러닝, 딥러닝 차이",-0.022259247,0.0,0.022259246557950974
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데 (남은 답변: 거대 언어 모델 정의),잠시 휴식,-0.0065660155,0.0,0.006566015537828207
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데 (남은 답변: 거대 언어 모델 정의),확률 예측에서 MSE Loss 미 사용 이유,-0.005015277,0.0,0.005015276838093996
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델 (남은 답변: 모든 질문 해결 완료),BCE Loss 설명,-0.003927917,0.0,0.003927917219698429
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델 (남은 답변: 모든 질문 해결 완료),LLM Fine-Tuning 의 PEFT,-0.0038755264,0.0,0.0038755263667553663
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델 (남은 답변: 모든 질문 해결 완료),LoRA,-0.0014419126,0.0,0.0014419126091524959
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델 (남은 답변: 모든 질문 해결 완료),LoRA 와 QLoRA 의 차이,-0.0018327342,0.0,0.0018327342113479972
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델 (남은 답변: 모든 질문 해결 완료),Loss Function 관련 실무 경험,-0.004128181,0.0,0.004128180909901857
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델 (남은 답변: 모든 질문 해결 완료),Loss Function 예시,-0.0074825026,0.0,0.007482502609491348
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델 (남은 답변: 모든 질문 해결 완료),Loss Function 정의,0.99803156,1.0,0.0019684433937072754
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델 (남은 답변: 모든 질문 해결 완료),MBTI / 좋아하는 아이돌,-0.0033329262,0.0,0.003332926193252206
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델 (남은 답변: 모든 질문 해결 완료),MSE Loss 설명,-0.004294345,0.0,0.0042943451553583145
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델 (남은 답변: 모든 질문 해결 완료),MSE Loss 용도,-0.0041385274,0.0,0.004138527438044548
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델 (남은 답변: 모든 질문 해결 완료),"Multi-Class, Multi-Label 중 BCE 가 좋은 task",-0.0002281057,0.0,0.00022810569498687983
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델 (남은 답변: 모든 질문 해결 완료),Multi-Label 에서 CE + Softmax 적용 문제점,-0.0012045904,0.0,0.0012045904295518994
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델 (남은 답변: 모든 질문 해결 완료),PEFT 방법 5가지,-0.0015804522,0.0,0.0015804521972313523
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델 (남은 답변: 모든 질문 해결 완료),거대 언어 모델 정의,-0.008269835,0.0,0.008269835263490677
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델 (남은 답변: 모든 질문 해결 완료),마지막 할 말,-0.0041754707,0.0,0.0041754706762731075
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델 (남은 답변: 모든 질문 해결 완료),면접 시작 인사,-0.0017719718,0.0,0.0017719718161970377
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델 (남은 답변: 모든 질문 해결 완료),면접 종료,-0.0016109247,0.0,0.0016109247226268053
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델 (남은 답변: 모든 질문 해결 완료),"인공지능, 머신러닝, 딥러닝 차이",-0.0024659394,0.0,0.0024659393820911646
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델 (남은 답변: 모든 질문 해결 완료),잠시 휴식,-0.0036510236,0.0,0.0036510236095637083
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델 (남은 답변: 모든 질문 해결 완료),확률 예측에서 MSE Loss 미 사용 이유,-0.0059873024,0.0,0.005987302400171757
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지. (남은 답변: 거대 언어 모델 정의),BCE Loss 설명,-0.0031235688,0.0,0.0031235688365995884
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지. (남은 답변: 거대 언어 모델 정의),LLM Fine-Tuning 의 PEFT,-0.0016355097,0.0,0.0016355096595361829
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지. (남은 답변: 거대 언어 모델 정의),LoRA,-0.0015367843,0.0,0.0015367843443527818
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지. (남은 답변: 거대 언어 모델 정의),LoRA 와 QLoRA 의 차이,-0.0014642114,0.0,0.001464211381971836
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지. (남은 답변: 거대 언어 모델 정의),Loss Function 관련 실무 경험,-0.003201982,0.0,0.003201982006430626
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지. (남은 답변: 거대 언어 모델 정의),Loss Function 예시,-0.001751762,0.0,0.00175176199991256
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지. (남은 답변: 거대 언어 모델 정의),Loss Function 정의,-0.0048649264,0.0,0.00486492644995451
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지. (남은 답변: 거대 언어 모델 정의),MBTI / 좋아하는 아이돌,-0.0033679716,0.0,0.0033679716289043427
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지. (남은 답변: 거대 언어 모델 정의),MSE Loss 설명,-0.0014161541,0.0,0.001416154089383781
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지. (남은 답변: 거대 언어 모델 정의),MSE Loss 용도,-0.0038638264,0.0,0.0038638263940811157
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지. (남은 답변: 거대 언어 모델 정의),"Multi-Class, Multi-Label 중 BCE 가 좋은 task",0.0020423178,0.0,0.0020423177629709244
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지. (남은 답변: 거대 언어 모델 정의),Multi-Label 에서 CE + Softmax 적용 문제점,-0.003173104,0.0,0.0031731040216982365
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지. (남은 답변: 거대 언어 모델 정의),PEFT 방법 5가지,-0.0031665086,0.0,0.0031665086280554533
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지. (남은 답변: 거대 언어 모델 정의),거대 언어 모델 정의,0.99882555,1.0,0.0011744499206542969
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지. (남은 답변: 거대 언어 모델 정의),마지막 할 말,0.0011364884,0.0,0.0011364883976057172
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지. (남은 답변: 거대 언어 모델 정의),면접 시작 인사,-0.0021449174,0.0,0.002144917380064726
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지. (남은 답변: 거대 언어 모델 정의),면접 종료,-0.0011395357,0.0,0.001139535685069859
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지. (남은 답변: 거대 언어 모델 정의),"인공지능, 머신러닝, 딥러닝 차이",-0.0032857007,0.0,0.003285700688138604
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지. (남은 답변: 거대 언어 모델 정의),잠시 휴식,0.0021711825,0.0,0.0021711825393140316
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지. (남은 답변: 거대 언어 모델 정의),확률 예측에서 MSE Loss 미 사용 이유,0.00071585976,0.0,0.0007158597582019866
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야 (남은 답변: Loss Function 정의),BCE Loss 설명,-0.007754507,0.0,0.0077545070089399815
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야 (남은 답변: Loss Function 정의),LLM Fine-Tuning 의 PEFT,-0.0019490493,0.0,0.001949049299582839
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야 (남은 답변: Loss Function 정의),LoRA,-0.005563135,0.0,0.005563134793192148
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야 (남은 답변: Loss Function 정의),LoRA 와 QLoRA 의 차이,0.0012494341,0.0,0.001249434077180922
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야 (남은 답변: Loss Function 정의),Loss Function 관련 실무 경험,-0.0062901326,0.0,0.006290132645517588
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야 (남은 답변: Loss Function 정의),Loss Function 예시,0.06930316,0.0,0.06930316239595413
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야 (남은 답변: Loss Function 정의),Loss Function 정의,0.9926264,1.0,0.0073735713958740234
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야 (남은 답변: Loss Function 정의),MBTI / 좋아하는 아이돌,-0.004574734,0.0,0.0045747337862849236
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야 (남은 답변: Loss Function 정의),MSE Loss 설명,-0.012541038,0.0,0.012541037984192371
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야 (남은 답변: Loss Function 정의),MSE Loss 용도,-0.004166967,0.0,0.004166966769844294
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야 (남은 답변: Loss Function 정의),"Multi-Class, Multi-Label 중 BCE 가 좋은 task",-0.0062184623,0.0,0.006218462251126766
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야 (남은 답변: Loss Function 정의),Multi-Label 에서 CE + Softmax 적용 문제점,-0.0033408182,0.0,0.0033408182207494974
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야 (남은 답변: Loss Function 정의),PEFT 방법 5가지,-0.007494568,0.0,0.007494567893445492
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야 (남은 답변: Loss Function 정의),거대 언어 모델 정의,-0.013020839,0.0,0.013020838610827923
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야 (남은 답변: Loss Function 정의),마지막 할 말,-0.0047990684,0.0,0.0047990684397518635
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야 (남은 답변: Loss Function 정의),면접 시작 인사,-0.007100774,0.0,0.007100773975253105
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야 (남은 답변: Loss Function 정의),면접 종료,-0.004289322,0.0,0.004289322067052126
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야 (남은 답변: Loss Function 정의),"인공지능, 머신러닝, 딥러닝 차이",-0.006626708,0.0,0.0066267079673707485
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야 (남은 답변: Loss Function 정의),잠시 휴식,-0.00036688062,0.0,0.00036688061663880944
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야 (남은 답변: Loss Function 정의),확률 예측에서 MSE Loss 미 사용 이유,-0.0030496235,0.0,0.003049623453989625
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지! (남은 답변: 모든 질문 해결 완료)",BCE Loss 설명,-0.009163911,0.0,0.009163911454379559
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지! (남은 답변: 모든 질문 해결 완료)",LLM Fine-Tuning 의 PEFT,-0.00555136,0.0,0.005551360081881285
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지! (남은 답변: 모든 질문 해결 완료)",LoRA,-0.0055012363,0.0,0.00550123630091548
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지! (남은 답변: 모든 질문 해결 완료)",LoRA 와 QLoRA 의 차이,-4.4030792e-05,0.0,4.4030792196281254e-05
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지! (남은 답변: 모든 질문 해결 완료)",Loss Function 관련 실무 경험,-0.0045918887,0.0,0.004591888748109341
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지! (남은 답변: 모든 질문 해결 완료)",Loss Function 예시,0.9959555,1.0,0.004044473171234131
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지! (남은 답변: 모든 질문 해결 완료)",Loss Function 정의,-0.0105957845,0.0,0.010595784522593021
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지! (남은 답변: 모든 질문 해결 완료)",MBTI / 좋아하는 아이돌,-0.0037662203,0.0,0.0037662202958017588
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지! (남은 답변: 모든 질문 해결 완료)",MSE Loss 설명,-0.005702768,0.0,0.005702767986804247
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지! (남은 답변: 모든 질문 해결 완료)",MSE Loss 용도,-0.0076714074,0.0,0.007671407423913479
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지! (남은 답변: 모든 질문 해결 완료)","Multi-Class, Multi-Label 중 BCE 가 좋은 task",-0.00576675,0.0,0.00576674984768033
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지! (남은 답변: 모든 질문 해결 완료)",Multi-Label 에서 CE + Softmax 적용 문제점,-0.004835315,0.0,0.004835315048694611
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지! (남은 답변: 모든 질문 해결 완료)",PEFT 방법 5가지,-0.001976611,0.0,0.001976611092686653
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지! (남은 답변: 모든 질문 해결 완료)",거대 언어 모델 정의,-0.0027990662,0.0,0.002799066249281168
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지! (남은 답변: 모든 질문 해결 완료)",마지막 할 말,-0.00956581,0.0,0.009565809741616249
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지! (남은 답변: 모든 질문 해결 완료)",면접 시작 인사,-0.010813996,0.0,0.010813996195793152
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지! (남은 답변: 모든 질문 해결 완료)",면접 종료,-0.010743798,0.0,0.01074379775673151
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지! (남은 답변: 모든 질문 해결 완료)","인공지능, 머신러닝, 딥러닝 차이",-0.002276446,0.0,0.002276445971801877
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지! (남은 답변: 모든 질문 해결 완료)",잠시 휴식,-0.0071426006,0.0,0.007142600603401661
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지! (남은 답변: 모든 질문 해결 완료)",확률 예측에서 MSE Loss 미 사용 이유,-0.009503874,0.0,0.009503873996436596
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야 (남은 답변: Loss Function 정의),BCE Loss 설명,-0.004445529,0.0,0.004445529077202082
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야 (남은 답변: Loss Function 정의),LLM Fine-Tuning 의 PEFT,-0.0016158238,0.0,0.0016158238286152482
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야 (남은 답변: Loss Function 정의),LoRA,-0.0019792651,0.0,0.0019792651291936636
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야 (남은 답변: Loss Function 정의),LoRA 와 QLoRA 의 차이,0.0011457865,0.0,0.0011457864893600345
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야 (남은 답변: Loss Function 정의),Loss Function 관련 실무 경험,-0.003331652,0.0,0.0033316519111394882
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야 (남은 답변: Loss Function 정의),Loss Function 예시,-0.0047382344,0.0,0.00473823444917798
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야 (남은 답변: Loss Function 정의),Loss Function 정의,0.99886346,1.0,0.0011365413665771484
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야 (남은 답변: Loss Function 정의),MBTI / 좋아하는 아이돌,-0.0032997136,0.0,0.0032997136004269123
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야 (남은 답변: Loss Function 정의),MSE Loss 설명,-0.004790218,0.0,0.004790218081325293
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야 (남은 답변: Loss Function 정의),MSE Loss 용도,-0.0038195006,0.0,0.0038195005618035793
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야 (남은 답변: Loss Function 정의),"Multi-Class, Multi-Label 중 BCE 가 좋은 task",-0.00016526082,0.0,0.00016526081890333444
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야 (남은 답변: Loss Function 정의),Multi-Label 에서 CE + Softmax 적용 문제점,-0.0003622768,0.0,0.00036227679811418056
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야 (남은 답변: Loss Function 정의),PEFT 방법 5가지,-0.0021337648,0.0,0.0021337647922337055
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야 (남은 답변: Loss Function 정의),거대 언어 모델 정의,-0.007048485,0.0,0.007048484869301319
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야 (남은 답변: Loss Function 정의),마지막 할 말,-0.0062482767,0.0,0.0062482766807079315
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야 (남은 답변: Loss Function 정의),면접 시작 인사,-0.003921956,0.0,0.003921955823898315
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야 (남은 답변: Loss Function 정의),면접 종료,-0.0015515296,0.0,0.0015515296254307032
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야 (남은 답변: Loss Function 정의),"인공지능, 머신러닝, 딥러닝 차이",-0.0016009244,0.0,0.001600924413651228
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야 (남은 답변: Loss Function 정의),잠시 휴식,-0.002599178,0.0,0.0025991781149059534
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야 (남은 답변: Loss Function 정의),확률 예측에서 MSE Loss 미 사용 이유,-0.0032745225,0.0,0.003274522488936782
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지 (남은 답변: 모든 질문 해결 완료),BCE Loss 설명,-0.009275954,0.0,0.009275954216718674
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지 (남은 답변: 모든 질문 해결 완료),LLM Fine-Tuning 의 PEFT,-0.0036907915,0.0,0.0036907915491610765
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지 (남은 답변: 모든 질문 해결 완료),LoRA,-0.0055154194,0.0,0.005515419412404299
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지 (남은 답변: 모든 질문 해결 완료),LoRA 와 QLoRA 의 차이,-0.0026873846,0.0,0.00268738460727036
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지 (남은 답변: 모든 질문 해결 완료),Loss Function 관련 실무 경험,-0.0062338808,0.0,0.006233880762010813
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지 (남은 답변: 모든 질문 해결 완료),Loss Function 예시,0.9960129,1.0,0.00398707389831543
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지 (남은 답변: 모든 질문 해결 완료),Loss Function 정의,-0.008837409,0.0,0.008837409317493439
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지 (남은 답변: 모든 질문 해결 완료),MBTI / 좋아하는 아이돌,-0.005636664,0.0,0.005636664107441902
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지 (남은 답변: 모든 질문 해결 완료),MSE Loss 설명,-0.0063136606,0.0,0.0063136606477200985
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지 (남은 답변: 모든 질문 해결 완료),MSE Loss 용도,-0.0055841524,0.0,0.005584152415394783
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지 (남은 답변: 모든 질문 해결 완료),"Multi-Class, Multi-Label 중 BCE 가 좋은 task",-0.005335784,0.0,0.005335784051567316
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지 (남은 답변: 모든 질문 해결 완료),Multi-Label 에서 CE + Softmax 적용 문제점,-0.0049089943,0.0,0.004908994305878878
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지 (남은 답변: 모든 질문 해결 완료),PEFT 방법 5가지,-0.00559668,0.0,0.005596680101007223
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지 (남은 답변: 모든 질문 해결 완료),거대 언어 모델 정의,-0.0010296279,0.0,0.001029627863317728
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지 (남은 답변: 모든 질문 해결 완료),마지막 할 말,-0.011187949,0.0,0.01118794921785593
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지 (남은 답변: 모든 질문 해결 완료),면접 시작 인사,-0.008841506,0.0,0.008841506205499172
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지 (남은 답변: 모든 질문 해결 완료),면접 종료,-0.010700456,0.0,0.010700455866754055
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지 (남은 답변: 모든 질문 해결 완료),"인공지능, 머신러닝, 딥러닝 차이",-0.0019056443,0.0,0.0019056443125009537
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지 (남은 답변: 모든 질문 해결 완료),잠시 휴식,-0.004778652,0.0,0.004778651986271143
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지 (남은 답변: 모든 질문 해결 완료),확률 예측에서 MSE Loss 미 사용 이유,-0.005991431,0.0,0.005991430953145027
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데 (남은 답변: Loss Function 예시),BCE Loss 설명,-0.0040192036,0.0,0.004019203595817089
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데 (남은 답변: Loss Function 예시),LLM Fine-Tuning 의 PEFT,-0.006589041,0.0,0.006589041091501713
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데 (남은 답변: Loss Function 예시),LoRA,-0.008501423,0.0,0.008501422591507435
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데 (남은 답변: Loss Function 예시),LoRA 와 QLoRA 의 차이,-0.004002955,0.0,0.004002954810857773
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데 (남은 답변: Loss Function 예시),Loss Function 관련 실무 경험,-0.004039604,0.0,0.004039604216814041
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데 (남은 답변: Loss Function 예시),Loss Function 예시,0.9970933,1.0,0.0029066801071166992
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데 (남은 답변: Loss Function 예시),Loss Function 정의,-0.01093304,0.0,0.010933039709925652
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데 (남은 답변: Loss Function 예시),MBTI / 좋아하는 아이돌,-0.0058583226,0.0,0.0058583226054906845
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데 (남은 답변: Loss Function 예시),MSE Loss 설명,-0.0070120683,0.0,0.007012068293988705
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데 (남은 답변: Loss Function 예시),MSE Loss 용도,-0.0056279646,0.0,0.005627964623272419
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데 (남은 답변: Loss Function 예시),"Multi-Class, Multi-Label 중 BCE 가 좋은 task",-0.005918356,0.0,0.0059183561243116856
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데 (남은 답변: Loss Function 예시),Multi-Label 에서 CE + Softmax 적용 문제점,-0.0048497063,0.0,0.004849706310778856
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데 (남은 답변: Loss Function 예시),PEFT 방법 5가지,-0.005909164,0.0,0.005909163970500231
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데 (남은 답변: Loss Function 예시),거대 언어 모델 정의,-0.0037146288,0.0,0.0037146287504583597
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데 (남은 답변: Loss Function 예시),마지막 할 말,-0.010332083,0.0,0.010332083329558372
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데 (남은 답변: Loss Function 예시),면접 시작 인사,-0.009600472,0.0,0.009600471705198288
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데 (남은 답변: Loss Function 예시),면접 종료,-0.0066507794,0.0,0.0066507793962955475
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데 (남은 답변: Loss Function 예시),"인공지능, 머신러닝, 딥러닝 차이",-0.00088767696,0.0,0.0008876769570633769
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데 (남은 답변: Loss Function 예시),잠시 휴식,-0.0044183945,0.0,0.004418394528329372
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데 (남은 답변: Loss Function 예시),확률 예측에서 MSE Loss 미 사용 이유,-0.0085870195,0.0,0.008587019518017769
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지 (남은 답변: 모든 질문 해결 완료)",BCE Loss 설명,-0.002566551,0.0,0.0025665510911494493
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지 (남은 답변: 모든 질문 해결 완료)",LLM Fine-Tuning 의 PEFT,-0.0016874577,0.0,0.001687457668595016
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지 (남은 답변: 모든 질문 해결 완료)",LoRA,-0.0067800917,0.0,0.0067800916731357574
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지 (남은 답변: 모든 질문 해결 완료)",LoRA 와 QLoRA 의 차이,0.0031232706,0.0,0.003123270580545068
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지 (남은 답변: 모든 질문 해결 완료)",Loss Function 관련 실무 경험,-0.0010463153,0.0,0.0010463153012096882
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지 (남은 답변: 모든 질문 해결 완료)",Loss Function 예시,-0.0027283975,0.0,0.0027283974923193455
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지 (남은 답변: 모든 질문 해결 완료)",Loss Function 정의,-0.0037455726,0.0,0.00374557264149189
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지 (남은 답변: 모든 질문 해결 완료)",MBTI / 좋아하는 아이돌,-0.0042464207,0.0,0.004246420692652464
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지 (남은 답변: 모든 질문 해결 완료)",MSE Loss 설명,0.9976071,1.0,0.002392888069152832
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지 (남은 답변: 모든 질문 해결 완료)",MSE Loss 용도,-0.003846311,0.0,0.0038463110104203224
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지 (남은 답변: 모든 질문 해결 완료)","Multi-Class, Multi-Label 중 BCE 가 좋은 task",-0.0024972584,0.0,0.002497258363291621
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지 (남은 답변: 모든 질문 해결 완료)",Multi-Label 에서 CE + Softmax 적용 문제점,-0.00060246885,0.0,0.0006024688482284546
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지 (남은 답변: 모든 질문 해결 완료)",PEFT 방법 5가지,-0.0024678935,0.0,0.0024678935296833515
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지 (남은 답변: 모든 질문 해결 완료)",거대 언어 모델 정의,-0.0019740593,0.0,0.0019740592688322067
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지 (남은 답변: 모든 질문 해결 완료)",마지막 할 말,-0.0025593718,0.0,0.0025593717582523823
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지 (남은 답변: 모든 질문 해결 완료)",면접 시작 인사,-0.0037665234,0.0,0.003766523441299796
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지 (남은 답변: 모든 질문 해결 완료)",면접 종료,-0.002647608,0.0,0.0026476080529391766
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지 (남은 답변: 모든 질문 해결 완료)","인공지능, 머신러닝, 딥러닝 차이",-0.0049418868,0.0,0.004941886756569147
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지 (남은 답변: 모든 질문 해결 완료)",잠시 휴식,-0.000911109,0.0,0.0009111089748330414
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지 (남은 답변: 모든 질문 해결 완료)",확률 예측에서 MSE Loss 미 사용 이유,-0.0016507459,0.0,0.0016507458640262485
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어 (남은 답변: 모든 질문 해결 완료)",BCE Loss 설명,0.99734515,1.0,0.002654850482940674
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어 (남은 답변: 모든 질문 해결 완료)",LLM Fine-Tuning 의 PEFT,-0.0029780117,0.0,0.0029780117329210043
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어 (남은 답변: 모든 질문 해결 완료)",LoRA,-0.0074982513,0.0,0.007498251274228096
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어 (남은 답변: 모든 질문 해결 완료)",LoRA 와 QLoRA 의 차이,-0.0044077407,0.0,0.004407740663737059
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어 (남은 답변: 모든 질문 해결 완료)",Loss Function 관련 실무 경험,-0.006222352,0.0,0.006222351919859648
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어 (남은 답변: 모든 질문 해결 완료)",Loss Function 예시,-0.014577165,0.0,0.014577165246009827
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어 (남은 답변: 모든 질문 해결 완료)",Loss Function 정의,-0.004070355,0.0,0.004070355091243982
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어 (남은 답변: 모든 질문 해결 완료)",MBTI / 좋아하는 아이돌,-0.0024790028,0.0,0.0024790028110146523
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어 (남은 답변: 모든 질문 해결 완료)",MSE Loss 설명,-0.00011494929,0.0,0.000114949289127253
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어 (남은 답변: 모든 질문 해결 완료)",MSE Loss 용도,-0.007850039,0.0,0.007850038819015026
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어 (남은 답변: 모든 질문 해결 완료)","Multi-Class, Multi-Label 중 BCE 가 좋은 task",-0.003000768,0.0,0.0030007679015398026
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어 (남은 답변: 모든 질문 해결 완료)",Multi-Label 에서 CE + Softmax 적용 문제점,-0.0033942675,0.0,0.0033942675217986107
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어 (남은 답변: 모든 질문 해결 완료)",PEFT 방법 5가지,-0.0024454023,0.0,0.0024454023223370314
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어 (남은 답변: 모든 질문 해결 완료)",거대 언어 모델 정의,-0.0036557715,0.0,0.003655771492049098
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어 (남은 답변: 모든 질문 해결 완료)",마지막 할 말,-0.0047962866,0.0,0.004796286579221487
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어 (남은 답변: 모든 질문 해결 완료)",면접 시작 인사,-0.0041309944,0.0,0.004130994435399771
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어 (남은 답변: 모든 질문 해결 완료)",면접 종료,-0.00062860176,0.0,0.0006286017596721649
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어 (남은 답변: 모든 질문 해결 완료)","인공지능, 머신러닝, 딥러닝 차이",-0.0034765077,0.0,0.003476507728919387
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어 (남은 답변: 모든 질문 해결 완료)",잠시 휴식,-0.0013728036,0.0,0.0013728035846725106
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어 (남은 답변: 모든 질문 해결 완료)",확률 예측에서 MSE Loss 미 사용 이유,-0.0023042345,0.0,0.00230423454195261
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야? (남은 답변: Loss Function 예시)",BCE Loss 설명,-0.0053860047,0.0,0.005386004690080881
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야? (남은 답변: Loss Function 예시)",LLM Fine-Tuning 의 PEFT,-0.005496105,0.0,0.005496105179190636
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야? (남은 답변: Loss Function 예시)",LoRA,-0.009324968,0.0,0.009324967861175537
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야? (남은 답변: Loss Function 예시)",LoRA 와 QLoRA 의 차이,-0.0053962814,0.0,0.005396281369030476
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야? (남은 답변: Loss Function 예시)",Loss Function 관련 실무 경험,-0.0053080856,0.0,0.005308085586875677
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야? (남은 답변: Loss Function 예시)",Loss Function 예시,0.9967001,1.0,0.003299891948699951
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야? (남은 답변: Loss Function 예시)",Loss Function 정의,-0.010907615,0.0,0.010907614603638649
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야? (남은 답변: Loss Function 예시)",MBTI / 좋아하는 아이돌,-0.006944712,0.0,0.006944711785763502
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야? (남은 답변: Loss Function 예시)",MSE Loss 설명,-0.006934405,0.0,0.006934404838830233
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야? (남은 답변: Loss Function 예시)",MSE Loss 용도,-0.0065656463,0.0,0.006565646268427372
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야? (남은 답변: Loss Function 예시)","Multi-Class, Multi-Label 중 BCE 가 좋은 task",-0.005726513,0.0,0.005726512987166643
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야? (남은 답변: Loss Function 예시)",Multi-Label 에서 CE + Softmax 적용 문제점,-0.0037899818,0.0,0.0037899818271398544
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야? (남은 답변: Loss Function 예시)",PEFT 방법 5가지,-0.0055547976,0.0,0.0055547975935041904
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야? (남은 답변: Loss Function 예시)",거대 언어 모델 정의,-0.0032373713,0.0,0.0032373713329434395
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야? (남은 답변: Loss Function 예시)",마지막 할 말,-0.012150058,0.0,0.012150057591497898
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야? (남은 답변: Loss Function 예시)",면접 시작 인사,-0.010259816,0.0,0.01025981642305851
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야? (남은 답변: Loss Function 예시)",면접 종료,-0.010030866,0.0,0.010030865669250488
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야? (남은 답변: Loss Function 예시)","인공지능, 머신러닝, 딥러닝 차이",0.0005157932,0.0,0.0005157932173460722
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야? (남은 답변: Loss Function 예시)",잠시 휴식,-0.0042065657,0.0,0.004206565674394369
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야? (남은 답변: Loss Function 예시)",확률 예측에서 MSE Loss 미 사용 이유,-0.008179325,0.0,0.008179324679076672
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어 (남은 답변: 모든 질문 해결 완료)",BCE Loss 설명,-0.0017246519,0.0,0.0017246518982574344
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어 (남은 답변: 모든 질문 해결 완료)",LLM Fine-Tuning 의 PEFT,-0.0003600662,0.0,0.0003600661875680089
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어 (남은 답변: 모든 질문 해결 완료)",LoRA,-0.004752605,0.0,0.0047526052221655846
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어 (남은 답변: 모든 질문 해결 완료)",LoRA 와 QLoRA 의 차이,0.0028987825,0.0,0.00289878249168396
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어 (남은 답변: 모든 질문 해결 완료)",Loss Function 관련 실무 경험,-0.001074963,0.0,0.001074963016435504
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어 (남은 답변: 모든 질문 해결 완료)",Loss Function 예시,-0.005486268,0.0,0.00548626808449626
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어 (남은 답변: 모든 질문 해결 완료)",Loss Function 정의,-0.005366286,0.0,0.005366285797208548
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어 (남은 답변: 모든 질문 해결 완료)",MBTI / 좋아하는 아이돌,-0.0024542892,0.0,0.002454289235174656
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어 (남은 답변: 모든 질문 해결 완료)",MSE Loss 설명,0.9970494,1.0,0.002950608730316162
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어 (남은 답변: 모든 질문 해결 완료)",MSE Loss 용도,-0.008966122,0.0,0.008966121822595596
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어 (남은 답변: 모든 질문 해결 완료)","Multi-Class, Multi-Label 중 BCE 가 좋은 task",-0.0024597468,0.0,0.0024597467854619026
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어 (남은 답변: 모든 질문 해결 완료)",Multi-Label 에서 CE + Softmax 적용 문제점,-0.0031516838,0.0,0.003151683835312724
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어 (남은 답변: 모든 질문 해결 완료)",PEFT 방법 5가지,4.9807622e-06,0.0,4.980762241757475e-06
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어 (남은 답변: 모든 질문 해결 완료)",거대 언어 모델 정의,-0.0050877365,0.0,0.005087736528366804
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어 (남은 답변: 모든 질문 해결 완료)",마지막 할 말,-0.0011837943,0.0,0.0011837943457067013
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어 (남은 답변: 모든 질문 해결 완료)",면접 시작 인사,-0.0028168268,0.0,0.002816826803609729
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어 (남은 답변: 모든 질문 해결 완료)",면접 종료,-0.00043973586,0.0,0.000439735857071355
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어 (남은 답변: 모든 질문 해결 완료)","인공지능, 머신러닝, 딥러닝 차이",-0.0020885777,0.0,0.0020885777194052935
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어 (남은 답변: 모든 질문 해결 완료)",잠시 휴식,-0.0017021492,0.0,0.0017021491657942533
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어 (남은 답변: 모든 질문 해결 완료)",확률 예측에서 MSE Loss 미 사용 이유,-0.0007845324,0.0,0.0007845323998481035
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아 (남은 답변: 모든 질문 해결 완료)",BCE Loss 설명,0.99775463,1.0,0.0022453665733337402
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아 (남은 답변: 모든 질문 해결 완료)",LLM Fine-Tuning 의 PEFT,-0.0031599863,0.0,0.0031599863432347775
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아 (남은 답변: 모든 질문 해결 완료)",LoRA,-0.009575383,0.0,0.009575382806360722
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아 (남은 답변: 모든 질문 해결 완료)",LoRA 와 QLoRA 의 차이,-0.0018394341,0.0,0.001839434145949781
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아 (남은 답변: 모든 질문 해결 완료)",Loss Function 관련 실무 경험,-0.0052598664,0.0,0.005259866360574961
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아 (남은 답변: 모든 질문 해결 완료)",Loss Function 예시,-0.011624152,0.0,0.011624151840806007
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아 (남은 답변: 모든 질문 해결 완료)",Loss Function 정의,-0.002342742,0.0,0.002342741936445236
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아 (남은 답변: 모든 질문 해결 완료)",MBTI / 좋아하는 아이돌,-0.004747831,0.0,0.004747830796986818
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아 (남은 답변: 모든 질문 해결 완료)",MSE Loss 설명,-0.0012032085,0.0,0.0012032084632664919
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아 (남은 답변: 모든 질문 해결 완료)",MSE Loss 용도,-0.00863442,0.0,0.008634420111775398
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아 (남은 답변: 모든 질문 해결 완료)","Multi-Class, Multi-Label 중 BCE 가 좋은 task",-0.0025340985,0.0,0.002534098457545042
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아 (남은 답변: 모든 질문 해결 완료)",Multi-Label 에서 CE + Softmax 적용 문제점,-0.0036723416,0.0,0.0036723415832966566
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아 (남은 답변: 모든 질문 해결 완료)",PEFT 방법 5가지,3.145991e-05,0.0,3.145990922348574e-05
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아 (남은 답변: 모든 질문 해결 완료)",거대 언어 모델 정의,-0.0024357254,0.0,0.0024357254151254892
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아 (남은 답변: 모든 질문 해결 완료)",마지막 할 말,-0.0034225737,0.0,0.0034225736744701862
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아 (남은 답변: 모든 질문 해결 완료)",면접 시작 인사,-0.0027768225,0.0,0.002776822540909052
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아 (남은 답변: 모든 질문 해결 완료)",면접 종료,-0.0030225755,0.0,0.003022575518116355
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아 (남은 답변: 모든 질문 해결 완료)","인공지능, 머신러닝, 딥러닝 차이",-0.0040943664,0.0,0.004094366449862719
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아 (남은 답변: 모든 질문 해결 완료)",잠시 휴식,-0.0027300757,0.0,0.0027300757355988026
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아 (남은 답변: 모든 질문 해결 완료)",확률 예측에서 MSE Loss 미 사용 이유,-0.004754136,0.0,0.004754135850816965
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야 (남은 답변: 모든 질문 해결 완료),BCE Loss 설명,-0.005616769,0.0,0.005616769194602966
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야 (남은 답변: 모든 질문 해결 완료),LLM Fine-Tuning 의 PEFT,-0.0009102389,0.0,0.0009102388867177069
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야 (남은 답변: 모든 질문 해결 완료),LoRA,-0.0065988908,0.0,0.006598890759050846
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야 (남은 답변: 모든 질문 해결 완료),LoRA 와 QLoRA 의 차이,-0.0011189327,0.0,0.0011189327342435718
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야 (남은 답변: 모든 질문 해결 완료),Loss Function 관련 실무 경험,-0.0045629614,0.0,0.004562961403280497
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야 (남은 답변: 모든 질문 해결 완료),Loss Function 예시,-0.0103107495,0.0,0.010310749523341656
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야 (남은 답변: 모든 질문 해결 완료),Loss Function 정의,-0.0032576823,0.0,0.003257682314142585
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야 (남은 답변: 모든 질문 해결 완료),MBTI / 좋아하는 아이돌,-0.0050261607,0.0,0.00502616073936224
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야 (남은 답변: 모든 질문 해결 완료),MSE Loss 설명,-0.004244946,0.0,0.00424494594335556
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야 (남은 답변: 모든 질문 해결 완료),MSE Loss 용도,0.9975977,1.0,0.0024023056030273438
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야 (남은 답변: 모든 질문 해결 완료),"Multi-Class, Multi-Label 중 BCE 가 좋은 task",-0.00050264737,0.0,0.0005026473663747311
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야 (남은 답변: 모든 질문 해결 완료),Multi-Label 에서 CE + Softmax 적용 문제점,-0.0020218184,0.0,0.002021818421781063
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야 (남은 답변: 모든 질문 해결 완료),PEFT 방법 5가지,-0.0025960212,0.0,0.0025960211642086506
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야 (남은 답변: 모든 질문 해결 완료),거대 언어 모델 정의,-0.0042304294,0.0,0.004230429418385029
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야 (남은 답변: 모든 질문 해결 완료),마지막 할 말,-0.004253167,0.0,0.0042531671933829784
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야 (남은 답변: 모든 질문 해결 완료),면접 시작 인사,-0.0019554715,0.0,0.0019554714672267437
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야 (남은 답변: 모든 질문 해결 완료),면접 종료,-0.0034708818,0.0,0.0034708818420767784
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야 (남은 답변: 모든 질문 해결 완료),"인공지능, 머신러닝, 딥러닝 차이",-0.0023769154,0.0,0.0023769154213368893
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야 (남은 답변: 모든 질문 해결 완료),잠시 휴식,-0.003613025,0.0,0.003613024950027466
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야 (남은 답변: 모든 질문 해결 완료),확률 예측에서 MSE Loss 미 사용 이유,-0.0031482945,0.0,0.0031482945196330547
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지? (남은 답변: MSE Loss 설명),BCE Loss 설명,-0.0012889585,0.0,0.0012889584759250283
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지? (남은 답변: MSE Loss 설명),LLM Fine-Tuning 의 PEFT,-0.00037162766,0.0,0.0003716276551131159
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지? (남은 답변: MSE Loss 설명),LoRA,-0.0029651735,0.0,0.00296517345122993
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지? (남은 답변: MSE Loss 설명),LoRA 와 QLoRA 의 차이,0.0012034881,0.0,0.0012034880928695202
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지? (남은 답변: MSE Loss 설명),Loss Function 관련 실무 경험,-0.0018218736,0.0,0.0018218735931441188
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지? (남은 답변: MSE Loss 설명),Loss Function 예시,-0.0066488907,0.0,0.006648890674114227
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지? (남은 답변: MSE Loss 설명),Loss Function 정의,-0.0029447835,0.0,0.002944783540442586
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지? (남은 답변: MSE Loss 설명),MBTI / 좋아하는 아이돌,-0.003959091,0.0,0.003959090914577246
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지? (남은 답변: MSE Loss 설명),MSE Loss 설명,0.9980415,1.0,0.001958489418029785
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지? (남은 답변: MSE Loss 설명),MSE Loss 용도,-0.0039532054,0.0,0.003953205421566963
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지? (남은 답변: MSE Loss 설명),"Multi-Class, Multi-Label 중 BCE 가 좋은 task",-0.00052427367,0.0,0.000524273666087538
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지? (남은 답변: MSE Loss 설명),Multi-Label 에서 CE + Softmax 적용 문제점,-0.0026361207,0.0,0.002636120654642582
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지? (남은 답변: MSE Loss 설명),PEFT 방법 5가지,-0.0030868573,0.0,0.0030868572648614645
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지? (남은 답변: MSE Loss 설명),거대 언어 모델 정의,-0.0032568027,0.0,0.0032568026799708605
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지? (남은 답변: MSE Loss 설명),마지막 할 말,-0.005329558,0.0,0.005329558160156012
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지? (남은 답변: MSE Loss 설명),면접 시작 인사,-0.002593243,0.0,0.0025932430289685726
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지? (남은 답변: MSE Loss 설명),면접 종료,-0.0060407417,0.0,0.006040741689503193
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지? (남은 답변: MSE Loss 설명),"인공지능, 머신러닝, 딥러닝 차이",-0.0015825533,0.0,0.0015825532609596848
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지? (남은 답변: MSE Loss 설명),잠시 휴식,-0.0034893055,0.0,0.0034893054980784655
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지? (남은 답변: MSE Loss 설명),확률 예측에서 MSE Loss 미 사용 이유,-0.0015128612,0.0,0.0015128612285479903
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지 (남은 답변: 모든 질문 해결 완료)",BCE Loss 설명,-0.0047268746,0.0,0.004726874642074108
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지 (남은 답변: 모든 질문 해결 완료)",LLM Fine-Tuning 의 PEFT,-0.0015110078,0.0,0.0015110077802091837
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지 (남은 답변: 모든 질문 해결 완료)",LoRA,-0.0046761474,0.0,0.004676147364079952
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지 (남은 답변: 모든 질문 해결 완료)",LoRA 와 QLoRA 의 차이,-0.0002687052,0.0,0.00026870518922805786
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지 (남은 답변: 모든 질문 해결 완료)",Loss Function 관련 실무 경험,-0.0021903994,0.0,0.0021903994493186474
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지 (남은 답변: 모든 질문 해결 완료)",Loss Function 예시,-0.0074932165,0.0,0.007493216544389725
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지 (남은 답변: 모든 질문 해결 완료)",Loss Function 정의,-0.007252669,0.0,0.007252668961882591
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지 (남은 답변: 모든 질문 해결 완료)",MBTI / 좋아하는 아이돌,-0.00504473,0.0,0.0050447299145162106
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지 (남은 답변: 모든 질문 해결 완료)",MSE Loss 설명,-0.0033615194,0.0,0.0033615194261074066
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지 (남은 답변: 모든 질문 해결 완료)",MSE Loss 용도,-0.0054195207,0.0,0.005419520661234856
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지 (남은 답변: 모든 질문 해결 완료)","Multi-Class, Multi-Label 중 BCE 가 좋은 task",-0.0019394212,0.0,0.0019394211703911424
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지 (남은 답변: 모든 질문 해결 완료)",Multi-Label 에서 CE + Softmax 적용 문제점,0.0005288201,0.0,0.0005288200918585062
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지 (남은 답변: 모든 질문 해결 완료)",PEFT 방법 5가지,-0.0029877927,0.0,0.0029877927154302597
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지 (남은 답변: 모든 질문 해결 완료)",거대 언어 모델 정의,0.0023400222,0.0,0.0023400222416967154
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지 (남은 답변: 모든 질문 해결 완료)",마지막 할 말,0.0001384977,0.0,0.000138497693114914
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지 (남은 답변: 모든 질문 해결 완료)",면접 시작 인사,-0.0045853546,0.0,0.0045853545889258385
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지 (남은 답변: 모든 질문 해결 완료)",면접 종료,-0.0068503036,0.0,0.006850303616374731
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지 (남은 답변: 모든 질문 해결 완료)","인공지능, 머신러닝, 딥러닝 차이",-0.003390071,0.0,0.0033900709822773933
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지 (남은 답변: 모든 질문 해결 완료)",잠시 휴식,-0.0022194483,0.0,0.0022194483317434788
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지 (남은 답변: 모든 질문 해결 완료)",확률 예측에서 MSE Loss 미 사용 이유,0.9986388,1.0,0.0013611912727355957
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지 (남은 답변: MSE Loss 용도),BCE Loss 설명,-0.0045886803,0.0,0.00458868034183979
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지 (남은 답변: MSE Loss 용도),LLM Fine-Tuning 의 PEFT,-0.00010301105,0.0,0.00010301105066901073
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지 (남은 답변: MSE Loss 용도),LoRA,-0.0055187494,0.0,0.005518749356269836
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지 (남은 답변: MSE Loss 용도),LoRA 와 QLoRA 의 차이,-0.002483945,0.0,0.002483945107087493
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지 (남은 답변: MSE Loss 용도),Loss Function 관련 실무 경험,-0.0040516984,0.0,0.004051698371767998
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지 (남은 답변: MSE Loss 용도),Loss Function 예시,-0.007340112,0.0,0.007340111769735813
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지 (남은 답변: MSE Loss 용도),Loss Function 정의,-0.0027880822,0.0,0.002788082230836153
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지 (남은 답변: MSE Loss 용도),MBTI / 좋아하는 아이돌,-0.0037679304,0.0,0.0037679304368793964
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지 (남은 답변: MSE Loss 용도),MSE Loss 설명,-0.0049914033,0.0,0.004991403315216303
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지 (남은 답변: MSE Loss 용도),MSE Loss 용도,0.99874747,1.0,0.0012525320053100586
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지 (남은 답변: MSE Loss 용도),"Multi-Class, Multi-Label 중 BCE 가 좋은 task",0.00071951194,0.0,0.0007195119396783412
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지 (남은 답변: MSE Loss 용도),Multi-Label 에서 CE + Softmax 적용 문제점,-0.0024743027,0.0,0.0024743026588112116
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지 (남은 답변: MSE Loss 용도),PEFT 방법 5가지,-0.0017484267,0.0,0.0017484267009422183
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지 (남은 답변: MSE Loss 용도),거대 언어 모델 정의,-0.0034143606,0.0,0.003414360573515296
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지 (남은 답변: MSE Loss 용도),마지막 할 말,-0.0027197844,0.0,0.002719784388318658
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지 (남은 답변: MSE Loss 용도),면접 시작 인사,-0.0017093351,0.0,0.0017093351343646646
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지 (남은 답변: MSE Loss 용도),면접 종료,-0.0046075867,0.0,0.004607586655765772
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지 (남은 답변: MSE Loss 용도),"인공지능, 머신러닝, 딥러닝 차이",-0.0035457287,0.0,0.0035457287449389696
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지 (남은 답변: MSE Loss 용도),잠시 휴식,-0.0040821726,0.0,0.004082172643393278
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지 (남은 답변: MSE Loss 용도),확률 예측에서 MSE Loss 미 사용 이유,-0.0044432515,0.0,0.0044432515278458595
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야? (남은 답변: 확률 예측에서 MSE Loss 미 사용 이유),BCE Loss 설명,-0.0018813865,0.0,0.0018813865026459098
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야? (남은 답변: 확률 예측에서 MSE Loss 미 사용 이유),LLM Fine-Tuning 의 PEFT,-0.0015263587,0.0,0.0015263586537912488
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야? (남은 답변: 확률 예측에서 MSE Loss 미 사용 이유),LoRA,-0.004354699,0.0,0.004354699049144983
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야? (남은 답변: 확률 예측에서 MSE Loss 미 사용 이유),LoRA 와 QLoRA 의 차이,-0.0019058781,0.0,0.0019058780744671822
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야? (남은 답변: 확률 예측에서 MSE Loss 미 사용 이유),Loss Function 관련 실무 경험,-0.003517284,0.0,0.00351728405803442
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야? (남은 답변: 확률 예측에서 MSE Loss 미 사용 이유),Loss Function 예시,-0.0068518184,0.0,0.006851818412542343
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야? (남은 답변: 확률 예측에서 MSE Loss 미 사용 이유),Loss Function 정의,-0.0031334313,0.0,0.0031334313098341227
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야? (남은 답변: 확률 예측에서 MSE Loss 미 사용 이유),MBTI / 좋아하는 아이돌,-0.0030813462,0.0,0.0030813461635261774
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야? (남은 답변: 확률 예측에서 MSE Loss 미 사용 이유),MSE Loss 설명,-5.2535695e-05,0.0,5.253569543128833e-05
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야? (남은 답변: 확률 예측에서 MSE Loss 미 사용 이유),MSE Loss 용도,-0.0025899813,0.0,0.0025899813044816256
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야? (남은 답변: 확률 예측에서 MSE Loss 미 사용 이유),"Multi-Class, Multi-Label 중 BCE 가 좋은 task",-0.0027327812,0.0,0.0027327812276780605
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야? (남은 답변: 확률 예측에서 MSE Loss 미 사용 이유),Multi-Label 에서 CE + Softmax 적용 문제점,-0.0019087193,0.0,0.0019087193068116903
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야? (남은 답변: 확률 예측에서 MSE Loss 미 사용 이유),PEFT 방법 5가지,-0.003865546,0.0,0.003865546081215143
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야? (남은 답변: 확률 예측에서 MSE Loss 미 사용 이유),거대 언어 모델 정의,-0.00010414603,0.0,0.0001041460272972472
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야? (남은 답변: 확률 예측에서 MSE Loss 미 사용 이유),마지막 할 말,-0.002339963,0.0,0.0023399631027132273
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야? (남은 답변: 확률 예측에서 MSE Loss 미 사용 이유),면접 시작 인사,-0.0049097114,0.0,0.0049097114242613316
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야? (남은 답변: 확률 예측에서 MSE Loss 미 사용 이유),면접 종료,-0.0053413594,0.0,0.005341359414160252
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야? (남은 답변: 확률 예측에서 MSE Loss 미 사용 이유),"인공지능, 머신러닝, 딥러닝 차이",-0.0044399244,0.0,0.004439924377948046
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야? (남은 답변: 확률 예측에서 MSE Loss 미 사용 이유),잠시 휴식,-0.0019714427,0.0,0.0019714427180588245
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야? (남은 답변: 확률 예측에서 MSE Loss 미 사용 이유),확률 예측에서 MSE Loss 미 사용 이유,0.99925786,1.0,0.0007421374320983887
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고? (남은 답변: 모든 질문 해결 완료),BCE Loss 설명,0.99707735,1.0,0.002922654151916504
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고? (남은 답변: 모든 질문 해결 완료),LLM Fine-Tuning 의 PEFT,-0.006718234,0.0,0.006718234159052372
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고? (남은 답변: 모든 질문 해결 완료),LoRA,-0.009926999,0.0,0.009926998987793922
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고? (남은 답변: 모든 질문 해결 완료),LoRA 와 QLoRA 의 차이,-0.003248555,0.0,0.003248554887250066
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고? (남은 답변: 모든 질문 해결 완료),Loss Function 관련 실무 경험,-0.007653844,0.0,0.007653844077140093
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고? (남은 답변: 모든 질문 해결 완료),Loss Function 예시,-0.0012775456,0.0,0.001277545583434403
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고? (남은 답변: 모든 질문 해결 완료),Loss Function 정의,-0.0043157875,0.0,0.004315787460654974
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고? (남은 답변: 모든 질문 해결 완료),MBTI / 좋아하는 아이돌,-0.0072009675,0.0,0.007200967520475388
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고? (남은 답변: 모든 질문 해결 완료),MSE Loss 설명,-0.0015056825,0.0,0.0015056824777275324
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고? (남은 답변: 모든 질문 해결 완료),MSE Loss 용도,-0.0056059393,0.0,0.00560593931004405
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고? (남은 답변: 모든 질문 해결 완료),"Multi-Class, Multi-Label 중 BCE 가 좋은 task",-0.0028420265,0.0,0.0028420265298336744
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고? (남은 답변: 모든 질문 해결 완료),Multi-Label 에서 CE + Softmax 적용 문제점,-0.003504571,0.0,0.0035045710392296314
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고? (남은 답변: 모든 질문 해결 완료),PEFT 방법 5가지,-0.003639145,0.0,0.003639145055785775
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고? (남은 답변: 모든 질문 해결 완료),거대 언어 모델 정의,-0.0038711291,0.0,0.0038711291272193193
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고? (남은 답변: 모든 질문 해결 완료),마지막 할 말,-0.005316408,0.0,0.005316407885402441
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고? (남은 답변: 모든 질문 해결 완료),면접 시작 인사,-0.0016665024,0.0,0.0016665024450048804
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고? (남은 답변: 모든 질문 해결 완료),면접 종료,-0.0043853763,0.0,0.00438537634909153
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고? (남은 답변: 모든 질문 해결 완료),"인공지능, 머신러닝, 딥러닝 차이",-0.0012158115,0.0,0.001215811469592154
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고? (남은 답변: 모든 질문 해결 완료),잠시 휴식,-0.0013030247,0.0,0.0013030247064307332
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고? (남은 답변: 모든 질문 해결 완료),확률 예측에서 MSE Loss 미 사용 이유,-0.0045962604,0.0,0.004596260376274586
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고 (남은 답변: 핵심 아이디어),BCE Loss 설명,0.9986182,1.0,0.0013818144798278809
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고 (남은 답변: 핵심 아이디어),LLM Fine-Tuning 의 PEFT,-0.0057820235,0.0,0.005782023537904024
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고 (남은 답변: 핵심 아이디어),LoRA,-0.009374011,0.0,0.009374011307954788
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고 (남은 답변: 핵심 아이디어),LoRA 와 QLoRA 의 차이,-0.0049096113,0.0,0.00490961130708456
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고 (남은 답변: 핵심 아이디어),Loss Function 관련 실무 경험,-0.003706748,0.0,0.0037067478988319635
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고 (남은 답변: 핵심 아이디어),Loss Function 예시,-0.0061637564,0.0,0.006163756363093853
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고 (남은 답변: 핵심 아이디어),Loss Function 정의,-0.0039310525,0.0,0.003931052517145872
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고 (남은 답변: 핵심 아이디어),MBTI / 좋아하는 아이돌,-0.0033838835,0.0,0.0033838835079222918
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고 (남은 답변: 핵심 아이디어),MSE Loss 설명,-0.00094184617,0.0,0.0009418461704626679
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고 (남은 답변: 핵심 아이디어),MSE Loss 용도,-0.007297763,0.0,0.007297763135284185
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고 (남은 답변: 핵심 아이디어),"Multi-Class, Multi-Label 중 BCE 가 좋은 task",-0.003939125,0.0,0.003939125221222639
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고 (남은 답변: 핵심 아이디어),Multi-Label 에서 CE + Softmax 적용 문제점,-0.0030845243,0.0,0.0030845243018120527
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고 (남은 답변: 핵심 아이디어),PEFT 방법 5가지,-0.0016961352,0.0,0.001696135150268674
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고 (남은 답변: 핵심 아이디어),거대 언어 모델 정의,-0.0025249517,0.0,0.0025249517057090998
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고 (남은 답변: 핵심 아이디어),마지막 할 말,-0.0045107733,0.0,0.004510773345828056
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고 (남은 답변: 핵심 아이디어),면접 시작 인사,-0.0051439553,0.0,0.00514395534992218
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고 (남은 답변: 핵심 아이디어),면접 종료,-0.0039011382,0.0,0.0039011382032185793
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고 (남은 답변: 핵심 아이디어),"인공지능, 머신러닝, 딥러닝 차이",-0.003503876,0.0,0.0035038760397583246
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고 (남은 답변: 핵심 아이디어),잠시 휴식,-0.0037267434,0.0,0.003726743394508958
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고 (남은 답변: 핵심 아이디어),확률 예측에서 MSE Loss 미 사용 이유,-0.0042731487,0.0,0.0042731487192213535
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거 (남은 답변: 모든 질문 해결 완료)",BCE Loss 설명,-0.004515135,0.0,0.004515135195106268
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거 (남은 답변: 모든 질문 해결 완료)",LLM Fine-Tuning 의 PEFT,-0.0015525754,0.0,0.0015525753842666745
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거 (남은 답변: 모든 질문 해결 완료)",LoRA,-0.002741301,0.0,0.002741300966590643
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거 (남은 답변: 모든 질문 해결 완료)",LoRA 와 QLoRA 의 차이,0.0010323562,0.0,0.001032356172800064
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거 (남은 답변: 모든 질문 해결 완료)",Loss Function 관련 실무 경험,-0.00038059993,0.0,0.00038059992948547006
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거 (남은 답변: 모든 질문 해결 완료)",Loss Function 예시,-0.008427286,0.0,0.008427285589277744
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거 (남은 답변: 모든 질문 해결 완료)",Loss Function 정의,-0.002504315,0.0,0.0025043149944394827
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거 (남은 답변: 모든 질문 해결 완료)",MBTI / 좋아하는 아이돌,-0.0008667887,0.0,0.0008667886722832918
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거 (남은 답변: 모든 질문 해결 완료)",MSE Loss 설명,0.0015677409,0.0,0.0015677409246563911
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거 (남은 답변: 모든 질문 해결 완료)",MSE Loss 용도,-0.0028002579,0.0,0.0028002578765153885
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거 (남은 답변: 모든 질문 해결 완료)","Multi-Class, Multi-Label 중 BCE 가 좋은 task",0.9982106,1.0,0.001789391040802002
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거 (남은 답변: 모든 질문 해결 완료)",Multi-Label 에서 CE + Softmax 적용 문제점,-0.0025739532,0.0,0.002573953242972493
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거 (남은 답변: 모든 질문 해결 완료)",PEFT 방법 5가지,-0.0005358173,0.0,0.000535817292984575
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거 (남은 답변: 모든 질문 해결 완료)",거대 언어 모델 정의,0.00071981054,0.0,0.0007198105449788272
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거 (남은 답변: 모든 질문 해결 완료)",마지막 할 말,-0.0030072848,0.0,0.0030072848312556744
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거 (남은 답변: 모든 질문 해결 완료)",면접 시작 인사,-0.0026906342,0.0,0.002690634224563837
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거 (남은 답변: 모든 질문 해결 완료)",면접 종료,-0.0016566536,0.0,0.001656653592363
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거 (남은 답변: 모든 질문 해결 완료)","인공지능, 머신러닝, 딥러닝 차이",-0.0013765768,0.0,0.0013765768380835652
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거 (남은 답변: 모든 질문 해결 완료)",잠시 휴식,-0.0012146818,0.0,0.0012146817753091455
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거 (남은 답변: 모든 질문 해결 완료)",확률 예측에서 MSE Loss 미 사용 이유,-0.0024136421,0.0,0.0024136421270668507
"BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야? (남은 답변: 핵심 아이디어, 수식)",BCE Loss 설명,0.99767125,1.0,0.0023287534713745117
"BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야? (남은 답변: 핵심 아이디어, 수식)",LLM Fine-Tuning 의 PEFT,-0.004496764,0.0,0.004496763925999403
"BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야? (남은 답변: 핵심 아이디어, 수식)",LoRA,-0.009587154,0.0,0.009587153792381287
"BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야? (남은 답변: 핵심 아이디어, 수식)",LoRA 와 QLoRA 의 차이,-0.0037765473,0.0,0.0037765472661703825
"BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야? (남은 답변: 핵심 아이디어, 수식)",Loss Function 관련 실무 경험,-0.0020620644,0.0,0.0020620643626898527
"BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야? (남은 답변: 핵심 아이디어, 수식)",Loss Function 예시,0.00136796,0.0,0.0013679600087925792
"BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야? (남은 답변: 핵심 아이디어, 수식)",Loss Function 정의,-0.005306394,0.0,0.005306393839418888
"BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야? (남은 답변: 핵심 아이디어, 수식)",MBTI / 좋아하는 아이돌,-0.0052072373,0.0,0.00520723732188344
"BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야? (남은 답변: 핵심 아이디어, 수식)",MSE Loss 설명,-0.0024562955,0.0,0.0024562955368310213
"BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야? (남은 답변: 핵심 아이디어, 수식)",MSE Loss 용도,-0.010240159,0.0,0.010240158997476101
"BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야? (남은 답변: 핵심 아이디어, 수식)","Multi-Class, Multi-Label 중 BCE 가 좋은 task",0.0023668706,0.0,0.002366870641708374
"BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야? (남은 답변: 핵심 아이디어, 수식)",Multi-Label 에서 CE + Softmax 적용 문제점,-0.002010517,0.0,0.002010517055168748
"BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야? (남은 답변: 핵심 아이디어, 수식)",PEFT 방법 5가지,-0.0021830283,0.0,0.0021830282639712095
"BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야? (남은 답변: 핵심 아이디어, 수식)",거대 언어 모델 정의,-0.002821455,0.0,0.0028214550111442804
"BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야? (남은 답변: 핵심 아이디어, 수식)",마지막 할 말,-0.0065851985,0.0,0.006585198454558849
"BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야? (남은 답변: 핵심 아이디어, 수식)",면접 시작 인사,-0.010538583,0.0,0.010538582690060139
"BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야? (남은 답변: 핵심 아이디어, 수식)",면접 종료,-0.008654952,0.0,0.008654952049255371
"BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야? (남은 답변: 핵심 아이디어, 수식)","인공지능, 머신러닝, 딥러닝 차이",-0.0052207704,0.0,0.0052207703702151775
"BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야? (남은 답변: 핵심 아이디어, 수식)",잠시 휴식,-0.0057509486,0.0,0.005750948563218117
"BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야? (남은 답변: 핵심 아이디어, 수식)",확률 예측에서 MSE Loss 미 사용 이유,-0.004227761,0.0,0.0042277611792087555
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고 (남은 답변: 수식)",BCE Loss 설명,0.9981898,1.0,0.0018101930618286133
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고 (남은 답변: 수식)",LLM Fine-Tuning 의 PEFT,-0.0058739237,0.0,0.005873923655599356
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고 (남은 답변: 수식)",LoRA,-0.00910597,0.0,0.009105970151722431
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고 (남은 답변: 수식)",LoRA 와 QLoRA 의 차이,-0.0056668962,0.0,0.0056668962351977825
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고 (남은 답변: 수식)",Loss Function 관련 실무 경험,-0.0043080198,0.0,0.004308019764721394
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고 (남은 답변: 수식)",Loss Function 예시,-0.00075167586,0.0,0.0007516758632846177
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고 (남은 답변: 수식)",Loss Function 정의,-0.007277628,0.0,0.007277627941220999
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고 (남은 답변: 수식)",MBTI / 좋아하는 아이돌,-0.005605305,0.0,0.005605305079370737
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고 (남은 답변: 수식)",MSE Loss 설명,-0.0026863893,0.0,0.0026863892562687397
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고 (남은 답변: 수식)",MSE Loss 용도,-0.0049134237,0.0,0.004913423676043749
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고 (남은 답변: 수식)","Multi-Class, Multi-Label 중 BCE 가 좋은 task",-0.0028958523,0.0,0.002895852318033576
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고 (남은 답변: 수식)",Multi-Label 에서 CE + Softmax 적용 문제점,-0.004130293,0.0,0.004130293149501085
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고 (남은 답변: 수식)",PEFT 방법 5가지,-0.0057626097,0.0,0.005762609653174877
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고 (남은 답변: 수식)",거대 언어 모델 정의,-0.0037966846,0.0,0.003796684555709362
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고 (남은 답변: 수식)",마지막 할 말,-0.00624425,0.0,0.0062442501075565815
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고 (남은 답변: 수식)",면접 시작 인사,-0.006314893,0.0,0.006314892787486315
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고 (남은 답변: 수식)",면접 종료,-0.0051969206,0.0,0.005196920596063137
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고 (남은 답변: 수식)","인공지능, 머신러닝, 딥러닝 차이",-0.0021680668,0.0,0.0021680667996406555
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고 (남은 답변: 수식)",잠시 휴식,-0.002224703,0.0,0.002224703086540103
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고 (남은 답변: 수식)",확률 예측에서 MSE Loss 미 사용 이유,-0.0034589027,0.0,0.003458902705460787
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')] (남은 답변: 핵심 아이디어)",BCE Loss 설명,0.99865633,1.0,0.0013436675071716309
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')] (남은 답변: 핵심 아이디어)",LLM Fine-Tuning 의 PEFT,-0.005858874,0.0,0.005858873948454857
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')] (남은 답변: 핵심 아이디어)",LoRA,-0.009597859,0.0,0.009597859345376492
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')] (남은 답변: 핵심 아이디어)",LoRA 와 QLoRA 의 차이,-0.005166157,0.0,0.005166157148778439
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')] (남은 답변: 핵심 아이디어)",Loss Function 관련 실무 경험,-0.004149855,0.0,0.004149855114519596
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')] (남은 답변: 핵심 아이디어)",Loss Function 예시,-0.00651675,0.0,0.00651674997061491
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')] (남은 답변: 핵심 아이디어)",Loss Function 정의,-0.004697131,0.0,0.004697130993008614
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')] (남은 답변: 핵심 아이디어)",MBTI / 좋아하는 아이돌,-0.005505739,0.0,0.005505738779902458
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')] (남은 답변: 핵심 아이디어)",MSE Loss 설명,0.00015212887,0.0,0.0001521288650110364
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')] (남은 답변: 핵심 아이디어)",MSE Loss 용도,-0.0065371417,0.0,0.006537141744047403
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')] (남은 답변: 핵심 아이디어)","Multi-Class, Multi-Label 중 BCE 가 좋은 task",-0.004226275,0.0,0.004226274788379669
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')] (남은 답변: 핵심 아이디어)",Multi-Label 에서 CE + Softmax 적용 문제점,-0.0037169391,0.0,0.003716939128935337
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')] (남은 답변: 핵심 아이디어)",PEFT 방법 5가지,-0.0018693133,0.0,0.001869313302449882
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')] (남은 답변: 핵심 아이디어)",거대 언어 모델 정의,-0.0031450938,0.0,0.003145093796774745
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')] (남은 답변: 핵심 아이디어)",마지막 할 말,-0.0034062564,0.0,0.0034062564373016357
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')] (남은 답변: 핵심 아이디어)",면접 시작 인사,-0.006018001,0.0,0.0060180011205375195
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')] (남은 답변: 핵심 아이디어)",면접 종료,-0.00378721,0.0,0.003787209978327155
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')] (남은 답변: 핵심 아이디어)","인공지능, 머신러닝, 딥러닝 차이",-0.0029084755,0.0,0.002908475464209914
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')] (남은 답변: 핵심 아이디어)",잠시 휴식,-0.0035947782,0.0,0.0035947782453149557
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')] (남은 답변: 핵심 아이디어)",확률 예측에서 MSE Loss 미 사용 이유,-0.0032786573,0.0,0.003278657328337431
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다 (남은 답변: 모든 질문 해결 완료)",BCE Loss 설명,-0.0039227824,0.0,0.003922782372683287
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다 (남은 답변: 모든 질문 해결 완료)",LLM Fine-Tuning 의 PEFT,-0.0017567886,0.0,0.0017567885806784034
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다 (남은 답변: 모든 질문 해결 완료)",LoRA,-0.0024238224,0.0,0.0024238224141299725
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다 (남은 답변: 모든 질문 해결 완료)",LoRA 와 QLoRA 의 차이,0.00047014272,0.0,0.00047014272422529757
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다 (남은 답변: 모든 질문 해결 완료)",Loss Function 관련 실무 경험,-0.0015118854,0.0,0.001511885435320437
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다 (남은 답변: 모든 질문 해결 완료)",Loss Function 예시,-0.008566508,0.0,0.008566508069634438
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다 (남은 답변: 모든 질문 해결 완료)",Loss Function 정의,-0.0027325638,0.0,0.002732563763856888
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다 (남은 답변: 모든 질문 해결 완료)",MBTI / 좋아하는 아이돌,-0.00030108533,0.0,0.00030108532519079745
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다 (남은 답변: 모든 질문 해결 완료)",MSE Loss 설명,0.0016682325,0.0,0.0016682324931025505
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다 (남은 답변: 모든 질문 해결 완료)",MSE Loss 용도,-0.0020242597,0.0,0.002024259651079774
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다 (남은 답변: 모든 질문 해결 완료)","Multi-Class, Multi-Label 중 BCE 가 좋은 task",0.99835896,1.0,0.0016410350799560547
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다 (남은 답변: 모든 질문 해결 완료)",Multi-Label 에서 CE + Softmax 적용 문제점,-0.0026034967,0.0,0.0026034966576844454
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다 (남은 답변: 모든 질문 해결 완료)",PEFT 방법 5가지,0.0002644743,0.0,0.00026447430718690157
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다 (남은 답변: 모든 질문 해결 완료)",거대 언어 모델 정의,0.00087514526,0.0,0.0008751452551223338
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다 (남은 답변: 모든 질문 해결 완료)",마지막 할 말,-0.0026590272,0.0,0.0026590272318571806
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다 (남은 답변: 모든 질문 해결 완료)",면접 시작 인사,-0.004282865,0.0,0.004282865207642317
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다 (남은 답변: 모든 질문 해결 완료)",면접 종료,-0.0015801704,0.0,0.0015801703557372093
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다 (남은 답변: 모든 질문 해결 완료)","인공지능, 머신러닝, 딥러닝 차이",-0.0016221768,0.0,0.0016221768455579877
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다 (남은 답변: 모든 질문 해결 완료)",잠시 휴식,-0.0007428949,0.0,0.0007428948883898556
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다 (남은 답변: 모든 질문 해결 완료)",확률 예측에서 MSE Loss 미 사용 이유,-0.0031272736,0.0,0.0031272736378014088
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거 (남은 답변: 핵심 아이디어),BCE Loss 설명,0.99901706,1.0,0.0009829401969909668
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거 (남은 답변: 핵심 아이디어),LLM Fine-Tuning 의 PEFT,-0.006043147,0.0,0.006043146830052137
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거 (남은 답변: 핵심 아이디어),LoRA,-0.009146627,0.0,0.00914662703871727
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거 (남은 답변: 핵심 아이디어),LoRA 와 QLoRA 의 차이,-0.004432562,0.0,0.004432561807334423
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거 (남은 답변: 핵심 아이디어),Loss Function 관련 실무 경험,-0.004694241,0.0,0.004694241099059582
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거 (남은 답변: 핵심 아이디어),Loss Function 예시,-0.007835802,0.0,0.00783580169081688
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거 (남은 답변: 핵심 아이디어),Loss Function 정의,-0.0060576773,0.0,0.006057677324861288
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거 (남은 답변: 핵심 아이디어),MBTI / 좋아하는 아이돌,-0.0045062397,0.0,0.004506239667534828
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거 (남은 답변: 핵심 아이디어),MSE Loss 설명,-0.00060724205,0.0,0.0006072420510463417
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거 (남은 답변: 핵심 아이디어),MSE Loss 용도,-0.0062688966,0.0,0.0062688966281712055
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거 (남은 답변: 핵심 아이디어),"Multi-Class, Multi-Label 중 BCE 가 좋은 task",-0.0044973413,0.0,0.004497341345995665
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거 (남은 답변: 핵심 아이디어),Multi-Label 에서 CE + Softmax 적용 문제점,-0.0024809884,0.0,0.0024809883907437325
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거 (남은 답변: 핵심 아이디어),PEFT 방법 5가지,-0.0011312853,0.0,0.0011312853312119842
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거 (남은 답변: 핵심 아이디어),거대 언어 모델 정의,-0.0021288265,0.0,0.002128826454281807
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거 (남은 답변: 핵심 아이디어),마지막 할 말,-0.0038203336,0.0,0.003820333629846573
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거 (남은 답변: 핵심 아이디어),면접 시작 인사,-0.0070340936,0.0,0.0070340936072170734
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거 (남은 답변: 핵심 아이디어),면접 종료,-0.0030235113,0.0,0.0030235112644732
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거 (남은 답변: 핵심 아이디어),"인공지능, 머신러닝, 딥러닝 차이",-0.0015654404,0.0,0.001565440441481769
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거 (남은 답변: 핵심 아이디어),잠시 휴식,-0.0047085923,0.0,0.004708592314273119
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거 (남은 답변: 핵심 아이디어),확률 예측에서 MSE Loss 미 사용 이유,-0.002915871,0.0,0.0029158710967749357
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지 (남은 답변: 모든 질문 해결 완료)",BCE Loss 설명,-0.0039699255,0.0,0.003969925455749035
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지 (남은 답변: 모든 질문 해결 완료)",LLM Fine-Tuning 의 PEFT,-0.001339896,0.0,0.0013398959999904037
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지 (남은 답변: 모든 질문 해결 완료)",LoRA,-0.0032185407,0.0,0.0032185406889766455
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지 (남은 답변: 모든 질문 해결 완료)",LoRA 와 QLoRA 의 차이,0.00070227426,0.0,0.000702274264767766
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지 (남은 답변: 모든 질문 해결 완료)",Loss Function 관련 실무 경험,-0.0008430183,0.0,0.0008430182933807373
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지 (남은 답변: 모든 질문 해결 완료)",Loss Function 예시,-0.009281271,0.0,0.009281271137297153
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지 (남은 답변: 모든 질문 해결 완료)",Loss Function 정의,-0.0033877033,0.0,0.003387703327462077
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지 (남은 답변: 모든 질문 해결 완료)",MBTI / 좋아하는 아이돌,-0.0007019929,0.0,0.0007019928889349103
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지 (남은 답변: 모든 질문 해결 완료)",MSE Loss 설명,0.0017040288,0.0,0.001704028807580471
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지 (남은 답변: 모든 질문 해결 완료)",MSE Loss 용도,-0.0025451828,0.0,0.002545182825997472
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지 (남은 답변: 모든 질문 해결 완료)","Multi-Class, Multi-Label 중 BCE 가 좋은 task",0.99812883,1.0,0.0018711686134338379
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지 (남은 답변: 모든 질문 해결 완료)",Multi-Label 에서 CE + Softmax 적용 문제점,-0.0027611607,0.0,0.002761160722002387
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지 (남은 답변: 모든 질문 해결 완료)",PEFT 방법 5가지,0.00033366904,0.0,0.0003336690424475819
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지 (남은 답변: 모든 질문 해결 완료)",거대 언어 모델 정의,0.0011750308,0.0,0.0011750308331102133
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지 (남은 답변: 모든 질문 해결 완료)",마지막 할 말,-0.0037146737,0.0,0.003714673686772585
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지 (남은 답변: 모든 질문 해결 완료)",면접 시작 인사,-0.0019349188,0.0,0.0019349188078194857
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지 (남은 답변: 모든 질문 해결 완료)",면접 종료,-0.0019164826,0.0,0.0019164825789630413
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지 (남은 답변: 모든 질문 해결 완료)","인공지능, 머신러닝, 딥러닝 차이",-0.0023199231,0.0,0.002319923136383295
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지 (남은 답변: 모든 질문 해결 완료)",잠시 휴식,-0.0031776486,0.0,0.0031776486430317163
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지 (남은 답변: 모든 질문 해결 완료)",확률 예측에서 MSE Loss 미 사용 이유,-0.0028782724,0.0,0.0028782724402844906
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야? (남은 답변: BCE 가 좋은 task, BCE 가 좋은 이유)",BCE Loss 설명,-0.0045968033,0.0,0.0045968033373355865
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야? (남은 답변: BCE 가 좋은 task, BCE 가 좋은 이유)",LLM Fine-Tuning 의 PEFT,-0.0023119266,0.0,0.002311926567927003
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야? (남은 답변: BCE 가 좋은 task, BCE 가 좋은 이유)",LoRA,-0.00289789,0.0,0.0028978900518268347
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야? (남은 답변: BCE 가 좋은 task, BCE 가 좋은 이유)",LoRA 와 QLoRA 의 차이,0.0005777911,0.0,0.000577791128307581
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야? (남은 답변: BCE 가 좋은 task, BCE 가 좋은 이유)",Loss Function 관련 실무 경험,-0.0030369223,0.0,0.0030369223095476627
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야? (남은 답변: BCE 가 좋은 task, BCE 가 좋은 이유)",Loss Function 예시,-0.0055029737,0.0,0.005502973683178425
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야? (남은 답변: BCE 가 좋은 task, BCE 가 좋은 이유)",Loss Function 정의,-0.000516475,0.0,0.0005164750036783516
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야? (남은 답변: BCE 가 좋은 task, BCE 가 좋은 이유)",MBTI / 좋아하는 아이돌,-0.0014881049,0.0,0.0014881049282848835
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야? (남은 답변: BCE 가 좋은 task, BCE 가 좋은 이유)",MSE Loss 설명,-0.0010988492,0.0,0.0010988492285832763
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야? (남은 답변: BCE 가 좋은 task, BCE 가 좋은 이유)",MSE Loss 용도,-0.0011196279,0.0,0.0011196278501302004
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야? (남은 답변: BCE 가 좋은 task, BCE 가 좋은 이유)","Multi-Class, Multi-Label 중 BCE 가 좋은 task",0.99940026,1.0,0.0005997419357299805
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야? (남은 답변: BCE 가 좋은 task, BCE 가 좋은 이유)",Multi-Label 에서 CE + Softmax 적용 문제점,-0.0039894073,0.0,0.003989407327026129
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야? (남은 답변: BCE 가 좋은 task, BCE 가 좋은 이유)",PEFT 방법 5가지,-0.001014484,0.0,0.0010144839761778712
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야? (남은 답변: BCE 가 좋은 task, BCE 가 좋은 이유)",거대 언어 모델 정의,-0.0004966144,0.0,0.0004966143751516938
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야? (남은 답변: BCE 가 좋은 task, BCE 가 좋은 이유)",마지막 할 말,-0.0007582999,0.0,0.0007582998950965703
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야? (남은 답변: BCE 가 좋은 task, BCE 가 좋은 이유)",면접 시작 인사,-0.0018678538,0.0,0.0018678538035601377
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야? (남은 답변: BCE 가 좋은 task, BCE 가 좋은 이유)",면접 종료,-0.0040895026,0.0,0.004089502617716789
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야? (남은 답변: BCE 가 좋은 task, BCE 가 좋은 이유)","인공지능, 머신러닝, 딥러닝 차이",-0.0015150881,0.0,0.0015150881372392178
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야? (남은 답변: BCE 가 좋은 task, BCE 가 좋은 이유)",잠시 휴식,-0.00040215647,0.0,0.0004021564673166722
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야? (남은 답변: BCE 가 좋은 task, BCE 가 좋은 이유)",확률 예측에서 MSE Loss 미 사용 이유,-0.0028174818,0.0,0.002817481756210327
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!! (남은 답변: BCE 가 좋은 task, BCE 가 좋은 이유)",BCE Loss 설명,-0.0039120605,0.0,0.003912060521543026
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!! (남은 답변: BCE 가 좋은 task, BCE 가 좋은 이유)",LLM Fine-Tuning 의 PEFT,-0.0021669718,0.0,0.0021669717971235514
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!! (남은 답변: BCE 가 좋은 task, BCE 가 좋은 이유)",LoRA,-0.0029771668,0.0,0.0029771667905151844
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!! (남은 답변: BCE 가 좋은 task, BCE 가 좋은 이유)",LoRA 와 QLoRA 의 차이,0.0004666314,0.0,0.00046663140528835356
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!! (남은 답변: BCE 가 좋은 task, BCE 가 좋은 이유)",Loss Function 관련 실무 경험,-0.0027991426,0.0,0.0027991426177322865
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!! (남은 답변: BCE 가 좋은 task, BCE 가 좋은 이유)",Loss Function 예시,-0.005710611,0.0,0.005710611119866371
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!! (남은 답변: BCE 가 좋은 task, BCE 가 좋은 이유)",Loss Function 정의,-0.00041019425,0.0,0.0004101942467968911
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!! (남은 답변: BCE 가 좋은 task, BCE 가 좋은 이유)",MBTI / 좋아하는 아이돌,-0.0011036874,0.0,0.0011036874493584037
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!! (남은 답변: BCE 가 좋은 task, BCE 가 좋은 이유)",MSE Loss 설명,-0.0008208923,0.0,0.0008208922808989882
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!! (남은 답변: BCE 가 좋은 task, BCE 가 좋은 이유)",MSE Loss 용도,-0.0011947616,0.0,0.0011947616003453732
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!! (남은 답변: BCE 가 좋은 task, BCE 가 좋은 이유)","Multi-Class, Multi-Label 중 BCE 가 좋은 task",0.9995677,1.0,0.0004323124885559082
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!! (남은 답변: BCE 가 좋은 task, BCE 가 좋은 이유)",Multi-Label 에서 CE + Softmax 적용 문제점,-0.0037486288,0.0,0.0037486287765204906
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!! (남은 답변: BCE 가 좋은 task, BCE 가 좋은 이유)",PEFT 방법 5가지,-0.0011633503,0.0,0.0011633503017947078
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!! (남은 답변: BCE 가 좋은 task, BCE 가 좋은 이유)",거대 언어 모델 정의,0.00010585122,0.0,0.00010585122072370723
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!! (남은 답변: BCE 가 좋은 task, BCE 가 좋은 이유)",마지막 할 말,-0.0008773428,0.0,0.0008773428271524608
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!! (남은 답변: BCE 가 좋은 task, BCE 가 좋은 이유)",면접 시작 인사,-0.0013766722,0.0,0.0013766721822321415
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!! (남은 답변: BCE 가 좋은 task, BCE 가 좋은 이유)",면접 종료,-0.0035949445,0.0,0.0035949444863945246
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!! (남은 답변: BCE 가 좋은 task, BCE 가 좋은 이유)","인공지능, 머신러닝, 딥러닝 차이",-0.0012756065,0.0,0.0012756064534187317
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!! (남은 답변: BCE 가 좋은 task, BCE 가 좋은 이유)",잠시 휴식,-0.00028570028,0.0,0.00028570028371177614
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!! (남은 답변: BCE 가 좋은 task, BCE 가 좋은 이유)",확률 예측에서 MSE Loss 미 사용 이유,-0.0032965112,0.0,0.003296511247754097
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어 (남은 답변: BCE 가 좋은 이유)",BCE Loss 설명,-0.003273519,0.0,0.0032735189888626337
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어 (남은 답변: BCE 가 좋은 이유)",LLM Fine-Tuning 의 PEFT,-0.0015598472,0.0,0.0015598471509292722
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어 (남은 답변: BCE 가 좋은 이유)",LoRA,-0.0030142686,0.0,0.003014268586412072
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어 (남은 답변: BCE 가 좋은 이유)",LoRA 와 QLoRA 의 차이,-0.0011814607,0.0,0.0011814606841653585
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어 (남은 답변: BCE 가 좋은 이유)",Loss Function 관련 실무 경험,-0.0027560028,0.0,0.002756002824753523
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어 (남은 답변: BCE 가 좋은 이유)",Loss Function 예시,-0.0043642465,0.0,0.004364246502518654
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어 (남은 답변: BCE 가 좋은 이유)",Loss Function 정의,0.00011866265,0.0,0.00011866264685522765
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어 (남은 답변: BCE 가 좋은 이유)",MBTI / 좋아하는 아이돌,-0.0010459512,0.0,0.0010459511540830135
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어 (남은 답변: BCE 가 좋은 이유)",MSE Loss 설명,-0.0006994728,0.0,0.0006994727882556617
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어 (남은 답변: BCE 가 좋은 이유)",MSE Loss 용도,-0.0012690533,0.0,0.0012690533185377717
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어 (남은 답변: BCE 가 좋은 이유)","Multi-Class, Multi-Label 중 BCE 가 좋은 task",0.999185,1.0,0.000814974308013916
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어 (남은 답변: BCE 가 좋은 이유)",Multi-Label 에서 CE + Softmax 적용 문제점,-0.0042371517,0.0,0.004237151704728603
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어 (남은 답변: BCE 가 좋은 이유)",PEFT 방법 5가지,-0.000332846,0.0,0.00033284598612226546
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어 (남은 답변: BCE 가 좋은 이유)",거대 언어 모델 정의,-0.001424651,0.0,0.0014246510108932853
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어 (남은 답변: BCE 가 좋은 이유)",마지막 할 말,-0.0008683402,0.0,0.0008683401974849403
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어 (남은 답변: BCE 가 좋은 이유)",면접 시작 인사,-0.0008382982,0.0,0.0008382981759496033
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어 (남은 답변: BCE 가 좋은 이유)",면접 종료,-0.003762132,0.0,0.0037621320225298405
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어 (남은 답변: BCE 가 좋은 이유)","인공지능, 머신러닝, 딥러닝 차이",-0.0010427955,0.0,0.0010427954839542508
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어 (남은 답변: BCE 가 좋은 이유)",잠시 휴식,-0.00018443704,0.0,0.0001844370417529717
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어 (남은 답변: BCE 가 좋은 이유)",확률 예측에서 MSE Loss 미 사용 이유,-0.002751283,0.0,0.0027512831147760153
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아 (남은 답변: 모든 질문 해결 완료)",BCE Loss 설명,-0.001936405,0.0,0.0019364049658179283
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아 (남은 답변: 모든 질문 해결 완료)",LLM Fine-Tuning 의 PEFT,-0.002233287,0.0,0.002233287086710334
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아 (남은 답변: 모든 질문 해결 완료)",LoRA,-0.0022791328,0.0,0.0022791328374296427
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아 (남은 답변: 모든 질문 해결 완료)",LoRA 와 QLoRA 의 차이,0.0007656317,0.0,0.0007656316738575697
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아 (남은 답변: 모든 질문 해결 완료)",Loss Function 관련 실무 경험,-0.0042563397,0.0,0.004256339743733406
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아 (남은 답변: 모든 질문 해결 완료)",Loss Function 예시,-0.0018155844,0.0,0.0018155843717977405
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아 (남은 답변: 모든 질문 해결 완료)",Loss Function 정의,-0.002622906,0.0,0.0026229058858007193
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아 (남은 답변: 모든 질문 해결 완료)",MBTI / 좋아하는 아이돌,-0.0020495905,0.0,0.0020495904609560966
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아 (남은 답변: 모든 질문 해결 완료)",MSE Loss 설명,-0.00252665,0.0,0.002526649972423911
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아 (남은 답변: 모든 질문 해결 완료)",MSE Loss 용도,-0.0048246006,0.0,0.004824600648134947
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아 (남은 답변: 모든 질문 해결 완료)","Multi-Class, Multi-Label 중 BCE 가 좋은 task",-0.0011895089,0.0,0.0011895089410245419
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아 (남은 답변: 모든 질문 해결 완료)",Multi-Label 에서 CE + Softmax 적용 문제점,0.9965943,1.0,0.0034056901931762695
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아 (남은 답변: 모든 질문 해결 완료)",PEFT 방법 5가지,0.0014817545,0.0,0.0014817544724792242
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아 (남은 답변: 모든 질문 해결 완료)",거대 언어 모델 정의,-0.0047359634,0.0,0.004735963419079781
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아 (남은 답변: 모든 질문 해결 완료)",마지막 할 말,-0.008694284,0.0,0.008694283664226532
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아 (남은 답변: 모든 질문 해결 완료)",면접 시작 인사,-0.0027051815,0.0,0.002705181483179331
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아 (남은 답변: 모든 질문 해결 완료)",면접 종료,-0.000757351,0.0,0.0007573509938083589
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아 (남은 답변: 모든 질문 해결 완료)","인공지능, 머신러닝, 딥러닝 차이",-0.0037529834,0.0,0.003752983408048749
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아 (남은 답변: 모든 질문 해결 완료)",잠시 휴식,-0.002315942,0.0,0.0023159419652074575
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아 (남은 답변: 모든 질문 해결 완료)",확률 예측에서 MSE Loss 미 사용 이유,-0.0027749739,0.0,0.00277497386559844
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지 (남은 답변: 모든 질문 해결 완료),BCE Loss 설명,-0.009472773,0.0,0.009472773410379887
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지 (남은 답변: 모든 질문 해결 완료),LLM Fine-Tuning 의 PEFT,-0.000632868,0.0,0.0006328679737634957
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지 (남은 답변: 모든 질문 해결 완료),LoRA,-0.0026996788,0.0,0.0026996787637472153
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지 (남은 답변: 모든 질문 해결 완료),LoRA 와 QLoRA 의 차이,0.0030600075,0.0,0.003060007467865944
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지 (남은 답변: 모든 질문 해결 완료),Loss Function 관련 실무 경험,0.99604785,1.0,0.003952145576477051
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지 (남은 답변: 모든 질문 해결 완료),Loss Function 예시,-0.0009169989,0.0,0.0009169988916255534
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지 (남은 답변: 모든 질문 해결 완료),Loss Function 정의,-0.006613107,0.0,0.006613106932491064
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지 (남은 답변: 모든 질문 해결 완료),MBTI / 좋아하는 아이돌,-0.0032362419,0.0,0.0032362418714910746
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지 (남은 답변: 모든 질문 해결 완료),MSE Loss 설명,-0.0033113186,0.0,0.003311318578198552
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지 (남은 답변: 모든 질문 해결 완료),MSE Loss 용도,0.00079404336,0.0,0.0007940433570183814
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지 (남은 답변: 모든 질문 해결 완료),"Multi-Class, Multi-Label 중 BCE 가 좋은 task",-0.0028463383,0.0,0.0028463383205235004
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지 (남은 답변: 모든 질문 해결 완료),Multi-Label 에서 CE + Softmax 적용 문제점,0.0033304647,0.0,0.003330464707687497
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지 (남은 답변: 모든 질문 해결 완료),PEFT 방법 5가지,-0.004929852,0.0,0.004929852206259966
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지 (남은 답변: 모든 질문 해결 완료),거대 언어 모델 정의,-0.0009121968,0.0,0.0009121968178078532
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지 (남은 답변: 모든 질문 해결 완료),마지막 할 말,-0.0008138952,0.0,0.0008138951961882412
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지 (남은 답변: 모든 질문 해결 완료),면접 시작 인사,-0.003352125,0.0,0.0033521249424666166
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지 (남은 답변: 모든 질문 해결 완료),면접 종료,-0.0030013141,0.0,0.0030013141222298145
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지 (남은 답변: 모든 질문 해결 완료),"인공지능, 머신러닝, 딥러닝 차이",-0.0039739916,0.0,0.003973991610109806
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지 (남은 답변: 모든 질문 해결 완료),잠시 휴식,-0.0014926712,0.0,0.0014926712028682232
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지 (남은 답변: 모든 질문 해결 완료),확률 예측에서 MSE Loss 미 사용 이유,-0.004577495,0.0,0.0045774951577186584
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나 (남은 답변: Multi-Label 에서 CE + Softmax 적용 문제점),BCE Loss 설명,-0.0030087302,0.0,0.0030087302438914776
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나 (남은 답변: Multi-Label 에서 CE + Softmax 적용 문제점),LLM Fine-Tuning 의 PEFT,-0.0026436036,0.0,0.0026436035986989737
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나 (남은 답변: Multi-Label 에서 CE + Softmax 적용 문제점),LoRA,-0.001653449,0.0,0.0016534490277990699
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나 (남은 답변: Multi-Label 에서 CE + Softmax 적용 문제점),LoRA 와 QLoRA 의 차이,-0.0024676633,0.0,0.002467663260176778
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나 (남은 답변: Multi-Label 에서 CE + Softmax 적용 문제점),Loss Function 관련 실무 경험,-0.0047090226,0.0,0.004709022585302591
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나 (남은 답변: Multi-Label 에서 CE + Softmax 적용 문제점),Loss Function 예시,-0.002759081,0.0,0.0027590810786932707
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나 (남은 답변: Multi-Label 에서 CE + Softmax 적용 문제점),Loss Function 정의,-0.0020049233,0.0,0.0020049232989549637
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나 (남은 답변: Multi-Label 에서 CE + Softmax 적용 문제점),MBTI / 좋아하는 아이돌,0.0003411349,0.0,0.0003411349025554955
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나 (남은 답변: Multi-Label 에서 CE + Softmax 적용 문제점),MSE Loss 설명,-0.002647131,0.0,0.0026471309829503298
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나 (남은 답변: Multi-Label 에서 CE + Softmax 적용 문제점),MSE Loss 용도,-0.0038656138,0.0,0.0038656138349324465
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나 (남은 답변: Multi-Label 에서 CE + Softmax 적용 문제점),"Multi-Class, Multi-Label 중 BCE 가 좋은 task",-0.0030268214,0.0,0.003026821417734027
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나 (남은 답변: Multi-Label 에서 CE + Softmax 적용 문제점),Multi-Label 에서 CE + Softmax 적용 문제점,0.998825,1.0,0.0011749863624572754
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나 (남은 답변: Multi-Label 에서 CE + Softmax 적용 문제점),PEFT 방법 5가지,-0.002015115,0.0,0.0020151149947196245
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나 (남은 답변: Multi-Label 에서 CE + Softmax 적용 문제점),거대 언어 모델 정의,-0.0032160087,0.0,0.0032160086557269096
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나 (남은 답변: Multi-Label 에서 CE + Softmax 적용 문제점),마지막 할 말,-0.005088942,0.0,0.005088942125439644
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나 (남은 답변: Multi-Label 에서 CE + Softmax 적용 문제점),면접 시작 인사,-0.0041113594,0.0,0.004111359361559153
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나 (남은 답변: Multi-Label 에서 CE + Softmax 적용 문제점),면접 종료,-0.00233523,0.0,0.0023352298885583878
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나 (남은 답변: Multi-Label 에서 CE + Softmax 적용 문제점),"인공지능, 머신러닝, 딥러닝 차이",-0.0028146787,0.0,0.002814678708091378
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나 (남은 답변: Multi-Label 에서 CE + Softmax 적용 문제점),잠시 휴식,-0.0007376658,0.0,0.0007376658031716943
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나 (남은 답변: Multi-Label 에서 CE + Softmax 적용 문제점),확률 예측에서 MSE Loss 미 사용 이유,-0.0021947927,0.0,0.0021947927307337523
Loss Function 관련 실무 경험 -> 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어 (남은 답변: 상세 경험),BCE Loss 설명,-0.0053673987,0.0,0.005367398727685213
Loss Function 관련 실무 경험 -> 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어 (남은 답변: 상세 경험),LLM Fine-Tuning 의 PEFT,-0.0032500701,0.0,0.003250070149078965
Loss Function 관련 실무 경험 -> 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어 (남은 답변: 상세 경험),LoRA,-0.0010895644,0.0,0.0010895644081756473
Loss Function 관련 실무 경험 -> 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어 (남은 답변: 상세 경험),LoRA 와 QLoRA 의 차이,-0.0034728006,0.0,0.00347280059941113
Loss Function 관련 실무 경험 -> 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어 (남은 답변: 상세 경험),Loss Function 관련 실무 경험,0.99882454,1.0,0.0011754631996154785
Loss Function 관련 실무 경험 -> 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어 (남은 답변: 상세 경험),Loss Function 예시,-0.0031143741,0.0,0.0031143741216510534
Loss Function 관련 실무 경험 -> 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어 (남은 답변: 상세 경험),Loss Function 정의,-0.003402243,0.0,0.0034022429026663303
Loss Function 관련 실무 경험 -> 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어 (남은 답변: 상세 경험),MBTI / 좋아하는 아이돌,-0.0028864425,0.0,0.002886442467570305
Loss Function 관련 실무 경험 -> 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어 (남은 답변: 상세 경험),MSE Loss 설명,-0.0028612877,0.0,0.0028612876776605844
Loss Function 관련 실무 경험 -> 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어 (남은 답변: 상세 경험),MSE Loss 용도,-0.004503715,0.0,0.004503714852035046
Loss Function 관련 실무 경험 -> 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어 (남은 답변: 상세 경험),"Multi-Class, Multi-Label 중 BCE 가 좋은 task",-0.0025576206,0.0,0.0025576206389814615
Loss Function 관련 실무 경험 -> 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어 (남은 답변: 상세 경험),Multi-Label 에서 CE + Softmax 적용 문제점,-0.0029724042,0.0,0.0029724042396992445
Loss Function 관련 실무 경험 -> 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어 (남은 답변: 상세 경험),PEFT 방법 5가지,-0.0029800006,0.0,0.0029800005722790956
Loss Function 관련 실무 경험 -> 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어 (남은 답변: 상세 경험),거대 언어 모델 정의,-0.003009658,0.0,0.0030096580740064383
Loss Function 관련 실무 경험 -> 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어 (남은 답변: 상세 경험),마지막 할 말,-0.00063852704,0.0,0.0006385270389728248
Loss Function 관련 실무 경험 -> 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어 (남은 답변: 상세 경험),면접 시작 인사,-0.0019408775,0.0,0.0019408775260671973
Loss Function 관련 실무 경험 -> 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어 (남은 답변: 상세 경험),면접 종료,-0.0022629162,0.0,0.0022629161830991507
Loss Function 관련 실무 경험 -> 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어 (남은 답변: 상세 경험),"인공지능, 머신러닝, 딥러닝 차이",-0.00404233,0.0,0.004042330197989941
Loss Function 관련 실무 경험 -> 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어 (남은 답변: 상세 경험),잠시 휴식,-0.0029724503,0.0,0.002972450340166688
Loss Function 관련 실무 경험 -> 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어 (남은 답변: 상세 경험),확률 예측에서 MSE Loss 미 사용 이유,-0.0024842315,0.0,0.002484231488779187
Loss Function 관련 실무 경험 -> 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지? (남은 답변: 모든 질문 해결 완료),BCE Loss 설명,-0.0046467795,0.0,0.004646779503673315
Loss Function 관련 실무 경험 -> 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지? (남은 답변: 모든 질문 해결 완료),LLM Fine-Tuning 의 PEFT,-0.00075671123,0.0,0.000756711233407259
Loss Function 관련 실무 경험 -> 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지? (남은 답변: 모든 질문 해결 완료),LoRA,0.0001763946,0.0,0.00017639460565987974
Loss Function 관련 실무 경험 -> 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지? (남은 답변: 모든 질문 해결 완료),LoRA 와 QLoRA 의 차이,-0.001988162,0.0,0.0019881620537489653
Loss Function 관련 실무 경험 -> 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지? (남은 답변: 모든 질문 해결 완료),Loss Function 관련 실무 경험,-0.0014309735,0.0,0.001430973527021706
Loss Function 관련 실무 경험 -> 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지? (남은 답변: 모든 질문 해결 완료),Loss Function 예시,-0.005580002,0.0,0.005580001976341009
Loss Function 관련 실무 경험 -> 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지? (남은 답변: 모든 질문 해결 완료),Loss Function 정의,-0.00026026584,0.0,0.00026026583509519696
Loss Function 관련 실무 경험 -> 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지? (남은 답변: 모든 질문 해결 완료),MBTI / 좋아하는 아이돌,0.99855554,1.0,0.0014444589614868164
Loss Function 관련 실무 경험 -> 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지? (남은 답변: 모든 질문 해결 완료),MSE Loss 설명,-0.0023088327,0.0,0.0023088327143341303
Loss Function 관련 실무 경험 -> 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지? (남은 답변: 모든 질문 해결 완료),MSE Loss 용도,-0.0019657474,0.0,0.0019657474476844072
Loss Function 관련 실무 경험 -> 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지? (남은 답변: 모든 질문 해결 완료),"Multi-Class, Multi-Label 중 BCE 가 좋은 task",-0.00087860605,0.0,0.0008786060498096049
Loss Function 관련 실무 경험 -> 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지? (남은 답변: 모든 질문 해결 완료),Multi-Label 에서 CE + Softmax 적용 문제점,-0.0012724198,0.0,0.001272419816814363
Loss Function 관련 실무 경험 -> 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지? (남은 답변: 모든 질문 해결 완료),PEFT 방법 5가지,-0.005767406,0.0,0.005767405964434147
Loss Function 관련 실무 경험 -> 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지? (남은 답변: 모든 질문 해결 완료),거대 언어 모델 정의,-0.0041605053,0.0,0.004160505253821611
Loss Function 관련 실무 경험 -> 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지? (남은 답변: 모든 질문 해결 완료),마지막 할 말,-0.0023120025,0.0,0.002312002470716834
Loss Function 관련 실무 경험 -> 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지? (남은 답변: 모든 질문 해결 완료),면접 시작 인사,-0.0017047613,0.0,0.0017047612927854061
Loss Function 관련 실무 경험 -> 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지? (남은 답변: 모든 질문 해결 완료),면접 종료,-0.0030529897,0.0,0.0030529897194355726
Loss Function 관련 실무 경험 -> 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지? (남은 답변: 모든 질문 해결 완료),"인공지능, 머신러닝, 딥러닝 차이",-0.0012899986,0.0,0.001289998646825552
Loss Function 관련 실무 경험 -> 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지? (남은 답변: 모든 질문 해결 완료),잠시 휴식,-0.0033055828,0.0,0.003305582795292139
Loss Function 관련 실무 경험 -> 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지? (남은 답변: 모든 질문 해결 완료),확률 예측에서 MSE Loss 미 사용 이유,-0.0012828925,0.0,0.001282892539165914
Loss Function 관련 실무 경험 -> 근데 기억 진짜 안나는데 어떡하지? (남은 답변: 상세 경험),BCE Loss 설명,-0.0029825845,0.0,0.0029825845267623663
Loss Function 관련 실무 경험 -> 근데 기억 진짜 안나는데 어떡하지? (남은 답변: 상세 경험),LLM Fine-Tuning 의 PEFT,-0.0021769772,0.0,0.002176977228373289
Loss Function 관련 실무 경험 -> 근데 기억 진짜 안나는데 어떡하지? (남은 답변: 상세 경험),LoRA,-0.003784878,0.0,0.003784877946600318
Loss Function 관련 실무 경험 -> 근데 기억 진짜 안나는데 어떡하지? (남은 답변: 상세 경험),LoRA 와 QLoRA 의 차이,-0.00034529588,0.0,0.0003452958771958947
Loss Function 관련 실무 경험 -> 근데 기억 진짜 안나는데 어떡하지? (남은 답변: 상세 경험),Loss Function 관련 실무 경험,0.00061732583,0.0,0.0006173258298076689
Loss Function 관련 실무 경험 -> 근데 기억 진짜 안나는데 어떡하지? (남은 답변: 상세 경험),Loss Function 예시,0.00028566105,0.0,0.00028566105174832046
Loss Function 관련 실무 경험 -> 근데 기억 진짜 안나는데 어떡하지? (남은 답변: 상세 경험),Loss Function 정의,-0.0024340309,0.0,0.0024340308737009764
Loss Function 관련 실무 경험 -> 근데 기억 진짜 안나는데 어떡하지? (남은 답변: 상세 경험),MBTI / 좋아하는 아이돌,-0.0033945746,0.0,0.00339457462541759
Loss Function 관련 실무 경험 -> 근데 기억 진짜 안나는데 어떡하지? (남은 답변: 상세 경험),MSE Loss 설명,-0.004166646,0.0,0.0041666459292173386
Loss Function 관련 실무 경험 -> 근데 기억 진짜 안나는데 어떡하지? (남은 답변: 상세 경험),MSE Loss 용도,-0.0045279167,0.0,0.004527916666120291
Loss Function 관련 실무 경험 -> 근데 기억 진짜 안나는데 어떡하지? (남은 답변: 상세 경험),"Multi-Class, Multi-Label 중 BCE 가 좋은 task",-0.00047551078,0.0,0.0004755107802338898
Loss Function 관련 실무 경험 -> 근데 기억 진짜 안나는데 어떡하지? (남은 답변: 상세 경험),Multi-Label 에서 CE + Softmax 적용 문제점,-0.003686702,0.0,0.00368670211173594
Loss Function 관련 실무 경험 -> 근데 기억 진짜 안나는데 어떡하지? (남은 답변: 상세 경험),PEFT 방법 5가지,-0.0010836825,0.0,0.0010836825240403414
Loss Function 관련 실무 경험 -> 근데 기억 진짜 안나는데 어떡하지? (남은 답변: 상세 경험),거대 언어 모델 정의,0.00035348625,0.0,0.00035348624805919826
Loss Function 관련 실무 경험 -> 근데 기억 진짜 안나는데 어떡하지? (남은 답변: 상세 경험),마지막 할 말,0.0035841768,0.0,0.003584176767617464
Loss Function 관련 실무 경험 -> 근데 기억 진짜 안나는데 어떡하지? (남은 답변: 상세 경험),면접 시작 인사,-0.004971115,0.0,0.004971114918589592
Loss Function 관련 실무 경험 -> 근데 기억 진짜 안나는데 어떡하지? (남은 답변: 상세 경험),면접 종료,-0.00190026,0.0,0.0019002599874511361
Loss Function 관련 실무 경험 -> 근데 기억 진짜 안나는데 어떡하지? (남은 답변: 상세 경험),"인공지능, 머신러닝, 딥러닝 차이",-0.0044924566,0.0,0.004492456559091806
Loss Function 관련 실무 경험 -> 근데 기억 진짜 안나는데 어떡하지? (남은 답변: 상세 경험),잠시 휴식,0.9952007,1.0,0.004799306392669678
Loss Function 관련 실무 경험 -> 근데 기억 진짜 안나는데 어떡하지? (남은 답변: 상세 경험),확률 예측에서 MSE Loss 미 사용 이유,-0.0034026941,0.0,0.0034026941284537315
Loss Function 관련 실무 경험 -> 나 아직 실무 경험 없는데 ㅠㅠ (남은 답변: 모든 질문 해결 완료),BCE Loss 설명,-0.004551144,0.0,0.004551143851131201
Loss Function 관련 실무 경험 -> 나 아직 실무 경험 없는데 ㅠㅠ (남은 답변: 모든 질문 해결 완료),LLM Fine-Tuning 의 PEFT,-0.0006834839,0.0,0.0006834839005023241
Loss Function 관련 실무 경험 -> 나 아직 실무 경험 없는데 ㅠㅠ (남은 답변: 모든 질문 해결 완료),LoRA,0.00025597692,0.0,0.00025597692001610994
Loss Function 관련 실무 경험 -> 나 아직 실무 경험 없는데 ㅠㅠ (남은 답변: 모든 질문 해결 완료),LoRA 와 QLoRA 의 차이,-0.003986092,0.0,0.003986091818660498
Loss Function 관련 실무 경험 -> 나 아직 실무 경험 없는데 ㅠㅠ (남은 답변: 모든 질문 해결 완료),Loss Function 관련 실무 경험,-0.0022658024,0.0,0.002265802351757884
Loss Function 관련 실무 경험 -> 나 아직 실무 경험 없는데 ㅠㅠ (남은 답변: 모든 질문 해결 완료),Loss Function 예시,-0.0048821163,0.0,0.004882116336375475
Loss Function 관련 실무 경험 -> 나 아직 실무 경험 없는데 ㅠㅠ (남은 답변: 모든 질문 해결 완료),Loss Function 정의,-0.0033124012,0.0,0.0033124012406915426
Loss Function 관련 실무 경험 -> 나 아직 실무 경험 없는데 ㅠㅠ (남은 답변: 모든 질문 해결 완료),MBTI / 좋아하는 아이돌,0.9981418,1.0,0.0018581748008728027
Loss Function 관련 실무 경험 -> 나 아직 실무 경험 없는데 ㅠㅠ (남은 답변: 모든 질문 해결 완료),MSE Loss 설명,-0.0020501206,0.0,0.0020501206163316965
Loss Function 관련 실무 경험 -> 나 아직 실무 경험 없는데 ㅠㅠ (남은 답변: 모든 질문 해결 완료),MSE Loss 용도,-0.0025239508,0.0,0.002523950766772032
Loss Function 관련 실무 경험 -> 나 아직 실무 경험 없는데 ㅠㅠ (남은 답변: 모든 질문 해결 완료),"Multi-Class, Multi-Label 중 BCE 가 좋은 task",-0.0014562249,0.0,0.0014562249416485429
Loss Function 관련 실무 경험 -> 나 아직 실무 경험 없는데 ㅠㅠ (남은 답변: 모든 질문 해결 완료),Multi-Label 에서 CE + Softmax 적용 문제점,-0.001070047,0.0,0.0010700470302253962
Loss Function 관련 실무 경험 -> 나 아직 실무 경험 없는데 ㅠㅠ (남은 답변: 모든 질문 해결 완료),PEFT 방법 5가지,-0.0064467024,0.0,0.006446702405810356
Loss Function 관련 실무 경험 -> 나 아직 실무 경험 없는데 ㅠㅠ (남은 답변: 모든 질문 해결 완료),거대 언어 모델 정의,-0.0056348788,0.0,0.005634878762066364
Loss Function 관련 실무 경험 -> 나 아직 실무 경험 없는데 ㅠㅠ (남은 답변: 모든 질문 해결 완료),마지막 할 말,-0.0013116726,0.0,0.001311672618612647
Loss Function 관련 실무 경험 -> 나 아직 실무 경험 없는데 ㅠㅠ (남은 답변: 모든 질문 해결 완료),면접 시작 인사,-0.0010297525,0.0,0.0010297525441274047
Loss Function 관련 실무 경험 -> 나 아직 실무 경험 없는데 ㅠㅠ (남은 답변: 모든 질문 해결 완료),면접 종료,-0.005227002,0.0,0.00522700184956193
Loss Function 관련 실무 경험 -> 나 아직 실무 경험 없는데 ㅠㅠ (남은 답변: 모든 질문 해결 완료),"인공지능, 머신러닝, 딥러닝 차이",-0.0014237997,0.0,0.001423799665644765
Loss Function 관련 실무 경험 -> 나 아직 실무 경험 없는데 ㅠㅠ (남은 답변: 모든 질문 해결 완료),잠시 휴식,0.0007470957,0.0,0.0007470956770703197
Loss Function 관련 실무 경험 -> 나 아직 실무 경험 없는데 ㅠㅠ (남은 답변: 모든 질문 해결 완료),확률 예측에서 MSE Loss 미 사용 이유,-0.002084953,0.0,0.00208495301194489
"Loss Function 관련 실무 경험 -> 대답하기 싫은데 힝 (남은 답변: 기본 경험, 상세 경험)",BCE Loss 설명,-0.004266933,0.0,0.00426693307235837
"Loss Function 관련 실무 경험 -> 대답하기 싫은데 힝 (남은 답변: 기본 경험, 상세 경험)",LLM Fine-Tuning 의 PEFT,-0.0014592673,0.0,0.001459267339669168
"Loss Function 관련 실무 경험 -> 대답하기 싫은데 힝 (남은 답변: 기본 경험, 상세 경험)",LoRA,-0.0010675555,0.0,0.001067555509507656
"Loss Function 관련 실무 경험 -> 대답하기 싫은데 힝 (남은 답변: 기본 경험, 상세 경험)",LoRA 와 QLoRA 의 차이,0.00058816443,0.0,0.0005881644319742918
"Loss Function 관련 실무 경험 -> 대답하기 싫은데 힝 (남은 답변: 기본 경험, 상세 경험)",Loss Function 관련 실무 경험,-0.004016097,0.0,0.004016097169369459
"Loss Function 관련 실무 경험 -> 대답하기 싫은데 힝 (남은 답변: 기본 경험, 상세 경험)",Loss Function 예시,-0.0059471647,0.0,0.005947164725512266
"Loss Function 관련 실무 경험 -> 대답하기 싫은데 힝 (남은 답변: 기본 경험, 상세 경험)",Loss Function 정의,-0.002774562,0.0,0.0027745619881898165
"Loss Function 관련 실무 경험 -> 대답하기 싫은데 힝 (남은 답변: 기본 경험, 상세 경험)",MBTI / 좋아하는 아이돌,-0.0019856258,0.0,0.0019856258295476437
"Loss Function 관련 실무 경험 -> 대답하기 싫은데 힝 (남은 답변: 기본 경험, 상세 경험)",MSE Loss 설명,-0.0004825634,0.0,0.00048256339505314827
"Loss Function 관련 실무 경험 -> 대답하기 싫은데 힝 (남은 답변: 기본 경험, 상세 경험)",MSE Loss 용도,-0.0030753878,0.0,0.0030753877945244312
"Loss Function 관련 실무 경험 -> 대답하기 싫은데 힝 (남은 답변: 기본 경험, 상세 경험)","Multi-Class, Multi-Label 중 BCE 가 좋은 task",0.0014057175,0.0,0.0014057174557819963
"Loss Function 관련 실무 경험 -> 대답하기 싫은데 힝 (남은 답변: 기본 경험, 상세 경험)",Multi-Label 에서 CE + Softmax 적용 문제점,-0.0024266865,0.0,0.0024266864638775587
"Loss Function 관련 실무 경험 -> 대답하기 싫은데 힝 (남은 답변: 기본 경험, 상세 경험)",PEFT 방법 5가지,-0.0012009098,0.0,0.001200909842737019
"Loss Function 관련 실무 경험 -> 대답하기 싫은데 힝 (남은 답변: 기본 경험, 상세 경험)",거대 언어 모델 정의,-0.0004860442,0.0,0.0004860442131757736
"Loss Function 관련 실무 경험 -> 대답하기 싫은데 힝 (남은 답변: 기본 경험, 상세 경험)",마지막 할 말,-0.003882883,0.0,0.003882882883772254
"Loss Function 관련 실무 경험 -> 대답하기 싫은데 힝 (남은 답변: 기본 경험, 상세 경험)",면접 시작 인사,-0.0036857806,0.0,0.003685780568048358
"Loss Function 관련 실무 경험 -> 대답하기 싫은데 힝 (남은 답변: 기본 경험, 상세 경험)",면접 종료,-0.0005552172,0.0,0.0005552172078751028
"Loss Function 관련 실무 경험 -> 대답하기 싫은데 힝 (남은 답변: 기본 경험, 상세 경험)","인공지능, 머신러닝, 딥러닝 차이",-0.0016589739,0.0,0.0016589738661423326
"Loss Function 관련 실무 경험 -> 대답하기 싫은데 힝 (남은 답변: 기본 경험, 상세 경험)",잠시 휴식,0.9973505,1.0,0.0026494860649108887
"Loss Function 관련 실무 경험 -> 대답하기 싫은데 힝 (남은 답변: 기본 경험, 상세 경험)",확률 예측에서 MSE Loss 미 사용 이유,-0.0031591875,0.0,0.003159187501296401
Loss Function 관련 실무 경험 -> 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ (남은 답변: 상세 경험),BCE Loss 설명,-0.0054193437,0.0,0.005419343709945679
Loss Function 관련 실무 경험 -> 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ (남은 답변: 상세 경험),LLM Fine-Tuning 의 PEFT,-0.0023739098,0.0,0.0023739098105579615
Loss Function 관련 실무 경험 -> 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ (남은 답변: 상세 경험),LoRA,-0.0029284658,0.0,0.002928465837612748
Loss Function 관련 실무 경험 -> 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ (남은 답변: 상세 경험),LoRA 와 QLoRA 의 차이,-0.0030097181,0.0,0.003009718144312501
Loss Function 관련 실무 경험 -> 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ (남은 답변: 상세 경험),Loss Function 관련 실무 경험,0.9989688,1.0,0.0010312199592590332
Loss Function 관련 실무 경험 -> 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ (남은 답변: 상세 경험),Loss Function 예시,-0.003142498,0.0,0.003142497967928648
Loss Function 관련 실무 경험 -> 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ (남은 답변: 상세 경험),Loss Function 정의,-0.0035975375,0.0,0.0035975375212728977
Loss Function 관련 실무 경험 -> 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ (남은 답변: 상세 경험),MBTI / 좋아하는 아이돌,-0.0001401343,0.0,0.00014013430336490273
Loss Function 관련 실무 경험 -> 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ (남은 답변: 상세 경험),MSE Loss 설명,-0.0033916067,0.0,0.0033916067332029343
Loss Function 관련 실무 경험 -> 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ (남은 답변: 상세 경험),MSE Loss 용도,-0.006724646,0.0,0.006724645849317312
Loss Function 관련 실무 경험 -> 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ (남은 답변: 상세 경험),"Multi-Class, Multi-Label 중 BCE 가 좋은 task",-0.0031170275,0.0,0.003117027459666133
Loss Function 관련 실무 경험 -> 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ (남은 답변: 상세 경험),Multi-Label 에서 CE + Softmax 적용 문제점,-0.0019894375,0.0,0.001989437500014901
Loss Function 관련 실무 경험 -> 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ (남은 답변: 상세 경험),PEFT 방법 5가지,-0.0035010898,0.0,0.0035010897554457188
Loss Function 관련 실무 경험 -> 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ (남은 답변: 상세 경험),거대 언어 모델 정의,-0.0032126645,0.0,0.003212664509192109
Loss Function 관련 실무 경험 -> 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ (남은 답변: 상세 경험),마지막 할 말,-0.00050828827,0.0,0.0005082882707938552
Loss Function 관련 실무 경험 -> 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ (남은 답변: 상세 경험),면접 시작 인사,-0.0026964126,0.0,0.002696412615478039
Loss Function 관련 실무 경험 -> 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ (남은 답변: 상세 경험),면접 종료,-0.00354261,0.0,0.003542609978467226
Loss Function 관련 실무 경험 -> 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ (남은 답변: 상세 경험),"인공지능, 머신러닝, 딥러닝 차이",-0.0038507266,0.0,0.003850726643577218
Loss Function 관련 실무 경험 -> 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ (남은 답변: 상세 경험),잠시 휴식,-0.0035634667,0.0,0.003563466714695096
Loss Function 관련 실무 경험 -> 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ (남은 답변: 상세 경험),확률 예측에서 MSE Loss 미 사용 이유,-0.0036799258,0.0,0.0036799258086830378
Loss Function 관련 실무 경험 -> 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어 (남은 답변: 모든 질문 해결 완료),BCE Loss 설명,-0.0033261618,0.0,0.00332616176456213
Loss Function 관련 실무 경험 -> 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어 (남은 답변: 모든 질문 해결 완료),LLM Fine-Tuning 의 PEFT,-0.0020827446,0.0,0.002082744613289833
Loss Function 관련 실무 경험 -> 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어 (남은 답변: 모든 질문 해결 완료),LoRA,-0.00078979996,0.0,0.0007897999603301287
Loss Function 관련 실무 경험 -> 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어 (남은 답변: 모든 질문 해결 완료),LoRA 와 QLoRA 의 차이,-0.00088304136,0.0,0.0008830413571558893
Loss Function 관련 실무 경험 -> 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어 (남은 답변: 모든 질문 해결 완료),Loss Function 관련 실무 경험,-0.0010441298,0.0,0.0010441298363730311
Loss Function 관련 실무 경험 -> 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어 (남은 답변: 모든 질문 해결 완료),Loss Function 예시,-0.00061262934,0.0,0.0006126293446868658
Loss Function 관련 실무 경험 -> 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어 (남은 답변: 모든 질문 해결 완료),Loss Function 정의,-0.005145702,0.0,0.0051457020454108715
Loss Function 관련 실무 경험 -> 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어 (남은 답변: 모든 질문 해결 완료),MBTI / 좋아하는 아이돌,0.99785036,1.0,0.002149641513824463
Loss Function 관련 실무 경험 -> 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어 (남은 답변: 모든 질문 해결 완료),MSE Loss 설명,-0.0038164414,0.0,0.003816441399976611
Loss Function 관련 실무 경험 -> 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어 (남은 답변: 모든 질문 해결 완료),MSE Loss 용도,-0.001587468,0.0,0.0015874679666012526
Loss Function 관련 실무 경험 -> 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어 (남은 답변: 모든 질문 해결 완료),"Multi-Class, Multi-Label 중 BCE 가 좋은 task",-0.0007920084,0.0,0.0007920084171928465
Loss Function 관련 실무 경험 -> 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어 (남은 답변: 모든 질문 해결 완료),Multi-Label 에서 CE + Softmax 적용 문제점,-0.0038103382,0.0,0.0038103382103145123
Loss Function 관련 실무 경험 -> 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어 (남은 답변: 모든 질문 해결 완료),PEFT 방법 5가지,-0.004472758,0.0,0.004472758155316114
Loss Function 관련 실무 경험 -> 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어 (남은 답변: 모든 질문 해결 완료),거대 언어 모델 정의,-0.0052522905,0.0,0.005252290517091751
Loss Function 관련 실무 경험 -> 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어 (남은 답변: 모든 질문 해결 완료),마지막 할 말,9.642607e-05,0.0,9.642606892157346e-05
Loss Function 관련 실무 경험 -> 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어 (남은 답변: 모든 질문 해결 완료),면접 시작 인사,-0.0009651932,0.0,0.0009651932050473988
Loss Function 관련 실무 경험 -> 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어 (남은 답변: 모든 질문 해결 완료),면접 종료,-0.0030504044,0.0,0.00305040436796844
Loss Function 관련 실무 경험 -> 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어 (남은 답변: 모든 질문 해결 완료),"인공지능, 머신러닝, 딥러닝 차이",-0.0025211456,0.0,0.00252114562317729
Loss Function 관련 실무 경험 -> 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어 (남은 답변: 모든 질문 해결 완료),잠시 휴식,-0.004378613,0.0,0.004378613084554672
Loss Function 관련 실무 경험 -> 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어 (남은 답변: 모든 질문 해결 완료),확률 예측에서 MSE Loss 미 사용 이유,-0.0017363318,0.0,0.0017363318474963307
Loss Function 관련 실무 경험 -> 기억 진짜 안 나는데 가물가물하다 (남은 답변: 상세 경험),BCE Loss 설명,-0.003068495,0.0,0.003068495076149702
Loss Function 관련 실무 경험 -> 기억 진짜 안 나는데 가물가물하다 (남은 답변: 상세 경험),LLM Fine-Tuning 의 PEFT,-0.0006953915,0.0,0.000695391499903053
Loss Function 관련 실무 경험 -> 기억 진짜 안 나는데 가물가물하다 (남은 답변: 상세 경험),LoRA,-0.0049325605,0.0,0.004932560492306948
Loss Function 관련 실무 경험 -> 기억 진짜 안 나는데 가물가물하다 (남은 답변: 상세 경험),LoRA 와 QLoRA 의 차이,0.00045070212,0.0,0.00045070212217979133
Loss Function 관련 실무 경험 -> 기억 진짜 안 나는데 가물가물하다 (남은 답변: 상세 경험),Loss Function 관련 실무 경험,0.021344459,0.0,0.021344458684325218
Loss Function 관련 실무 경험 -> 기억 진짜 안 나는데 가물가물하다 (남은 답변: 상세 경험),Loss Function 예시,0.0024220378,0.0,0.0024220377672463655
Loss Function 관련 실무 경험 -> 기억 진짜 안 나는데 가물가물하다 (남은 답변: 상세 경험),Loss Function 정의,-0.0017352133,0.0,0.0017352133290842175
Loss Function 관련 실무 경험 -> 기억 진짜 안 나는데 가물가물하다 (남은 답변: 상세 경험),MBTI / 좋아하는 아이돌,-0.0048554656,0.0,0.004855465609580278
Loss Function 관련 실무 경험 -> 기억 진짜 안 나는데 가물가물하다 (남은 답변: 상세 경험),MSE Loss 설명,-0.0034101214,0.0,0.00341012142598629
Loss Function 관련 실무 경험 -> 기억 진짜 안 나는데 가물가물하다 (남은 답변: 상세 경험),MSE Loss 용도,-0.005578332,0.0,0.005578332114964724
Loss Function 관련 실무 경험 -> 기억 진짜 안 나는데 가물가물하다 (남은 답변: 상세 경험),"Multi-Class, Multi-Label 중 BCE 가 좋은 task",-0.000453555,0.0,0.0004535549960564822
Loss Function 관련 실무 경험 -> 기억 진짜 안 나는데 가물가물하다 (남은 답변: 상세 경험),Multi-Label 에서 CE + Softmax 적용 문제점,-0.0047147362,0.0,0.004714736249297857
Loss Function 관련 실무 경험 -> 기억 진짜 안 나는데 가물가물하다 (남은 답변: 상세 경험),PEFT 방법 5가지,-0.0009892429,0.0,0.000989242922514677
Loss Function 관련 실무 경험 -> 기억 진짜 안 나는데 가물가물하다 (남은 답변: 상세 경험),거대 언어 모델 정의,-0.0031052588,0.0,0.0031052588019520044
Loss Function 관련 실무 경험 -> 기억 진짜 안 나는데 가물가물하다 (남은 답변: 상세 경험),마지막 할 말,0.0030085715,0.0,0.0030085714533925056
Loss Function 관련 실무 경험 -> 기억 진짜 안 나는데 가물가물하다 (남은 답변: 상세 경험),면접 시작 인사,-0.0037642168,0.0,0.0037642167881131172
Loss Function 관련 실무 경험 -> 기억 진짜 안 나는데 가물가물하다 (남은 답변: 상세 경험),면접 종료,-0.0045437603,0.0,0.004543760325759649
Loss Function 관련 실무 경험 -> 기억 진짜 안 나는데 가물가물하다 (남은 답변: 상세 경험),"인공지능, 머신러닝, 딥러닝 차이",-0.0050163507,0.0,0.005016350653022528
Loss Function 관련 실무 경험 -> 기억 진짜 안 나는데 가물가물하다 (남은 답변: 상세 경험),잠시 휴식,0.99178445,1.0,0.008215546607971191
Loss Function 관련 실무 경험 -> 기억 진짜 안 나는데 가물가물하다 (남은 답변: 상세 경험),확률 예측에서 MSE Loss 미 사용 이유,-0.0040045576,0.0,0.004004557617008686
Loss Function 관련 실무 경험 -> 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ (남은 답변: 모든 질문 해결 완료),BCE Loss 설명,-0.004466237,0.0,0.004466237034648657
Loss Function 관련 실무 경험 -> 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ (남은 답변: 모든 질문 해결 완료),LLM Fine-Tuning 의 PEFT,-0.0004943553,0.0,0.0004943552776239812
Loss Function 관련 실무 경험 -> 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ (남은 답변: 모든 질문 해결 완료),LoRA,7.944413e-05,0.0,7.944412936922163e-05
Loss Function 관련 실무 경험 -> 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ (남은 답변: 모든 질문 해결 완료),LoRA 와 QLoRA 의 차이,-0.0023274154,0.0,0.002327415393665433
Loss Function 관련 실무 경험 -> 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ (남은 답변: 모든 질문 해결 완료),Loss Function 관련 실무 경험,-0.0016517603,0.0,0.0016517603071406484
Loss Function 관련 실무 경험 -> 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ (남은 답변: 모든 질문 해결 완료),Loss Function 예시,-0.0054477896,0.0,0.005447789561003447
Loss Function 관련 실무 경험 -> 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ (남은 답변: 모든 질문 해결 완료),Loss Function 정의,-0.0034692339,0.0,0.0034692338667809963
Loss Function 관련 실무 경험 -> 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ (남은 답변: 모든 질문 해결 완료),MBTI / 좋아하는 아이돌,0.99842024,1.0,0.0015797615051269531
Loss Function 관련 실무 경험 -> 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ (남은 답변: 모든 질문 해결 완료),MSE Loss 설명,-0.0019524262,0.0,0.001952426158823073
Loss Function 관련 실무 경험 -> 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ (남은 답변: 모든 질문 해결 완료),MSE Loss 용도,-0.0030451661,0.0,0.0030451661441475153
Loss Function 관련 실무 경험 -> 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ (남은 답변: 모든 질문 해결 완료),"Multi-Class, Multi-Label 중 BCE 가 좋은 task",-0.0013612981,0.0,0.0013612981420010328
Loss Function 관련 실무 경험 -> 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ (남은 답변: 모든 질문 해결 완료),Multi-Label 에서 CE + Softmax 적용 문제점,-0.0011119923,0.0,0.0011119922855868936
Loss Function 관련 실무 경험 -> 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ (남은 답변: 모든 질문 해결 완료),PEFT 방법 5가지,-0.0060404367,0.0,0.006040436681360006
Loss Function 관련 실무 경험 -> 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ (남은 답변: 모든 질문 해결 완료),거대 언어 모델 정의,-0.005467685,0.0,0.00546768493950367
Loss Function 관련 실무 경험 -> 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ (남은 답변: 모든 질문 해결 완료),마지막 할 말,-0.0010397345,0.0,0.0010397344594821334
Loss Function 관련 실무 경험 -> 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ (남은 답변: 모든 질문 해결 완료),면접 시작 인사,-0.0011582331,0.0,0.001158233149908483
Loss Function 관련 실무 경험 -> 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ (남은 답변: 모든 질문 해결 완료),면접 종료,-0.0040439856,0.0,0.00404398562386632
Loss Function 관련 실무 경험 -> 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ (남은 답변: 모든 질문 해결 완료),"인공지능, 머신러닝, 딥러닝 차이",-0.0010521199,0.0,0.001052119885571301
Loss Function 관련 실무 경험 -> 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ (남은 답변: 모든 질문 해결 완료),잠시 휴식,-0.0011528938,0.0,0.0011528937611728907
Loss Function 관련 실무 경험 -> 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ (남은 답변: 모든 질문 해결 완료),확률 예측에서 MSE Loss 미 사용 이유,-0.001649887,0.0,0.0016498869517818093
"Loss Function 관련 실무 경험 -> 이걸 왜 알려줘야 하지 (남은 답변: 기본 경험, 상세 경험)",BCE Loss 설명,-0.0021914928,0.0,0.002191492822021246
"Loss Function 관련 실무 경험 -> 이걸 왜 알려줘야 하지 (남은 답변: 기본 경험, 상세 경험)",LLM Fine-Tuning 의 PEFT,-0.0028348959,0.0,0.002834895858541131
"Loss Function 관련 실무 경험 -> 이걸 왜 알려줘야 하지 (남은 답변: 기본 경험, 상세 경험)",LoRA,-0.0025146091,0.0,0.0025146091356873512
"Loss Function 관련 실무 경험 -> 이걸 왜 알려줘야 하지 (남은 답변: 기본 경험, 상세 경험)",LoRA 와 QLoRA 의 차이,-0.0011515542,0.0,0.0011515541700646281
"Loss Function 관련 실무 경험 -> 이걸 왜 알려줘야 하지 (남은 답변: 기본 경험, 상세 경험)",Loss Function 관련 실무 경험,-0.0062049576,0.0,0.0062049576081335545
"Loss Function 관련 실무 경험 -> 이걸 왜 알려줘야 하지 (남은 답변: 기본 경험, 상세 경험)",Loss Function 예시,-0.0032480252,0.0,0.0032480251975357533
"Loss Function 관련 실무 경험 -> 이걸 왜 알려줘야 하지 (남은 답변: 기본 경험, 상세 경험)",Loss Function 정의,-0.0020090332,0.0,0.002009033225476742
"Loss Function 관련 실무 경험 -> 이걸 왜 알려줘야 하지 (남은 답변: 기본 경험, 상세 경험)",MBTI / 좋아하는 아이돌,-0.004012017,0.0,0.004012017045170069
"Loss Function 관련 실무 경험 -> 이걸 왜 알려줘야 하지 (남은 답변: 기본 경험, 상세 경험)",MSE Loss 설명,-0.00062406546,0.0,0.0006240654620341957
"Loss Function 관련 실무 경험 -> 이걸 왜 알려줘야 하지 (남은 답변: 기본 경험, 상세 경험)",MSE Loss 용도,-0.0045933714,0.0,0.0045933714136481285
"Loss Function 관련 실무 경험 -> 이걸 왜 알려줘야 하지 (남은 답변: 기본 경험, 상세 경험)","Multi-Class, Multi-Label 중 BCE 가 좋은 task",-0.00096604956,0.0,0.0009660495561547577
"Loss Function 관련 실무 경험 -> 이걸 왜 알려줘야 하지 (남은 답변: 기본 경험, 상세 경험)",Multi-Label 에서 CE + Softmax 적용 문제점,-0.0044360342,0.0,0.004436034243553877
"Loss Function 관련 실무 경험 -> 이걸 왜 알려줘야 하지 (남은 답변: 기본 경험, 상세 경험)",PEFT 방법 5가지,0.00084135524,0.0,0.0008413552423007786
"Loss Function 관련 실무 경험 -> 이걸 왜 알려줘야 하지 (남은 답변: 기본 경험, 상세 경험)",거대 언어 모델 정의,0.00030604395,0.0,0.00030604394851252437
"Loss Function 관련 실무 경험 -> 이걸 왜 알려줘야 하지 (남은 답변: 기본 경험, 상세 경험)",마지막 할 말,-0.0021778052,0.0,0.0021778051741421223
"Loss Function 관련 실무 경험 -> 이걸 왜 알려줘야 하지 (남은 답변: 기본 경험, 상세 경험)",면접 시작 인사,-0.005474386,0.0,0.005474385805428028
"Loss Function 관련 실무 경험 -> 이걸 왜 알려줘야 하지 (남은 답변: 기본 경험, 상세 경험)",면접 종료,0.00068003015,0.0,0.0006800301489420235
"Loss Function 관련 실무 경험 -> 이걸 왜 알려줘야 하지 (남은 답변: 기본 경험, 상세 경험)","인공지능, 머신러닝, 딥러닝 차이",-0.0034684117,0.0,0.0034684117417782545
"Loss Function 관련 실무 경험 -> 이걸 왜 알려줘야 하지 (남은 답변: 기본 경험, 상세 경험)",잠시 휴식,0.9967359,1.0,0.003264129161834717
"Loss Function 관련 실무 경험 -> 이걸 왜 알려줘야 하지 (남은 답변: 기본 경험, 상세 경험)",확률 예측에서 MSE Loss 미 사용 이유,-0.002157653,0.0,0.002157652983441949
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야! (남은 답변: 모든 질문 해결 완료),BCE Loss 설명,-0.0028895144,0.0,0.002889514435082674
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야! (남은 답변: 모든 질문 해결 완료),LLM Fine-Tuning 의 PEFT,-0.0011493762,0.0,0.001149376155808568
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야! (남은 답변: 모든 질문 해결 완료),LoRA,-0.0011473378,0.0,0.0011473378399387002
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야! (남은 답변: 모든 질문 해결 완료),LoRA 와 QLoRA 의 차이,-0.00385575,0.0,0.0038557499647140503
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야! (남은 답변: 모든 질문 해결 완료),Loss Function 관련 실무 경험,-0.0014512899,0.0,0.0014512898633256555
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야! (남은 답변: 모든 질문 해결 완료),Loss Function 예시,-0.0048472453,0.0,0.004847245290875435
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야! (남은 답변: 모든 질문 해결 완료),Loss Function 정의,-0.005724637,0.0,0.00572463683784008
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야! (남은 답변: 모든 질문 해결 완료),MBTI / 좋아하는 아이돌,-0.0025720806,0.0,0.002572080586105585
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야! (남은 답변: 모든 질문 해결 완료),MSE Loss 설명,0.0004947921,0.0,0.0004947921261191368
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야! (남은 답변: 모든 질문 해결 완료),MSE Loss 용도,-0.0025578663,0.0,0.0025578662753105164
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야! (남은 답변: 모든 질문 해결 완료),"Multi-Class, Multi-Label 중 BCE 가 좋은 task",-0.00090486975,0.0,0.0009048697538673878
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야! (남은 답변: 모든 질문 해결 완료),Multi-Label 에서 CE + Softmax 적용 문제점,-0.0015859434,0.0,0.001585943391546607
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야! (남은 답변: 모든 질문 해결 완료),PEFT 방법 5가지,-0.0038671521,0.0,0.0038671521469950676
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야! (남은 답변: 모든 질문 해결 완료),거대 언어 모델 정의,-0.00010830523,0.0,0.00010830522660398856
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야! (남은 답변: 모든 질문 해결 완료),마지막 할 말,-0.0058495956,0.0,0.00584959564730525
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야! (남은 답변: 모든 질문 해결 완료),면접 시작 인사,-0.0017421822,0.0,0.0017421821830794215
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야! (남은 답변: 모든 질문 해결 완료),면접 종료,-0.002238398,0.0,0.00223839795216918
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야! (남은 답변: 모든 질문 해결 완료),"인공지능, 머신러닝, 딥러닝 차이",-0.0017334368,0.0,0.0017334368312731385
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야! (남은 답변: 모든 질문 해결 완료),잠시 휴식,0.9973885,1.0,0.002611517906188965
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야! (남은 답변: 모든 질문 해결 완료),확률 예측에서 MSE Loss 미 사용 이유,-7.998285e-05,0.0,7.998284854693338e-05
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해 (남은 답변: 모든 질문 해결 완료),BCE Loss 설명,-0.004650614,0.0,0.004650614224374294
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해 (남은 답변: 모든 질문 해결 완료),LLM Fine-Tuning 의 PEFT,-0.0014935742,0.0,0.0014935742365196347
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해 (남은 답변: 모든 질문 해결 완료),LoRA,-0.000974671,0.0,0.000974670983850956
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해 (남은 답변: 모든 질문 해결 완료),LoRA 와 QLoRA 의 차이,-0.0040609553,0.0,0.004060955252498388
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해 (남은 답변: 모든 질문 해결 완료),Loss Function 관련 실무 경험,-0.0014646472,0.0,0.0014646472409367561
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해 (남은 답변: 모든 질문 해결 완료),Loss Function 예시,-0.003915341,0.0,0.003915341105312109
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해 (남은 답변: 모든 질문 해결 완료),Loss Function 정의,-0.0022864006,0.0,0.0022864006459712982
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해 (남은 답변: 모든 질문 해결 완료),MBTI / 좋아하는 아이돌,-0.0045563914,0.0,0.004556391388177872
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해 (남은 답변: 모든 질문 해결 완료),MSE Loss 설명,-0.0022910337,0.0,0.0022910337429493666
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해 (남은 답변: 모든 질문 해결 완료),MSE Loss 용도,-0.0046118665,0.0,0.004611866548657417
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해 (남은 답변: 모든 질문 해결 완료),"Multi-Class, Multi-Label 중 BCE 가 좋은 task",-6.462347e-05,0.0,6.46234693704173e-05
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해 (남은 답변: 모든 질문 해결 완료),Multi-Label 에서 CE + Softmax 적용 문제점,-0.0024378286,0.0,0.0024378285743296146
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해 (남은 답변: 모든 질문 해결 완료),PEFT 방법 5가지,-0.002722616,0.0,0.0027226160746067762
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해 (남은 답변: 모든 질문 해결 완료),거대 언어 모델 정의,-0.00032809202,0.0,0.0003280920209363103
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해 (남은 답변: 모든 질문 해결 완료),마지막 할 말,-0.0030609216,0.0,0.003060921560972929
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해 (남은 답변: 모든 질문 해결 완료),면접 시작 인사,-0.0037948347,0.0,0.003794834716245532
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해 (남은 답변: 모든 질문 해결 완료),면접 종료,0.0015655948,0.0,0.0015655948081985116
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해 (남은 답변: 모든 질문 해결 완료),"인공지능, 머신러닝, 딥러닝 차이",-0.00069362746,0.0,0.0006936274585314095
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해 (남은 답변: 모든 질문 해결 완료),잠시 휴식,0.9974724,1.0,0.002527594566345215
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해 (남은 답변: 모든 질문 해결 완료),확률 예측에서 MSE Loss 미 사용 이유,-0.0012503498,0.0,0.0012503498001024127
잠시 휴식 -> 재미있는 이야기 해줄래? (남은 답변: 잠시 휴식),BCE Loss 설명,-0.005441278,0.0,0.0054412782192230225
잠시 휴식 -> 재미있는 이야기 해줄래? (남은 답변: 잠시 휴식),LLM Fine-Tuning 의 PEFT,0.9989992,1.0,0.001000821590423584
잠시 휴식 -> 재미있는 이야기 해줄래? (남은 답변: 잠시 휴식),LoRA,-0.00089273136,0.0,0.0008927313610911369
잠시 휴식 -> 재미있는 이야기 해줄래? (남은 답변: 잠시 휴식),LoRA 와 QLoRA 의 차이,-0.0002619319,0.0,0.00026193191297352314
잠시 휴식 -> 재미있는 이야기 해줄래? (남은 답변: 잠시 휴식),Loss Function 관련 실무 경험,-0.0021596833,0.0,0.0021596832666546106
잠시 휴식 -> 재미있는 이야기 해줄래? (남은 답변: 잠시 휴식),Loss Function 예시,-0.0052839187,0.0,0.00528391869738698
잠시 휴식 -> 재미있는 이야기 해줄래? (남은 답변: 잠시 휴식),Loss Function 정의,-0.002496233,0.0,0.0024962329771369696
잠시 휴식 -> 재미있는 이야기 해줄래? (남은 답변: 잠시 휴식),MBTI / 좋아하는 아이돌,-0.0012544497,0.0,0.0012544497149065137
잠시 휴식 -> 재미있는 이야기 해줄래? (남은 답변: 잠시 휴식),MSE Loss 설명,-0.0013549888,0.0,0.0013549887808039784
잠시 휴식 -> 재미있는 이야기 해줄래? (남은 답변: 잠시 휴식),MSE Loss 용도,0.0004556098,0.0,0.00045560981379821897
잠시 휴식 -> 재미있는 이야기 해줄래? (남은 답변: 잠시 휴식),"Multi-Class, Multi-Label 중 BCE 가 좋은 task",-0.0024374921,0.0,0.002437492134049535
잠시 휴식 -> 재미있는 이야기 해줄래? (남은 답변: 잠시 휴식),Multi-Label 에서 CE + Softmax 적용 문제점,-0.0035053135,0.0,0.0035053135361522436
잠시 휴식 -> 재미있는 이야기 해줄래? (남은 답변: 잠시 휴식),PEFT 방법 5가지,0.0028334036,0.0,0.0028334036469459534
잠시 휴식 -> 재미있는 이야기 해줄래? (남은 답변: 잠시 휴식),거대 언어 모델 정의,-0.00069292414,0.0,0.0006929241353645921
잠시 휴식 -> 재미있는 이야기 해줄래? (남은 답변: 잠시 휴식),마지막 할 말,-0.00019953004,0.0,0.00019953004084527493
잠시 휴식 -> 재미있는 이야기 해줄래? (남은 답변: 잠시 휴식),면접 시작 인사,-0.005185045,0.0,0.005185044836252928
잠시 휴식 -> 재미있는 이야기 해줄래? (남은 답변: 잠시 휴식),면접 종료,-0.0005867573,0.0,0.0005867573199793696
잠시 휴식 -> 재미있는 이야기 해줄래? (남은 답변: 잠시 휴식),"인공지능, 머신러닝, 딥러닝 차이",0.0012876171,0.0,0.0012876171385869384
잠시 휴식 -> 재미있는 이야기 해줄래? (남은 답변: 잠시 휴식),잠시 휴식,-0.0021032195,0.0,0.0021032195072621107
잠시 휴식 -> 재미있는 이야기 해줄래? (남은 답변: 잠시 휴식),확률 예측에서 MSE Loss 미 사용 이유,-0.0020047098,0.0,0.002004709793254733
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야 (남은 답변: 모든 질문 해결 완료)",BCE Loss 설명,-0.0026409633,0.0,0.0026409632991999388
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야 (남은 답변: 모든 질문 해결 완료)",LLM Fine-Tuning 의 PEFT,-0.0028868085,0.0,0.0028868084773421288
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야 (남은 답변: 모든 질문 해결 완료)",LoRA,-0.0056550056,0.0,0.005655005574226379
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야 (남은 답변: 모든 질문 해결 완료)",LoRA 와 QLoRA 의 차이,-0.0033209282,0.0,0.0033209281973540783
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야 (남은 답변: 모든 질문 해결 완료)",Loss Function 관련 실무 경험,-0.004076336,0.0,0.004076336044818163
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야 (남은 답변: 모든 질문 해결 완료)",Loss Function 예시,-0.0014893927,0.0,0.001489392714574933
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야 (남은 답변: 모든 질문 해결 완료)",Loss Function 정의,0.0003650259,0.0,0.0003650258877314627
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야 (남은 답변: 모든 질문 해결 완료)",MBTI / 좋아하는 아이돌,-0.0033717435,0.0,0.0033717434853315353
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야 (남은 답변: 모든 질문 해결 완료)",MSE Loss 설명,-0.003913146,0.0,0.00391314597800374
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야 (남은 답변: 모든 질문 해결 완료)",MSE Loss 용도,-0.0018548546,0.0,0.0018548546358942986
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야 (남은 답변: 모든 질문 해결 완료)","Multi-Class, Multi-Label 중 BCE 가 좋은 task",-0.0013598425,0.0,0.0013598424848169088
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야 (남은 답변: 모든 질문 해결 완료)",Multi-Label 에서 CE + Softmax 적용 문제점,0.0008726017,0.0,0.0008726016967557371
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야 (남은 답변: 모든 질문 해결 완료)",PEFT 방법 5가지,0.9969494,1.0,0.0030506253242492676
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야 (남은 답변: 모든 질문 해결 완료)",거대 언어 모델 정의,-0.0020252059,0.0,0.0020252058748155832
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야 (남은 답변: 모든 질문 해결 완료)",마지막 할 말,-0.0025682994,0.0,0.0025682994164526463
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야 (남은 답변: 모든 질문 해결 완료)",면접 시작 인사,-0.001912046,0.0,0.0019120459910482168
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야 (남은 답변: 모든 질문 해결 완료)",면접 종료,-0.005070689,0.0,0.005070689134299755
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야 (남은 답변: 모든 질문 해결 완료)","인공지능, 머신러닝, 딥러닝 차이",-0.004836901,0.0,0.004836901091039181
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야 (남은 답변: 모든 질문 해결 완료)",잠시 휴식,0.0015267101,0.0,0.0015267101116478443
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야 (남은 답변: 모든 질문 해결 완료)",확률 예측에서 MSE Loss 미 사용 이유,-0.0038875423,0.0,0.0038875422906130552
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지? (남은 답변: LLM Fine-Tuning 의 PEFT),BCE Loss 설명,-0.0039351103,0.0,0.003935110289603472
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지? (남은 답변: LLM Fine-Tuning 의 PEFT),LLM Fine-Tuning 의 PEFT,0.99912155,1.0,0.000878453254699707
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지? (남은 답변: LLM Fine-Tuning 의 PEFT),LoRA,0.00027268758,0.0,0.00027268758276477456
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지? (남은 답변: LLM Fine-Tuning 의 PEFT),LoRA 와 QLoRA 의 차이,-0.0019543308,0.0,0.0019543308299034834
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지? (남은 답변: LLM Fine-Tuning 의 PEFT),Loss Function 관련 실무 경험,-0.001624362,0.0,0.0016243619611486793
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지? (남은 답변: LLM Fine-Tuning 의 PEFT),Loss Function 예시,-0.0052049155,0.0,0.005204915534704924
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지? (남은 답변: LLM Fine-Tuning 의 PEFT),Loss Function 정의,-0.0020650541,0.0,0.002065054140985012
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지? (남은 답변: LLM Fine-Tuning 의 PEFT),MBTI / 좋아하는 아이돌,-0.0022007097,0.0,0.002200709655880928
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지? (남은 답변: LLM Fine-Tuning 의 PEFT),MSE Loss 설명,-0.0012992658,0.0,0.0012992657721042633
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지? (남은 답변: LLM Fine-Tuning 의 PEFT),MSE Loss 용도,-0.00065237714,0.0,0.000652377144433558
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지? (남은 답변: LLM Fine-Tuning 의 PEFT),"Multi-Class, Multi-Label 중 BCE 가 좋은 task",-0.001385811,0.0,0.0013858110178261995
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지? (남은 답변: LLM Fine-Tuning 의 PEFT),Multi-Label 에서 CE + Softmax 적용 문제점,-0.0029753994,0.0,0.002975399373099208
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지? (남은 답변: LLM Fine-Tuning 의 PEFT),PEFT 방법 5가지,0.00047228826,0.0,0.000472288258606568
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지? (남은 답변: LLM Fine-Tuning 의 PEFT),거대 언어 모델 정의,-0.0032138498,0.0,0.003213849849998951
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지? (남은 답변: LLM Fine-Tuning 의 PEFT),마지막 할 말,-0.0027738072,0.0,0.0027738071512430906
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지? (남은 답변: LLM Fine-Tuning 의 PEFT),면접 시작 인사,-0.0026717002,0.0,0.0026717002037912607
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지? (남은 답변: LLM Fine-Tuning 의 PEFT),면접 종료,-0.0032598965,0.0,0.003259896533563733
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지? (남은 답변: LLM Fine-Tuning 의 PEFT),"인공지능, 머신러닝, 딥러닝 차이",-0.001068893,0.0,0.0010688930051401258
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지? (남은 답변: LLM Fine-Tuning 의 PEFT),잠시 휴식,-0.0022410573,0.0,0.0022410573437809944
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지? (남은 답변: LLM Fine-Tuning 의 PEFT),확률 예측에서 MSE Loss 미 사용 이유,0.0011670114,0.0,0.001167011447250843
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야! (남은 답변: 모든 질문 해결 완료),BCE Loss 설명,-0.0053188032,0.0,0.005318803247064352
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야! (남은 답변: 모든 질문 해결 완료),LLM Fine-Tuning 의 PEFT,-0.0005033974,0.0,0.0005033973720856011
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야! (남은 답변: 모든 질문 해결 완료),LoRA,-0.0035259533,0.0,0.003525953274220228
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야! (남은 답변: 모든 질문 해결 완료),LoRA 와 QLoRA 의 차이,-0.0017367956,0.0,0.0017367956461384892
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야! (남은 답변: 모든 질문 해결 완료),Loss Function 관련 실무 경험,-0.0020775765,0.0,0.002077576471492648
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야! (남은 답변: 모든 질문 해결 완료),Loss Function 예시,-0.005323882,0.0,0.0053238822147250175
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야! (남은 답변: 모든 질문 해결 완료),Loss Function 정의,0.0010604257,0.0,0.0010604256531223655
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야! (남은 답변: 모든 질문 해결 완료),MBTI / 좋아하는 아이돌,-0.0062070745,0.0,0.0062070745043456554
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야! (남은 답변: 모든 질문 해결 완료),MSE Loss 설명,-0.003935838,0.0,0.003935838118195534
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야! (남은 답변: 모든 질문 해결 완료),MSE Loss 용도,-0.00041351322,0.0,0.00041351321851834655
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야! (남은 답변: 모든 질문 해결 완료),"Multi-Class, Multi-Label 중 BCE 가 좋은 task",-0.0017718886,0.0,0.0017718885792419314
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야! (남은 답변: 모든 질문 해결 완료),Multi-Label 에서 CE + Softmax 적용 문제점,-0.0037667744,0.0,0.003766774432733655
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야! (남은 답변: 모든 질문 해결 완료),PEFT 방법 5가지,0.99773705,1.0,0.0022629499435424805
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야! (남은 답변: 모든 질문 해결 완료),거대 언어 모델 정의,-0.0027757625,0.0,0.002775762462988496
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야! (남은 답변: 모든 질문 해결 완료),마지막 할 말,-0.0045316806,0.0,0.004531680606305599
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야! (남은 답변: 모든 질문 해결 완료),면접 시작 인사,-0.0013801517,0.0,0.0013801517197862267
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야! (남은 답변: 모든 질문 해결 완료),면접 종료,-0.0051467945,0.0,0.0051467944867908955
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야! (남은 답변: 모든 질문 해결 완료),"인공지능, 머신러닝, 딥러닝 차이",-0.0048717717,0.0,0.0048717716708779335
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야! (남은 답변: 모든 질문 해결 완료),잠시 휴식,5.235245e-05,0.0,5.235245043877512e-05
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야! (남은 답변: 모든 질문 해결 완료),확률 예측에서 MSE Loss 미 사용 이유,-0.002787453,0.0,0.0027874528896063566
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다 (남은 답변: LLM Fine-Tuning 의 PEFT),BCE Loss 설명,-0.00432305,0.0,0.0043230499140918255
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다 (남은 답변: LLM Fine-Tuning 의 PEFT),LLM Fine-Tuning 의 PEFT,0.9991414,1.0,0.000858604907989502
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다 (남은 답변: LLM Fine-Tuning 의 PEFT),LoRA,0.002414265,0.0,0.0024142649490386248
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다 (남은 답변: LLM Fine-Tuning 의 PEFT),LoRA 와 QLoRA 의 차이,-0.0014094873,0.0,0.0014094873331487179
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다 (남은 답변: LLM Fine-Tuning 의 PEFT),Loss Function 관련 실무 경험,-0.0012186647,0.0,0.0012186646927148104
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다 (남은 답변: LLM Fine-Tuning 의 PEFT),Loss Function 예시,-0.0056766383,0.0,0.005676638334989548
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다 (남은 답변: LLM Fine-Tuning 의 PEFT),Loss Function 정의,-0.0016575751,0.0,0.001657575136050582
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다 (남은 답변: LLM Fine-Tuning 의 PEFT),MBTI / 좋아하는 아이돌,-0.0019194687,0.0,0.001919468748383224
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다 (남은 답변: LLM Fine-Tuning 의 PEFT),MSE Loss 설명,-0.0015904427,0.0,0.0015904427273198962
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다 (남은 답변: LLM Fine-Tuning 의 PEFT),MSE Loss 용도,-0.0014150455,0.0,0.001415045466274023
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다 (남은 답변: LLM Fine-Tuning 의 PEFT),"Multi-Class, Multi-Label 중 BCE 가 좋은 task",-0.0019762886,0.0,0.0019762886222451925
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다 (남은 답변: LLM Fine-Tuning 의 PEFT),Multi-Label 에서 CE + Softmax 적용 문제점,-0.0030377402,0.0,0.0030377402435988188
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다 (남은 답변: LLM Fine-Tuning 의 PEFT),PEFT 방법 5가지,-0.0007510455,0.0,0.0007510454743169248
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다 (남은 답변: LLM Fine-Tuning 의 PEFT),거대 언어 모델 정의,-0.0023354073,0.0,0.002335407305508852
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다 (남은 답변: LLM Fine-Tuning 의 PEFT),마지막 할 말,-0.0020357913,0.0,0.0020357912871986628
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다 (남은 답변: LLM Fine-Tuning 의 PEFT),면접 시작 인사,-0.0023600569,0.0,0.0023600568529218435
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다 (남은 답변: LLM Fine-Tuning 의 PEFT),면접 종료,-0.002300496,0.0,0.00230049598030746
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다 (남은 답변: LLM Fine-Tuning 의 PEFT),"인공지능, 머신러닝, 딥러닝 차이",-0.0019838505,0.0,0.001983850495889783
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다 (남은 답변: LLM Fine-Tuning 의 PEFT),잠시 휴식,-0.001742598,0.0,0.0017425980186089873
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다 (남은 답변: LLM Fine-Tuning 의 PEFT),확률 예측에서 MSE Loss 미 사용 이유,1.7398655e-05,0.0,1.739865547278896e-05
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지? (남은 답변: 모든 질문 해결 완료)",BCE Loss 설명,-0.0045466074,0.0,0.004546607378870249
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지? (남은 답변: 모든 질문 해결 완료)",LLM Fine-Tuning 의 PEFT,-0.001549453,0.0,0.0015494530089199543
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지? (남은 답변: 모든 질문 해결 완료)",LoRA,0.9977074,1.0,0.0022925734519958496
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지? (남은 답변: 모든 질문 해결 완료)",LoRA 와 QLoRA 의 차이,-0.0031766577,0.0,0.0031766577158123255
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지? (남은 답변: 모든 질문 해결 완료)",Loss Function 관련 실무 경험,-1.789904e-05,0.0,1.7899039448820986e-05
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지? (남은 답변: 모든 질문 해결 완료)",Loss Function 예시,-0.0099416515,0.0,0.009941651485860348
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지? (남은 답변: 모든 질문 해결 완료)",Loss Function 정의,-0.0030206258,0.0,0.0030206257943063974
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지? (남은 답변: 모든 질문 해결 완료)",MBTI / 좋아하는 아이돌,-0.0003049848,0.0,0.0003049848019145429
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지? (남은 답변: 모든 질문 해결 완료)",MSE Loss 설명,-0.0043616095,0.0,0.00436160946264863
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지? (남은 답변: 모든 질문 해결 완료)",MSE Loss 용도,-0.00612421,0.0,0.006124210078269243
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지? (남은 답변: 모든 질문 해결 완료)","Multi-Class, Multi-Label 중 BCE 가 좋은 task",-0.0014632829,0.0,0.0014632828533649445
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지? (남은 답변: 모든 질문 해결 완료)",Multi-Label 에서 CE + Softmax 적용 문제점,0.00023493754,0.0,0.00023493754270020872
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지? (남은 답변: 모든 질문 해결 완료)",PEFT 방법 5가지,-0.0037017597,0.0,0.003701759735122323
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지? (남은 답변: 모든 질문 해결 완료)",거대 언어 모델 정의,-0.0020529458,0.0,0.0020529457833617926
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지? (남은 답변: 모든 질문 해결 완료)",마지막 할 말,-0.0037803177,0.0,0.0037803177256137133
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지? (남은 답변: 모든 질문 해결 완료)",면접 시작 인사,-0.0058927084,0.0,0.00589270843192935
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지? (남은 답변: 모든 질문 해결 완료)",면접 종료,-0.0060227374,0.0,0.0060227373614907265
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지? (남은 답변: 모든 질문 해결 완료)","인공지능, 머신러닝, 딥러닝 차이",-0.0011248176,0.0,0.0011248176451772451
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지? (남은 답변: 모든 질문 해결 완료)",잠시 휴식,-0.0014035245,0.0,0.0014035245403647423
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지? (남은 답변: 모든 질문 해결 완료)",확률 예측에서 MSE Loss 미 사용 이유,-0.0040690824,0.0,0.00406908243894577
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야? (남은 답변: PEFT 방법 5가지),BCE Loss 설명,-0.0025372785,0.0,0.0025372784584760666
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야? (남은 답변: PEFT 방법 5가지),LLM Fine-Tuning 의 PEFT,-0.00232431,0.0,0.0023243098985403776
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야? (남은 답변: PEFT 방법 5가지),LoRA,-0.0024322425,0.0,0.002432242501527071
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야? (남은 답변: PEFT 방법 5가지),LoRA 와 QLoRA 의 차이,-0.002987812,0.0,0.002987812040373683
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야? (남은 답변: PEFT 방법 5가지),Loss Function 관련 실무 경험,-0.0027050637,0.0,0.002705063670873642
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야? (남은 답변: PEFT 방법 5가지),Loss Function 예시,-0.0034183504,0.0,0.0034183503594249487
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야? (남은 답변: PEFT 방법 5가지),Loss Function 정의,-0.0028191607,0.0,0.002819160697981715
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야? (남은 답변: PEFT 방법 5가지),MBTI / 좋아하는 아이돌,-0.0028643676,0.0,0.002864367561414838
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야? (남은 답변: PEFT 방법 5가지),MSE Loss 설명,-0.0046565845,0.0,0.004656584467738867
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야? (남은 답변: PEFT 방법 5가지),MSE Loss 용도,-0.0016298692,0.0,0.001629869220778346
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야? (남은 답변: PEFT 방법 5가지),"Multi-Class, Multi-Label 중 BCE 가 좋은 task",-0.0003387014,0.0,0.0003387013857718557
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야? (남은 답변: PEFT 방법 5가지),Multi-Label 에서 CE + Softmax 적용 문제점,-0.0024977662,0.0,0.0024977661669254303
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야? (남은 답변: PEFT 방법 5가지),PEFT 방법 5가지,0.99871874,1.0,0.0012812614440917969
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야? (남은 답변: PEFT 방법 5가지),거대 언어 모델 정의,-0.0025006987,0.0,0.002500698668882251
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야? (남은 답변: PEFT 방법 5가지),마지막 할 말,-0.0040534697,0.0,0.004053469747304916
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야? (남은 답변: PEFT 방법 5가지),면접 시작 인사,-0.003914368,0.0,0.003914367873221636
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야? (남은 답변: PEFT 방법 5가지),면접 종료,-0.0053509306,0.0,0.005350930616259575
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야? (남은 답변: PEFT 방법 5가지),"인공지능, 머신러닝, 딥러닝 차이",-0.0071352273,0.0,0.00713522732257843
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야? (남은 답변: PEFT 방법 5가지),잠시 휴식,-0.0012024546,0.0,0.0012024545576423407
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야? (남은 답변: PEFT 방법 5가지),확률 예측에서 MSE Loss 미 사용 이유,-0.0043767346,0.0,0.004376734606921673
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야! (남은 답변: 모든 질문 해결 완료),BCE Loss 설명,-0.0029422478,0.0,0.0029422477819025517
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야! (남은 답변: 모든 질문 해결 완료),LLM Fine-Tuning 의 PEFT,-0.0020557204,0.0,0.0020557204261422157
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야! (남은 답변: 모든 질문 해결 완료),LoRA,-0.004223142,0.0,0.004223141819238663
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야! (남은 답변: 모든 질문 해결 완료),LoRA 와 QLoRA 의 차이,0.99845994,1.0,0.001540064811706543
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야! (남은 답변: 모든 질문 해결 완료),Loss Function 관련 실무 경험,-0.003087258,0.0,0.003087257966399193
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야! (남은 답변: 모든 질문 해결 완료),Loss Function 예시,-0.007647322,0.0,0.007647322025150061
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야! (남은 답변: 모든 질문 해결 완료),Loss Function 정의,-0.0020655813,0.0,0.0020655812695622444
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야! (남은 답변: 모든 질문 해결 완료),MBTI / 좋아하는 아이돌,0.001859243,0.0,0.0018592430278658867
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야! (남은 답변: 모든 질문 해결 완료),MSE Loss 설명,0.0031993487,0.0,0.0031993486918509007
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야! (남은 답변: 모든 질문 해결 완료),MSE Loss 용도,-0.0053820508,0.0,0.005382050760090351
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야! (남은 답변: 모든 질문 해결 완료),"Multi-Class, Multi-Label 중 BCE 가 좋은 task",-0.0012983064,0.0,0.0012983063934370875
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야! (남은 답변: 모든 질문 해결 완료),Multi-Label 에서 CE + Softmax 적용 문제점,-0.0020198084,0.0,0.0020198083948343992
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야! (남은 답변: 모든 질문 해결 완료),PEFT 방법 5가지,-0.0014318752,0.0,0.0014318751636892557
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야! (남은 답변: 모든 질문 해결 완료),거대 언어 모델 정의,0.00056517264,0.0,0.0005651726387441158
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야! (남은 답변: 모든 질문 해결 완료),마지막 할 말,0.0007340279,0.0,0.0007340278825722635
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야! (남은 답변: 모든 질문 해결 완료),면접 시작 인사,-0.003629382,0.0,0.003629382001236081
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야! (남은 답변: 모든 질문 해결 완료),면접 종료,0.00027553685,0.0,0.0002755368477664888
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야! (남은 답변: 모든 질문 해결 완료),"인공지능, 머신러닝, 딥러닝 차이",-0.0021409753,0.0,0.002140975324437022
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야! (남은 답변: 모든 질문 해결 완료),잠시 휴식,-0.004780795,0.0,0.004780794959515333
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야! (남은 답변: 모든 질문 해결 완료),확률 예측에서 MSE Loss 미 사용 이유,6.149779e-05,0.0,6.149779073894024e-05
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아? (남은 답변: LoRA),BCE Loss 설명,-0.0060111308,0.0,0.006011130753904581
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아? (남은 답변: LoRA),LLM Fine-Tuning 의 PEFT,-0.0022249757,0.0,0.0022249757312238216
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아? (남은 답변: LoRA),LoRA,0.9985602,1.0,0.001439809799194336
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아? (남은 답변: LoRA),LoRA 와 QLoRA 의 차이,-0.005770801,0.0,0.005770801100879908
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아? (남은 답변: LoRA),Loss Function 관련 실무 경험,-0.0033603476,0.0,0.0033603475894778967
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아? (남은 답변: LoRA),Loss Function 예시,-0.008011044,0.0,0.0080110440030694
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아? (남은 답변: LoRA),Loss Function 정의,-0.0014265315,0.0,0.0014265314675867558
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아? (남은 답변: LoRA),MBTI / 좋아하는 아이돌,0.0002247514,0.0,0.00022475140576716512
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아? (남은 답변: LoRA),MSE Loss 설명,-0.0030312596,0.0,0.003031259635463357
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아? (남은 답변: LoRA),MSE Loss 용도,-0.004386891,0.0,0.004386891145259142
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아? (남은 답변: LoRA),"Multi-Class, Multi-Label 중 BCE 가 좋은 task",-0.0026525634,0.0,0.002652563387528062
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아? (남은 답변: LoRA),Multi-Label 에서 CE + Softmax 적용 문제점,-0.002505583,0.0,0.0025055829901248217
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아? (남은 답변: LoRA),PEFT 방법 5가지,-0.0043621445,0.0,0.004362144507467747
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아? (남은 답변: LoRA),거대 언어 모델 정의,-0.00048693098,0.0,0.00048693097778595984
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아? (남은 답변: LoRA),마지막 할 말,-0.005238322,0.0,0.005238322075456381
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아? (남은 답변: LoRA),면접 시작 인사,-0.006336203,0.0,0.006336202844977379
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아? (남은 답변: LoRA),면접 종료,-0.0041998005,0.0,0.004199800547212362
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아? (남은 답변: LoRA),"인공지능, 머신러닝, 딥러닝 차이",-0.001051478,0.0,0.0010514779714867473
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아? (남은 답변: LoRA),잠시 휴식,-0.0007259259,0.0,0.000725925900042057
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아? (남은 답변: LoRA),확률 예측에서 MSE Loss 미 사용 이유,-0.004243695,0.0,0.004243695177137852
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야! (남은 답변: 모든 질문 해결 완료),BCE Loss 설명,-0.0027144155,0.0,0.0027144155465066433
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야! (남은 답변: 모든 질문 해결 완료),LLM Fine-Tuning 의 PEFT,-0.0016685405,0.0,0.0016685405280441046
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야! (남은 답변: 모든 질문 해결 완료),LoRA,-0.0034283686,0.0,0.0034283685963600874
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야! (남은 답변: 모든 질문 해결 완료),LoRA 와 QLoRA 의 차이,0.9982803,1.0,0.0017197132110595703
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야! (남은 답변: 모든 질문 해결 완료),Loss Function 관련 실무 경험,-0.005787207,0.0,0.005787206813693047
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야! (남은 답변: 모든 질문 해결 완료),Loss Function 예시,-0.006711519,0.0,0.006711518857628107
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야! (남은 답변: 모든 질문 해결 완료),Loss Function 정의,0.0009412736,0.0,0.0009412735817022622
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야! (남은 답변: 모든 질문 해결 완료),MBTI / 좋아하는 아이돌,0.00095005933,0.0,0.0009500593296252191
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야! (남은 답변: 모든 질문 해결 완료),MSE Loss 설명,0.002677171,0.0,0.0026771710254251957
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야! (남은 답변: 모든 질문 해결 완료),MSE Loss 용도,-0.0067716707,0.0,0.006771670654416084
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야! (남은 답변: 모든 질문 해결 완료),"Multi-Class, Multi-Label 중 BCE 가 좋은 task",-0.0005630877,0.0,0.0005630876985378563
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야! (남은 답변: 모든 질문 해결 완료),Multi-Label 에서 CE + Softmax 적용 문제점,-0.00011328984,0.0,0.00011328983964631334
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야! (남은 답변: 모든 질문 해결 완료),PEFT 방법 5가지,-0.002759173,0.0,0.002759173046797514
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야! (남은 답변: 모든 질문 해결 완료),거대 언어 모델 정의,-0.00020055682,0.0,0.0002005568239837885
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야! (남은 답변: 모든 질문 해결 완료),마지막 할 말,0.0006038831,0.0,0.0006038831197656691
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야! (남은 답변: 모든 질문 해결 완료),면접 시작 인사,-0.0045159278,0.0,0.004515927750617266
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야! (남은 답변: 모든 질문 해결 완료),면접 종료,-0.0033868651,0.0,0.003386865137144923
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야! (남은 답변: 모든 질문 해결 완료),"인공지능, 머신러닝, 딥러닝 차이",-0.0030229227,0.0,0.003022922668606043
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야! (남은 답변: 모든 질문 해결 완료),잠시 휴식,-0.0014632279,0.0,0.0014632279053330421
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야! (남은 답변: 모든 질문 해결 완료),확률 예측에서 MSE Loss 미 사용 이유,0.00036683626,0.0,0.0003668362624011934
LoRA -> 무슨 OOM 없앤다는 것 같은데 (남은 답변: LoRA),BCE Loss 설명,-0.0053400146,0.0,0.005340014584362507
LoRA -> 무슨 OOM 없앤다는 것 같은데 (남은 답변: LoRA),LLM Fine-Tuning 의 PEFT,-0.002184501,0.0,0.0021845009177923203
LoRA -> 무슨 OOM 없앤다는 것 같은데 (남은 답변: LoRA),LoRA,0.9987124,1.0,0.0012875795364379883
LoRA -> 무슨 OOM 없앤다는 것 같은데 (남은 답변: LoRA),LoRA 와 QLoRA 의 차이,-0.005740373,0.0,0.005740372929722071
LoRA -> 무슨 OOM 없앤다는 것 같은데 (남은 답변: LoRA),Loss Function 관련 실무 경험,-0.0037523932,0.0,0.0037523931823670864
LoRA -> 무슨 OOM 없앤다는 것 같은데 (남은 답변: LoRA),Loss Function 예시,-0.009428096,0.0,0.009428096003830433
LoRA -> 무슨 OOM 없앤다는 것 같은데 (남은 답변: LoRA),Loss Function 정의,-0.0012647639,0.0,0.0012647638795897365
LoRA -> 무슨 OOM 없앤다는 것 같은데 (남은 답변: LoRA),MBTI / 좋아하는 아이돌,0.00065907673,0.0,0.0006590767297893763
LoRA -> 무슨 OOM 없앤다는 것 같은데 (남은 답변: LoRA),MSE Loss 설명,-0.0020952947,0.0,0.002095294650644064
LoRA -> 무슨 OOM 없앤다는 것 같은데 (남은 답변: LoRA),MSE Loss 용도,-0.00565293,0.0,0.005652930121868849
LoRA -> 무슨 OOM 없앤다는 것 같은데 (남은 답변: LoRA),"Multi-Class, Multi-Label 중 BCE 가 좋은 task",-0.0019437773,0.0,0.0019437773153185844
LoRA -> 무슨 OOM 없앤다는 것 같은데 (남은 답변: LoRA),Multi-Label 에서 CE + Softmax 적용 문제점,-0.0016231166,0.0,0.0016231165500357747
LoRA -> 무슨 OOM 없앤다는 것 같은데 (남은 답변: LoRA),PEFT 방법 5가지,-0.004087789,0.0,0.004087788984179497
LoRA -> 무슨 OOM 없앤다는 것 같은데 (남은 답변: LoRA),거대 언어 모델 정의,-0.0014038691,0.0,0.00140386912971735
LoRA -> 무슨 OOM 없앤다는 것 같은데 (남은 답변: LoRA),마지막 할 말,-0.0051033706,0.0,0.005103370640426874
LoRA -> 무슨 OOM 없앤다는 것 같은데 (남은 답변: LoRA),면접 시작 인사,-0.0067331847,0.0,0.006733184680342674
LoRA -> 무슨 OOM 없앤다는 것 같은데 (남은 답변: LoRA),면접 종료,-0.0043402924,0.0,0.0043402924202382565
LoRA -> 무슨 OOM 없앤다는 것 같은데 (남은 답변: LoRA),"인공지능, 머신러닝, 딥러닝 차이",-0.0013317926,0.0,0.0013317925622686744
LoRA -> 무슨 OOM 없앤다는 것 같은데 (남은 답변: LoRA),잠시 휴식,-0.00105217,0.0,0.0010521699441596866
LoRA -> 무슨 OOM 없앤다는 것 같은데 (남은 답변: LoRA),확률 예측에서 MSE Loss 미 사용 이유,-0.004328048,0.0,0.0043280478566884995
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야! (남은 답변: 모든 질문 해결 완료),BCE Loss 설명,-0.0063681933,0.0,0.006368193309754133
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야! (남은 답변: 모든 질문 해결 완료),LLM Fine-Tuning 의 PEFT,-0.0010288962,0.0,0.0010288961930200458
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야! (남은 답변: 모든 질문 해결 완료),LoRA,-0.0064635435,0.0,0.006463543511927128
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야! (남은 답변: 모든 질문 해결 완료),LoRA 와 QLoRA 의 차이,0.0024413657,0.0,0.0024413657374680042
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야! (남은 답변: 모든 질문 해결 완료),Loss Function 관련 실무 경험,-0.0014325901,0.0,0.001432590070180595
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야! (남은 답변: 모든 질문 해결 완료),Loss Function 예시,-0.00621785,0.0,0.0062178499065339565
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야! (남은 답변: 모든 질문 해결 완료),Loss Function 정의,-0.0039657103,0.0,0.003965710289776325
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야! (남은 답변: 모든 질문 해결 완료),MBTI / 좋아하는 아이돌,-0.006072419,0.0,0.0060724192298948765
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야! (남은 답변: 모든 질문 해결 완료),MSE Loss 설명,-0.004751531,0.0,0.004751530941575766
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야! (남은 답변: 모든 질문 해결 완료),MSE Loss 용도,0.004128906,0.0,0.0041289059445261955
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야! (남은 답변: 모든 질문 해결 완료),"Multi-Class, Multi-Label 중 BCE 가 좋은 task",-0.0020394777,0.0,0.0020394776947796345
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야! (남은 답변: 모든 질문 해결 완료),Multi-Label 에서 CE + Softmax 적용 문제점,-0.009493429,0.0,0.009493429213762283
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야! (남은 답변: 모든 질문 해결 완료),PEFT 방법 5가지,-0.006816548,0.0,0.006816547829657793
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야! (남은 답변: 모든 질문 해결 완료),거대 언어 모델 정의,-0.0028301496,0.0,0.002830149605870247
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야! (남은 답변: 모든 질문 해결 완료),마지막 할 말,0.9942449,1.0,0.005755126476287842
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야! (남은 답변: 모든 질문 해결 완료),면접 시작 인사,-0.002799151,0.0,0.002799150999635458
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야! (남은 답변: 모든 질문 해결 완료),면접 종료,-0.00086118106,0.0,0.0008611810626462102
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야! (남은 답변: 모든 질문 해결 완료),"인공지능, 머신러닝, 딥러닝 차이",-0.007937785,0.0,0.007937785238027573
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야! (남은 답변: 모든 질문 해결 완료),잠시 휴식,-0.0049823546,0.0,0.004982354585081339
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야! (남은 답변: 모든 질문 해결 완료),확률 예측에서 MSE Loss 미 사용 이유,-0.00591347,0.0,0.0059134699404239655
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데 (남은 답변: LoRA 와 QLoRA 의 차이),BCE Loss 설명,-0.0036475249,0.0,0.0036475248634815216
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데 (남은 답변: LoRA 와 QLoRA 의 차이),LLM Fine-Tuning 의 PEFT,-0.0003822553,0.0,0.00038225529715418816
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데 (남은 답변: LoRA 와 QLoRA 의 차이),LoRA,-0.005144375,0.0,0.0051443749107420444
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데 (남은 답변: LoRA 와 QLoRA 의 차이),LoRA 와 QLoRA 의 차이,0.9990129,1.0,0.0009871125221252441
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데 (남은 답변: LoRA 와 QLoRA 의 차이),Loss Function 관련 실무 경험,-0.0051891967,0.0,0.005189196672290564
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데 (남은 답변: LoRA 와 QLoRA 의 차이),Loss Function 예시,-0.005808991,0.0,0.00580899091437459
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데 (남은 답변: LoRA 와 QLoRA 의 차이),Loss Function 정의,0.00018292958,0.0,0.00018292957975063473
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데 (남은 답변: LoRA 와 QLoRA 의 차이),MBTI / 좋아하는 아이돌,0.0002846953,0.0,0.00028469529934227467
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데 (남은 답변: LoRA 와 QLoRA 의 차이),MSE Loss 설명,0.0031871006,0.0,0.003187100635841489
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데 (남은 답변: LoRA 와 QLoRA 의 차이),MSE Loss 용도,-0.0018523668,0.0,0.001852366840466857
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데 (남은 답변: LoRA 와 QLoRA 의 차이),"Multi-Class, Multi-Label 중 BCE 가 좋은 task",-0.00078837154,0.0,0.0007883715443313122
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데 (남은 답변: LoRA 와 QLoRA 의 차이),Multi-Label 에서 CE + Softmax 적용 문제점,-0.0026648142,0.0,0.0026648142375051975
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데 (남은 답변: LoRA 와 QLoRA 의 차이),PEFT 방법 5가지,-0.0031567311,0.0,0.0031567311380058527
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데 (남은 답변: LoRA 와 QLoRA 의 차이),거대 언어 모델 정의,-0.0013874053,0.0,0.0013874053256586194
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데 (남은 답변: LoRA 와 QLoRA 의 차이),마지막 할 말,0.0014166298,0.0,0.0014166297623887658
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데 (남은 답변: LoRA 와 QLoRA 의 차이),면접 시작 인사,-0.0028446433,0.0,0.0028446433134377003
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데 (남은 답변: LoRA 와 QLoRA 의 차이),면접 종료,-0.0021621895,0.0,0.002162189455702901
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데 (남은 답변: LoRA 와 QLoRA 의 차이),"인공지능, 머신러닝, 딥러닝 차이",-0.0038381584,0.0,0.003838158445432782
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데 (남은 답변: LoRA 와 QLoRA 의 차이),잠시 휴식,-0.0025093176,0.0,0.0025093175936490297
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데 (남은 답변: LoRA 와 QLoRA 의 차이),확률 예측에서 MSE Loss 미 사용 이유,5.6473327e-05,0.0,5.6473327276762575e-05
마지막 할 말 -> 로라야 정말 고마워! (남은 답변: 마지막 할 말),BCE Loss 설명,-0.0029919553,0.0,0.0029919552616775036
마지막 할 말 -> 로라야 정말 고마워! (남은 답변: 마지막 할 말),LLM Fine-Tuning 의 PEFT,-0.003088217,0.0,0.003088216995820403
마지막 할 말 -> 로라야 정말 고마워! (남은 답변: 마지막 할 말),LoRA,-0.0032009492,0.0,0.0032009491696953773
마지막 할 말 -> 로라야 정말 고마워! (남은 답변: 마지막 할 말),LoRA 와 QLoRA 의 차이,-0.0005401726,0.0,0.0005401726230047643
마지막 할 말 -> 로라야 정말 고마워! (남은 답변: 마지막 할 말),Loss Function 관련 실무 경험,-0.003779535,0.0,0.003779534948989749
마지막 할 말 -> 로라야 정말 고마워! (남은 답변: 마지막 할 말),Loss Function 예시,-0.008507103,0.0,0.008507102727890015
마지막 할 말 -> 로라야 정말 고마워! (남은 답변: 마지막 할 말),Loss Function 정의,-0.0012219198,0.0,0.0012219197815284133
마지막 할 말 -> 로라야 정말 고마워! (남은 답변: 마지막 할 말),MBTI / 좋아하는 아이돌,-0.0041173585,0.0,0.004117358475923538
마지막 할 말 -> 로라야 정말 고마워! (남은 답변: 마지막 할 말),MSE Loss 설명,-0.003252439,0.0,0.0032524389680474997
마지막 할 말 -> 로라야 정말 고마워! (남은 답변: 마지막 할 말),MSE Loss 용도,-0.0046417676,0.0,0.004641767591238022
마지막 할 말 -> 로라야 정말 고마워! (남은 답변: 마지막 할 말),"Multi-Class, Multi-Label 중 BCE 가 좋은 task",-0.004659223,0.0,0.0046592229045927525
마지막 할 말 -> 로라야 정말 고마워! (남은 답변: 마지막 할 말),Multi-Label 에서 CE + Softmax 적용 문제점,-0.0022454136,0.0,0.0022454136051237583
마지막 할 말 -> 로라야 정말 고마워! (남은 답변: 마지막 할 말),PEFT 방법 5가지,-0.0040902654,0.0,0.004090265370905399
마지막 할 말 -> 로라야 정말 고마워! (남은 답변: 마지막 할 말),거대 언어 모델 정의,-0.0028036968,0.0,0.002803696785122156
마지막 할 말 -> 로라야 정말 고마워! (남은 답변: 마지막 할 말),마지막 할 말,-0.0022853666,0.0,0.0022853666450828314
마지막 할 말 -> 로라야 정말 고마워! (남은 답변: 마지막 할 말),면접 시작 인사,-0.002328239,0.0,0.0023282389156520367
마지막 할 말 -> 로라야 정말 고마워! (남은 답변: 마지막 할 말),면접 종료,0.9984924,1.0,0.001507580280303955
마지막 할 말 -> 로라야 정말 고마워! (남은 답변: 마지막 할 말),"인공지능, 머신러닝, 딥러닝 차이",-0.005608644,0.0,0.0056086438708007336
마지막 할 말 -> 로라야 정말 고마워! (남은 답변: 마지막 할 말),잠시 휴식,-0.0055592824,0.0,0.005559282377362251
마지막 할 말 -> 로라야 정말 고마워! (남은 답변: 마지막 할 말),확률 예측에서 MSE Loss 미 사용 이유,-0.0032624968,0.0,0.0032624967861920595
마지막 할 말 -> 로라야 사랑해 (남은 답변: 마지막 할 말),BCE Loss 설명,-0.0028142352,0.0,0.0028142351657152176
마지막 할 말 -> 로라야 사랑해 (남은 답변: 마지막 할 말),LLM Fine-Tuning 의 PEFT,-0.0026303416,0.0,0.0026303415652364492
마지막 할 말 -> 로라야 사랑해 (남은 답변: 마지막 할 말),LoRA,-0.002752923,0.0,0.0027529229409992695
마지막 할 말 -> 로라야 사랑해 (남은 답변: 마지막 할 말),LoRA 와 QLoRA 의 차이,0.00018411445,0.0,0.00018411445489618927
마지막 할 말 -> 로라야 사랑해 (남은 답변: 마지막 할 말),Loss Function 관련 실무 경험,-0.0039482675,0.0,0.003948267549276352
마지막 할 말 -> 로라야 사랑해 (남은 답변: 마지막 할 말),Loss Function 예시,-0.008483483,0.0,0.008483483456075191
마지막 할 말 -> 로라야 사랑해 (남은 답변: 마지막 할 말),Loss Function 정의,-0.0014075997,0.0,0.0014075996587052941
마지막 할 말 -> 로라야 사랑해 (남은 답변: 마지막 할 말),MBTI / 좋아하는 아이돌,-0.004209385,0.0,0.00420938478782773
마지막 할 말 -> 로라야 사랑해 (남은 답변: 마지막 할 말),MSE Loss 설명,-0.0035761404,0.0,0.003576140385121107
마지막 할 말 -> 로라야 사랑해 (남은 답변: 마지막 할 말),MSE Loss 용도,-0.0047732084,0.0,0.0047732084058225155
마지막 할 말 -> 로라야 사랑해 (남은 답변: 마지막 할 말),"Multi-Class, Multi-Label 중 BCE 가 좋은 task",-0.0049251947,0.0,0.004925194662064314
마지막 할 말 -> 로라야 사랑해 (남은 답변: 마지막 할 말),Multi-Label 에서 CE + Softmax 적용 문제점,-0.0029343748,0.0,0.0029343748465180397
마지막 할 말 -> 로라야 사랑해 (남은 답변: 마지막 할 말),PEFT 방법 5가지,-0.003798809,0.0,0.00379880890250206
마지막 할 말 -> 로라야 사랑해 (남은 답변: 마지막 할 말),거대 언어 모델 정의,-0.0020680686,0.0,0.0020680685993283987
마지막 할 말 -> 로라야 사랑해 (남은 답변: 마지막 할 말),마지막 할 말,-0.0020422,0.0,0.0020421999506652355
마지막 할 말 -> 로라야 사랑해 (남은 답변: 마지막 할 말),면접 시작 인사,-0.0023403494,0.0,0.002340349368751049
마지막 할 말 -> 로라야 사랑해 (남은 답변: 마지막 할 말),면접 종료,0.9985255,1.0,0.0014744997024536133
마지막 할 말 -> 로라야 사랑해 (남은 답변: 마지막 할 말),"인공지능, 머신러닝, 딥러닝 차이",-0.0058482853,0.0,0.005848285276442766
마지막 할 말 -> 로라야 사랑해 (남은 답변: 마지막 할 말),잠시 휴식,-0.005394638,0.0,0.005394638050347567
마지막 할 말 -> 로라야 사랑해 (남은 답변: 마지막 할 말),확률 예측에서 MSE Loss 미 사용 이유,-0.0038775003,0.0,0.003877500304952264
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어 (남은 답변: 마지막 할 말),BCE Loss 설명,-0.0033143198,0.0,0.0033143197651952505
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어 (남은 답변: 마지막 할 말),LLM Fine-Tuning 의 PEFT,-0.0021037385,0.0,0.002103738486766815
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어 (남은 답변: 마지막 할 말),LoRA,-0.0030655942,0.0,0.0030655942391604185
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어 (남은 답변: 마지막 할 말),LoRA 와 QLoRA 의 차이,-0.0005612417,0.0,0.0005612417007796466
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어 (남은 답변: 마지막 할 말),Loss Function 관련 실무 경험,-0.0036347136,0.0,0.003634713590145111
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어 (남은 답변: 마지막 할 말),Loss Function 예시,-0.00887815,0.0,0.008878150023519993
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어 (남은 답변: 마지막 할 말),Loss Function 정의,-0.0023496584,0.0,0.002349658403545618
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어 (남은 답변: 마지막 할 말),MBTI / 좋아하는 아이돌,-0.0048617674,0.0,0.004861767403781414
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어 (남은 답변: 마지막 할 말),MSE Loss 설명,-0.0037593357,0.0,0.0037593357264995575
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어 (남은 답변: 마지막 할 말),MSE Loss 용도,-0.003817774,0.0,0.0038177738897502422
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어 (남은 답변: 마지막 할 말),"Multi-Class, Multi-Label 중 BCE 가 좋은 task",-0.0047306176,0.0,0.004730617627501488
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어 (남은 답변: 마지막 할 말),Multi-Label 에서 CE + Softmax 적용 문제점,-0.0034214226,0.0,0.0034214225597679615
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어 (남은 답변: 마지막 할 말),PEFT 방법 5가지,-0.0043461327,0.0,0.00434613274410367
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어 (남은 답변: 마지막 할 말),거대 언어 모델 정의,-0.003041418,0.0,0.0030414180364459753
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어 (남은 답변: 마지막 할 말),마지막 할 말,-0.002012464,0.0,0.0020124639850109816
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어 (남은 답변: 마지막 할 말),면접 시작 인사,-0.002964335,0.0,0.0029643350280821323
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어 (남은 답변: 마지막 할 말),면접 종료,0.998116,1.0,0.0018839836120605469
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어 (남은 답변: 마지막 할 말),"인공지능, 머신러닝, 딥러닝 차이",-0.0055108103,0.0,0.005510810296982527
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어 (남은 답변: 마지막 할 말),잠시 휴식,-0.006432798,0.0,0.006432798225432634
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어 (남은 답변: 마지막 할 말),확률 예측에서 MSE Loss 미 사용 이유,-0.0033114625,0.0,0.0033114624675363302
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워 (남은 답변: 마지막 할 말),BCE Loss 설명,-0.0037084091,0.0,0.003708409145474434
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워 (남은 답변: 마지막 할 말),LLM Fine-Tuning 의 PEFT,-0.003191008,0.0,0.003191007999703288
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워 (남은 답변: 마지막 할 말),LoRA,-0.004062552,0.0,0.0040625520050525665
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워 (남은 답변: 마지막 할 말),LoRA 와 QLoRA 의 차이,0.00026498947,0.0,0.0002649894740898162
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워 (남은 답변: 마지막 할 말),Loss Function 관련 실무 경험,-0.0031414942,0.0,0.003141494235023856
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워 (남은 답변: 마지막 할 말),Loss Function 예시,-0.009235256,0.0,0.009235256351530552
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워 (남은 답변: 마지막 할 말),Loss Function 정의,-0.0016023441,0.0,0.0016023440985009074
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워 (남은 답변: 마지막 할 말),MBTI / 좋아하는 아이돌,-0.0050962963,0.0,0.005096296314150095
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워 (남은 답변: 마지막 할 말),MSE Loss 설명,-0.0028974863,0.0,0.002897486323490739
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워 (남은 답변: 마지막 할 말),MSE Loss 용도,-0.004108606,0.0,0.004108605906367302
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워 (남은 답변: 마지막 할 말),"Multi-Class, Multi-Label 중 BCE 가 좋은 task",-0.0051165004,0.0,0.005116500426083803
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워 (남은 답변: 마지막 할 말),Multi-Label 에서 CE + Softmax 적용 문제점,-0.0023187005,0.0,0.0023187005426734686
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워 (남은 답변: 마지막 할 말),PEFT 방법 5가지,-0.0030765245,0.0,0.0030765244737267494
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워 (남은 답변: 마지막 할 말),거대 언어 모델 정의,-0.003442699,0.0,0.0034426990896463394
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워 (남은 답변: 마지막 할 말),마지막 할 말,-0.002277651,0.0,0.0022776511032134295
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워 (남은 답변: 마지막 할 말),면접 시작 인사,-0.0012101368,0.0,0.0012101368047297
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워 (남은 답변: 마지막 할 말),면접 종료,0.9981398,1.0,0.001860201358795166
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워 (남은 답변: 마지막 할 말),"인공지능, 머신러닝, 딥러닝 차이",-0.0060146702,0.0,0.006014670245349407
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워 (남은 답변: 마지막 할 말),잠시 휴식,-0.006288175,0.0,0.006288175005465746
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워 (남은 답변: 마지막 할 말),확률 예측에서 MSE Loss 미 사용 이유,-0.0032131088,0.0,0.0032131087500602007
