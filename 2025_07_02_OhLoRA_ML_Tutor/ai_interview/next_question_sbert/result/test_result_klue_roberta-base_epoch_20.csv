input_part,next_question,predicted_similarity,ground_truth_similarity,absolute_error
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서? (남은 답변: 면접 시작 인사),BCE Loss 설명,-0.0027838359,0.0,0.0027838358655571938
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서? (남은 답변: 면접 시작 인사),LLM Fine-Tuning 의 PEFT,0.0033293217,0.0,0.0033293217420578003
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서? (남은 답변: 면접 시작 인사),LoRA,-0.0013109599,0.0,0.0013109599240124226
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서? (남은 답변: 면접 시작 인사),LoRA 와 QLoRA 의 차이,-0.0023073652,0.0,0.00230736518278718
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서? (남은 답변: 면접 시작 인사),Loss Function 관련 실무 경험,-0.00956,0.0,0.009560000151395798
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서? (남은 답변: 면접 시작 인사),Loss Function 예시,0.00082469726,0.0,0.0008246972574852407
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서? (남은 답변: 면접 시작 인사),Loss Function 정의,0.0053380975,0.0,0.005338097456842661
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서? (남은 답변: 면접 시작 인사),MBTI / 좋아하는 아이돌,0.0013569361,0.0,0.0013569360598921776
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서? (남은 답변: 면접 시작 인사),MSE Loss 설명,-0.01584427,0.0,0.015844270586967468
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서? (남은 답변: 면접 시작 인사),MSE Loss 용도,-0.0070044566,0.0,0.007004456594586372
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서? (남은 답변: 면접 시작 인사),"Multi-Class, Multi-Label 중 BCE 가 좋은 task",0.0015821297,0.0,0.0015821297420188785
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서? (남은 답변: 면접 시작 인사),Multi-Label 에서 CE + Softmax 적용 문제점,-0.00676179,0.0,0.006761789787560701
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서? (남은 답변: 면접 시작 인사),PEFT 방법 5가지,-0.003808454,0.0,0.0038084539119154215
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서? (남은 답변: 면접 시작 인사),거대 언어 모델 정의,-0.0063175294,0.0,0.006317529361695051
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서? (남은 답변: 면접 시작 인사),마지막 할 말,-0.0018327183,0.0,0.001832718262448907
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서? (남은 답변: 면접 시작 인사),면접 시작 인사,0.9873478,1.0,0.012652218341827393
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서? (남은 답변: 면접 시작 인사),면접 종료,-0.022998065,0.0,0.022998064756393433
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서? (남은 답변: 면접 시작 인사),"인공지능, 머신러닝, 딥러닝 차이",-0.0010831648,0.0,0.001083164825104177
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서? (남은 답변: 면접 시작 인사),잠시 휴식,-0.018832238,0.0,0.018832238391041756
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서? (남은 답변: 면접 시작 인사),확률 예측에서 MSE Loss 미 사용 이유,-0.0022141263,0.0,0.0022141262888908386
면접 시작 인사 -> 로라야 안녕 정말 반가워 (남은 답변: 면접 시작 인사),BCE Loss 설명,-0.0054491074,0.0,0.0054491073824465275
면접 시작 인사 -> 로라야 안녕 정말 반가워 (남은 답변: 면접 시작 인사),LLM Fine-Tuning 의 PEFT,0.0016652244,0.0,0.0016652244376018643
면접 시작 인사 -> 로라야 안녕 정말 반가워 (남은 답변: 면접 시작 인사),LoRA,-0.0005857193,0.0,0.0005857193027622998
면접 시작 인사 -> 로라야 안녕 정말 반가워 (남은 답변: 면접 시작 인사),LoRA 와 QLoRA 의 차이,0.00017204841,0.0,0.00017204841424245387
면접 시작 인사 -> 로라야 안녕 정말 반가워 (남은 답변: 면접 시작 인사),Loss Function 관련 실무 경험,-0.003964183,0.0,0.003964182920753956
면접 시작 인사 -> 로라야 안녕 정말 반가워 (남은 답변: 면접 시작 인사),Loss Function 예시,-0.0031696798,0.0,0.003169679781422019
면접 시작 인사 -> 로라야 안녕 정말 반가워 (남은 답변: 면접 시작 인사),Loss Function 정의,0.0016779425,0.0,0.0016779424622654915
면접 시작 인사 -> 로라야 안녕 정말 반가워 (남은 답변: 면접 시작 인사),MBTI / 좋아하는 아이돌,-0.0048843715,0.0,0.004884371533989906
면접 시작 인사 -> 로라야 안녕 정말 반가워 (남은 답변: 면접 시작 인사),MSE Loss 설명,-0.0039137825,0.0,0.00391378253698349
면접 시작 인사 -> 로라야 안녕 정말 반가워 (남은 답변: 면접 시작 인사),MSE Loss 용도,-0.004224885,0.0,0.004224884789437056
면접 시작 인사 -> 로라야 안녕 정말 반가워 (남은 답변: 면접 시작 인사),"Multi-Class, Multi-Label 중 BCE 가 좋은 task",0.0083622,0.0,0.008362200111150742
면접 시작 인사 -> 로라야 안녕 정말 반가워 (남은 답변: 면접 시작 인사),Multi-Label 에서 CE + Softmax 적용 문제점,-0.009184769,0.0,0.009184769354760647
면접 시작 인사 -> 로라야 안녕 정말 반가워 (남은 답변: 면접 시작 인사),PEFT 방법 5가지,-0.010400575,0.0,0.010400574654340744
면접 시작 인사 -> 로라야 안녕 정말 반가워 (남은 답변: 면접 시작 인사),거대 언어 모델 정의,-0.003979826,0.0,0.003979825880378485
면접 시작 인사 -> 로라야 안녕 정말 반가워 (남은 답변: 면접 시작 인사),마지막 할 말,-0.0026690986,0.0,0.0026690985541790724
면접 시작 인사 -> 로라야 안녕 정말 반가워 (남은 답변: 면접 시작 인사),면접 시작 인사,0.98715985,1.0,0.0128401517868042
면접 시작 인사 -> 로라야 안녕 정말 반가워 (남은 답변: 면접 시작 인사),면접 종료,-0.011615264,0.0,0.011615264229476452
면접 시작 인사 -> 로라야 안녕 정말 반가워 (남은 답변: 면접 시작 인사),"인공지능, 머신러닝, 딥러닝 차이",-0.0005002299,0.0,0.0005002298858016729
면접 시작 인사 -> 로라야 안녕 정말 반가워 (남은 답변: 면접 시작 인사),잠시 휴식,-0.004890272,0.0,0.004890271928161383
면접 시작 인사 -> 로라야 안녕 정말 반가워 (남은 답변: 면접 시작 인사),확률 예측에서 MSE Loss 미 사용 이유,-0.008688726,0.0,0.008688726462423801
면접 시작 인사 -> 로라야 그럼 네가 면접관이야? (남은 답변: 면접 시작 인사),BCE Loss 설명,-0.00383728,0.0,0.003837279975414276
면접 시작 인사 -> 로라야 그럼 네가 면접관이야? (남은 답변: 면접 시작 인사),LLM Fine-Tuning 의 PEFT,0.0040937793,0.0,0.0040937792509794235
면접 시작 인사 -> 로라야 그럼 네가 면접관이야? (남은 답변: 면접 시작 인사),LoRA,0.0045722104,0.0,0.004572210367769003
면접 시작 인사 -> 로라야 그럼 네가 면접관이야? (남은 답변: 면접 시작 인사),LoRA 와 QLoRA 의 차이,-0.0021179449,0.0,0.0021179448813199997
면접 시작 인사 -> 로라야 그럼 네가 면접관이야? (남은 답변: 면접 시작 인사),Loss Function 관련 실무 경험,-0.004023296,0.0,0.0040232958272099495
면접 시작 인사 -> 로라야 그럼 네가 면접관이야? (남은 답변: 면접 시작 인사),Loss Function 예시,-0.002241192,0.0,0.0022411919198930264
면접 시작 인사 -> 로라야 그럼 네가 면접관이야? (남은 답변: 면접 시작 인사),Loss Function 정의,0.0019111129,0.0,0.001911112922243774
면접 시작 인사 -> 로라야 그럼 네가 면접관이야? (남은 답변: 면접 시작 인사),MBTI / 좋아하는 아이돌,-0.0023843718,0.0,0.0023843718226999044
면접 시작 인사 -> 로라야 그럼 네가 면접관이야? (남은 답변: 면접 시작 인사),MSE Loss 설명,-0.010129497,0.0,0.01012949738651514
면접 시작 인사 -> 로라야 그럼 네가 면접관이야? (남은 답변: 면접 시작 인사),MSE Loss 용도,-0.0037443293,0.0,0.0037443293258547783
면접 시작 인사 -> 로라야 그럼 네가 면접관이야? (남은 답변: 면접 시작 인사),"Multi-Class, Multi-Label 중 BCE 가 좋은 task",0.0039341394,0.0,0.003934139385819435
면접 시작 인사 -> 로라야 그럼 네가 면접관이야? (남은 답변: 면접 시작 인사),Multi-Label 에서 CE + Softmax 적용 문제점,-0.008345696,0.0,0.00834569614380598
면접 시작 인사 -> 로라야 그럼 네가 면접관이야? (남은 답변: 면접 시작 인사),PEFT 방법 5가지,-0.007851914,0.0,0.007851913571357727
면접 시작 인사 -> 로라야 그럼 네가 면접관이야? (남은 답변: 면접 시작 인사),거대 언어 모델 정의,-0.003475686,0.0,0.0034756860695779324
면접 시작 인사 -> 로라야 그럼 네가 면접관이야? (남은 답변: 면접 시작 인사),마지막 할 말,-0.008407631,0.0,0.00840763095766306
면접 시작 인사 -> 로라야 그럼 네가 면접관이야? (남은 답변: 면접 시작 인사),면접 시작 인사,0.98567903,1.0,0.014320969581604004
면접 시작 인사 -> 로라야 그럼 네가 면접관이야? (남은 답변: 면접 시작 인사),면접 종료,-0.021191882,0.0,0.021191881969571114
면접 시작 인사 -> 로라야 그럼 네가 면접관이야? (남은 답변: 면접 시작 인사),"인공지능, 머신러닝, 딥러닝 차이",0.0054376214,0.0,0.005437621381133795
면접 시작 인사 -> 로라야 그럼 네가 면접관이야? (남은 답변: 면접 시작 인사),잠시 휴식,-0.014995766,0.0,0.014995765872299671
면접 시작 인사 -> 로라야 그럼 네가 면접관이야? (남은 답변: 면접 시작 인사),확률 예측에서 MSE Loss 미 사용 이유,-0.0069183046,0.0,0.0069183045998215675
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까 (남은 답변: 면접 시작 인사),BCE Loss 설명,-0.009853891,0.0,0.009853891097009182
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까 (남은 답변: 면접 시작 인사),LLM Fine-Tuning 의 PEFT,0.007340555,0.0,0.00734055507928133
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까 (남은 답변: 면접 시작 인사),LoRA,0.009133658,0.0,0.00913365837186575
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까 (남은 답변: 면접 시작 인사),LoRA 와 QLoRA 의 차이,0.0042828037,0.0,0.004282803740352392
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까 (남은 답변: 면접 시작 인사),Loss Function 관련 실무 경험,-0.013641217,0.0,0.013641216792166233
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까 (남은 답변: 면접 시작 인사),Loss Function 예시,-0.027511973,0.0,0.027511972934007645
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까 (남은 답변: 면접 시작 인사),Loss Function 정의,0.0065814643,0.0,0.006581464316695929
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까 (남은 답변: 면접 시작 인사),MBTI / 좋아하는 아이돌,-0.015072804,0.0,0.015072803944349289
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까 (남은 답변: 면접 시작 인사),MSE Loss 설명,-0.02481763,0.0,0.024817630648612976
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까 (남은 답변: 면접 시작 인사),MSE Loss 용도,-0.022656046,0.0,0.022656045854091644
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까 (남은 답변: 면접 시작 인사),"Multi-Class, Multi-Label 중 BCE 가 좋은 task",-0.017207049,0.0,0.01720704883337021
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까 (남은 답변: 면접 시작 인사),Multi-Label 에서 CE + Softmax 적용 문제점,0.0012697163,0.0,0.001269716303795576
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까 (남은 답변: 면접 시작 인사),PEFT 방법 5가지,0.0011215282,0.0,0.0011215282138437033
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까 (남은 답변: 면접 시작 인사),거대 언어 모델 정의,-0.002348442,0.0,0.0023484420962631702
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까 (남은 답변: 면접 시작 인사),마지막 할 말,-0.023812035,0.0,0.023812035098671913
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까 (남은 답변: 면접 시작 인사),면접 시작 인사,0.9371486,1.0,0.0628514289855957
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까 (남은 답변: 면접 시작 인사),면접 종료,-0.01568063,0.0,0.015680629760026932
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까 (남은 답변: 면접 시작 인사),"인공지능, 머신러닝, 딥러닝 차이",0.1378298,0.0,0.13782979547977448
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까 (남은 답변: 면접 시작 인사),잠시 휴식,-0.015308147,0.0,0.015308147296309471
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까 (남은 답변: 면접 시작 인사),확률 예측에서 MSE Loss 미 사용 이유,-0.030003149,0.0,0.030003149062395096
면접 시작 인사 -> 파이팅! 시작하자 (남은 답변: 면접 시작 인사),BCE Loss 설명,-0.008540487,0.0,0.008540486916899681
면접 시작 인사 -> 파이팅! 시작하자 (남은 답변: 면접 시작 인사),LLM Fine-Tuning 의 PEFT,0.0013053645,0.0,0.0013053645379841328
면접 시작 인사 -> 파이팅! 시작하자 (남은 답변: 면접 시작 인사),LoRA,-0.004411117,0.0,0.004411117173731327
면접 시작 인사 -> 파이팅! 시작하자 (남은 답변: 면접 시작 인사),LoRA 와 QLoRA 의 차이,0.0010309011,0.0,0.0010309010976925492
면접 시작 인사 -> 파이팅! 시작하자 (남은 답변: 면접 시작 인사),Loss Function 관련 실무 경험,-0.0005289674,0.0,0.0005289674154482782
면접 시작 인사 -> 파이팅! 시작하자 (남은 답변: 면접 시작 인사),Loss Function 예시,0.0048185443,0.0,0.004818544257432222
면접 시작 인사 -> 파이팅! 시작하자 (남은 답변: 면접 시작 인사),Loss Function 정의,-0.0028414975,0.0,0.002841497538611293
면접 시작 인사 -> 파이팅! 시작하자 (남은 답변: 면접 시작 인사),MBTI / 좋아하는 아이돌,-0.004946413,0.0,0.004946412984281778
면접 시작 인사 -> 파이팅! 시작하자 (남은 답변: 면접 시작 인사),MSE Loss 설명,-0.00089176017,0.0,0.0008917601662687957
면접 시작 인사 -> 파이팅! 시작하자 (남은 답변: 면접 시작 인사),MSE Loss 용도,0.005767032,0.0,0.005767032038420439
면접 시작 인사 -> 파이팅! 시작하자 (남은 답변: 면접 시작 인사),"Multi-Class, Multi-Label 중 BCE 가 좋은 task",-0.0021583326,0.0,0.0021583326160907745
면접 시작 인사 -> 파이팅! 시작하자 (남은 답변: 면접 시작 인사),Multi-Label 에서 CE + Softmax 적용 문제점,-0.002242606,0.0,0.0022426059003919363
면접 시작 인사 -> 파이팅! 시작하자 (남은 답변: 면접 시작 인사),PEFT 방법 5가지,0.0044183685,0.0,0.004418368451297283
면접 시작 인사 -> 파이팅! 시작하자 (남은 답변: 면접 시작 인사),거대 언어 모델 정의,-0.004204069,0.0,0.004204068798571825
면접 시작 인사 -> 파이팅! 시작하자 (남은 답변: 면접 시작 인사),마지막 할 말,0.0010533092,0.0,0.001053309184499085
면접 시작 인사 -> 파이팅! 시작하자 (남은 답변: 면접 시작 인사),면접 시작 인사,0.00014875285,0.0,0.00014875284978188574
면접 시작 인사 -> 파이팅! 시작하자 (남은 답변: 면접 시작 인사),면접 종료,0.0011800365,0.0,0.0011800364591181278
면접 시작 인사 -> 파이팅! 시작하자 (남은 답변: 면접 시작 인사),"인공지능, 머신러닝, 딥러닝 차이",0.98782486,1.0,0.012175142765045166
면접 시작 인사 -> 파이팅! 시작하자 (남은 답변: 면접 시작 인사),잠시 휴식,-0.003211512,0.0,0.0032115119975060225
면접 시작 인사 -> 파이팅! 시작하자 (남은 답변: 면접 시작 인사),확률 예측에서 MSE Loss 미 사용 이유,-0.00912949,0.0,0.00912948977202177
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데 (남은 답변: 인공지능, 머신러닝, 딥러닝)",BCE Loss 설명,-0.0063974448,0.0,0.0063974447548389435
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데 (남은 답변: 인공지능, 머신러닝, 딥러닝)",LLM Fine-Tuning 의 PEFT,-0.0039437152,0.0,0.0039437152445316315
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데 (남은 답변: 인공지능, 머신러닝, 딥러닝)",LoRA,-0.00066525815,0.0,0.0006652581505477428
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데 (남은 답변: 인공지능, 머신러닝, 딥러닝)",LoRA 와 QLoRA 의 차이,0.00025239037,0.0,0.0002523903676774353
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데 (남은 답변: 인공지능, 머신러닝, 딥러닝)",Loss Function 관련 실무 경험,0.0016616699,0.0,0.0016616699285805225
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데 (남은 답변: 인공지능, 머신러닝, 딥러닝)",Loss Function 예시,0.0018606685,0.0,0.0018606685334816575
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데 (남은 답변: 인공지능, 머신러닝, 딥러닝)",Loss Function 정의,-0.00040588572,0.0,0.00040588571573607624
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데 (남은 답변: 인공지능, 머신러닝, 딥러닝)",MBTI / 좋아하는 아이돌,-0.011444608,0.0,0.011444607749581337
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데 (남은 답변: 인공지능, 머신러닝, 딥러닝)",MSE Loss 설명,-0.0051246593,0.0,0.005124659277498722
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데 (남은 답변: 인공지능, 머신러닝, 딥러닝)",MSE Loss 용도,0.0028370812,0.0,0.0028370812069624662
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데 (남은 답변: 인공지능, 머신러닝, 딥러닝)","Multi-Class, Multi-Label 중 BCE 가 좋은 task",-0.0014680544,0.0,0.001468054368160665
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데 (남은 답변: 인공지능, 머신러닝, 딥러닝)",Multi-Label 에서 CE + Softmax 적용 문제점,-0.0026502348,0.0,0.0026502348482608795
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데 (남은 답변: 인공지능, 머신러닝, 딥러닝)",PEFT 방법 5가지,0.0023331975,0.0,0.002333197509869933
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데 (남은 답변: 인공지능, 머신러닝, 딥러닝)",거대 언어 모델 정의,-0.0072680153,0.0,0.007268015295267105
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데 (남은 답변: 인공지능, 머신러닝, 딥러닝)",마지막 할 말,0.0018337903,0.0,0.0018337903311476111
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데 (남은 답변: 인공지능, 머신러닝, 딥러닝)",면접 시작 인사,-0.004280888,0.0,0.004280888009816408
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데 (남은 답변: 인공지능, 머신러닝, 딥러닝)",면접 종료,-0.0006989947,0.0,0.0006989947287365794
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데 (남은 답변: 인공지능, 머신러닝, 딥러닝)","인공지능, 머신러닝, 딥러닝 차이",0.9975214,1.0,0.0024785995483398438
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데 (남은 답변: 인공지능, 머신러닝, 딥러닝)",잠시 휴식,-0.0004765676,0.0,0.00047656759852543473
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데 (남은 답변: 인공지능, 머신러닝, 딥러닝)",확률 예측에서 MSE Loss 미 사용 이유,-0.004535965,0.0,0.004535965155810118
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거! (남은 답변: 머신러닝, 딥러닝)",BCE Loss 설명,-0.008406733,0.0,0.00840673316270113
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거! (남은 답변: 머신러닝, 딥러닝)",LLM Fine-Tuning 의 PEFT,-0.0031517542,0.0,0.0031517541501671076
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거! (남은 답변: 머신러닝, 딥러닝)",LoRA,-0.0010636295,0.0,0.0010636295191943645
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거! (남은 답변: 머신러닝, 딥러닝)",LoRA 와 QLoRA 의 차이,0.0018721776,0.0,0.001872177585028112
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거! (남은 답변: 머신러닝, 딥러닝)",Loss Function 관련 실무 경험,0.00047869456,0.0,0.00047869456466287374
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거! (남은 답변: 머신러닝, 딥러닝)",Loss Function 예시,0.0021821063,0.0,0.00218210625462234
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거! (남은 답변: 머신러닝, 딥러닝)",Loss Function 정의,0.0005944534,0.0,0.0005944534204900265
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거! (남은 답변: 머신러닝, 딥러닝)",MBTI / 좋아하는 아이돌,-0.010621693,0.0,0.01062169298529625
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거! (남은 답변: 머신러닝, 딥러닝)",MSE Loss 설명,-0.0008592176,0.0,0.0008592176018282771
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거! (남은 답변: 머신러닝, 딥러닝)",MSE Loss 용도,0.002521123,0.0,0.0025211230386048555
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거! (남은 답변: 머신러닝, 딥러닝)","Multi-Class, Multi-Label 중 BCE 가 좋은 task",0.000672661,0.0,0.0006726610008627176
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거! (남은 답변: 머신러닝, 딥러닝)",Multi-Label 에서 CE + Softmax 적용 문제점,-0.0017696639,0.0,0.0017696638824418187
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거! (남은 답변: 머신러닝, 딥러닝)",PEFT 방법 5가지,0.0010428565,0.0,0.0010428564855828881
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거! (남은 답변: 머신러닝, 딥러닝)",거대 언어 모델 정의,-0.008385188,0.0,0.008385187946259975
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거! (남은 답변: 머신러닝, 딥러닝)",마지막 할 말,6.201611e-06,0.0,6.201611085998593e-06
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거! (남은 답변: 머신러닝, 딥러닝)",면접 시작 인사,-0.0035329121,0.0,0.003532912116497755
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거! (남은 답변: 머신러닝, 딥러닝)",면접 종료,-0.00030455383,0.0,0.0003045538323931396
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거! (남은 답변: 머신러닝, 딥러닝)","인공지능, 머신러닝, 딥러닝 차이",0.99744034,1.0,0.002559661865234375
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거! (남은 답변: 머신러닝, 딥러닝)",잠시 휴식,-0.0017529068,0.0,0.001752906828187406
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거! (남은 답변: 머신러닝, 딥러닝)",확률 예측에서 MSE Loss 미 사용 이유,-0.0051187985,0.0,0.005118798464536667
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지 (남은 답변: 딥러닝)",BCE Loss 설명,-0.0071608177,0.0,0.007160817738622427
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지 (남은 답변: 딥러닝)",LLM Fine-Tuning 의 PEFT,-0.0032530297,0.0,0.0032530296593904495
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지 (남은 답변: 딥러닝)",LoRA,-0.00078110344,0.0,0.0007811034447513521
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지 (남은 답변: 딥러닝)",LoRA 와 QLoRA 의 차이,0.0045223376,0.0,0.004522337578237057
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지 (남은 답변: 딥러닝)",Loss Function 관련 실무 경험,-5.1844683e-05,0.0,5.1844683184754103e-05
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지 (남은 답변: 딥러닝)",Loss Function 예시,0.002305435,0.0,0.0023054350167512894
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지 (남은 답변: 딥러닝)",Loss Function 정의,-8.5406435e-05,0.0,8.540643466403708e-05
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지 (남은 답변: 딥러닝)",MBTI / 좋아하는 아이돌,-0.011348928,0.0,0.011348928324878216
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지 (남은 답변: 딥러닝)",MSE Loss 설명,-0.0028946595,0.0,0.0028946595266461372
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지 (남은 답변: 딥러닝)",MSE Loss 용도,0.0045039468,0.0,0.004503946751356125
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지 (남은 답변: 딥러닝)","Multi-Class, Multi-Label 중 BCE 가 좋은 task",-0.0020850024,0.0,0.0020850023720413446
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지 (남은 답변: 딥러닝)",Multi-Label 에서 CE + Softmax 적용 문제점,-0.0025770734,0.0,0.0025770734064280987
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지 (남은 답변: 딥러닝)",PEFT 방법 5가지,0.0016562201,0.0,0.0016562200617045164
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지 (남은 답변: 딥러닝)",거대 언어 모델 정의,-0.0077202558,0.0,0.007720255758613348
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지 (남은 답변: 딥러닝)",마지막 할 말,-0.00030942567,0.0,0.00030942566809244454
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지 (남은 답변: 딥러닝)",면접 시작 인사,-0.0029489503,0.0,0.0029489502776414156
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지 (남은 답변: 딥러닝)",면접 종료,3.954162e-05,0.0,3.9541620935779065e-05
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지 (남은 답변: 딥러닝)","인공지능, 머신러닝, 딥러닝 차이",0.9974613,1.0,0.0025386810302734375
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지 (남은 답변: 딥러닝)",잠시 휴식,-0.00026187542,0.0,0.0002618754224386066
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지 (남은 답변: 딥러닝)",확률 예측에서 MSE Loss 미 사용 이유,-0.0036105246,0.0,0.003610524581745267
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지? (남은 답변: 인공지능, 머신러닝)",BCE Loss 설명,-0.008948746,0.0,0.008948746137320995
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지? (남은 답변: 인공지능, 머신러닝)",LLM Fine-Tuning 의 PEFT,-0.003615908,0.0,0.0036159080918878317
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지? (남은 답변: 인공지능, 머신러닝)",LoRA,-0.00021571384,0.0,0.0002157138369511813
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지? (남은 답변: 인공지능, 머신러닝)",LoRA 와 QLoRA 의 차이,0.0024453397,0.0,0.0024453396908938885
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지? (남은 답변: 인공지능, 머신러닝)",Loss Function 관련 실무 경험,-0.0002978444,0.0,0.00029784440994262695
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지? (남은 답변: 인공지능, 머신러닝)",Loss Function 예시,0.0020479485,0.0,0.0020479485392570496
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지? (남은 답변: 인공지능, 머신러닝)",Loss Function 정의,-0.00063615513,0.0,0.0006361551349982619
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지? (남은 답변: 인공지능, 머신러닝)",MBTI / 좋아하는 아이돌,-0.010917473,0.0,0.010917472653090954
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지? (남은 답변: 인공지능, 머신러닝)",MSE Loss 설명,-0.00077466987,0.0,0.0007746698684059083
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지? (남은 답변: 인공지능, 머신러닝)",MSE Loss 용도,0.0026785063,0.0,0.0026785063091665506
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지? (남은 답변: 인공지능, 머신러닝)","Multi-Class, Multi-Label 중 BCE 가 좋은 task",-0.0018017082,0.0,0.001801708247512579
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지? (남은 답변: 인공지능, 머신러닝)",Multi-Label 에서 CE + Softmax 적용 문제점,-0.0023609595,0.0,0.0023609595373272896
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지? (남은 답변: 인공지능, 머신러닝)",PEFT 방법 5가지,0.0015327978,0.0,0.0015327978180721402
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지? (남은 답변: 인공지능, 머신러닝)",거대 언어 모델 정의,-0.0066405884,0.0,0.006640588399022818
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지? (남은 답변: 인공지능, 머신러닝)",마지막 할 말,-0.00057070964,0.0,0.0005707096424885094
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지? (남은 답변: 인공지능, 머신러닝)",면접 시작 인사,-0.005391785,0.0,0.005391784943640232
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지? (남은 답변: 인공지능, 머신러닝)",면접 종료,-0.0012238097,0.0,0.0012238096678629518
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지? (남은 답변: 인공지능, 머신러닝)","인공지능, 머신러닝, 딥러닝 차이",0.99744207,1.0,0.0025579333305358887
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지? (남은 답변: 인공지능, 머신러닝)",잠시 휴식,-0.0023896173,0.0,0.0023896172642707825
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지? (남은 답변: 인공지능, 머신러닝)",확률 예측에서 MSE Loss 미 사용 이유,-0.0033593373,0.0,0.0033593373373150826
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야 (남은 답변: 모든 질문 해결 완료)",BCE Loss 설명,-0.0019514074,0.0,0.0019514074083417654
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야 (남은 답변: 모든 질문 해결 완료)",LLM Fine-Tuning 의 PEFT,-0.0069859712,0.0,0.006985971238464117
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야 (남은 답변: 모든 질문 해결 완료)",LoRA,0.005136749,0.0,0.005136748775839806
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야 (남은 답변: 모든 질문 해결 완료)",LoRA 와 QLoRA 의 차이,-0.0057784594,0.0,0.005778459366410971
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야 (남은 답변: 모든 질문 해결 완료)",Loss Function 관련 실무 경험,0.0012976994,0.0,0.001297699403949082
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야 (남은 답변: 모든 질문 해결 완료)",Loss Function 예시,-0.0032049648,0.0,0.0032049647998064756
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야 (남은 답변: 모든 질문 해결 완료)",Loss Function 정의,-0.0022736937,0.0,0.0022736936807632446
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야 (남은 답변: 모든 질문 해결 완료)",MBTI / 좋아하는 아이돌,-0.0044082114,0.0,0.004408211447298527
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야 (남은 답변: 모든 질문 해결 완료)",MSE Loss 설명,0.00024518074,0.0,0.00024518073769286275
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야 (남은 답변: 모든 질문 해결 완료)",MSE Loss 용도,0.0009070791,0.0,0.0009070790838450193
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야 (남은 답변: 모든 질문 해결 완료)","Multi-Class, Multi-Label 중 BCE 가 좋은 task",0.00031488482,0.0,0.00031488481909036636
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야 (남은 답변: 모든 질문 해결 완료)",Multi-Label 에서 CE + Softmax 적용 문제점,-0.00044115656,0.0,0.00044115656055510044
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야 (남은 답변: 모든 질문 해결 완료)",PEFT 방법 5가지,-0.008454276,0.0,0.008454276248812675
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야 (남은 답변: 모든 질문 해결 완료)",거대 언어 모델 정의,0.99524343,1.0,0.004756569862365723
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야 (남은 답변: 모든 질문 해결 완료)",마지막 할 말,-0.0045175306,0.0,0.004517530556768179
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야 (남은 답변: 모든 질문 해결 완료)",면접 시작 인사,-0.008889067,0.0,0.008889066986739635
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야 (남은 답변: 모든 질문 해결 완료)",면접 종료,-0.00028356008,0.0,0.0002835600753314793
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야 (남은 답변: 모든 질문 해결 완료)","인공지능, 머신러닝, 딥러닝 차이",0.0046273824,0.0,0.004627382382750511
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야 (남은 답변: 모든 질문 해결 완료)",잠시 휴식,-0.010706065,0.0,0.010706065222620964
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야 (남은 답변: 모든 질문 해결 완료)",확률 예측에서 MSE Loss 미 사용 이유,-0.0018574084,0.0,0.001857408438809216
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야? (남은 답변: 머신러닝)",BCE Loss 설명,-0.006557098,0.0,0.006557098124176264
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야? (남은 답변: 머신러닝)",LLM Fine-Tuning 의 PEFT,-0.003137242,0.0,0.0031372420489788055
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야? (남은 답변: 머신러닝)",LoRA,-0.0023050318,0.0,0.002305031754076481
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야? (남은 답변: 머신러닝)",LoRA 와 QLoRA 의 차이,0.0019095717,0.0,0.0019095716997981071
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야? (남은 답변: 머신러닝)",Loss Function 관련 실무 경험,-0.00023625427,0.0,0.0002362542727496475
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야? (남은 답변: 머신러닝)",Loss Function 예시,0.0035559295,0.0,0.003555929521098733
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야? (남은 답변: 머신러닝)",Loss Function 정의,-0.0009671054,0.0,0.0009671053849160671
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야? (남은 답변: 머신러닝)",MBTI / 좋아하는 아이돌,-0.012600746,0.0,0.012600746005773544
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야? (남은 답변: 머신러닝)",MSE Loss 설명,-0.0014468839,0.0,0.0014468838926404715
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야? (남은 답변: 머신러닝)",MSE Loss 용도,0.0022254272,0.0,0.0022254271898418665
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야? (남은 답변: 머신러닝)","Multi-Class, Multi-Label 중 BCE 가 좋은 task",-0.0005090975,0.0,0.0005090974736958742
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야? (남은 답변: 머신러닝)",Multi-Label 에서 CE + Softmax 적용 문제점,-0.002254104,0.0,0.002254104008898139
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야? (남은 답변: 머신러닝)",PEFT 방법 5가지,-3.184579e-05,0.0,3.1845789635553956e-05
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야? (남은 답변: 머신러닝)",거대 언어 모델 정의,-0.0074716043,0.0,0.007471604272723198
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야? (남은 답변: 머신러닝)",마지막 할 말,0.0012953694,0.0,0.0012953693512827158
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야? (남은 답변: 머신러닝)",면접 시작 인사,-0.0035597526,0.0,0.0035597526002675295
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야? (남은 답변: 머신러닝)",면접 종료,-0.0010139842,0.0,0.0010139842052012682
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야? (남은 답변: 머신러닝)","인공지능, 머신러닝, 딥러닝 차이",0.99766535,1.0,0.0023346543312072754
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야? (남은 답변: 머신러닝)",잠시 휴식,0.0009787262,0.0,0.0009787261951714754
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야? (남은 답변: 머신러닝)",확률 예측에서 MSE Loss 미 사용 이유,-0.0036086028,0.0,0.003608602797612548
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아 (남은 답변: 인공지능, 머신러닝)",BCE Loss 설명,-0.0037315884,0.0,0.0037315883673727512
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아 (남은 답변: 인공지능, 머신러닝)",LLM Fine-Tuning 의 PEFT,-0.003217575,0.0,0.0032175749074667692
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아 (남은 답변: 인공지능, 머신러닝)",LoRA,-0.0022922975,0.0,0.0022922975476831198
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아 (남은 답변: 인공지능, 머신러닝)",LoRA 와 QLoRA 의 차이,0.0005271974,0.0,0.0005271973786875606
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아 (남은 답변: 인공지능, 머신러닝)",Loss Function 관련 실무 경험,7.443458e-05,0.0,7.443458162015304e-05
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아 (남은 답변: 인공지능, 머신러닝)",Loss Function 예시,0.0016223257,0.0,0.0016223257407546043
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아 (남은 답변: 인공지능, 머신러닝)",Loss Function 정의,-0.00045813105,0.0,0.00045813104952685535
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아 (남은 답변: 인공지능, 머신러닝)",MBTI / 좋아하는 아이돌,-0.008911082,0.0,0.008911081589758396
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아 (남은 답변: 인공지능, 머신러닝)",MSE Loss 설명,-0.005336342,0.0,0.005336341913789511
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아 (남은 답변: 인공지능, 머신러닝)",MSE Loss 용도,0.0004123531,0.0,0.0004123531107325107
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아 (남은 답변: 인공지능, 머신러닝)","Multi-Class, Multi-Label 중 BCE 가 좋은 task",-0.0021740608,0.0,0.0021740607917308807
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아 (남은 답변: 인공지능, 머신러닝)",Multi-Label 에서 CE + Softmax 적용 문제점,-0.002465891,0.0,0.0024658909533172846
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아 (남은 답변: 인공지능, 머신러닝)",PEFT 방법 5가지,2.6987951e-05,0.0,2.6987951059709303e-05
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아 (남은 답변: 인공지능, 머신러닝)",거대 언어 모델 정의,-0.01146777,0.0,0.011467769742012024
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아 (남은 답변: 인공지능, 머신러닝)",마지막 할 말,0.001167879,0.0,0.0011678789742290974
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아 (남은 답변: 인공지능, 머신러닝)",면접 시작 인사,-0.004385194,0.0,0.004385193809866905
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아 (남은 답변: 인공지능, 머신러닝)",면접 종료,0.00033263038,0.0,0.000332630384946242
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아 (남은 답변: 인공지능, 머신러닝)","인공지능, 머신러닝, 딥러닝 차이",0.99731696,1.0,0.0026830434799194336
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아 (남은 답변: 인공지능, 머신러닝)",잠시 휴식,0.0020441003,0.0,0.0020441003143787384
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아 (남은 답변: 인공지능, 머신러닝)",확률 예측에서 MSE Loss 미 사용 이유,-0.005206585,0.0,0.005206584930419922
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지? (남은 답변: 머신러닝)",BCE Loss 설명,-0.0071359645,0.0,0.007135964464396238
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지? (남은 답변: 머신러닝)",LLM Fine-Tuning 의 PEFT,-0.0027259076,0.0,0.002725907601416111
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지? (남은 답변: 머신러닝)",LoRA,-0.0010241342,0.0,0.001024134224280715
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지? (남은 답변: 머신러닝)",LoRA 와 QLoRA 의 차이,0.0018061579,0.0,0.001806157873943448
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지? (남은 답변: 머신러닝)",Loss Function 관련 실무 경험,7.424765e-05,0.0,7.424764771712944e-05
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지? (남은 답변: 머신러닝)",Loss Function 예시,0.0039619524,0.0,0.003961952403187752
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지? (남은 답변: 머신러닝)",Loss Function 정의,-0.0010554657,0.0,0.001055465661920607
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지? (남은 답변: 머신러닝)",MBTI / 좋아하는 아이돌,-0.011307193,0.0,0.011307192966341972
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지? (남은 답변: 머신러닝)",MSE Loss 설명,-0.00028215811,0.0,0.000282158114714548
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지? (남은 답변: 머신러닝)",MSE Loss 용도,0.0026166548,0.0,0.0026166548486799
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지? (남은 답변: 머신러닝)","Multi-Class, Multi-Label 중 BCE 가 좋은 task",-0.0013983402,0.0,0.0013983402168378234
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지? (남은 답변: 머신러닝)",Multi-Label 에서 CE + Softmax 적용 문제점,-0.0033102683,0.0,0.0033102682791650295
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지? (남은 답변: 머신러닝)",PEFT 방법 5가지,0.0008936889,0.0,0.0008936888771131635
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지? (남은 답변: 머신러닝)",거대 언어 모델 정의,-0.008228128,0.0,0.008228127844631672
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지? (남은 답변: 머신러닝)",마지막 할 말,0.0031841595,0.0,0.003184159519150853
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지? (남은 답변: 머신러닝)",면접 시작 인사,-0.0042494386,0.0,0.004249438643455505
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지? (남은 답변: 머신러닝)",면접 종료,-0.0013806279,0.0,0.001380627858452499
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지? (남은 답변: 머신러닝)","인공지능, 머신러닝, 딥러닝 차이",0.9963328,1.0,0.003667175769805908
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지? (남은 답변: 머신러닝)",잠시 휴식,0.00046634296,0.0,0.0004663429572246969
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지? (남은 답변: 머신러닝)",확률 예측에서 MSE Loss 미 사용 이유,-0.0030619465,0.0,0.0030619464814662933
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야! (남은 답변: 모든 질문 해결 완료)",BCE Loss 설명,-0.0011289092,0.0,0.0011289091780781746
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야! (남은 답변: 모든 질문 해결 완료)",LLM Fine-Tuning 의 PEFT,-0.007979594,0.0,0.00797959417104721
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야! (남은 답변: 모든 질문 해결 완료)",LoRA,0.0053606844,0.0,0.005360684357583523
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야! (남은 답변: 모든 질문 해결 완료)",LoRA 와 QLoRA 의 차이,-0.0052556354,0.0,0.005255635362118483
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야! (남은 답변: 모든 질문 해결 완료)",Loss Function 관련 실무 경험,0.0015784813,0.0,0.0015784812858328223
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야! (남은 답변: 모든 질문 해결 완료)",Loss Function 예시,-0.0036028982,0.0,0.0036028982140123844
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야! (남은 답변: 모든 질문 해결 완료)",Loss Function 정의,-0.0006534306,0.0,0.0006534305866807699
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야! (남은 답변: 모든 질문 해결 완료)",MBTI / 좋아하는 아이돌,0.00084265496,0.0,0.0008426549611613154
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야! (남은 답변: 모든 질문 해결 완료)",MSE Loss 설명,-0.0005182331,0.0,0.000518233107868582
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야! (남은 답변: 모든 질문 해결 완료)",MSE Loss 용도,0.0010134676,0.0,0.0010134675540030003
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야! (남은 답변: 모든 질문 해결 완료)","Multi-Class, Multi-Label 중 BCE 가 좋은 task",0.0013805468,0.0,0.0013805468333885074
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야! (남은 답변: 모든 질문 해결 완료)",Multi-Label 에서 CE + Softmax 적용 문제점,-4.0513507e-05,0.0,4.0513506974093616e-05
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야! (남은 답변: 모든 질문 해결 완료)",PEFT 방법 5가지,-0.003919454,0.0,0.003919453825801611
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야! (남은 답변: 모든 질문 해결 완료)",거대 언어 모델 정의,0.9954366,1.0,0.004563391208648682
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야! (남은 답변: 모든 질문 해결 완료)",마지막 할 말,-0.00355338,0.0,0.003553380025550723
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야! (남은 답변: 모든 질문 해결 완료)",면접 시작 인사,-0.0059233895,0.0,0.005923389457166195
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야! (남은 답변: 모든 질문 해결 완료)",면접 종료,-0.0008032314,0.0,0.0008032313780859113
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야! (남은 답변: 모든 질문 해결 완료)","인공지능, 머신러닝, 딥러닝 차이",-0.0017549954,0.0,0.001754995435476303
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야! (남은 답변: 모든 질문 해결 완료)",잠시 휴식,-0.010092502,0.0,0.01009250245988369
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야! (남은 답변: 모든 질문 해결 완료)",확률 예측에서 MSE Loss 미 사용 이유,-0.0049193385,0.0,0.004919338505715132
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지 (남은 답변: 모든 질문 해결 완료)",BCE Loss 설명,-0.0015191671,0.0,0.00151916709728539
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지 (남은 답변: 모든 질문 해결 완료)",LLM Fine-Tuning 의 PEFT,-0.0065979613,0.0,0.00659796129912138
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지 (남은 답변: 모든 질문 해결 완료)",LoRA,0.0030092008,0.0,0.003009200794622302
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지 (남은 답변: 모든 질문 해결 완료)",LoRA 와 QLoRA 의 차이,-0.0047174217,0.0,0.004717421717941761
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지 (남은 답변: 모든 질문 해결 완료)",Loss Function 관련 실무 경험,0.0017041017,0.0,0.0017041016835719347
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지 (남은 답변: 모든 질문 해결 완료)",Loss Function 예시,-0.003121587,0.0,0.0031215869821608067
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지 (남은 답변: 모든 질문 해결 완료)",Loss Function 정의,-0.0014752835,0.0,0.001475283526815474
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지 (남은 답변: 모든 질문 해결 완료)",MBTI / 좋아하는 아이돌,-0.0042493986,0.0,0.004249398596584797
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지 (남은 답변: 모든 질문 해결 완료)",MSE Loss 설명,0.0016257584,0.0,0.0016257583629339933
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지 (남은 답변: 모든 질문 해결 완료)",MSE Loss 용도,0.002180078,0.0,0.0021800780668854713
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지 (남은 답변: 모든 질문 해결 완료)","Multi-Class, Multi-Label 중 BCE 가 좋은 task",7.510621e-05,0.0,7.510621071560308e-05
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지 (남은 답변: 모든 질문 해결 완료)",Multi-Label 에서 CE + Softmax 적용 문제점,-0.0005561677,0.0,0.0005561676807701588
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지 (남은 답변: 모든 질문 해결 완료)",PEFT 방법 5가지,-0.005322292,0.0,0.005322291981428862
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지 (남은 답변: 모든 질문 해결 완료)",거대 언어 모델 정의,0.9953011,1.0,0.0046988725662231445
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지 (남은 답변: 모든 질문 해결 완료)",마지막 할 말,-0.0032642304,0.0,0.0032642304431647062
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지 (남은 답변: 모든 질문 해결 완료)",면접 시작 인사,-0.008116863,0.0,0.00811686273664236
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지 (남은 답변: 모든 질문 해결 완료)",면접 종료,0.00080121483,0.0,0.0008012148318812251
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지 (남은 답변: 모든 질문 해결 완료)","인공지능, 머신러닝, 딥러닝 차이",0.0011755888,0.0,0.0011755888117477298
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지 (남은 답변: 모든 질문 해결 완료)",잠시 휴식,-0.010247139,0.0,0.010247139260172844
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지 (남은 답변: 모든 질문 해결 완료)",확률 예측에서 MSE Loss 미 사용 이유,-0.0024928518,0.0,0.002492851810529828
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야. (남은 답변: 모든 질문 해결 완료)",BCE Loss 설명,-0.00147488,0.0,0.0014748800313100219
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야. (남은 답변: 모든 질문 해결 완료)",LLM Fine-Tuning 의 PEFT,-0.0065938006,0.0,0.006593800615519285
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야. (남은 답변: 모든 질문 해결 완료)",LoRA,0.0037058962,0.0,0.0037058962043374777
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야. (남은 답변: 모든 질문 해결 완료)",LoRA 와 QLoRA 의 차이,-0.0059604887,0.0,0.0059604886919260025
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야. (남은 답변: 모든 질문 해결 완료)",Loss Function 관련 실무 경험,0.0016464051,0.0,0.0016464050859212875
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야. (남은 답변: 모든 질문 해결 완료)",Loss Function 예시,-0.0030734357,0.0,0.003073435742408037
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야. (남은 답변: 모든 질문 해결 완료)",Loss Function 정의,-0.00090964435,0.0,0.0009096443536691368
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야. (남은 답변: 모든 질문 해결 완료)",MBTI / 좋아하는 아이돌,-0.0021923964,0.0,0.0021923964377492666
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야. (남은 답변: 모든 질문 해결 완료)",MSE Loss 설명,0.0015848349,0.0,0.001584834884852171
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야. (남은 답변: 모든 질문 해결 완료)",MSE Loss 용도,0.0027009293,0.0,0.00270092929713428
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야. (남은 답변: 모든 질문 해결 완료)","Multi-Class, Multi-Label 중 BCE 가 좋은 task",0.00062839495,0.0,0.0006283949478529394
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야. (남은 답변: 모든 질문 해결 완료)",Multi-Label 에서 CE + Softmax 적용 문제점,-0.0007153872,0.0,0.000715387228410691
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야. (남은 답변: 모든 질문 해결 완료)",PEFT 방법 5가지,-0.005965925,0.0,0.005965924821794033
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야. (남은 답변: 모든 질문 해결 완료)",거대 언어 모델 정의,0.99530005,1.0,0.0046999454498291016
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야. (남은 답변: 모든 질문 해결 완료)",마지막 할 말,-0.0028162634,0.0,0.0028162633534520864
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야. (남은 답변: 모든 질문 해결 완료)",면접 시작 인사,-0.007259083,0.0,0.007259082980453968
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야. (남은 답변: 모든 질문 해결 완료)",면접 종료,0.00014824588,0.0,0.00014824587560724467
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야. (남은 답변: 모든 질문 해결 완료)","인공지능, 머신러닝, 딥러닝 차이",0.00046925375,0.0,0.00046925374772399664
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야. (남은 답변: 모든 질문 해결 완료)",잠시 휴식,-0.0102989795,0.0,0.010298979468643665
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야. (남은 답변: 모든 질문 해결 완료)",확률 예측에서 MSE Loss 미 사용 이유,-0.003716472,0.0,0.0037164720706641674
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로 (남은 답변: 머신러닝, 딥러닝)",BCE Loss 설명,-0.0069090864,0.0,0.0069090863689780235
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로 (남은 답변: 머신러닝, 딥러닝)",LLM Fine-Tuning 의 PEFT,-0.002447344,0.0,0.002447343897074461
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로 (남은 답변: 머신러닝, 딥러닝)",LoRA,-0.0018847462,0.0,0.0018847462488338351
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로 (남은 답변: 머신러닝, 딥러닝)",LoRA 와 QLoRA 의 차이,0.0023646185,0.0,0.00236461847089231
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로 (남은 답변: 머신러닝, 딥러닝)",Loss Function 관련 실무 경험,0.0007606493,0.0,0.0007606492727063596
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로 (남은 답변: 머신러닝, 딥러닝)",Loss Function 예시,0.0021682102,0.0,0.0021682102233171463
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로 (남은 답변: 머신러닝, 딥러닝)",Loss Function 정의,-0.0001210346,0.0,0.00012103460176149383
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로 (남은 답변: 머신러닝, 딥러닝)",MBTI / 좋아하는 아이돌,-0.012689137,0.0,0.012689136900007725
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로 (남은 답변: 머신러닝, 딥러닝)",MSE Loss 설명,-0.0019648673,0.0,0.0019648673478513956
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로 (남은 답변: 머신러닝, 딥러닝)",MSE Loss 용도,0.0021441968,0.0,0.002144196769222617
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로 (남은 답변: 머신러닝, 딥러닝)","Multi-Class, Multi-Label 중 BCE 가 좋은 task",-0.00015414774,0.0,0.0001541477395221591
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로 (남은 답변: 머신러닝, 딥러닝)",Multi-Label 에서 CE + Softmax 적용 문제점,-0.0006052332,0.0,0.0006052331882528961
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로 (남은 답변: 머신러닝, 딥러닝)",PEFT 방법 5가지,0.00045333308,0.0,0.0004533330793492496
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로 (남은 답변: 머신러닝, 딥러닝)",거대 언어 모델 정의,-0.0074206754,0.0,0.007420675363391638
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로 (남은 답변: 머신러닝, 딥러닝)",마지막 할 말,0.0009809315,0.0,0.000980931450612843
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로 (남은 답변: 머신러닝, 딥러닝)",면접 시작 인사,-0.0037169135,0.0,0.003716913517564535
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로 (남은 답변: 머신러닝, 딥러닝)",면접 종료,-0.00023137766,0.0,0.00023137766402214766
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로 (남은 답변: 머신러닝, 딥러닝)","인공지능, 머신러닝, 딥러닝 차이",0.9978203,1.0,0.0021796822547912598
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로 (남은 답변: 머신러닝, 딥러닝)",잠시 휴식,0.00016502022,0.0,0.00016502021753694862
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로 (남은 답변: 머신러닝, 딥러닝)",확률 예측에서 MSE Loss 미 사용 이유,-0.0052138614,0.0,0.005213861353695393
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야 (남은 답변: 모든 질문 해결 완료),BCE Loss 설명,-0.002954827,0.0,0.0029548269230872393
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야 (남은 답변: 모든 질문 해결 완료),LLM Fine-Tuning 의 PEFT,0.0036429013,0.0,0.003642901312559843
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야 (남은 답변: 모든 질문 해결 완료),LoRA,-0.0009984231,0.0,0.0009984230855479836
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야 (남은 답변: 모든 질문 해결 완료),LoRA 와 QLoRA 의 차이,-0.0009611431,0.0,0.0009611431160010397
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야 (남은 답변: 모든 질문 해결 완료),Loss Function 관련 실무 경험,-0.005140312,0.0,0.005140312016010284
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야 (남은 답변: 모든 질문 해결 완료),Loss Function 예시,-0.0044529364,0.0,0.004452936351299286
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야 (남은 답변: 모든 질문 해결 완료),Loss Function 정의,0.996617,1.0,0.0033829808235168457
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야 (남은 답변: 모든 질문 해결 완료),MBTI / 좋아하는 아이돌,-0.0051215333,0.0,0.005121533293277025
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야 (남은 답변: 모든 질문 해결 완료),MSE Loss 설명,-0.0040356875,0.0,0.004035687539726496
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야 (남은 답변: 모든 질문 해결 완료),MSE Loss 용도,-0.003801298,0.0,0.0038012980949133635
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야 (남은 답변: 모든 질문 해결 완료),"Multi-Class, Multi-Label 중 BCE 가 좋은 task",3.617144e-05,0.0,3.617144102463499e-05
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야 (남은 답변: 모든 질문 해결 완료),Multi-Label 에서 CE + Softmax 적용 문제점,0.0031840838,0.0,0.0031840838491916656
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야 (남은 답변: 모든 질문 해결 완료),PEFT 방법 5가지,-0.0061059156,0.0,0.006105915643274784
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야 (남은 답변: 모든 질문 해결 완료),거대 언어 모델 정의,-0.0017072724,0.0,0.001707272371277213
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야 (남은 답변: 모든 질문 해결 완료),마지막 할 말,-0.00852251,0.0,0.008522509597241879
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야 (남은 답변: 모든 질문 해결 완료),면접 시작 인사,0.0036246777,0.0,0.0036246776580810547
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야 (남은 답변: 모든 질문 해결 완료),면접 종료,-0.0019188644,0.0,0.0019188644364476204
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야 (남은 답변: 모든 질문 해결 완료),"인공지능, 머신러닝, 딥러닝 차이",-0.0008075237,0.0,0.0008075237274169922
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야 (남은 답변: 모든 질문 해결 완료),잠시 휴식,-0.0070339907,0.0,0.007033990696072578
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야 (남은 답변: 모든 질문 해결 완료),확률 예측에서 MSE Loss 미 사용 이유,-0.0037066024,0.0,0.00370660237967968
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데 (남은 답변: 거대 언어 모델 정의),BCE Loss 설명,-0.039410945,0.0,0.039410945028066635
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데 (남은 답변: 거대 언어 모델 정의),LLM Fine-Tuning 의 PEFT,-0.0043661087,0.0,0.0043661086820065975
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데 (남은 답변: 거대 언어 모델 정의),LoRA,-0.041994825,0.0,0.04199482500553131
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데 (남은 답변: 거대 언어 모델 정의),LoRA 와 QLoRA 의 차이,-0.020961195,0.0,0.02096119523048401
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데 (남은 답변: 거대 언어 모델 정의),Loss Function 관련 실무 경험,-0.025919458,0.0,0.025919457897543907
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데 (남은 답변: 거대 언어 모델 정의),Loss Function 예시,-0.009859188,0.0,0.009859188459813595
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데 (남은 답변: 거대 언어 모델 정의),Loss Function 정의,0.48182914,0.0,0.48182913661003113
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데 (남은 답변: 거대 언어 모델 정의),MBTI / 좋아하는 아이돌,-0.01326832,0.0,0.013268319889903069
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데 (남은 답변: 거대 언어 모델 정의),MSE Loss 설명,-0.022862302,0.0,0.022862302139401436
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데 (남은 답변: 거대 언어 모델 정의),MSE Loss 용도,-0.0062062023,0.0,0.006206202320754528
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데 (남은 답변: 거대 언어 모델 정의),"Multi-Class, Multi-Label 중 BCE 가 좋은 task",0.0015754765,0.0,0.0015754764899611473
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데 (남은 답변: 거대 언어 모델 정의),Multi-Label 에서 CE + Softmax 적용 문제점,-0.004704062,0.0,0.004704061895608902
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데 (남은 답변: 거대 언어 모델 정의),PEFT 방법 5가지,-0.011150911,0.0,0.011150911450386047
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데 (남은 답변: 거대 언어 모델 정의),거대 언어 모델 정의,0.807995,1.0,0.1920049786567688
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데 (남은 답변: 거대 언어 모델 정의),마지막 할 말,-0.025507918,0.0,0.025507917627692223
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데 (남은 답변: 거대 언어 모델 정의),면접 시작 인사,-0.02469302,0.0,0.024693019688129425
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데 (남은 답변: 거대 언어 모델 정의),면접 종료,-0.013282383,0.0,0.013282382860779762
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데 (남은 답변: 거대 언어 모델 정의),"인공지능, 머신러닝, 딥러닝 차이",0.0046262834,0.0,0.004626283422112465
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데 (남은 답변: 거대 언어 모델 정의),잠시 휴식,-0.03264373,0.0,0.03264373168349266
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데 (남은 답변: 거대 언어 모델 정의),확률 예측에서 MSE Loss 미 사용 이유,-0.00543294,0.0,0.00543294008821249
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델 (남은 답변: 모든 질문 해결 완료),BCE Loss 설명,-0.0035131879,0.0,0.0035131878685206175
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델 (남은 답변: 모든 질문 해결 완료),LLM Fine-Tuning 의 PEFT,0.0032602586,0.0,0.003260258585214615
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델 (남은 답변: 모든 질문 해결 완료),LoRA,-0.00095314457,0.0,0.0009531445684842765
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델 (남은 답변: 모든 질문 해결 완료),LoRA 와 QLoRA 의 차이,-0.000855178,0.0,0.0008551779901608825
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델 (남은 답변: 모든 질문 해결 완료),Loss Function 관련 실무 경험,-0.0029637963,0.0,0.0029637962579727173
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델 (남은 답변: 모든 질문 해결 완료),Loss Function 예시,-0.0027422667,0.0,0.002742266748100519
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델 (남은 답변: 모든 질문 해결 완료),Loss Function 정의,0.99594367,1.0,0.004056334495544434
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델 (남은 답변: 모든 질문 해결 완료),MBTI / 좋아하는 아이돌,-0.0036635003,0.0,0.0036635003052651882
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델 (남은 답변: 모든 질문 해결 완료),MSE Loss 설명,-0.0055888575,0.0,0.00558885745704174
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델 (남은 답변: 모든 질문 해결 완료),MSE Loss 용도,-0.00029395838,0.0,0.0002939583791885525
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델 (남은 답변: 모든 질문 해결 완료),"Multi-Class, Multi-Label 중 BCE 가 좋은 task",-0.0012781649,0.0,0.0012781649129465222
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델 (남은 답변: 모든 질문 해결 완료),Multi-Label 에서 CE + Softmax 적용 문제점,0.004429522,0.0,0.004429521970450878
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델 (남은 답변: 모든 질문 해결 완료),PEFT 방법 5가지,-0.005314292,0.0,0.005314291920512915
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델 (남은 답변: 모든 질문 해결 완료),거대 언어 모델 정의,-0.0096071055,0.0,0.009607105515897274
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델 (남은 답변: 모든 질문 해결 완료),마지막 할 말,-0.0032964535,0.0,0.003296453505754471
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델 (남은 답변: 모든 질문 해결 완료),면접 시작 인사,0.0051914877,0.0,0.005191487725824118
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델 (남은 답변: 모든 질문 해결 완료),면접 종료,-0.0009492804,0.0,0.0009492803947068751
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델 (남은 답변: 모든 질문 해결 완료),"인공지능, 머신러닝, 딥러닝 차이",-0.0026044277,0.0,0.002604427747428417
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델 (남은 답변: 모든 질문 해결 완료),잠시 휴식,-0.008235766,0.0,0.008235765621066093
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델 (남은 답변: 모든 질문 해결 완료),확률 예측에서 MSE Loss 미 사용 이유,-0.0062834816,0.0,0.006283481605350971
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지. (남은 답변: 거대 언어 모델 정의),BCE Loss 설명,-0.0036734913,0.0,0.0036734913010150194
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지. (남은 답변: 거대 언어 모델 정의),LLM Fine-Tuning 의 PEFT,-0.0069144857,0.0,0.006914485711604357
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지. (남은 답변: 거대 언어 모델 정의),LoRA,0.0001577013,0.0,0.00015770130266901106
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지. (남은 답변: 거대 언어 모델 정의),LoRA 와 QLoRA 의 차이,-0.0060335714,0.0,0.006033571437001228
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지. (남은 답변: 거대 언어 모델 정의),Loss Function 관련 실무 경험,0.0026714003,0.0,0.0026714003179222345
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지. (남은 답변: 거대 언어 모델 정의),Loss Function 예시,-0.0018036052,0.0,0.0018036052351817489
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지. (남은 답변: 거대 언어 모델 정의),Loss Function 정의,-0.0032207367,0.0,0.0032207367476075888
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지. (남은 답변: 거대 언어 모델 정의),MBTI / 좋아하는 아이돌,-0.0034721615,0.0,0.0034721614792943
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지. (남은 답변: 거대 언어 모델 정의),MSE Loss 설명,6.901053e-05,0.0,6.901052984176204e-05
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지. (남은 답변: 거대 언어 모델 정의),MSE Loss 용도,-0.00020342109,0.0,0.00020342109201010317
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지. (남은 답변: 거대 언어 모델 정의),"Multi-Class, Multi-Label 중 BCE 가 좋은 task",0.00031974365,0.0,0.00031974364537745714
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지. (남은 답변: 거대 언어 모델 정의),Multi-Label 에서 CE + Softmax 적용 문제점,0.0002639413,0.0,0.00026394129963591695
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지. (남은 답변: 거대 언어 모델 정의),PEFT 방법 5가지,-0.00859497,0.0,0.008594970218837261
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지. (남은 답변: 거대 언어 모델 정의),거대 언어 모델 정의,0.9962702,1.0,0.0037298202514648438
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지. (남은 답변: 거대 언어 모델 정의),마지막 할 말,-0.0007845738,0.0,0.000784573785495013
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지. (남은 답변: 거대 언어 모델 정의),면접 시작 인사,-0.005052226,0.0,0.0050522261299192905
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지. (남은 답변: 거대 언어 모델 정의),면접 종료,0.0015381615,0.0,0.0015381615376099944
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지. (남은 답변: 거대 언어 모델 정의),"인공지능, 머신러닝, 딥러닝 차이",-0.0002534894,0.0,0.00025348938652314246
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지. (남은 답변: 거대 언어 모델 정의),잠시 휴식,-0.012227616,0.0,0.012227616272866726
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지. (남은 답변: 거대 언어 모델 정의),확률 예측에서 MSE Loss 미 사용 이유,-0.005190507,0.0,0.0051905070431530476
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야 (남은 답변: Loss Function 정의),BCE Loss 설명,-0.008453898,0.0,0.008453898131847382
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야 (남은 답변: Loss Function 정의),LLM Fine-Tuning 의 PEFT,0.0012612029,0.0,0.0012612028513103724
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야 (남은 답변: Loss Function 정의),LoRA,-0.0005187776,0.0,0.0005187775823287666
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야 (남은 답변: Loss Function 정의),LoRA 와 QLoRA 의 차이,-0.0003714151,0.0,0.0003714151098392904
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야 (남은 답변: Loss Function 정의),Loss Function 관련 실무 경험,-0.007134945,0.0,0.007134945131838322
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야 (남은 답변: Loss Function 정의),Loss Function 예시,-0.0030387307,0.0,0.0030387307051569223
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야 (남은 답변: Loss Function 정의),Loss Function 정의,0.9974105,1.0,0.0025895237922668457
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야 (남은 답변: Loss Function 정의),MBTI / 좋아하는 아이돌,-0.00042531983,0.0,0.0004253198276273906
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야 (남은 답변: Loss Function 정의),MSE Loss 설명,-0.0054332446,0.0,0.005433244630694389
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야 (남은 답변: Loss Function 정의),MSE Loss 용도,-0.0038854112,0.0,0.0038854111917316914
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야 (남은 답변: Loss Function 정의),"Multi-Class, Multi-Label 중 BCE 가 좋은 task",-0.00016910677,0.0,0.00016910677368286997
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야 (남은 답변: Loss Function 정의),Multi-Label 에서 CE + Softmax 적용 문제점,0.004530395,0.0,0.0045303949154913425
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야 (남은 답변: Loss Function 정의),PEFT 방법 5가지,-0.006563152,0.0,0.006563152186572552
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야 (남은 답변: Loss Function 정의),거대 언어 모델 정의,-0.005376778,0.0,0.005376778077334166
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야 (남은 답변: Loss Function 정의),마지막 할 말,-0.008380388,0.0,0.008380387909710407
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야 (남은 답변: Loss Function 정의),면접 시작 인사,0.000746428,0.0,0.0007464279769919813
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야 (남은 답변: Loss Function 정의),면접 종료,0.0009995141,0.0,0.0009995141299441457
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야 (남은 답변: Loss Function 정의),"인공지능, 머신러닝, 딥러닝 차이",0.0009105215,0.0,0.0009105214849114418
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야 (남은 답변: Loss Function 정의),잠시 휴식,-0.00783476,0.0,0.007834759540855885
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야 (남은 답변: Loss Function 정의),확률 예측에서 MSE Loss 미 사용 이유,-0.0027362898,0.0,0.0027362897526472807
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지! (남은 답변: 모든 질문 해결 완료)",BCE Loss 설명,-0.008360265,0.0,0.00836026482284069
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지! (남은 답변: 모든 질문 해결 완료)",LLM Fine-Tuning 의 PEFT,-0.00051943754,0.0,0.0005194375407882035
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지! (남은 답변: 모든 질문 해결 완료)",LoRA,0.00059539254,0.0,0.0005953925428912044
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지! (남은 답변: 모든 질문 해결 완료)",LoRA 와 QLoRA 의 차이,0.0032210338,0.0,0.003221033839508891
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지! (남은 답변: 모든 질문 해결 완료)",Loss Function 관련 실무 경험,-0.011397444,0.0,0.01139744371175766
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지! (남은 답변: 모든 질문 해결 완료)",Loss Function 예시,0.99618274,1.0,0.003817260265350342
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지! (남은 답변: 모든 질문 해결 완료)",Loss Function 정의,-0.004908842,0.0,0.004908842034637928
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지! (남은 답변: 모든 질문 해결 완료)",MBTI / 좋아하는 아이돌,-0.008056637,0.0,0.008056636899709702
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지! (남은 답변: 모든 질문 해결 완료)",MSE Loss 설명,-0.010033957,0.0,0.010033956728875637
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지! (남은 답변: 모든 질문 해결 완료)",MSE Loss 용도,-0.00024778378,0.0,0.000247783784288913
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지! (남은 답변: 모든 질문 해결 완료)","Multi-Class, Multi-Label 중 BCE 가 좋은 task",-0.00035443367,0.0,0.0003544336650520563
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지! (남은 답변: 모든 질문 해결 완료)",Multi-Label 에서 CE + Softmax 적용 문제점,-0.010152873,0.0,0.010152872651815414
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지! (남은 답변: 모든 질문 해결 완료)",PEFT 방법 5가지,0.010281804,0.0,0.010281804017722607
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지! (남은 답변: 모든 질문 해결 완료)",거대 언어 모델 정의,-0.003994925,0.0,0.003994924947619438
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지! (남은 답변: 모든 질문 해결 완료)",마지막 할 말,-0.005914624,0.0,0.005914623849093914
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지! (남은 답변: 모든 질문 해결 완료)",면접 시작 인사,-0.0008491872,0.0,0.0008491871994920075
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지! (남은 답변: 모든 질문 해결 완료)",면접 종료,-0.0026800376,0.0,0.002680037636309862
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지! (남은 답변: 모든 질문 해결 완료)","인공지능, 머신러닝, 딥러닝 차이",0.003158673,0.0,0.003158672945573926
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지! (남은 답변: 모든 질문 해결 완료)",잠시 휴식,-0.00891502,0.0,0.008915020152926445
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지! (남은 답변: 모든 질문 해결 완료)",확률 예측에서 MSE Loss 미 사용 이유,-0.00019405619,0.0,0.00019405619241297245
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야 (남은 답변: Loss Function 정의),BCE Loss 설명,-0.008046572,0.0,0.008046572096645832
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야 (남은 답변: Loss Function 정의),LLM Fine-Tuning 의 PEFT,0.0023965228,0.0,0.0023965227883309126
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야 (남은 답변: Loss Function 정의),LoRA,-0.0019979207,0.0,0.00199792068451643
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야 (남은 답변: Loss Function 정의),LoRA 와 QLoRA 의 차이,-0.00043597864,0.0,0.00043597863987088203
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야 (남은 답변: Loss Function 정의),Loss Function 관련 실무 경험,-0.0060279006,0.0,0.006027900613844395
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야 (남은 답변: Loss Function 정의),Loss Function 예시,-0.005793264,0.0,0.005793264135718346
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야 (남은 답변: Loss Function 정의),Loss Function 정의,0.99768496,1.0,0.002315044403076172
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야 (남은 답변: Loss Function 정의),MBTI / 좋아하는 아이돌,0.0007915696,0.0,0.0007915695896372199
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야 (남은 답변: Loss Function 정의),MSE Loss 설명,-0.00461694,0.0,0.004616939928382635
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야 (남은 답변: Loss Function 정의),MSE Loss 용도,-0.0052783806,0.0,0.005278380587697029
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야 (남은 답변: Loss Function 정의),"Multi-Class, Multi-Label 중 BCE 가 좋은 task",0.0002905887,0.0,0.0002905887085944414
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야 (남은 답변: Loss Function 정의),Multi-Label 에서 CE + Softmax 적용 문제점,0.0055312496,0.0,0.005531249567866325
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야 (남은 답변: Loss Function 정의),PEFT 방법 5가지,-0.007641152,0.0,0.007641152013093233
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야 (남은 답변: Loss Function 정의),거대 언어 모델 정의,-0.0043577356,0.0,0.004357735626399517
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야 (남은 답변: Loss Function 정의),마지막 할 말,-0.007742307,0.0,0.007742307148873806
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야 (남은 답변: Loss Function 정의),면접 시작 인사,0.0017751203,0.0,0.0017751202685758471
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야 (남은 답변: Loss Function 정의),면접 종료,0.0019039044,0.0,0.0019039043691009283
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야 (남은 답변: Loss Function 정의),"인공지능, 머신러닝, 딥러닝 차이",0.00043710723,0.0,0.00043710722820833325
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야 (남은 답변: Loss Function 정의),잠시 휴식,-0.010404964,0.0,0.010404963977634907
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야 (남은 답변: Loss Function 정의),확률 예측에서 MSE Loss 미 사용 이유,-0.003681035,0.0,0.003681035013869405
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지 (남은 답변: 모든 질문 해결 완료),BCE Loss 설명,-0.003383956,0.0,0.003383955918252468
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지 (남은 답변: 모든 질문 해결 완료),LLM Fine-Tuning 의 PEFT,-0.001375257,0.0,0.0013752570375800133
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지 (남은 답변: 모든 질문 해결 완료),LoRA,0.00029995717,0.0,0.0002999571734108031
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지 (남은 답변: 모든 질문 해결 완료),LoRA 와 QLoRA 의 차이,0.00077652326,0.0,0.0007765232585370541
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지 (남은 답변: 모든 질문 해결 완료),Loss Function 관련 실무 경험,-0.008131061,0.0,0.008131060749292374
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지 (남은 답변: 모든 질문 해결 완료),Loss Function 예시,0.9961269,1.0,0.003873109817504883
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지 (남은 답변: 모든 질문 해결 완료),Loss Function 정의,-0.008585281,0.0,0.008585280738770962
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지 (남은 답변: 모든 질문 해결 완료),MBTI / 좋아하는 아이돌,-0.0069832546,0.0,0.006983254570513964
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지 (남은 답변: 모든 질문 해결 완료),MSE Loss 설명,-0.0050774515,0.0,0.005077451467514038
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지 (남은 답변: 모든 질문 해결 완료),MSE Loss 용도,-0.00033694037,0.0,0.00033694037119857967
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지 (남은 답변: 모든 질문 해결 완료),"Multi-Class, Multi-Label 중 BCE 가 좋은 task",0.004010781,0.0,0.004010781180113554
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지 (남은 답변: 모든 질문 해결 완료),Multi-Label 에서 CE + Softmax 적용 문제점,-0.007740036,0.0,0.007740036118775606
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지 (남은 답변: 모든 질문 해결 완료),PEFT 방법 5가지,0.0057679918,0.0,0.00576799176633358
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지 (남은 답변: 모든 질문 해결 완료),거대 언어 모델 정의,-0.0060199895,0.0,0.0060199894942343235
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지 (남은 답변: 모든 질문 해결 완료),마지막 할 말,-0.00648272,0.0,0.006482719909399748
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지 (남은 답변: 모든 질문 해결 완료),면접 시작 인사,-0.00033394285,0.0,0.00033394285128451884
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지 (남은 답변: 모든 질문 해결 완료),면접 종료,-0.005912843,0.0,0.005912843160331249
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지 (남은 답변: 모든 질문 해결 완료),"인공지능, 머신러닝, 딥러닝 차이",0.004371359,0.0,0.004371359013020992
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지 (남은 답변: 모든 질문 해결 완료),잠시 휴식,-0.008302514,0.0,0.00830251444131136
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지 (남은 답변: 모든 질문 해결 완료),확률 예측에서 MSE Loss 미 사용 이유,0.0026613101,0.0,0.002661310136318207
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데 (남은 답변: Loss Function 예시),BCE Loss 설명,-0.0034129773,0.0,0.0034129773266613483
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데 (남은 답변: Loss Function 예시),LLM Fine-Tuning 의 PEFT,-0.005637931,0.0,0.0056379311718046665
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데 (남은 답변: Loss Function 예시),LoRA,-0.0029545212,0.0,0.0029545212164521217
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데 (남은 답변: Loss Function 예시),LoRA 와 QLoRA 의 차이,0.005220723,0.0,0.005220722872763872
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데 (남은 답변: Loss Function 예시),Loss Function 관련 실무 경험,-0.010086057,0.0,0.010086056776344776
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데 (남은 답변: Loss Function 예시),Loss Function 예시,0.99671245,1.0,0.0032875537872314453
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데 (남은 답변: Loss Function 예시),Loss Function 정의,-0.0059080375,0.0,0.005908037535846233
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데 (남은 답변: Loss Function 예시),MBTI / 좋아하는 아이돌,-0.00581779,0.0,0.005817790050059557
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데 (남은 답변: Loss Function 예시),MSE Loss 설명,-0.013465185,0.0,0.013465184718370438
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데 (남은 답변: Loss Function 예시),MSE Loss 용도,-0.0030327993,0.0,0.00303279934450984
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데 (남은 답변: Loss Function 예시),"Multi-Class, Multi-Label 중 BCE 가 좋은 task",0.0012501769,0.0,0.0012501769233494997
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데 (남은 답변: Loss Function 예시),Multi-Label 에서 CE + Softmax 적용 문제점,-0.008086008,0.0,0.00808600801974535
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데 (남은 답변: Loss Function 예시),PEFT 방법 5가지,0.006293688,0.0,0.006293687969446182
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데 (남은 답변: Loss Function 예시),거대 언어 모델 정의,-0.003840077,0.0,0.00384007696993649
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데 (남은 답변: Loss Function 예시),마지막 할 말,0.0004973477,0.0,0.0004973476752638817
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데 (남은 답변: Loss Function 예시),면접 시작 인사,-0.00069974613,0.0,0.0006997461314313114
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데 (남은 답변: Loss Function 예시),면접 종료,-0.0010551048,0.0,0.0010551047744229436
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데 (남은 답변: Loss Function 예시),"인공지능, 머신러닝, 딥러닝 차이",0.004760019,0.0,0.004760019015520811
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데 (남은 답변: Loss Function 예시),잠시 휴식,-0.0037401926,0.0,0.00374019262380898
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데 (남은 답변: Loss Function 예시),확률 예측에서 MSE Loss 미 사용 이유,-0.0005749359,0.0,0.0005749359261244535
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지 (남은 답변: 모든 질문 해결 완료)",BCE Loss 설명,0.0041828416,0.0,0.004182841628789902
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지 (남은 답변: 모든 질문 해결 완료)",LLM Fine-Tuning 의 PEFT,-0.00423773,0.0,0.00423773005604744
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지 (남은 답변: 모든 질문 해결 완료)",LoRA,-0.010478888,0.0,0.01047888770699501
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지 (남은 답변: 모든 질문 해결 완료)",LoRA 와 QLoRA 의 차이,0.0050153206,0.0,0.005015320610255003
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지 (남은 답변: 모든 질문 해결 완료)",Loss Function 관련 실무 경험,-0.0025257962,0.0,0.0025257961824536324
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지 (남은 답변: 모든 질문 해결 완료)",Loss Function 예시,-0.011669624,0.0,0.011669623665511608
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지 (남은 답변: 모든 질문 해결 완료)",Loss Function 정의,-0.00063156406,0.0,0.0006315640639513731
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지 (남은 답변: 모든 질문 해결 완료)",MBTI / 좋아하는 아이돌,-0.00020669369,0.0,0.00020669368677772582
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지 (남은 답변: 모든 질문 해결 완료)",MSE Loss 설명,0.9947969,1.0,0.005203127861022949
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지 (남은 답변: 모든 질문 해결 완료)",MSE Loss 용도,0.00046043645,0.0,0.0004604364512488246
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지 (남은 답변: 모든 질문 해결 완료)","Multi-Class, Multi-Label 중 BCE 가 좋은 task",-0.0023782651,0.0,0.0023782651405781507
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지 (남은 답변: 모든 질문 해결 완료)",Multi-Label 에서 CE + Softmax 적용 문제점,-0.002309274,0.0,0.0023092739284038544
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지 (남은 답변: 모든 질문 해결 완료)",PEFT 방법 5가지,0.0052722865,0.0,0.005272286478430033
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지 (남은 답변: 모든 질문 해결 완료)",거대 언어 모델 정의,-0.0013169834,0.0,0.00131698336917907
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지 (남은 답변: 모든 질문 해결 완료)",마지막 할 말,-0.010738608,0.0,0.010738608427345753
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지 (남은 답변: 모든 질문 해결 완료)",면접 시작 인사,-0.017451147,0.0,0.017451146617531776
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지 (남은 답변: 모든 질문 해결 완료)",면접 종료,-0.0032375907,0.0,0.0032375906594097614
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지 (남은 답변: 모든 질문 해결 완료)","인공지능, 머신러닝, 딥러닝 차이",-0.0059518865,0.0,0.005951886530965567
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지 (남은 답변: 모든 질문 해결 완료)",잠시 휴식,-0.008409519,0.0,0.008409518748521805
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지 (남은 답변: 모든 질문 해결 완료)",확률 예측에서 MSE Loss 미 사용 이유,-0.0017317199,0.0,0.0017317199381068349
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어 (남은 답변: 모든 질문 해결 완료)",BCE Loss 설명,0.9912201,1.0,0.00877988338470459
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어 (남은 답변: 모든 질문 해결 완료)",LLM Fine-Tuning 의 PEFT,0.0033570668,0.0,0.00335706677287817
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어 (남은 답변: 모든 질문 해결 완료)",LoRA,-0.003649324,0.0,0.003649323945865035
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어 (남은 답변: 모든 질문 해결 완료)",LoRA 와 QLoRA 의 차이,-0.007497714,0.0,0.007497713901102543
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어 (남은 답변: 모든 질문 해결 완료)",Loss Function 관련 실무 경험,0.0012132287,0.0,0.0012132286792621017
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어 (남은 답변: 모든 질문 해결 완료)",Loss Function 예시,-0.016631402,0.0,0.01663140207529068
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어 (남은 답변: 모든 질문 해결 완료)",Loss Function 정의,-0.0022443894,0.0,0.002244389383122325
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어 (남은 답변: 모든 질문 해결 완료)",MBTI / 좋아하는 아이돌,-0.003268161,0.0,0.003268161090090871
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어 (남은 답변: 모든 질문 해결 완료)",MSE Loss 설명,0.004890039,0.0,0.004890039097517729
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어 (남은 답변: 모든 질문 해결 완료)",MSE Loss 용도,-0.018028596,0.0,0.01802859641611576
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어 (남은 답변: 모든 질문 해결 완료)","Multi-Class, Multi-Label 중 BCE 가 좋은 task",-0.0054371157,0.0,0.005437115672975779
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어 (남은 답변: 모든 질문 해결 완료)",Multi-Label 에서 CE + Softmax 적용 문제점,-0.0032300735,0.0,0.0032300734892487526
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어 (남은 답변: 모든 질문 해결 완료)",PEFT 방법 5가지,0.0048026135,0.0,0.004802613519132137
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어 (남은 답변: 모든 질문 해결 완료)",거대 언어 모델 정의,-0.0032834755,0.0,0.003283475525677204
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어 (남은 답변: 모든 질문 해결 완료)",마지막 할 말,-0.017276062,0.0,0.017276061698794365
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어 (남은 답변: 모든 질문 해결 완료)",면접 시작 인사,0.00027931994,0.0,0.0002793199382722378
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어 (남은 답변: 모든 질문 해결 완료)",면접 종료,-0.0026697267,0.0,0.0026697267312556505
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어 (남은 답변: 모든 질문 해결 완료)","인공지능, 머신러닝, 딥러닝 차이",-0.0015461814,0.0,0.001546181389130652
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어 (남은 답변: 모든 질문 해결 완료)",잠시 휴식,-0.0018946195,0.0,0.0018946195486932993
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어 (남은 답변: 모든 질문 해결 완료)",확률 예측에서 MSE Loss 미 사용 이유,-0.008604426,0.0,0.008604425936937332
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야? (남은 답변: Loss Function 예시)",BCE Loss 설명,-0.008382527,0.0,0.008382527157664299
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야? (남은 답변: Loss Function 예시)",LLM Fine-Tuning 의 PEFT,-0.0073858276,0.0,0.007385827600955963
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야? (남은 답변: Loss Function 예시)",LoRA,0.0017422664,0.0,0.0017422663513571024
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야? (남은 답변: Loss Function 예시)",LoRA 와 QLoRA 의 차이,-0.00017740422,0.0,0.00017740421753842384
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야? (남은 답변: Loss Function 예시)",Loss Function 관련 실무 경험,-0.011809648,0.0,0.011809648014605045
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야? (남은 답변: Loss Function 예시)",Loss Function 예시,0.9961493,1.0,0.003850698471069336
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야? (남은 답변: Loss Function 예시)",Loss Function 정의,-0.0068664574,0.0,0.0068664574064314365
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야? (남은 답변: Loss Function 예시)",MBTI / 좋아하는 아이돌,-0.004051966,0.0,0.0040519661270082
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야? (남은 답변: Loss Function 예시)",MSE Loss 설명,-0.0037696129,0.0,0.0037696128711104393
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야? (남은 답변: Loss Function 예시)",MSE Loss 용도,-0.0066733006,0.0,0.006673300638794899
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야? (남은 답변: Loss Function 예시)","Multi-Class, Multi-Label 중 BCE 가 좋은 task",0.0035491681,0.0,0.0035491681192070246
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야? (남은 답변: Loss Function 예시)",Multi-Label 에서 CE + Softmax 적용 문제점,-0.007909159,0.0,0.007909159176051617
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야? (남은 답변: Loss Function 예시)",PEFT 방법 5가지,0.006045734,0.0,0.006045734044164419
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야? (남은 답변: Loss Function 예시)",거대 언어 모델 정의,-0.0018298805,0.0,0.0018298805225640535
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야? (남은 답변: Loss Function 예시)",마지막 할 말,-0.003376173,0.0,0.00337617308832705
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야? (남은 답변: Loss Function 예시)",면접 시작 인사,0.003792671,0.0,0.0037926710210740566
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야? (남은 답변: Loss Function 예시)",면접 종료,0.000129604,0.0,0.00012960399908479303
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야? (남은 답변: Loss Function 예시)","인공지능, 머신러닝, 딥러닝 차이",0.0052138222,0.0,0.005213822238147259
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야? (남은 답변: Loss Function 예시)",잠시 휴식,-0.0068833735,0.0,0.006883373484015465
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야? (남은 답변: Loss Function 예시)",확률 예측에서 MSE Loss 미 사용 이유,-0.0029242572,0.0,0.002924257190898061
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어 (남은 답변: 모든 질문 해결 완료)",BCE Loss 설명,0.005466831,0.0,0.005466830916702747
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어 (남은 답변: 모든 질문 해결 완료)",LLM Fine-Tuning 의 PEFT,-0.004564381,0.0,0.004564381204545498
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어 (남은 답변: 모든 질문 해결 완료)",LoRA,-0.020245323,0.0,0.020245322957634926
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어 (남은 답변: 모든 질문 해결 완료)",LoRA 와 QLoRA 의 차이,-0.00273493,0.0,0.002734930021688342
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어 (남은 답변: 모든 질문 해결 완료)",Loss Function 관련 실무 경험,0.0009948014,0.0,0.0009948014048859477
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어 (남은 답변: 모든 질문 해결 완료)",Loss Function 예시,0.0017125668,0.0,0.0017125668236985803
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어 (남은 답변: 모든 질문 해결 완료)",Loss Function 정의,-0.0033342242,0.0,0.003334224224090576
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어 (남은 답변: 모든 질문 해결 완료)",MBTI / 좋아하는 아이돌,0.0094535705,0.0,0.00945357047021389
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어 (남은 답변: 모든 질문 해결 완료)",MSE Loss 설명,0.9929803,1.0,0.007019698619842529
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어 (남은 답변: 모든 질문 해결 완료)",MSE Loss 용도,-0.0068098954,0.0,0.006809895392507315
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어 (남은 답변: 모든 질문 해결 완료)","Multi-Class, Multi-Label 중 BCE 가 좋은 task",-0.008611255,0.0,0.008611255325376987
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어 (남은 답변: 모든 질문 해결 완료)",Multi-Label 에서 CE + Softmax 적용 문제점,-0.0016176426,0.0,0.0016176425851881504
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어 (남은 답변: 모든 질문 해결 완료)",PEFT 방법 5가지,0.0066310223,0.0,0.006631022319197655
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어 (남은 답변: 모든 질문 해결 완료)",거대 언어 모델 정의,0.0048286347,0.0,0.004828634671866894
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어 (남은 답변: 모든 질문 해결 완료)",마지막 할 말,-0.004564471,0.0,0.004564471077173948
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어 (남은 답변: 모든 질문 해결 완료)",면접 시작 인사,-0.0119055575,0.0,0.011905557475984097
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어 (남은 답변: 모든 질문 해결 완료)",면접 종료,-0.0057045547,0.0,0.005704554729163647
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어 (남은 답변: 모든 질문 해결 완료)","인공지능, 머신러닝, 딥러닝 차이",-0.005823898,0.0,0.005823898129165173
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어 (남은 답변: 모든 질문 해결 완료)",잠시 휴식,-0.0101605505,0.0,0.010160550475120544
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어 (남은 답변: 모든 질문 해결 완료)",확률 예측에서 MSE Loss 미 사용 이유,-0.00575788,0.0,0.0057578799314796925
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아 (남은 답변: 모든 질문 해결 완료)",BCE Loss 설명,0.99510086,1.0,0.004899144172668457
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아 (남은 답변: 모든 질문 해결 완료)",LLM Fine-Tuning 의 PEFT,0.0028145872,0.0,0.0028145872056484222
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아 (남은 답변: 모든 질문 해결 완료)",LoRA,-0.005181947,0.0,0.005181946791708469
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아 (남은 답변: 모든 질문 해결 완료)",LoRA 와 QLoRA 의 차이,-0.006017729,0.0,0.006017729174345732
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아 (남은 답변: 모든 질문 해결 완료)",Loss Function 관련 실무 경험,-0.00049174234,0.0,0.0004917423357255757
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아 (남은 답변: 모든 질문 해결 완료)",Loss Function 예시,-0.006052147,0.0,0.006052147131413221
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아 (남은 답변: 모든 질문 해결 완료)",Loss Function 정의,-0.0007618139,0.0,0.0007618138915859163
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아 (남은 답변: 모든 질문 해결 완료)",MBTI / 좋아하는 아이돌,-0.0023250007,0.0,0.0023250007070600986
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아 (남은 답변: 모든 질문 해결 완료)",MSE Loss 설명,-0.0057917796,0.0,0.005791779607534409
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아 (남은 답변: 모든 질문 해결 완료)",MSE Loss 용도,-0.013296118,0.0,0.013296118006110191
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아 (남은 답변: 모든 질문 해결 완료)","Multi-Class, Multi-Label 중 BCE 가 좋은 task",0.00056974584,0.0,0.0005697458400391042
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아 (남은 답변: 모든 질문 해결 완료)",Multi-Label 에서 CE + Softmax 적용 문제점,-0.0005850407,0.0,0.0005850407178513706
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아 (남은 답변: 모든 질문 해결 완료)",PEFT 방법 5가지,0.0014633443,0.0,0.001463344320654869
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아 (남은 답변: 모든 질문 해결 완료)",거대 언어 모델 정의,-0.0076985266,0.0,0.007698526605963707
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아 (남은 답변: 모든 질문 해결 완료)",마지막 할 말,-0.013913833,0.0,0.013913832604885101
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아 (남은 답변: 모든 질문 해결 완료)",면접 시작 인사,-0.0021344586,0.0,0.002134458627551794
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아 (남은 답변: 모든 질문 해결 완료)",면접 종료,0.002298226,0.0,0.0022982261143624783
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아 (남은 답변: 모든 질문 해결 완료)","인공지능, 머신러닝, 딥러닝 차이",-0.003221708,0.0,0.003221707884222269
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아 (남은 답변: 모든 질문 해결 완료)",잠시 휴식,-0.0077767177,0.0,0.007776717655360699
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아 (남은 답변: 모든 질문 해결 완료)",확률 예측에서 MSE Loss 미 사용 이유,-0.008786026,0.0,0.008786026388406754
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야 (남은 답변: 모든 질문 해결 완료),BCE Loss 설명,-0.0053361533,0.0,0.005336153320968151
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야 (남은 답변: 모든 질문 해결 완료),LLM Fine-Tuning 의 PEFT,0.0012248656,0.0,0.001224865554831922
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야 (남은 답변: 모든 질문 해결 완료),LoRA,0.00034770637,0.0,0.00034770637284964323
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야 (남은 답변: 모든 질문 해결 완료),LoRA 와 QLoRA 의 차이,-0.0024021454,0.0,0.00240214541554451
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야 (남은 답변: 모든 질문 해결 완료),Loss Function 관련 실무 경험,-0.0034345244,0.0,0.003434524405747652
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야 (남은 답변: 모든 질문 해결 완료),Loss Function 예시,-0.0014515696,0.0,0.0014515696093440056
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야 (남은 답변: 모든 질문 해결 완료),Loss Function 정의,-0.007361187,0.0,0.007361187133938074
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야 (남은 답변: 모든 질문 해결 완료),MBTI / 좋아하는 아이돌,-0.0022359255,0.0,0.0022359255235642195
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야 (남은 답변: 모든 질문 해결 완료),MSE Loss 설명,0.005657193,0.0,0.005657192785292864
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야 (남은 답변: 모든 질문 해결 완료),MSE Loss 용도,0.9946696,1.0,0.005330383777618408
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야 (남은 답변: 모든 질문 해결 완료),"Multi-Class, Multi-Label 중 BCE 가 좋은 task",-0.015391524,0.0,0.015391523949801922
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야 (남은 답변: 모든 질문 해결 완료),Multi-Label 에서 CE + Softmax 적용 문제점,-0.0039249244,0.0,0.003924924414604902
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야 (남은 답변: 모든 질문 해결 완료),PEFT 방법 5가지,0.00020792612,0.0,0.00020792611758224666
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야 (남은 답변: 모든 질문 해결 완료),거대 언어 모델 정의,-0.0043823887,0.0,0.004382388666272163
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야 (남은 답변: 모든 질문 해결 완료),마지막 할 말,0.0020880532,0.0,0.0020880531519651413
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야 (남은 답변: 모든 질문 해결 완료),면접 시작 인사,-0.005026813,0.0,0.005026813130825758
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야 (남은 답변: 모든 질문 해결 완료),면접 종료,-0.0013788188,0.0,0.0013788187643513083
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야 (남은 답변: 모든 질문 해결 완료),"인공지능, 머신러닝, 딥러닝 차이",0.0011796622,0.0,0.0011796621838584542
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야 (남은 답변: 모든 질문 해결 완료),잠시 휴식,-0.0058334735,0.0,0.005833473522216082
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야 (남은 답변: 모든 질문 해결 완료),확률 예측에서 MSE Loss 미 사용 이유,-0.008382934,0.0,0.008382934145629406
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지? (남은 답변: MSE Loss 설명),BCE Loss 설명,0.0008282643,0.0,0.0008282642811536789
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지? (남은 답변: MSE Loss 설명),LLM Fine-Tuning 의 PEFT,-0.0045948047,0.0,0.004594804719090462
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지? (남은 답변: MSE Loss 설명),LoRA,-0.011238061,0.0,0.011238060891628265
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지? (남은 답변: MSE Loss 설명),LoRA 와 QLoRA 의 차이,0.0014583034,0.0,0.001458303420804441
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지? (남은 답변: MSE Loss 설명),Loss Function 관련 실무 경험,0.00017794853,0.0,0.0001779485319275409
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지? (남은 답변: MSE Loss 설명),Loss Function 예시,-0.015787674,0.0,0.015787674114108086
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지? (남은 답변: MSE Loss 설명),Loss Function 정의,-0.0023701612,0.0,0.002370161237195134
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지? (남은 답변: MSE Loss 설명),MBTI / 좋아하는 아이돌,-0.0022383693,0.0,0.0022383693140000105
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지? (남은 답변: MSE Loss 설명),MSE Loss 설명,0.99663305,1.0,0.0033669471740722656
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지? (남은 답변: MSE Loss 설명),MSE Loss 용도,-0.004786245,0.0,0.004786245059221983
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지? (남은 답변: MSE Loss 설명),"Multi-Class, Multi-Label 중 BCE 가 좋은 task",-0.0015040488,0.0,0.001504048821516335
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지? (남은 답변: MSE Loss 설명),Multi-Label 에서 CE + Softmax 적용 문제점,-0.00014269387,0.0,0.00014269386883825064
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지? (남은 답변: MSE Loss 설명),PEFT 방법 5가지,0.0063628573,0.0,0.006362857297062874
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지? (남은 답변: MSE Loss 설명),거대 언어 모델 정의,-0.0011762641,0.0,0.0011762641370296478
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지? (남은 답변: MSE Loss 설명),마지막 할 말,-0.006371101,0.0,0.006371100898832083
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지? (남은 답변: MSE Loss 설명),면접 시작 인사,-0.007708907,0.0,0.007708907127380371
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지? (남은 답변: MSE Loss 설명),면접 종료,-0.01189047,0.0,0.011890470050275326
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지? (남은 답변: MSE Loss 설명),"인공지능, 머신러닝, 딥러닝 차이",-0.0029585594,0.0,0.0029585594311356544
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지? (남은 답변: MSE Loss 설명),잠시 휴식,-0.0012897676,0.0,0.0012897675624117255
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지? (남은 답변: MSE Loss 설명),확률 예측에서 MSE Loss 미 사용 이유,-0.0030635505,0.0,0.003063550451770425
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지 (남은 답변: 모든 질문 해결 완료)",BCE Loss 설명,-0.011991416,0.0,0.011991416104137897
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지 (남은 답변: 모든 질문 해결 완료)",LLM Fine-Tuning 의 PEFT,-0.0068324166,0.0,0.006832416635006666
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지 (남은 답변: 모든 질문 해결 완료)",LoRA,-0.008407682,0.0,0.008407682180404663
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지 (남은 답변: 모든 질문 해결 완료)",LoRA 와 QLoRA 의 차이,0.007809152,0.0,0.007809151895344257
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지 (남은 답변: 모든 질문 해결 완료)",Loss Function 관련 실무 경험,0.002944716,0.0,0.0029447160195559263
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지 (남은 답변: 모든 질문 해결 완료)",Loss Function 예시,-0.0003716499,0.0,0.00037164989043958485
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지 (남은 답변: 모든 질문 해결 완료)",Loss Function 정의,-0.0032374237,0.0,0.0032374237198382616
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지 (남은 답변: 모든 질문 해결 완료)",MBTI / 좋아하는 아이돌,-0.0036057474,0.0,0.003605747362598777
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지 (남은 답변: 모든 질문 해결 완료)",MSE Loss 설명,-0.008128859,0.0,0.008128859102725983
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지 (남은 답변: 모든 질문 해결 완료)",MSE Loss 용도,-0.0065933466,0.0,0.00659334659576416
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지 (남은 답변: 모든 질문 해결 완료)","Multi-Class, Multi-Label 중 BCE 가 좋은 task",-0.002414279,0.0,0.002414278918877244
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지 (남은 답변: 모든 질문 해결 완료)",Multi-Label 에서 CE + Softmax 적용 문제점,-0.0019473131,0.0,0.001947313081473112
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지 (남은 답변: 모든 질문 해결 완료)",PEFT 방법 5가지,0.0018788642,0.0,0.0018788642482832074
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지 (남은 답변: 모든 질문 해결 완료)",거대 언어 모델 정의,0.00965278,0.0,0.009652780368924141
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지 (남은 답변: 모든 질문 해결 완료)",마지막 할 말,-0.008012265,0.0,0.008012264966964722
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지 (남은 답변: 모든 질문 해결 완료)",면접 시작 인사,-0.007657676,0.0,0.007657676003873348
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지 (남은 답변: 모든 질문 해결 완료)",면접 종료,0.0034050457,0.0,0.0034050457179546356
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지 (남은 답변: 모든 질문 해결 완료)","인공지능, 머신러닝, 딥러닝 차이",-0.005949015,0.0,0.00594901479780674
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지 (남은 답변: 모든 질문 해결 완료)",잠시 휴식,-0.00297244,0.0,0.002972440095618367
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지 (남은 답변: 모든 질문 해결 완료)",확률 예측에서 MSE Loss 미 사용 이유,0.9964891,1.0,0.003510892391204834
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지 (남은 답변: MSE Loss 용도),BCE Loss 설명,-0.00937854,0.0,0.009378540329635143
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지 (남은 답변: MSE Loss 용도),LLM Fine-Tuning 의 PEFT,3.0573927e-05,0.0,3.0573926778743044e-05
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지 (남은 답변: MSE Loss 용도),LoRA,0.0009183475,0.0,0.0009183475049212575
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지 (남은 답변: MSE Loss 용도),LoRA 와 QLoRA 의 차이,0.0025062202,0.0,0.0025062202475965023
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지 (남은 답변: MSE Loss 용도),Loss Function 관련 실무 경험,-0.0035702086,0.0,0.0035702085588127375
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지 (남은 답변: MSE Loss 용도),Loss Function 예시,-0.0058233854,0.0,0.005823385436087847
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지 (남은 답변: MSE Loss 용도),Loss Function 정의,-0.0030093598,0.0,0.0030093598179519176
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지 (남은 답변: MSE Loss 용도),MBTI / 좋아하는 아이돌,0.00038210774,0.0,0.0003821077407337725
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지 (남은 답변: MSE Loss 용도),MSE Loss 설명,-0.0006717106,0.0,0.0006717105861753225
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지 (남은 답변: MSE Loss 용도),MSE Loss 용도,0.9967928,1.0,0.0032072067260742188
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지 (남은 답변: MSE Loss 용도),"Multi-Class, Multi-Label 중 BCE 가 좋은 task",-0.007533585,0.0,0.00753358518704772
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지 (남은 답변: MSE Loss 용도),Multi-Label 에서 CE + Softmax 적용 문제점,-0.0039522485,0.0,0.003952248487621546
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지 (남은 답변: MSE Loss 용도),PEFT 방법 5가지,0.0014442328,0.0,0.0014442327665165067
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지 (남은 답변: MSE Loss 용도),거대 언어 모델 정의,-0.0040193712,0.0,0.00401937123388052
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지 (남은 답변: MSE Loss 용도),마지막 할 말,0.002696593,0.0,0.0026965930592268705
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지 (남은 답변: MSE Loss 용도),면접 시작 인사,-0.008900412,0.0,0.008900412358343601
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지 (남은 답변: MSE Loss 용도),면접 종료,-0.0004531682,0.0,0.0004531682061497122
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지 (남은 답변: MSE Loss 용도),"인공지능, 머신러닝, 딥러닝 차이",0.0024854743,0.0,0.0024854743387550116
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지 (남은 답변: MSE Loss 용도),잠시 휴식,-0.0072996398,0.0,0.007299639750272036
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지 (남은 답변: MSE Loss 용도),확률 예측에서 MSE Loss 미 사용 이유,-0.009505711,0.0,0.009505710564553738
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야? (남은 답변: 확률 예측에서 MSE Loss 미 사용 이유),BCE Loss 설명,-0.008940324,0.0,0.008940324187278748
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야? (남은 답변: 확률 예측에서 MSE Loss 미 사용 이유),LLM Fine-Tuning 의 PEFT,-0.0018730509,0.0,0.0018730509327724576
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야? (남은 답변: 확률 예측에서 MSE Loss 미 사용 이유),LoRA,-0.006259064,0.0,0.006259064190089703
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야? (남은 답변: 확률 예측에서 MSE Loss 미 사용 이유),LoRA 와 QLoRA 의 차이,0.005538465,0.0,0.005538464989513159
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야? (남은 답변: 확률 예측에서 MSE Loss 미 사용 이유),Loss Function 관련 실무 경험,0.002692622,0.0,0.00269262189976871
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야? (남은 답변: 확률 예측에서 MSE Loss 미 사용 이유),Loss Function 예시,-0.0013978892,0.0,0.0013978892238810658
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야? (남은 답변: 확률 예측에서 MSE Loss 미 사용 이유),Loss Function 정의,-0.005410617,0.0,0.005410617217421532
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야? (남은 답변: 확률 예측에서 MSE Loss 미 사용 이유),MBTI / 좋아하는 아이돌,-0.0034337738,0.0,0.003433773759752512
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야? (남은 답변: 확률 예측에서 MSE Loss 미 사용 이유),MSE Loss 설명,-0.0022512197,0.0,0.002251219702884555
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야? (남은 답변: 확률 예측에서 MSE Loss 미 사용 이유),MSE Loss 용도,-0.005208911,0.0,0.005208910908550024
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야? (남은 답변: 확률 예측에서 MSE Loss 미 사용 이유),"Multi-Class, Multi-Label 중 BCE 가 좋은 task",-0.00016863999,0.0,0.00016863999189808965
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야? (남은 답변: 확률 예측에서 MSE Loss 미 사용 이유),Multi-Label 에서 CE + Softmax 적용 문제점,-0.007374405,0.0,0.007374404929578304
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야? (남은 답변: 확률 예측에서 MSE Loss 미 사용 이유),PEFT 방법 5가지,-0.0017215478,0.0,0.001721547800116241
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야? (남은 답변: 확률 예측에서 MSE Loss 미 사용 이유),거대 언어 모델 정의,-0.007271661,0.0,0.007271660957485437
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야? (남은 답변: 확률 예측에서 MSE Loss 미 사용 이유),마지막 할 말,-0.0036509237,0.0,0.003650923725217581
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야? (남은 답변: 확률 예측에서 MSE Loss 미 사용 이유),면접 시작 인사,-0.0038130556,0.0,0.0038130555767565966
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야? (남은 답변: 확률 예측에서 MSE Loss 미 사용 이유),면접 종료,0.0010917261,0.0,0.0010917261242866516
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야? (남은 답변: 확률 예측에서 MSE Loss 미 사용 이유),"인공지능, 머신러닝, 딥러닝 차이",-0.005542683,0.0,0.005542682949453592
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야? (남은 답변: 확률 예측에서 MSE Loss 미 사용 이유),잠시 휴식,0.0029510944,0.0,0.002951094415038824
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야? (남은 답변: 확률 예측에서 MSE Loss 미 사용 이유),확률 예측에서 MSE Loss 미 사용 이유,0.9981942,1.0,0.0018057823181152344
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고? (남은 답변: 모든 질문 해결 완료),BCE Loss 설명,0.98512757,1.0,0.014872431755065918
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고? (남은 답변: 모든 질문 해결 완료),LLM Fine-Tuning 의 PEFT,-0.0028583447,0.0,0.0028583446983247995
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고? (남은 답변: 모든 질문 해결 완료),LoRA,0.0044398424,0.0,0.00443984242156148
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고? (남은 답변: 모든 질문 해결 완료),LoRA 와 QLoRA 의 차이,-0.0065240376,0.0,0.006524037569761276
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고? (남은 답변: 모든 질문 해결 완료),Loss Function 관련 실무 경험,-0.00097260624,0.0,0.0009726062417030334
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고? (남은 답변: 모든 질문 해결 완료),Loss Function 예시,-0.009715887,0.0,0.009715886786580086
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고? (남은 답변: 모든 질문 해결 완료),Loss Function 정의,-0.009503574,0.0,0.00950357411056757
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고? (남은 답변: 모든 질문 해결 완료),MBTI / 좋아하는 아이돌,-0.010081031,0.0,0.010081031359732151
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고? (남은 답변: 모든 질문 해결 완료),MSE Loss 설명,0.009058235,0.0,0.009058235213160515
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고? (남은 답변: 모든 질문 해결 완료),MSE Loss 용도,-0.0042474647,0.0,0.004247464705258608
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고? (남은 답변: 모든 질문 해결 완료),"Multi-Class, Multi-Label 중 BCE 가 좋은 task",0.013842793,0.0,0.013842793181538582
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고? (남은 답변: 모든 질문 해결 완료),Multi-Label 에서 CE + Softmax 적용 문제점,-0.0056469054,0.0,0.005646905396133661
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고? (남은 답변: 모든 질문 해결 완료),PEFT 방법 5가지,-0.010541344,0.0,0.010541344061493874
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고? (남은 답변: 모든 질문 해결 완료),거대 언어 모델 정의,-0.0076795495,0.0,0.007679549511522055
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고? (남은 답변: 모든 질문 해결 완료),마지막 할 말,-0.024332477,0.0,0.024332476779818535
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고? (남은 답변: 모든 질문 해결 완료),면접 시작 인사,-0.01419791,0.0,0.014197910204529762
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고? (남은 답변: 모든 질문 해결 완료),면접 종료,-0.011668319,0.0,0.011668318882584572
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고? (남은 답변: 모든 질문 해결 완료),"인공지능, 머신러닝, 딥러닝 차이",-0.007930974,0.0,0.007930973544716835
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고? (남은 답변: 모든 질문 해결 완료),잠시 휴식,-0.0042101815,0.0,0.004210181534290314
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고? (남은 답변: 모든 질문 해결 완료),확률 예측에서 MSE Loss 미 사용 이유,0.01595559,0.0,0.015955589711666107
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고 (남은 답변: 핵심 아이디어),BCE Loss 설명,0.9956092,1.0,0.00439077615737915
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고 (남은 답변: 핵심 아이디어),LLM Fine-Tuning 의 PEFT,-0.0008897437,0.0,0.0008897436782717705
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고 (남은 답변: 핵심 아이디어),LoRA,-0.004244475,0.0,0.004244475159794092
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고 (남은 답변: 핵심 아이디어),LoRA 와 QLoRA 의 차이,-0.0030020415,0.0,0.0030020414851605892
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고 (남은 답변: 핵심 아이디어),Loss Function 관련 실무 경험,-0.00044336074,0.0,0.00044336073915474117
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고 (남은 답변: 핵심 아이디어),Loss Function 예시,-0.0009682587,0.0,0.0009682587115094066
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고 (남은 답변: 핵심 아이디어),Loss Function 정의,-0.0048394823,0.0,0.0048394822515547276
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고 (남은 답변: 핵심 아이디어),MBTI / 좋아하는 아이돌,-0.0013697066,0.0,0.0013697065878659487
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고 (남은 답변: 핵심 아이디어),MSE Loss 설명,-0.0005233419,0.0,0.0005233418778516352
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고 (남은 답변: 핵심 아이디어),MSE Loss 용도,-0.011747348,0.0,0.011747348122298717
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고 (남은 답변: 핵심 아이디어),"Multi-Class, Multi-Label 중 BCE 가 좋은 task",-0.00058584247,0.0,0.0005858424701727927
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고 (남은 답변: 핵심 아이디어),Multi-Label 에서 CE + Softmax 적용 문제점,-0.0018065154,0.0,0.0018065153853967786
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고 (남은 답변: 핵심 아이디어),PEFT 방법 5가지,0.0008160949,0.0,0.0008160949219018221
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고 (남은 답변: 핵심 아이디어),거대 언어 모델 정의,-0.0035574574,0.0,0.0035574573557823896
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고 (남은 답변: 핵심 아이디어),마지막 할 말,-0.01505628,0.0,0.015056280419230461
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고 (남은 답변: 핵심 아이디어),면접 시작 인사,-0.0048619453,0.0,0.004861945286393166
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고 (남은 답변: 핵심 아이디어),면접 종료,0.0002587535,0.0,0.0002587535127531737
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고 (남은 답변: 핵심 아이디어),"인공지능, 머신러닝, 딥러닝 차이",-0.004707552,0.0,0.0047075520269572735
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고 (남은 답변: 핵심 아이디어),잠시 휴식,-0.00652289,0.0,0.00652289018034935
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고 (남은 답변: 핵심 아이디어),확률 예측에서 MSE Loss 미 사용 이유,-0.010302583,0.0,0.010302582755684853
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거 (남은 답변: 모든 질문 해결 완료)",BCE Loss 설명,-0.0132456,0.0,0.013245600275695324
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거 (남은 답변: 모든 질문 해결 완료)",LLM Fine-Tuning 의 PEFT,0.0063944357,0.0,0.006394435651600361
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거 (남은 답변: 모든 질문 해결 완료)",LoRA,-0.009031846,0.0,0.009031846188008785
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거 (남은 답변: 모든 질문 해결 완료)",LoRA 와 QLoRA 의 차이,0.002658876,0.0,0.0026588758919388056
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거 (남은 답변: 모든 질문 해결 완료)",Loss Function 관련 실무 경험,-0.0038453147,0.0,0.0038453147280961275
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거 (남은 답변: 모든 질문 해결 완료)",Loss Function 예시,-0.0009575461,0.0,0.0009575461153872311
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거 (남은 답변: 모든 질문 해결 완료)",Loss Function 정의,0.0013495985,0.0,0.0013495985185727477
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거 (남은 답변: 모든 질문 해결 완료)",MBTI / 좋아하는 아이돌,-0.0051590833,0.0,0.005159083288162947
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거 (남은 답변: 모든 질문 해결 완료)",MSE Loss 설명,-0.004412614,0.0,0.004412613809108734
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거 (남은 답변: 모든 질문 해결 완료)",MSE Loss 용도,-0.008887682,0.0,0.008887682110071182
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거 (남은 답변: 모든 질문 해결 완료)","Multi-Class, Multi-Label 중 BCE 가 좋은 task",0.9923226,1.0,0.0076773762702941895
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거 (남은 답변: 모든 질문 해결 완료)",Multi-Label 에서 CE + Softmax 적용 문제점,-0.0014021685,0.0,0.0014021685346961021
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거 (남은 답변: 모든 질문 해결 완료)",PEFT 방법 5가지,-0.0016175921,0.0,0.0016175920609384775
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거 (남은 답변: 모든 질문 해결 완료)",거대 언어 모델 정의,1.45935555e-05,0.0,1.4593555533792824e-05
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거 (남은 답변: 모든 질문 해결 완료)",마지막 할 말,0.00064482907,0.0,0.0006448290660046041
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거 (남은 답변: 모든 질문 해결 완료)",면접 시작 인사,0.0035674903,0.0,0.0035674902610480785
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거 (남은 답변: 모든 질문 해결 완료)",면접 종료,0.0059305755,0.0,0.005930575542151928
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거 (남은 답변: 모든 질문 해결 완료)","인공지능, 머신러닝, 딥러닝 차이",-9.819736e-05,0.0,9.819735714700073e-05
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거 (남은 답변: 모든 질문 해결 완료)",잠시 휴식,-0.0065048407,0.0,0.006504840683192015
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거 (남은 답변: 모든 질문 해결 완료)",확률 예측에서 MSE Loss 미 사용 이유,-0.0007373606,0.0,0.000737360620405525
"BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야? (남은 답변: 핵심 아이디어, 수식)",BCE Loss 설명,0.98855525,1.0,0.01144474744796753
"BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야? (남은 답변: 핵심 아이디어, 수식)",LLM Fine-Tuning 의 PEFT,-0.0037288596,0.0,0.003728859592229128
"BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야? (남은 답변: 핵심 아이디어, 수식)",LoRA,-0.0104627265,0.0,0.010462726466357708
"BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야? (남은 답변: 핵심 아이디어, 수식)",LoRA 와 QLoRA 의 차이,-0.0015603515,0.0,0.0015603514621034265
"BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야? (남은 답변: 핵심 아이디어, 수식)",Loss Function 관련 실무 경험,0.015192614,0.0,0.015192613936960697
"BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야? (남은 답변: 핵심 아이디어, 수식)",Loss Function 예시,0.00062935136,0.0,0.0006293513579294086
"BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야? (남은 답변: 핵심 아이디어, 수식)",Loss Function 정의,0.013361705,0.0,0.013361704535782337
"BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야? (남은 답변: 핵심 아이디어, 수식)",MBTI / 좋아하는 아이돌,4.731441e-05,0.0,4.731441003968939e-05
"BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야? (남은 답변: 핵심 아이디어, 수식)",MSE Loss 설명,-0.0004813601,0.0,0.0004813600971829146
"BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야? (남은 답변: 핵심 아이디어, 수식)",MSE Loss 용도,-0.010609049,0.0,0.01060904935002327
"BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야? (남은 답변: 핵심 아이디어, 수식)","Multi-Class, Multi-Label 중 BCE 가 좋은 task",-0.008553544,0.0,0.00855354405939579
"BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야? (남은 답변: 핵심 아이디어, 수식)",Multi-Label 에서 CE + Softmax 적용 문제점,-0.0007094358,0.0,0.0007094357861205935
"BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야? (남은 답변: 핵심 아이디어, 수식)",PEFT 방법 5가지,0.021631187,0.0,0.021631186828017235
"BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야? (남은 답변: 핵심 아이디어, 수식)",거대 언어 모델 정의,-0.007148664,0.0,0.007148663979023695
"BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야? (남은 답변: 핵심 아이디어, 수식)",마지막 할 말,-0.026941331,0.0,0.0269413311034441
"BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야? (남은 답변: 핵심 아이디어, 수식)",면접 시작 인사,-0.006871183,0.0,0.0068711829371750355
"BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야? (남은 답변: 핵심 아이디어, 수식)",면접 종료,-0.0015646596,0.0,0.0015646596439182758
"BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야? (남은 답변: 핵심 아이디어, 수식)","인공지능, 머신러닝, 딥러닝 차이",-0.014040667,0.0,0.01404066663235426
"BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야? (남은 답변: 핵심 아이디어, 수식)",잠시 휴식,-0.016082717,0.0,0.01608271710574627
"BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야? (남은 답변: 핵심 아이디어, 수식)",확률 예측에서 MSE Loss 미 사용 이유,-0.012292612,0.0,0.012292612344026566
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고 (남은 답변: 수식)",BCE Loss 설명,0.9956857,1.0,0.004314303398132324
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고 (남은 답변: 수식)",LLM Fine-Tuning 의 PEFT,-0.0012392297,0.0,0.001239229692146182
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고 (남은 답변: 수식)",LoRA,-0.0033171442,0.0,0.0033171442337334156
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고 (남은 답변: 수식)",LoRA 와 QLoRA 의 차이,-0.0013840821,0.0,0.0013840821338817477
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고 (남은 답변: 수식)",Loss Function 관련 실무 경험,-0.0012023699,0.0,0.0012023699237033725
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고 (남은 답변: 수식)",Loss Function 예시,-0.0023020185,0.0,0.0023020184598863125
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고 (남은 답변: 수식)",Loss Function 정의,-0.0043544625,0.0,0.004354462493211031
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고 (남은 답변: 수식)",MBTI / 좋아하는 아이돌,-0.0061504226,0.0,0.006150422617793083
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고 (남은 답변: 수식)",MSE Loss 설명,-0.0044637984,0.0,0.004463798366487026
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고 (남은 답변: 수식)",MSE Loss 용도,-0.010574309,0.0,0.010574309155344963
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고 (남은 답변: 수식)","Multi-Class, Multi-Label 중 BCE 가 좋은 task",-0.0009866327,0.0,0.0009866326581686735
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고 (남은 답변: 수식)",Multi-Label 에서 CE + Softmax 적용 문제점,-0.0001875807,0.0,0.00018758070655167103
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고 (남은 답변: 수식)",PEFT 방법 5가지,-0.0015568461,0.0,0.0015568460803478956
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고 (남은 답변: 수식)",거대 언어 모델 정의,-0.0045963125,0.0,0.004596312530338764
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고 (남은 답변: 수식)",마지막 할 말,-0.015630268,0.0,0.015630267560482025
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고 (남은 답변: 수식)",면접 시작 인사,-0.0052565928,0.0,0.005256592761725187
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고 (남은 답변: 수식)",면접 종료,-0.0014158131,0.0,0.0014158131089061499
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고 (남은 답변: 수식)","인공지능, 머신러닝, 딥러닝 차이",-0.0035051198,0.0,0.0035051198210567236
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고 (남은 답변: 수식)",잠시 휴식,-0.008548462,0.0,0.008548461832106113
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고 (남은 답변: 수식)",확률 예측에서 MSE Loss 미 사용 이유,-0.006284693,0.0,0.006284692790359259
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')] (남은 답변: 핵심 아이디어)",BCE Loss 설명,0.99507236,1.0,0.004927635192871094
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')] (남은 답변: 핵심 아이디어)",LLM Fine-Tuning 의 PEFT,-0.0029454285,0.0,0.002945428481325507
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')] (남은 답변: 핵심 아이디어)",LoRA,-0.0034952043,0.0,0.0034952042624354362
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')] (남은 답변: 핵심 아이디어)",LoRA 와 QLoRA 의 차이,-0.0033552793,0.0,0.0033552793320268393
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')] (남은 답변: 핵심 아이디어)",Loss Function 관련 실무 경험,-0.0004957318,0.0,0.0004957317723892629
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')] (남은 답변: 핵심 아이디어)",Loss Function 예시,-0.0019176153,0.0,0.0019176153000444174
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')] (남은 답변: 핵심 아이디어)",Loss Function 정의,-0.0017129859,0.0,0.0017129859188571572
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')] (남은 답변: 핵심 아이디어)",MBTI / 좋아하는 아이돌,-0.004244654,0.0,0.004244653973728418
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')] (남은 답변: 핵심 아이디어)",MSE Loss 설명,0.0017279923,0.0,0.0017279923195019364
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')] (남은 답변: 핵심 아이디어)",MSE Loss 용도,-0.009985921,0.0,0.00998592097312212
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')] (남은 답변: 핵심 아이디어)","Multi-Class, Multi-Label 중 BCE 가 좋은 task",-0.0006733404,0.0,0.0006733404006808996
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')] (남은 답변: 핵심 아이디어)",Multi-Label 에서 CE + Softmax 적용 문제점,-0.0034679135,0.0,0.0034679134842008352
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')] (남은 답변: 핵심 아이디어)",PEFT 방법 5가지,-0.0035846573,0.0,0.0035846573300659657
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')] (남은 답변: 핵심 아이디어)",거대 언어 모델 정의,-0.002956211,0.0,0.0029562111012637615
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')] (남은 답변: 핵심 아이디어)",마지막 할 말,-0.015741063,0.0,0.01574106328189373
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')] (남은 답변: 핵심 아이디어)",면접 시작 인사,-0.007893742,0.0,0.007893742062151432
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')] (남은 답변: 핵심 아이디어)",면접 종료,0.0002537187,0.0,0.00025371869560331106
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')] (남은 답변: 핵심 아이디어)","인공지능, 머신러닝, 딥러닝 차이",-0.004094309,0.0,0.00409430917352438
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')] (남은 답변: 핵심 아이디어)",잠시 휴식,-0.0063659814,0.0,0.0063659814186394215
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')] (남은 답변: 핵심 아이디어)",확률 예측에서 MSE Loss 미 사용 이유,-0.0052344115,0.0,0.0052344114519655704
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다 (남은 답변: 모든 질문 해결 완료)",BCE Loss 설명,-0.012152256,0.0,0.012152256444096565
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다 (남은 답변: 모든 질문 해결 완료)",LLM Fine-Tuning 의 PEFT,0.004260321,0.0,0.004260321147739887
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다 (남은 답변: 모든 질문 해결 완료)",LoRA,-0.009864995,0.0,0.009864995256066322
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다 (남은 답변: 모든 질문 해결 완료)",LoRA 와 QLoRA 의 차이,0.0049987976,0.0,0.0049987975507974625
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다 (남은 답변: 모든 질문 해결 완료)",Loss Function 관련 실무 경험,-0.003367851,0.0,0.00336785102263093
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다 (남은 답변: 모든 질문 해결 완료)",Loss Function 예시,-0.0021444506,0.0,0.0021444505546242
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다 (남은 답변: 모든 질문 해결 완료)",Loss Function 정의,-0.002111475,0.0,0.002111474983394146
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다 (남은 답변: 모든 질문 해결 완료)",MBTI / 좋아하는 아이돌,-0.006631869,0.0,0.00663186889141798
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다 (남은 답변: 모든 질문 해결 완료)",MSE Loss 설명,0.00028280303,0.0,0.00028280302649363875
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다 (남은 답변: 모든 질문 해결 완료)",MSE Loss 용도,-0.0059745195,0.0,0.005974519532173872
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다 (남은 답변: 모든 질문 해결 완료)","Multi-Class, Multi-Label 중 BCE 가 좋은 task",0.9920329,1.0,0.007967114448547363
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다 (남은 답변: 모든 질문 해결 완료)",Multi-Label 에서 CE + Softmax 적용 문제점,-0.00011340547,0.0,0.00011340546916471794
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다 (남은 답변: 모든 질문 해결 완료)",PEFT 방법 5가지,0.0015346945,0.0,0.0015346944564953446
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다 (남은 답변: 모든 질문 해결 완료)",거대 언어 모델 정의,0.0005435496,0.0,0.00054354959866032
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다 (남은 답변: 모든 질문 해결 완료)",마지막 할 말,-0.0049641207,0.0,0.00496412068605423
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다 (남은 답변: 모든 질문 해결 완료)",면접 시작 인사,0.005332477,0.0,0.0053324769251048565
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다 (남은 답변: 모든 질문 해결 완료)",면접 종료,0.0034404702,0.0,0.003440470201894641
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다 (남은 답변: 모든 질문 해결 완료)","인공지능, 머신러닝, 딥러닝 차이",-0.0030414711,0.0,0.0030414711218327284
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다 (남은 답변: 모든 질문 해결 완료)",잠시 휴식,-0.002586117,0.0,0.0025861170142889023
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다 (남은 답변: 모든 질문 해결 완료)",확률 예측에서 MSE Loss 미 사용 이유,-0.0001697103,0.0,0.00016971029981505126
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거 (남은 답변: 핵심 아이디어),BCE Loss 설명,0.9960612,1.0,0.003938794136047363
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거 (남은 답변: 핵심 아이디어),LLM Fine-Tuning 의 PEFT,-0.0012036143,0.0,0.0012036142870783806
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거 (남은 답변: 핵심 아이디어),LoRA,-0.0022352748,0.0,0.002235274761915207
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거 (남은 답변: 핵심 아이디어),LoRA 와 QLoRA 의 차이,-0.00080653315,0.0,0.0008065331494435668
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거 (남은 답변: 핵심 아이디어),Loss Function 관련 실무 경험,-0.0010091721,0.0,0.00100917206145823
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거 (남은 답변: 핵심 아이디어),Loss Function 예시,0.0010072846,0.0,0.00100728461984545
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거 (남은 답변: 핵심 아이디어),Loss Function 정의,-0.004882003,0.0,0.004882003180682659
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거 (남은 답변: 핵심 아이디어),MBTI / 좋아하는 아이돌,-0.0074352818,0.0,0.00743528176099062
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거 (남은 답변: 핵심 아이디어),MSE Loss 설명,-0.004340921,0.0,0.004340921062976122
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거 (남은 답변: 핵심 아이디어),MSE Loss 용도,-0.012175352,0.0,0.012175352312624454
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거 (남은 답변: 핵심 아이디어),"Multi-Class, Multi-Label 중 BCE 가 좋은 task",-0.005426122,0.0,0.00542612187564373
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거 (남은 답변: 핵심 아이디어),Multi-Label 에서 CE + Softmax 적용 문제점,-0.0003130469,0.0,0.00031304691219702363
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거 (남은 답변: 핵심 아이디어),PEFT 방법 5가지,-0.00092538085,0.0,0.0009253808530047536
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거 (남은 답변: 핵심 아이디어),거대 언어 모델 정의,-0.006184027,0.0,0.006184027064591646
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거 (남은 답변: 핵심 아이디어),마지막 할 말,-0.00982741,0.0,0.009827409870922565
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거 (남은 답변: 핵심 아이디어),면접 시작 인사,-0.005815751,0.0,0.005815750919282436
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거 (남은 답변: 핵심 아이디어),면접 종료,0.004811952,0.0,0.004811951890587807
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거 (남은 답변: 핵심 아이디어),"인공지능, 머신러닝, 딥러닝 차이",-0.0011426108,0.0,0.0011426107957959175
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거 (남은 답변: 핵심 아이디어),잠시 휴식,-0.0042584334,0.0,0.004258433356881142
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거 (남은 답변: 핵심 아이디어),확률 예측에서 MSE Loss 미 사용 이유,-0.0073068617,0.0,0.007306861691176891
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지 (남은 답변: 모든 질문 해결 완료)",BCE Loss 설명,-0.015642786,0.0,0.015642786398530006
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지 (남은 답변: 모든 질문 해결 완료)",LLM Fine-Tuning 의 PEFT,0.0060557937,0.0,0.006055793724954128
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지 (남은 답변: 모든 질문 해결 완료)",LoRA,-0.009212359,0.0,0.009212358854711056
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지 (남은 답변: 모든 질문 해결 완료)",LoRA 와 QLoRA 의 차이,0.0016914951,0.0,0.001691495068371296
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지 (남은 답변: 모든 질문 해결 완료)",Loss Function 관련 실무 경험,-0.0020702742,0.0,0.002070274204015732
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지 (남은 답변: 모든 질문 해결 완료)",Loss Function 예시,0.0003285934,0.0,0.00032859339262358844
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지 (남은 답변: 모든 질문 해결 완료)",Loss Function 정의,0.0005685699,0.0,0.0005685699288733304
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지 (남은 답변: 모든 질문 해결 완료)",MBTI / 좋아하는 아이돌,-0.0060978867,0.0,0.006097886711359024
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지 (남은 답변: 모든 질문 해결 완료)",MSE Loss 설명,0.00030061722,0.0,0.00030061721918173134
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지 (남은 답변: 모든 질문 해결 완료)",MSE Loss 용도,-0.0053040134,0.0,0.005304013378918171
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지 (남은 답변: 모든 질문 해결 완료)","Multi-Class, Multi-Label 중 BCE 가 좋은 task",0.99258214,1.0,0.007417857646942139
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지 (남은 답변: 모든 질문 해결 완료)",Multi-Label 에서 CE + Softmax 적용 문제점,0.000641824,0.0,0.0006418239790946245
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지 (남은 답변: 모든 질문 해결 완료)",PEFT 방법 5가지,-0.0016478939,0.0,0.0016478939214721322
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지 (남은 답변: 모든 질문 해결 완료)",거대 언어 모델 정의,-0.0008382891,0.0,0.0008382890955545008
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지 (남은 답변: 모든 질문 해결 완료)",마지막 할 말,-0.0016922087,0.0,0.001692208694294095
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지 (남은 답변: 모든 질문 해결 완료)",면접 시작 인사,0.0020102472,0.0,0.002010247204452753
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지 (남은 답변: 모든 질문 해결 완료)",면접 종료,0.0029775365,0.0,0.0029775365255773067
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지 (남은 답변: 모든 질문 해결 완료)","인공지능, 머신러닝, 딥러닝 차이",0.0003502005,0.0,0.0003502005129121244
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지 (남은 답변: 모든 질문 해결 완료)",잠시 휴식,-0.0062963907,0.0,0.006296390667557716
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지 (남은 답변: 모든 질문 해결 완료)",확률 예측에서 MSE Loss 미 사용 이유,-0.00083500694,0.0,0.0008350069401785731
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야? (남은 답변: BCE 가 좋은 task, BCE 가 좋은 이유)",BCE Loss 설명,-0.0033784043,0.0,0.0033784043043851852
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야? (남은 답변: BCE 가 좋은 task, BCE 가 좋은 이유)",LLM Fine-Tuning 의 PEFT,-0.00092624617,0.0,0.0009262461680918932
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야? (남은 답변: BCE 가 좋은 task, BCE 가 좋은 이유)",LoRA,-0.005616541,0.0,0.0056165410205721855
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야? (남은 답변: BCE 가 좋은 task, BCE 가 좋은 이유)",LoRA 와 QLoRA 의 차이,0.0006192243,0.0,0.0006192242726683617
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야? (남은 답변: BCE 가 좋은 task, BCE 가 좋은 이유)",Loss Function 관련 실무 경험,0.00033866783,0.0,0.0003386678290553391
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야? (남은 답변: BCE 가 좋은 task, BCE 가 좋은 이유)",Loss Function 예시,-0.00039740573,0.0,0.00039740573265589774
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야? (남은 답변: BCE 가 좋은 task, BCE 가 좋은 이유)",Loss Function 정의,-0.0018524161,0.0,0.0018524160841479897
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야? (남은 답변: BCE 가 좋은 task, BCE 가 좋은 이유)",MBTI / 좋아하는 아이돌,-0.0061343843,0.0,0.00613438431173563
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야? (남은 답변: BCE 가 좋은 task, BCE 가 좋은 이유)",MSE Loss 설명,-0.0015172994,0.0,0.0015172994462773204
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야? (남은 답변: BCE 가 좋은 task, BCE 가 좋은 이유)",MSE Loss 용도,-0.0073081474,0.0,0.007308147381991148
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야? (남은 답변: BCE 가 좋은 task, BCE 가 좋은 이유)","Multi-Class, Multi-Label 중 BCE 가 좋은 task",0.99862856,1.0,0.0013714432716369629
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야? (남은 답변: BCE 가 좋은 task, BCE 가 좋은 이유)",Multi-Label 에서 CE + Softmax 적용 문제점,-0.0011943082,0.0,0.0011943081626668572
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야? (남은 답변: BCE 가 좋은 task, BCE 가 좋은 이유)",PEFT 방법 5가지,-0.0025170746,0.0,0.002517074579373002
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야? (남은 답변: BCE 가 좋은 task, BCE 가 좋은 이유)",거대 언어 모델 정의,-0.0016102182,0.0,0.0016102181980386376
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야? (남은 답변: BCE 가 좋은 task, BCE 가 좋은 이유)",마지막 할 말,-0.0038869695,0.0,0.0038869695272296667
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야? (남은 답변: BCE 가 좋은 task, BCE 가 좋은 이유)",면접 시작 인사,0.0033027872,0.0,0.003302787197753787
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야? (남은 답변: BCE 가 좋은 task, BCE 가 좋은 이유)",면접 종료,0.0026246912,0.0,0.002624691231176257
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야? (남은 답변: BCE 가 좋은 task, BCE 가 좋은 이유)","인공지능, 머신러닝, 딥러닝 차이",-0.0020303715,0.0,0.002030371455475688
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야? (남은 답변: BCE 가 좋은 task, BCE 가 좋은 이유)",잠시 휴식,-0.0044960985,0.0,0.00449609849601984
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야? (남은 답변: BCE 가 좋은 task, BCE 가 좋은 이유)",확률 예측에서 MSE Loss 미 사용 이유,-0.0008832439,0.0,0.0008832439198158681
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!! (남은 답변: BCE 가 좋은 task, BCE 가 좋은 이유)",BCE Loss 설명,-0.002058314,0.0,0.002058313926681876
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!! (남은 답변: BCE 가 좋은 task, BCE 가 좋은 이유)",LLM Fine-Tuning 의 PEFT,-0.0012517603,0.0,0.0012517602881416678
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!! (남은 답변: BCE 가 좋은 task, BCE 가 좋은 이유)",LoRA,-0.005006189,0.0,0.005006188992410898
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!! (남은 답변: BCE 가 좋은 task, BCE 가 좋은 이유)",LoRA 와 QLoRA 의 차이,-0.0011570515,0.0,0.0011570515343919396
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!! (남은 답변: BCE 가 좋은 task, BCE 가 좋은 이유)",Loss Function 관련 실무 경험,-0.00073124416,0.0,0.0007312441593967378
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!! (남은 답변: BCE 가 좋은 task, BCE 가 좋은 이유)",Loss Function 예시,0.00055206363,0.0,0.0005520636332221329
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!! (남은 답변: BCE 가 좋은 task, BCE 가 좋은 이유)",Loss Function 정의,-0.00069683354,0.0,0.0006968335364945233
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!! (남은 답변: BCE 가 좋은 task, BCE 가 좋은 이유)",MBTI / 좋아하는 아이돌,-0.007241274,0.0,0.007241274230182171
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!! (남은 답변: BCE 가 좋은 task, BCE 가 좋은 이유)",MSE Loss 설명,-0.002030062,0.0,0.002030062023550272
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!! (남은 답변: BCE 가 좋은 task, BCE 가 좋은 이유)",MSE Loss 용도,-0.0079095755,0.0,0.00790957547724247
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!! (남은 답변: BCE 가 좋은 task, BCE 가 좋은 이유)","Multi-Class, Multi-Label 중 BCE 가 좋은 task",0.99880785,1.0,0.0011921525001525879
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!! (남은 답변: BCE 가 좋은 task, BCE 가 좋은 이유)",Multi-Label 에서 CE + Softmax 적용 문제점,0.0014346514,0.0,0.0014346514362841845
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!! (남은 답변: BCE 가 좋은 task, BCE 가 좋은 이유)",PEFT 방법 5가지,-0.0038570222,0.0,0.003857022151350975
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!! (남은 답변: BCE 가 좋은 task, BCE 가 좋은 이유)",거대 언어 모델 정의,0.0010383492,0.0,0.0010383492335677147
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!! (남은 답변: BCE 가 좋은 task, BCE 가 좋은 이유)",마지막 할 말,-0.003653512,0.0,0.003653512103483081
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!! (남은 답변: BCE 가 좋은 task, BCE 가 좋은 이유)",면접 시작 인사,0.000312676,0.0,0.000312676012981683
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!! (남은 답변: BCE 가 좋은 task, BCE 가 좋은 이유)",면접 종료,0.0020631615,0.0,0.0020631614606827497
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!! (남은 답변: BCE 가 좋은 task, BCE 가 좋은 이유)","인공지능, 머신러닝, 딥러닝 차이",-0.0021889643,0.0,0.002188964281231165
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!! (남은 답변: BCE 가 좋은 task, BCE 가 좋은 이유)",잠시 휴식,-0.002458281,0.0,0.002458280883729458
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!! (남은 답변: BCE 가 좋은 task, BCE 가 좋은 이유)",확률 예측에서 MSE Loss 미 사용 이유,0.00023374465,0.0,0.00023374464944936335
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어 (남은 답변: BCE 가 좋은 이유)",BCE Loss 설명,-0.0039622234,0.0,0.003962223418056965
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어 (남은 답변: BCE 가 좋은 이유)",LLM Fine-Tuning 의 PEFT,-0.00028231528,0.0,0.00028231527539901435
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어 (남은 답변: BCE 가 좋은 이유)",LoRA,-0.0069279238,0.0,0.0069279237650334835
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어 (남은 답변: BCE 가 좋은 이유)",LoRA 와 QLoRA 의 차이,0.001769442,0.0,0.0017694419948384166
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어 (남은 답변: BCE 가 좋은 이유)",Loss Function 관련 실무 경험,-0.00065154344,0.0,0.0006515434361062944
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어 (남은 답변: BCE 가 좋은 이유)",Loss Function 예시,-0.0010144858,0.0,0.0010144858388230205
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어 (남은 답변: BCE 가 좋은 이유)",Loss Function 정의,-0.00071775215,0.0,0.0007177521474659443
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어 (남은 답변: BCE 가 좋은 이유)",MBTI / 좋아하는 아이돌,-0.005844049,0.0,0.005844049155712128
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어 (남은 답변: BCE 가 좋은 이유)",MSE Loss 설명,0.00023395587,0.0,0.0002339558704989031
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어 (남은 답변: BCE 가 좋은 이유)",MSE Loss 용도,-0.007172845,0.0,0.007172844838351011
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어 (남은 답변: BCE 가 좋은 이유)","Multi-Class, Multi-Label 중 BCE 가 좋은 task",0.9984881,1.0,0.0015118718147277832
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어 (남은 답변: BCE 가 좋은 이유)",Multi-Label 에서 CE + Softmax 적용 문제점,-0.0011645574,0.0,0.0011645574122667313
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어 (남은 답변: BCE 가 좋은 이유)",PEFT 방법 5가지,-0.0026210614,0.0,0.0026210614014416933
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어 (남은 답변: BCE 가 좋은 이유)",거대 언어 모델 정의,-0.0015417072,0.0,0.0015417071990668774
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어 (남은 답변: BCE 가 좋은 이유)",마지막 할 말,-0.007338613,0.0,0.0073386128060519695
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어 (남은 답변: BCE 가 좋은 이유)",면접 시작 인사,0.0050949506,0.0,0.005094950553029776
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어 (남은 답변: BCE 가 좋은 이유)",면접 종료,0.0026028655,0.0,0.0026028654538094997
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어 (남은 답변: BCE 가 좋은 이유)","인공지능, 머신러닝, 딥러닝 차이",-0.0018793647,0.0,0.0018793647177517414
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어 (남은 답변: BCE 가 좋은 이유)",잠시 휴식,-0.0026767894,0.0,0.002676789416000247
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어 (남은 답변: BCE 가 좋은 이유)",확률 예측에서 MSE Loss 미 사용 이유,-0.0015843794,0.0,0.0015843793516978621
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아 (남은 답변: 모든 질문 해결 완료)",BCE Loss 설명,-0.007516566,0.0,0.007516566198319197
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아 (남은 답변: 모든 질문 해결 완료)",LLM Fine-Tuning 의 PEFT,-0.0074848006,0.0,0.007484800647944212
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아 (남은 답변: 모든 질문 해결 완료)",LoRA,-0.009721115,0.0,0.009721115231513977
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아 (남은 답변: 모든 질문 해결 완료)",LoRA 와 QLoRA 의 차이,-0.015774399,0.0,0.015774399042129517
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아 (남은 답변: 모든 질문 해결 완료)",Loss Function 관련 실무 경험,-0.009003203,0.0,0.009003203362226486
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아 (남은 답변: 모든 질문 해결 완료)",Loss Function 예시,-0.012306419,0.0,0.01230641920119524
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아 (남은 답변: 모든 질문 해결 완료)",Loss Function 정의,0.013810832,0.0,0.013810832053422928
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아 (남은 답변: 모든 질문 해결 완료)",MBTI / 좋아하는 아이돌,-0.0072231162,0.0,0.007223116233944893
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아 (남은 답변: 모든 질문 해결 완료)",MSE Loss 설명,-0.013072004,0.0,0.013072003610432148
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아 (남은 답변: 모든 질문 해결 완료)",MSE Loss 용도,-0.010496311,0.0,0.010496310889720917
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아 (남은 답변: 모든 질문 해결 완료)","Multi-Class, Multi-Label 중 BCE 가 좋은 task",0.24893144,0.0,0.24893143773078918
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아 (남은 답변: 모든 질문 해결 완료)",Multi-Label 에서 CE + Softmax 적용 문제점,0.9244314,1.0,0.07556861639022827
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아 (남은 답변: 모든 질문 해결 완료)",PEFT 방법 5가지,-0.0059107947,0.0,0.0059107947163283825
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아 (남은 답변: 모든 질문 해결 완료)",거대 언어 모델 정의,0.006065632,0.0,0.006065632216632366
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아 (남은 답변: 모든 질문 해결 완료)",마지막 할 말,0.009969628,0.0,0.009969628416001797
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아 (남은 답변: 모든 질문 해결 완료)",면접 시작 인사,-0.0039001298,0.0,0.0039001298137009144
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아 (남은 답변: 모든 질문 해결 완료)",면접 종료,0.013802532,0.0,0.013802532106637955
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아 (남은 답변: 모든 질문 해결 완료)","인공지능, 머신러닝, 딥러닝 차이",-0.014109428,0.0,0.01410942804068327
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아 (남은 답변: 모든 질문 해결 완료)",잠시 휴식,-0.0024764922,0.0,0.0024764921981841326
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아 (남은 답변: 모든 질문 해결 완료)",확률 예측에서 MSE Loss 미 사용 이유,-0.010973238,0.0,0.01097323838621378
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지 (남은 답변: 모든 질문 해결 완료),BCE Loss 설명,-0.006984816,0.0,0.0069848159328103065
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지 (남은 답변: 모든 질문 해결 완료),LLM Fine-Tuning 의 PEFT,-0.0030071393,0.0,0.0030071393121033907
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지 (남은 답변: 모든 질문 해결 완료),LoRA,0.004883409,0.0,0.004883409012109041
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지 (남은 답변: 모든 질문 해결 완료),LoRA 와 QLoRA 의 차이,0.0039448794,0.0,0.003944879397749901
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지 (남은 답변: 모든 질문 해결 완료),Loss Function 관련 실무 경험,0.9905499,1.0,0.009450078010559082
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지 (남은 답변: 모든 질문 해결 완료),Loss Function 예시,-0.00981475,0.0,0.009814750403165817
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지 (남은 답변: 모든 질문 해결 완료),Loss Function 정의,-0.0048223995,0.0,0.004822399467229843
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지 (남은 답변: 모든 질문 해결 완료),MBTI / 좋아하는 아이돌,-0.013418969,0.0,0.013418968766927719
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지 (남은 답변: 모든 질문 해결 완료),MSE Loss 설명,-0.008246981,0.0,0.008246980607509613
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지 (남은 답변: 모든 질문 해결 완료),MSE Loss 용도,0.006547399,0.0,0.006547398865222931
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지 (남은 답변: 모든 질문 해결 완료),"Multi-Class, Multi-Label 중 BCE 가 좋은 task",-0.005591022,0.0,0.005591021850705147
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지 (남은 답변: 모든 질문 해결 완료),Multi-Label 에서 CE + Softmax 적용 문제점,0.0037625553,0.0,0.003762555308640003
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지 (남은 답변: 모든 질문 해결 완료),PEFT 방법 5가지,0.00017339368,0.0,0.00017339368059765548
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지 (남은 답변: 모든 질문 해결 완료),거대 언어 모델 정의,0.0037180746,0.0,0.003718074643984437
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지 (남은 답변: 모든 질문 해결 완료),마지막 할 말,0.0001368443,0.0,0.00013684430450666696
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지 (남은 답변: 모든 질문 해결 완료),면접 시작 인사,-0.012552534,0.0,0.012552534230053425
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지 (남은 답변: 모든 질문 해결 완료),면접 종료,0.008399534,0.0,0.008399534039199352
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지 (남은 답변: 모든 질문 해결 완료),"인공지능, 머신러닝, 딥러닝 차이",0.0066374205,0.0,0.006637420505285263
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지 (남은 답변: 모든 질문 해결 완료),잠시 휴식,-0.012408258,0.0,0.012408258393406868
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지 (남은 답변: 모든 질문 해결 완료),확률 예측에서 MSE Loss 미 사용 이유,-1.6392456e-05,0.0,1.639245601836592e-05
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나 (남은 답변: Multi-Label 에서 CE + Softmax 적용 문제점),BCE Loss 설명,-0.00031179553,0.0,0.00031179553479887545
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나 (남은 답변: Multi-Label 에서 CE + Softmax 적용 문제점),LLM Fine-Tuning 의 PEFT,-0.005168966,0.0,0.005168966017663479
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나 (남은 답변: Multi-Label 에서 CE + Softmax 적용 문제점),LoRA,0.0043077297,0.0,0.004307729657739401
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나 (남은 답변: Multi-Label 에서 CE + Softmax 적용 문제점),LoRA 와 QLoRA 의 차이,0.0018463176,0.0,0.001846317551098764
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나 (남은 답변: Multi-Label 에서 CE + Softmax 적용 문제점),Loss Function 관련 실무 경험,-0.014695172,0.0,0.01469517219811678
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나 (남은 답변: Multi-Label 에서 CE + Softmax 적용 문제점),Loss Function 예시,-0.006444557,0.0,0.006444557104259729
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나 (남은 답변: Multi-Label 에서 CE + Softmax 적용 문제점),Loss Function 정의,0.00481119,0.0,0.004811190068721771
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나 (남은 답변: Multi-Label 에서 CE + Softmax 적용 문제점),MBTI / 좋아하는 아이돌,-0.013012056,0.0,0.013012056238949299
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나 (남은 답변: Multi-Label 에서 CE + Softmax 적용 문제점),MSE Loss 설명,0.001999723,0.0,0.0019997230265289545
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나 (남은 답변: Multi-Label 에서 CE + Softmax 적용 문제점),MSE Loss 용도,-0.0050161676,0.0,0.005016167648136616
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나 (남은 답변: Multi-Label 에서 CE + Softmax 적용 문제점),"Multi-Class, Multi-Label 중 BCE 가 좋은 task",0.0024355515,0.0,0.00243555149063468
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나 (남은 답변: Multi-Label 에서 CE + Softmax 적용 문제점),Multi-Label 에서 CE + Softmax 적용 문제점,0.996282,1.0,0.0037180185317993164
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나 (남은 답변: Multi-Label 에서 CE + Softmax 적용 문제점),PEFT 방법 5가지,0.00535584,0.0,0.00535584008321166
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나 (남은 답변: Multi-Label 에서 CE + Softmax 적용 문제점),거대 언어 모델 정의,-5.6618217e-05,0.0,5.6618217058712617e-05
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나 (남은 답변: Multi-Label 에서 CE + Softmax 적용 문제점),마지막 할 말,0.002625123,0.0,0.0026251228991895914
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나 (남은 답변: Multi-Label 에서 CE + Softmax 적용 문제점),면접 시작 인사,-0.008341302,0.0,0.008341302163898945
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나 (남은 답변: Multi-Label 에서 CE + Softmax 적용 문제점),면접 종료,0.0016637282,0.0,0.0016637281514704227
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나 (남은 답변: Multi-Label 에서 CE + Softmax 적용 문제점),"인공지능, 머신러닝, 딥러닝 차이",-0.0037611406,0.0,0.0037611406296491623
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나 (남은 답변: Multi-Label 에서 CE + Softmax 적용 문제점),잠시 휴식,-0.0009991119,0.0,0.0009991119150072336
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나 (남은 답변: Multi-Label 에서 CE + Softmax 적용 문제점),확률 예측에서 MSE Loss 미 사용 이유,-0.0045319395,0.0,0.004531939513981342
Loss Function 관련 실무 경험 -> [기본 경험] 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어 (남은 답변: 상세 경험),BCE Loss 설명,-0.00027391282,0.0,0.0002739128249231726
Loss Function 관련 실무 경험 -> [기본 경험] 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어 (남은 답변: 상세 경험),LLM Fine-Tuning 의 PEFT,-0.006560493,0.0,0.006560492794960737
Loss Function 관련 실무 경험 -> [기본 경험] 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어 (남은 답변: 상세 경험),LoRA,0.0025134666,0.0,0.0025134666357189417
Loss Function 관련 실무 경험 -> [기본 경험] 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어 (남은 답변: 상세 경험),LoRA 와 QLoRA 의 차이,0.00089417037,0.0,0.0008941703708842397
Loss Function 관련 실무 경험 -> [기본 경험] 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어 (남은 답변: 상세 경험),Loss Function 관련 실무 경험,0.9962623,1.0,0.0037376880645751953
Loss Function 관련 실무 경험 -> [기본 경험] 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어 (남은 답변: 상세 경험),Loss Function 예시,-0.012452064,0.0,0.012452064082026482
Loss Function 관련 실무 경험 -> [기본 경험] 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어 (남은 답변: 상세 경험),Loss Function 정의,-0.0034808782,0.0,0.0034808781929314137
Loss Function 관련 실무 경험 -> [기본 경험] 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어 (남은 답변: 상세 경험),MBTI / 좋아하는 아이돌,-0.004911402,0.0,0.004911401774734259
Loss Function 관련 실무 경험 -> [기본 경험] 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어 (남은 답변: 상세 경험),MSE Loss 설명,-0.002539434,0.0,0.002539434004575014
Loss Function 관련 실무 경험 -> [기본 경험] 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어 (남은 답변: 상세 경험),MSE Loss 용도,-0.0033205247,0.0,0.003320524701848626
Loss Function 관련 실무 경험 -> [기본 경험] 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어 (남은 답변: 상세 경험),"Multi-Class, Multi-Label 중 BCE 가 좋은 task",-0.0017792997,0.0,0.001779299695044756
Loss Function 관련 실무 경험 -> [기본 경험] 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어 (남은 답변: 상세 경험),Multi-Label 에서 CE + Softmax 적용 문제점,-0.0031483737,0.0,0.003148373682051897
Loss Function 관련 실무 경험 -> [기본 경험] 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어 (남은 답변: 상세 경험),PEFT 방법 5가지,-0.00567872,0.0,0.005678719840943813
Loss Function 관련 실무 경험 -> [기본 경험] 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어 (남은 답변: 상세 경험),거대 언어 모델 정의,-0.0015192505,0.0,0.001519250450655818
Loss Function 관련 실무 경험 -> [기본 경험] 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어 (남은 답변: 상세 경험),마지막 할 말,-0.007912772,0.0,0.00791277177631855
Loss Function 관련 실무 경험 -> [기본 경험] 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어 (남은 답변: 상세 경험),면접 시작 인사,-0.010861658,0.0,0.010861657559871674
Loss Function 관련 실무 경험 -> [기본 경험] 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어 (남은 답변: 상세 경험),면접 종료,0.00699464,0.0,0.006994639988988638
Loss Function 관련 실무 경험 -> [기본 경험] 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어 (남은 답변: 상세 경험),"인공지능, 머신러닝, 딥러닝 차이",0.00286024,0.0,0.002860239939764142
Loss Function 관련 실무 경험 -> [기본 경험] 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어 (남은 답변: 상세 경험),잠시 휴식,-0.0025707511,0.0,0.0025707511231303215
Loss Function 관련 실무 경험 -> [기본 경험] 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어 (남은 답변: 상세 경험),확률 예측에서 MSE Loss 미 사용 이유,0.0038355824,0.0,0.0038355824071913958
Loss Function 관련 실무 경험 -> [상세 경험] 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지? (남은 답변: 모든 질문 해결 완료),BCE Loss 설명,-0.0052064494,0.0,0.005206449422985315
Loss Function 관련 실무 경험 -> [상세 경험] 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지? (남은 답변: 모든 질문 해결 완료),LLM Fine-Tuning 의 PEFT,-0.002104737,0.0,0.0021047370973974466
Loss Function 관련 실무 경험 -> [상세 경험] 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지? (남은 답변: 모든 질문 해결 완료),LoRA,-0.006592742,0.0,0.006592742167413235
Loss Function 관련 실무 경험 -> [상세 경험] 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지? (남은 답변: 모든 질문 해결 완료),LoRA 와 QLoRA 의 차이,0.007947007,0.0,0.007947007194161415
Loss Function 관련 실무 경험 -> [상세 경험] 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지? (남은 답변: 모든 질문 해결 완료),Loss Function 관련 실무 경험,-0.017989064,0.0,0.017989063635468483
Loss Function 관련 실무 경험 -> [상세 경험] 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지? (남은 답변: 모든 질문 해결 완료),Loss Function 예시,-0.002697657,0.0,0.0026976570952683687
Loss Function 관련 실무 경험 -> [상세 경험] 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지? (남은 답변: 모든 질문 해결 완료),Loss Function 정의,-0.0037292177,0.0,0.0037292176857590675
Loss Function 관련 실무 경험 -> [상세 경험] 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지? (남은 답변: 모든 질문 해결 완료),MBTI / 좋아하는 아이돌,0.9942621,1.0,0.005737900733947754
Loss Function 관련 실무 경험 -> [상세 경험] 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지? (남은 답변: 모든 질문 해결 완료),MSE Loss 설명,0.0042259474,0.0,0.004225947428494692
Loss Function 관련 실무 경험 -> [상세 경험] 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지? (남은 답변: 모든 질문 해결 완료),MSE Loss 용도,0.0032703462,0.0,0.0032703462056815624
Loss Function 관련 실무 경험 -> [상세 경험] 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지? (남은 답변: 모든 질문 해결 완료),"Multi-Class, Multi-Label 중 BCE 가 좋은 task",-0.0066639553,0.0,0.00666395528241992
Loss Function 관련 실무 경험 -> [상세 경험] 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지? (남은 답변: 모든 질문 해결 완료),Multi-Label 에서 CE + Softmax 적용 문제점,-0.0077868486,0.0,0.007786848582327366
Loss Function 관련 실무 경험 -> [상세 경험] 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지? (남은 답변: 모든 질문 해결 완료),PEFT 방법 5가지,-0.003867133,0.0,0.003867133054882288
Loss Function 관련 실무 경험 -> [상세 경험] 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지? (남은 답변: 모든 질문 해결 완료),거대 언어 모델 정의,-0.001663395,0.0,0.001663394970819354
Loss Function 관련 실무 경험 -> [상세 경험] 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지? (남은 답변: 모든 질문 해결 완료),마지막 할 말,-0.018889371,0.0,0.018889371305704117
Loss Function 관련 실무 경험 -> [상세 경험] 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지? (남은 답변: 모든 질문 해결 완료),면접 시작 인사,-0.004082985,0.0,0.00408298522233963
Loss Function 관련 실무 경험 -> [상세 경험] 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지? (남은 답변: 모든 질문 해결 완료),면접 종료,-0.0015745071,0.0,0.0015745070995762944
Loss Function 관련 실무 경험 -> [상세 경험] 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지? (남은 답변: 모든 질문 해결 완료),"인공지능, 머신러닝, 딥러닝 차이",-0.00983694,0.0,0.009836940094828606
Loss Function 관련 실무 경험 -> [상세 경험] 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지? (남은 답변: 모든 질문 해결 완료),잠시 휴식,-0.012260524,0.0,0.01226052362471819
Loss Function 관련 실무 경험 -> [상세 경험] 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지? (남은 답변: 모든 질문 해결 완료),확률 예측에서 MSE Loss 미 사용 이유,-0.0022895555,0.0,0.002289555501192808
Loss Function 관련 실무 경험 -> [상세 경험] 근데 기억 진짜 안나는데 어떡하지? (남은 답변: 상세 경험),BCE Loss 설명,-0.0049231653,0.0,0.004923165310174227
Loss Function 관련 실무 경험 -> [상세 경험] 근데 기억 진짜 안나는데 어떡하지? (남은 답변: 상세 경험),LLM Fine-Tuning 의 PEFT,-0.00404838,0.0,0.004048380069434643
Loss Function 관련 실무 경험 -> [상세 경험] 근데 기억 진짜 안나는데 어떡하지? (남은 답변: 상세 경험),LoRA,-0.0076898015,0.0,0.007689801510423422
Loss Function 관련 실무 경험 -> [상세 경험] 근데 기억 진짜 안나는데 어떡하지? (남은 답변: 상세 경험),LoRA 와 QLoRA 의 차이,0.0018120365,0.0,0.0018120364984497428
Loss Function 관련 실무 경험 -> [상세 경험] 근데 기억 진짜 안나는데 어떡하지? (남은 답변: 상세 경험),Loss Function 관련 실무 경험,-0.0071812654,0.0,0.007181265391409397
Loss Function 관련 실무 경험 -> [상세 경험] 근데 기억 진짜 안나는데 어떡하지? (남은 답변: 상세 경험),Loss Function 예시,-0.005758916,0.0,0.005758916027843952
Loss Function 관련 실무 경험 -> [상세 경험] 근데 기억 진짜 안나는데 어떡하지? (남은 답변: 상세 경험),Loss Function 정의,-0.0050124046,0.0,0.005012404639273882
Loss Function 관련 실무 경험 -> [상세 경험] 근데 기억 진짜 안나는데 어떡하지? (남은 답변: 상세 경험),MBTI / 좋아하는 아이돌,-0.0069822515,0.0,0.006982251536101103
Loss Function 관련 실무 경험 -> [상세 경험] 근데 기억 진짜 안나는데 어떡하지? (남은 답변: 상세 경험),MSE Loss 설명,-0.0050660917,0.0,0.005066091660410166
Loss Function 관련 실무 경험 -> [상세 경험] 근데 기억 진짜 안나는데 어떡하지? (남은 답변: 상세 경험),MSE Loss 용도,-0.004893254,0.0,0.0048932540230453014
Loss Function 관련 실무 경험 -> [상세 경험] 근데 기억 진짜 안나는데 어떡하지? (남은 답변: 상세 경험),"Multi-Class, Multi-Label 중 BCE 가 좋은 task",-0.0050725215,0.0,0.005072521511465311
Loss Function 관련 실무 경험 -> [상세 경험] 근데 기억 진짜 안나는데 어떡하지? (남은 답변: 상세 경험),Multi-Label 에서 CE + Softmax 적용 문제점,-0.0031481322,0.0,0.003148132236674428
Loss Function 관련 실무 경험 -> [상세 경험] 근데 기억 진짜 안나는데 어떡하지? (남은 답변: 상세 경험),PEFT 방법 5가지,-0.006418297,0.0,0.006418297067284584
Loss Function 관련 실무 경험 -> [상세 경험] 근데 기억 진짜 안나는데 어떡하지? (남은 답변: 상세 경험),거대 언어 모델 정의,-0.0062556434,0.0,0.00625564344227314
Loss Function 관련 실무 경험 -> [상세 경험] 근데 기억 진짜 안나는데 어떡하지? (남은 답변: 상세 경험),마지막 할 말,-0.008022713,0.0,0.008022712543606758
Loss Function 관련 실무 경험 -> [상세 경험] 근데 기억 진짜 안나는데 어떡하지? (남은 답변: 상세 경험),면접 시작 인사,-0.007726993,0.0,0.007726992946118116
Loss Function 관련 실무 경험 -> [상세 경험] 근데 기억 진짜 안나는데 어떡하지? (남은 답변: 상세 경험),면접 종료,-0.006993203,0.0,0.006993202958256006
Loss Function 관련 실무 경험 -> [상세 경험] 근데 기억 진짜 안나는데 어떡하지? (남은 답변: 상세 경험),"인공지능, 머신러닝, 딥러닝 차이",-0.0014452264,0.0,0.0014452263712882996
Loss Function 관련 실무 경험 -> [상세 경험] 근데 기억 진짜 안나는데 어떡하지? (남은 답변: 상세 경험),잠시 휴식,0.9927976,1.0,0.0072023868560791016
Loss Function 관련 실무 경험 -> [상세 경험] 근데 기억 진짜 안나는데 어떡하지? (남은 답변: 상세 경험),확률 예측에서 MSE Loss 미 사용 이유,0.0067961686,0.0,0.006796168629080057
Loss Function 관련 실무 경험 -> [기본 경험] 나 아직 실무 경험 없는데 ㅠㅠ (남은 답변: 모든 질문 해결 완료),BCE Loss 설명,-0.0078639425,0.0,0.00786394253373146
Loss Function 관련 실무 경험 -> [기본 경험] 나 아직 실무 경험 없는데 ㅠㅠ (남은 답변: 모든 질문 해결 완료),LLM Fine-Tuning 의 PEFT,-0.002894106,0.0,0.002894106088206172
Loss Function 관련 실무 경험 -> [기본 경험] 나 아직 실무 경험 없는데 ㅠㅠ (남은 답변: 모든 질문 해결 완료),LoRA,-0.009059923,0.0,0.009059922769665718
Loss Function 관련 실무 경험 -> [기본 경험] 나 아직 실무 경험 없는데 ㅠㅠ (남은 답변: 모든 질문 해결 완료),LoRA 와 QLoRA 의 차이,0.0063369134,0.0,0.0063369134441018105
Loss Function 관련 실무 경험 -> [기본 경험] 나 아직 실무 경험 없는데 ㅠㅠ (남은 답변: 모든 질문 해결 완료),Loss Function 관련 실무 경험,-0.014697152,0.0,0.014697152189910412
Loss Function 관련 실무 경험 -> [기본 경험] 나 아직 실무 경험 없는데 ㅠㅠ (남은 답변: 모든 질문 해결 완료),Loss Function 예시,-0.002097906,0.0,0.0020979060791432858
Loss Function 관련 실무 경험 -> [기본 경험] 나 아직 실무 경험 없는데 ㅠㅠ (남은 답변: 모든 질문 해결 완료),Loss Function 정의,-0.00064957363,0.0,0.0006495736306533217
Loss Function 관련 실무 경험 -> [기본 경험] 나 아직 실무 경험 없는데 ㅠㅠ (남은 답변: 모든 질문 해결 완료),MBTI / 좋아하는 아이돌,0.9946122,1.0,0.005387783050537109
Loss Function 관련 실무 경험 -> [기본 경험] 나 아직 실무 경험 없는데 ㅠㅠ (남은 답변: 모든 질문 해결 완료),MSE Loss 설명,0.0037525112,0.0,0.003752511227503419
Loss Function 관련 실무 경험 -> [기본 경험] 나 아직 실무 경험 없는데 ㅠㅠ (남은 답변: 모든 질문 해결 완료),MSE Loss 용도,0.0035244361,0.0,0.0035244361497461796
Loss Function 관련 실무 경험 -> [기본 경험] 나 아직 실무 경험 없는데 ㅠㅠ (남은 답변: 모든 질문 해결 완료),"Multi-Class, Multi-Label 중 BCE 가 좋은 task",-0.007128942,0.0,0.00712894182652235
Loss Function 관련 실무 경험 -> [기본 경험] 나 아직 실무 경험 없는데 ㅠㅠ (남은 답변: 모든 질문 해결 완료),Multi-Label 에서 CE + Softmax 적용 문제점,-0.0101504745,0.0,0.01015047449618578
Loss Function 관련 실무 경험 -> [기본 경험] 나 아직 실무 경험 없는데 ㅠㅠ (남은 답변: 모든 질문 해결 완료),PEFT 방법 5가지,-0.004690401,0.0,0.004690400790423155
Loss Function 관련 실무 경험 -> [기본 경험] 나 아직 실무 경험 없는데 ㅠㅠ (남은 답변: 모든 질문 해결 완료),거대 언어 모델 정의,-0.005027256,0.0,0.005027255974709988
Loss Function 관련 실무 경험 -> [기본 경험] 나 아직 실무 경험 없는데 ㅠㅠ (남은 답변: 모든 질문 해결 완료),마지막 할 말,-0.01768247,0.0,0.017682470381259918
Loss Function 관련 실무 경험 -> [기본 경험] 나 아직 실무 경험 없는데 ㅠㅠ (남은 답변: 모든 질문 해결 완료),면접 시작 인사,-0.0038252913,0.0,0.0038252912927418947
Loss Function 관련 실무 경험 -> [기본 경험] 나 아직 실무 경험 없는데 ㅠㅠ (남은 답변: 모든 질문 해결 완료),면접 종료,-0.0014339145,0.0,0.00143391452729702
Loss Function 관련 실무 경험 -> [기본 경험] 나 아직 실무 경험 없는데 ㅠㅠ (남은 답변: 모든 질문 해결 완료),"인공지능, 머신러닝, 딥러닝 차이",-0.010870422,0.0,0.01087042223662138
Loss Function 관련 실무 경험 -> [기본 경험] 나 아직 실무 경험 없는데 ㅠㅠ (남은 답변: 모든 질문 해결 완료),잠시 휴식,-0.006672629,0.0,0.006672629155218601
Loss Function 관련 실무 경험 -> [기본 경험] 나 아직 실무 경험 없는데 ㅠㅠ (남은 답변: 모든 질문 해결 완료),확률 예측에서 MSE Loss 미 사용 이유,-0.0024714924,0.0,0.0024714923929423094
"Loss Function 관련 실무 경험 -> [기본 경험] 대답하기 싫은데 힝 (남은 답변: 기본 경험, 상세 경험)",BCE Loss 설명,-0.004203875,0.0,0.004203875083476305
"Loss Function 관련 실무 경험 -> [기본 경험] 대답하기 싫은데 힝 (남은 답변: 기본 경험, 상세 경험)",LLM Fine-Tuning 의 PEFT,-0.0056202854,0.0,0.005620285402983427
"Loss Function 관련 실무 경험 -> [기본 경험] 대답하기 싫은데 힝 (남은 답변: 기본 경험, 상세 경험)",LoRA,-0.007606729,0.0,0.007606728933751583
"Loss Function 관련 실무 경험 -> [기본 경험] 대답하기 싫은데 힝 (남은 답변: 기본 경험, 상세 경험)",LoRA 와 QLoRA 의 차이,0.0035273365,0.0,0.003527336521074176
"Loss Function 관련 실무 경험 -> [기본 경험] 대답하기 싫은데 힝 (남은 답변: 기본 경험, 상세 경험)",Loss Function 관련 실무 경험,-0.00550826,0.0,0.005508259870111942
"Loss Function 관련 실무 경험 -> [기본 경험] 대답하기 싫은데 힝 (남은 답변: 기본 경험, 상세 경험)",Loss Function 예시,-0.005177329,0.0,0.0051773288287222385
"Loss Function 관련 실무 경험 -> [기본 경험] 대답하기 싫은데 힝 (남은 답변: 기본 경험, 상세 경험)",Loss Function 정의,-0.007596567,0.0,0.007596566807478666
"Loss Function 관련 실무 경험 -> [기본 경험] 대답하기 싫은데 힝 (남은 답변: 기본 경험, 상세 경험)",MBTI / 좋아하는 아이돌,-0.012335419,0.0,0.012335418723523617
"Loss Function 관련 실무 경험 -> [기본 경험] 대답하기 싫은데 힝 (남은 답변: 기본 경험, 상세 경험)",MSE Loss 설명,-0.0053411224,0.0,0.005341122392565012
"Loss Function 관련 실무 경험 -> [기본 경험] 대답하기 싫은데 힝 (남은 답변: 기본 경험, 상세 경험)",MSE Loss 용도,-0.0040124264,0.0,0.004012426361441612
"Loss Function 관련 실무 경험 -> [기본 경험] 대답하기 싫은데 힝 (남은 답변: 기본 경험, 상세 경험)","Multi-Class, Multi-Label 중 BCE 가 좋은 task",-0.0018136224,0.0,0.0018136224243789911
"Loss Function 관련 실무 경험 -> [기본 경험] 대답하기 싫은데 힝 (남은 답변: 기본 경험, 상세 경험)",Multi-Label 에서 CE + Softmax 적용 문제점,-0.0013930516,0.0,0.0013930515851825476
"Loss Function 관련 실무 경험 -> [기본 경험] 대답하기 싫은데 힝 (남은 답변: 기본 경험, 상세 경험)",PEFT 방법 5가지,-0.0051537566,0.0,0.0051537565886974335
"Loss Function 관련 실무 경험 -> [기본 경험] 대답하기 싫은데 힝 (남은 답변: 기본 경험, 상세 경험)",거대 언어 모델 정의,-0.009756437,0.0,0.009756436571478844
"Loss Function 관련 실무 경험 -> [기본 경험] 대답하기 싫은데 힝 (남은 답변: 기본 경험, 상세 경험)",마지막 할 말,-0.009826782,0.0,0.009826782159507275
"Loss Function 관련 실무 경험 -> [기본 경험] 대답하기 싫은데 힝 (남은 답변: 기본 경험, 상세 경험)",면접 시작 인사,-0.0051129195,0.0,0.005112919490784407
"Loss Function 관련 실무 경험 -> [기본 경험] 대답하기 싫은데 힝 (남은 답변: 기본 경험, 상세 경험)",면접 종료,-0.008263638,0.0,0.008263638243079185
"Loss Function 관련 실무 경험 -> [기본 경험] 대답하기 싫은데 힝 (남은 답변: 기본 경험, 상세 경험)","인공지능, 머신러닝, 딥러닝 차이",0.003924221,0.0,0.00392422080039978
"Loss Function 관련 실무 경험 -> [기본 경험] 대답하기 싫은데 힝 (남은 답변: 기본 경험, 상세 경험)",잠시 휴식,0.9929753,1.0,0.007024705410003662
"Loss Function 관련 실무 경험 -> [기본 경험] 대답하기 싫은데 힝 (남은 답변: 기본 경험, 상세 경험)",확률 예측에서 MSE Loss 미 사용 이유,0.0074894447,0.0,0.007489444687962532
Loss Function 관련 실무 경험 -> [기본 경험] 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ (남은 답변: 상세 경험),BCE Loss 설명,0.0029664421,0.0,0.0029664421454072
Loss Function 관련 실무 경험 -> [기본 경험] 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ (남은 답변: 상세 경험),LLM Fine-Tuning 의 PEFT,-0.0045707286,0.0,0.00457072863355279
Loss Function 관련 실무 경험 -> [기본 경험] 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ (남은 답변: 상세 경험),LoRA,0.0022188053,0.0,0.002218805253505707
Loss Function 관련 실무 경험 -> [기본 경험] 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ (남은 답변: 상세 경험),LoRA 와 QLoRA 의 차이,-0.0003621039,0.0,0.0003621038922574371
Loss Function 관련 실무 경험 -> [기본 경험] 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ (남은 답변: 상세 경험),Loss Function 관련 실무 경험,0.9946549,1.0,0.00534510612487793
Loss Function 관련 실무 경험 -> [기본 경험] 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ (남은 답변: 상세 경험),Loss Function 예시,-0.008313112,0.0,0.008313111960887909
Loss Function 관련 실무 경험 -> [기본 경험] 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ (남은 답변: 상세 경험),Loss Function 정의,-0.007107232,0.0,0.007107232231646776
Loss Function 관련 실무 경험 -> [기본 경험] 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ (남은 답변: 상세 경험),MBTI / 좋아하는 아이돌,-0.0038598755,0.0,0.003859875490888953
Loss Function 관련 실무 경험 -> [기본 경험] 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ (남은 답변: 상세 경험),MSE Loss 설명,-0.0012334008,0.0,0.0012334007769823074
Loss Function 관련 실무 경험 -> [기본 경험] 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ (남은 답변: 상세 경험),MSE Loss 용도,-0.003236262,0.0,0.003236261894926429
Loss Function 관련 실무 경험 -> [기본 경험] 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ (남은 답변: 상세 경험),"Multi-Class, Multi-Label 중 BCE 가 좋은 task",-0.006640834,0.0,0.006640833802521229
Loss Function 관련 실무 경험 -> [기본 경험] 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ (남은 답변: 상세 경험),Multi-Label 에서 CE + Softmax 적용 문제점,-0.0040195794,0.0,0.004019579384475946
Loss Function 관련 실무 경험 -> [기본 경험] 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ (남은 답변: 상세 경험),PEFT 방법 5가지,-0.007456072,0.0,0.007456072140485048
Loss Function 관련 실무 경험 -> [기본 경험] 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ (남은 답변: 상세 경험),거대 언어 모델 정의,0.00049224537,0.0,0.0004922453663311899
Loss Function 관련 실무 경험 -> [기본 경험] 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ (남은 답변: 상세 경험),마지막 할 말,-0.006773841,0.0,0.006773841101676226
Loss Function 관련 실무 경험 -> [기본 경험] 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ (남은 답변: 상세 경험),면접 시작 인사,-0.011378257,0.0,0.011378256604075432
Loss Function 관련 실무 경험 -> [기본 경험] 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ (남은 답변: 상세 경험),면접 종료,0.0053350586,0.0,0.005335058551281691
Loss Function 관련 실무 경험 -> [기본 경험] 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ (남은 답변: 상세 경험),"인공지능, 머신러닝, 딥러닝 차이",0.001254191,0.0,0.0012541910400614142
Loss Function 관련 실무 경험 -> [기본 경험] 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ (남은 답변: 상세 경험),잠시 휴식,-0.0005656798,0.0,0.0005656798020936549
Loss Function 관련 실무 경험 -> [기본 경험] 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ (남은 답변: 상세 경험),확률 예측에서 MSE Loss 미 사용 이유,0.002436139,0.0,0.0024361389223486185
Loss Function 관련 실무 경험 -> [상세 경험] 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어 (남은 답변: 모든 질문 해결 완료),BCE Loss 설명,-0.0042397226,0.0,0.004239722620695829
Loss Function 관련 실무 경험 -> [상세 경험] 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어 (남은 답변: 모든 질문 해결 완료),LLM Fine-Tuning 의 PEFT,-0.00066165795,0.0,0.000661657948512584
Loss Function 관련 실무 경험 -> [상세 경험] 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어 (남은 답변: 모든 질문 해결 완료),LoRA,-0.005888775,0.0,0.005888774991035461
Loss Function 관련 실무 경험 -> [상세 경험] 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어 (남은 답변: 모든 질문 해결 완료),LoRA 와 QLoRA 의 차이,0.007475836,0.0,0.007475836202502251
Loss Function 관련 실무 경험 -> [상세 경험] 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어 (남은 답변: 모든 질문 해결 완료),Loss Function 관련 실무 경험,-0.015672987,0.0,0.015672987326979637
Loss Function 관련 실무 경험 -> [상세 경험] 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어 (남은 답변: 모든 질문 해결 완료),Loss Function 예시,-0.004820486,0.0,0.004820486065000296
Loss Function 관련 실무 경험 -> [상세 경험] 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어 (남은 답변: 모든 질문 해결 완료),Loss Function 정의,-0.0008593598,0.0,0.0008593598031438887
Loss Function 관련 실무 경험 -> [상세 경험] 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어 (남은 답변: 모든 질문 해결 완료),MBTI / 좋아하는 아이돌,0.99405366,1.0,0.005946338176727295
Loss Function 관련 실무 경험 -> [상세 경험] 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어 (남은 답변: 모든 질문 해결 완료),MSE Loss 설명,0.0041646636,0.0,0.0041646636091172695
Loss Function 관련 실무 경험 -> [상세 경험] 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어 (남은 답변: 모든 질문 해결 완료),MSE Loss 용도,0.0024552124,0.0,0.0024552124086767435
Loss Function 관련 실무 경험 -> [상세 경험] 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어 (남은 답변: 모든 질문 해결 완료),"Multi-Class, Multi-Label 중 BCE 가 좋은 task",-0.0044854814,0.0,0.004485481418669224
Loss Function 관련 실무 경험 -> [상세 경험] 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어 (남은 답변: 모든 질문 해결 완료),Multi-Label 에서 CE + Softmax 적용 문제점,-0.005653786,0.0,0.00565378600731492
Loss Function 관련 실무 경험 -> [상세 경험] 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어 (남은 답변: 모든 질문 해결 완료),PEFT 방법 5가지,-0.004813806,0.0,0.004813806153833866
Loss Function 관련 실무 경험 -> [상세 경험] 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어 (남은 답변: 모든 질문 해결 완료),거대 언어 모델 정의,-0.0005661683,0.0,0.0005661682807840407
Loss Function 관련 실무 경험 -> [상세 경험] 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어 (남은 답변: 모든 질문 해결 완료),마지막 할 말,-0.015864221,0.0,0.01586422137916088
Loss Function 관련 실무 경험 -> [상세 경험] 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어 (남은 답변: 모든 질문 해결 완료),면접 시작 인사,-0.00677253,0.0,0.006772529799491167
Loss Function 관련 실무 경험 -> [상세 경험] 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어 (남은 답변: 모든 질문 해결 완료),면접 종료,-0.0023902867,0.0,0.0023902866523712873
Loss Function 관련 실무 경험 -> [상세 경험] 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어 (남은 답변: 모든 질문 해결 완료),"인공지능, 머신러닝, 딥러닝 차이",-0.007394273,0.0,0.007394272834062576
Loss Function 관련 실무 경험 -> [상세 경험] 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어 (남은 답변: 모든 질문 해결 완료),잠시 휴식,-0.016590565,0.0,0.01659056544303894
Loss Function 관련 실무 경험 -> [상세 경험] 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어 (남은 답변: 모든 질문 해결 완료),확률 예측에서 MSE Loss 미 사용 이유,-0.004281383,0.0,0.004281383007764816
Loss Function 관련 실무 경험 -> [상세 경험] 기억 진짜 안 나는데 가물가물하다 (남은 답변: 상세 경험),BCE Loss 설명,-0.0054385145,0.0,0.005438514519482851
Loss Function 관련 실무 경험 -> [상세 경험] 기억 진짜 안 나는데 가물가물하다 (남은 답변: 상세 경험),LLM Fine-Tuning 의 PEFT,-0.0034926673,0.0,0.0034926673397421837
Loss Function 관련 실무 경험 -> [상세 경험] 기억 진짜 안 나는데 가물가물하다 (남은 답변: 상세 경험),LoRA,-0.0070998752,0.0,0.007099875248968601
Loss Function 관련 실무 경험 -> [상세 경험] 기억 진짜 안 나는데 가물가물하다 (남은 답변: 상세 경험),LoRA 와 QLoRA 의 차이,0.00251444,0.0,0.0025144401006400585
Loss Function 관련 실무 경험 -> [상세 경험] 기억 진짜 안 나는데 가물가물하다 (남은 답변: 상세 경험),Loss Function 관련 실무 경험,-0.008806775,0.0,0.008806775324046612
Loss Function 관련 실무 경험 -> [상세 경험] 기억 진짜 안 나는데 가물가물하다 (남은 답변: 상세 경험),Loss Function 예시,-0.0060316958,0.0,0.006031695753335953
Loss Function 관련 실무 경험 -> [상세 경험] 기억 진짜 안 나는데 가물가물하다 (남은 답변: 상세 경험),Loss Function 정의,-0.0033416068,0.0,0.003341606818139553
Loss Function 관련 실무 경험 -> [상세 경험] 기억 진짜 안 나는데 가물가물하다 (남은 답변: 상세 경험),MBTI / 좋아하는 아이돌,-0.009844604,0.0,0.009844603948295116
Loss Function 관련 실무 경험 -> [상세 경험] 기억 진짜 안 나는데 가물가물하다 (남은 답변: 상세 경험),MSE Loss 설명,-0.0039569205,0.0,0.003956920467317104
Loss Function 관련 실무 경험 -> [상세 경험] 기억 진짜 안 나는데 가물가물하다 (남은 답변: 상세 경험),MSE Loss 용도,-0.005559044,0.0,0.00555904395878315
Loss Function 관련 실무 경험 -> [상세 경험] 기억 진짜 안 나는데 가물가물하다 (남은 답변: 상세 경험),"Multi-Class, Multi-Label 중 BCE 가 좋은 task",-0.004071023,0.0,0.004071022849529982
Loss Function 관련 실무 경험 -> [상세 경험] 기억 진짜 안 나는데 가물가물하다 (남은 답변: 상세 경험),Multi-Label 에서 CE + Softmax 적용 문제점,-0.0023027328,0.0,0.0023027327843010426
Loss Function 관련 실무 경험 -> [상세 경험] 기억 진짜 안 나는데 가물가물하다 (남은 답변: 상세 경험),PEFT 방법 5가지,-0.006953071,0.0,0.006953070871531963
Loss Function 관련 실무 경험 -> [상세 경험] 기억 진짜 안 나는데 가물가물하다 (남은 답변: 상세 경험),거대 언어 모델 정의,-0.0072979983,0.0,0.007297998294234276
Loss Function 관련 실무 경험 -> [상세 경험] 기억 진짜 안 나는데 가물가물하다 (남은 답변: 상세 경험),마지막 할 말,-0.0066219484,0.0,0.006621948443353176
Loss Function 관련 실무 경험 -> [상세 경험] 기억 진짜 안 나는데 가물가물하다 (남은 답변: 상세 경험),면접 시작 인사,-0.0072746263,0.0,0.007274626288563013
Loss Function 관련 실무 경험 -> [상세 경험] 기억 진짜 안 나는데 가물가물하다 (남은 답변: 상세 경험),면접 종료,-0.009454803,0.0,0.009454802609980106
Loss Function 관련 실무 경험 -> [상세 경험] 기억 진짜 안 나는데 가물가물하다 (남은 답변: 상세 경험),"인공지능, 머신러닝, 딥러닝 차이",-0.001173784,0.0,0.0011737840250134468
Loss Function 관련 실무 경험 -> [상세 경험] 기억 진짜 안 나는데 가물가물하다 (남은 답변: 상세 경험),잠시 휴식,0.99266267,1.0,0.007337331771850586
Loss Function 관련 실무 경험 -> [상세 경험] 기억 진짜 안 나는데 가물가물하다 (남은 답변: 상세 경험),확률 예측에서 MSE Loss 미 사용 이유,0.0070270705,0.0,0.007027070503681898
Loss Function 관련 실무 경험 -> [기본 경험] 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ (남은 답변: 모든 질문 해결 완료),BCE Loss 설명,-0.008857642,0.0,0.00885764230042696
Loss Function 관련 실무 경험 -> [기본 경험] 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ (남은 답변: 모든 질문 해결 완료),LLM Fine-Tuning 의 PEFT,-0.003134228,0.0,0.003134228056296706
Loss Function 관련 실무 경험 -> [기본 경험] 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ (남은 답변: 모든 질문 해결 완료),LoRA,-0.009087985,0.0,0.009087985381484032
Loss Function 관련 실무 경험 -> [기본 경험] 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ (남은 답변: 모든 질문 해결 완료),LoRA 와 QLoRA 의 차이,0.0065724785,0.0,0.006572478450834751
Loss Function 관련 실무 경험 -> [기본 경험] 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ (남은 답변: 모든 질문 해결 완료),Loss Function 관련 실무 경험,-0.014342098,0.0,0.01434209756553173
Loss Function 관련 실무 경험 -> [기본 경험] 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ (남은 답변: 모든 질문 해결 완료),Loss Function 예시,-0.0028052272,0.0,0.002805227180942893
Loss Function 관련 실무 경험 -> [기본 경험] 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ (남은 답변: 모든 질문 해결 완료),Loss Function 정의,-0.00073792937,0.0,0.0007379293674603105
Loss Function 관련 실무 경험 -> [기본 경험] 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ (남은 답변: 모든 질문 해결 완료),MBTI / 좋아하는 아이돌,0.9946285,1.0,0.005371510982513428
Loss Function 관련 실무 경험 -> [기본 경험] 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ (남은 답변: 모든 질문 해결 완료),MSE Loss 설명,0.004566889,0.0,0.00456688879057765
Loss Function 관련 실무 경험 -> [기본 경험] 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ (남은 답변: 모든 질문 해결 완료),MSE Loss 용도,0.002576274,0.0,0.002576274098828435
Loss Function 관련 실무 경험 -> [기본 경험] 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ (남은 답변: 모든 질문 해결 완료),"Multi-Class, Multi-Label 중 BCE 가 좋은 task",-0.006369441,0.0,0.006369440816342831
Loss Function 관련 실무 경험 -> [기본 경험] 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ (남은 답변: 모든 질문 해결 완료),Multi-Label 에서 CE + Softmax 적용 문제점,-0.010731015,0.0,0.010731015354394913
Loss Function 관련 실무 경험 -> [기본 경험] 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ (남은 답변: 모든 질문 해결 완료),PEFT 방법 5가지,-0.004621924,0.0,0.00462192390114069
Loss Function 관련 실무 경험 -> [기본 경험] 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ (남은 답변: 모든 질문 해결 완료),거대 언어 모델 정의,-0.0023490158,0.0,0.0023490157909691334
Loss Function 관련 실무 경험 -> [기본 경험] 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ (남은 답변: 모든 질문 해결 완료),마지막 할 말,-0.018351464,0.0,0.018351463600993156
Loss Function 관련 실무 경험 -> [기본 경험] 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ (남은 답변: 모든 질문 해결 완료),면접 시작 인사,-0.0036368796,0.0,0.003636879613623023
Loss Function 관련 실무 경험 -> [기본 경험] 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ (남은 답변: 모든 질문 해결 완료),면접 종료,-0.0013237004,0.0,0.0013237004168331623
Loss Function 관련 실무 경험 -> [기본 경험] 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ (남은 답변: 모든 질문 해결 완료),"인공지능, 머신러닝, 딥러닝 차이",-0.0108930385,0.0,0.010893038474023342
Loss Function 관련 실무 경험 -> [기본 경험] 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ (남은 답변: 모든 질문 해결 완료),잠시 휴식,-0.008299573,0.0,0.008299573324620724
Loss Function 관련 실무 경험 -> [기본 경험] 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ (남은 답변: 모든 질문 해결 완료),확률 예측에서 MSE Loss 미 사용 이유,-0.0019513726,0.0,0.0019513726001605392
"Loss Function 관련 실무 경험 -> [기본 경험] 이걸 왜 알려줘야 하지 (남은 답변: 기본 경험, 상세 경험)",BCE Loss 설명,-0.0064964173,0.0,0.006496417336165905
"Loss Function 관련 실무 경험 -> [기본 경험] 이걸 왜 알려줘야 하지 (남은 답변: 기본 경험, 상세 경험)",LLM Fine-Tuning 의 PEFT,-0.00485262,0.0,0.004852619953453541
"Loss Function 관련 실무 경험 -> [기본 경험] 이걸 왜 알려줘야 하지 (남은 답변: 기본 경험, 상세 경험)",LoRA,-0.0062247617,0.0,0.006224761717021465
"Loss Function 관련 실무 경험 -> [기본 경험] 이걸 왜 알려줘야 하지 (남은 답변: 기본 경험, 상세 경험)",LoRA 와 QLoRA 의 차이,0.0026788835,0.0,0.00267888349480927
"Loss Function 관련 실무 경험 -> [기본 경험] 이걸 왜 알려줘야 하지 (남은 답변: 기본 경험, 상세 경험)",Loss Function 관련 실무 경험,-0.0016527001,0.0,0.0016527001280337572
"Loss Function 관련 실무 경험 -> [기본 경험] 이걸 왜 알려줘야 하지 (남은 답변: 기본 경험, 상세 경험)",Loss Function 예시,-0.0045559714,0.0,0.00455597136169672
"Loss Function 관련 실무 경험 -> [기본 경험] 이걸 왜 알려줘야 하지 (남은 답변: 기본 경험, 상세 경험)",Loss Function 정의,-0.007647864,0.0,0.007647864054888487
"Loss Function 관련 실무 경험 -> [기본 경험] 이걸 왜 알려줘야 하지 (남은 답변: 기본 경험, 상세 경험)",MBTI / 좋아하는 아이돌,-0.01101464,0.0,0.01101464033126831
"Loss Function 관련 실무 경험 -> [기본 경험] 이걸 왜 알려줘야 하지 (남은 답변: 기본 경험, 상세 경험)",MSE Loss 설명,-0.004729131,0.0,0.004729130771011114
"Loss Function 관련 실무 경험 -> [기본 경험] 이걸 왜 알려줘야 하지 (남은 답변: 기본 경험, 상세 경험)",MSE Loss 용도,-0.0054713823,0.0,0.005471382290124893
"Loss Function 관련 실무 경험 -> [기본 경험] 이걸 왜 알려줘야 하지 (남은 답변: 기본 경험, 상세 경험)","Multi-Class, Multi-Label 중 BCE 가 좋은 task",-0.003489421,0.0,0.0034894209820777178
"Loss Function 관련 실무 경험 -> [기본 경험] 이걸 왜 알려줘야 하지 (남은 답변: 기본 경험, 상세 경험)",Multi-Label 에서 CE + Softmax 적용 문제점,-0.0044032666,0.0,0.004403266590088606
"Loss Function 관련 실무 경험 -> [기본 경험] 이걸 왜 알려줘야 하지 (남은 답변: 기본 경험, 상세 경험)",PEFT 방법 5가지,-0.005520831,0.0,0.005520830862224102
"Loss Function 관련 실무 경험 -> [기본 경험] 이걸 왜 알려줘야 하지 (남은 답변: 기본 경험, 상세 경험)",거대 언어 모델 정의,-0.007908106,0.0,0.007908105850219727
"Loss Function 관련 실무 경험 -> [기본 경험] 이걸 왜 알려줘야 하지 (남은 답변: 기본 경험, 상세 경험)",마지막 할 말,-0.0096920235,0.0,0.009692023508250713
"Loss Function 관련 실무 경험 -> [기본 경험] 이걸 왜 알려줘야 하지 (남은 답변: 기본 경험, 상세 경험)",면접 시작 인사,-0.006468684,0.0,0.006468683946877718
"Loss Function 관련 실무 경험 -> [기본 경험] 이걸 왜 알려줘야 하지 (남은 답변: 기본 경험, 상세 경험)",면접 종료,-0.009184968,0.0,0.00918496772646904
"Loss Function 관련 실무 경험 -> [기본 경험] 이걸 왜 알려줘야 하지 (남은 답변: 기본 경험, 상세 경험)","인공지능, 머신러닝, 딥러닝 차이",0.0029583762,0.0,0.002958376193419099
"Loss Function 관련 실무 경험 -> [기본 경험] 이걸 왜 알려줘야 하지 (남은 답변: 기본 경험, 상세 경험)",잠시 휴식,0.9928494,1.0,0.007150590419769287
"Loss Function 관련 실무 경험 -> [기본 경험] 이걸 왜 알려줘야 하지 (남은 답변: 기본 경험, 상세 경험)",확률 예측에서 MSE Loss 미 사용 이유,0.008075241,0.0,0.00807524099946022
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야! (남은 답변: 모든 질문 해결 완료),BCE Loss 설명,-0.009079391,0.0,0.00907939113676548
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야! (남은 답변: 모든 질문 해결 완료),LLM Fine-Tuning 의 PEFT,-0.005835412,0.0,0.005835412070155144
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야! (남은 답변: 모든 질문 해결 완료),LoRA,-0.0139091,0.0,0.013909099623560905
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야! (남은 답변: 모든 질문 해결 완료),LoRA 와 QLoRA 의 차이,-0.004229776,0.0,0.004229776095598936
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야! (남은 답변: 모든 질문 해결 완료),Loss Function 관련 실무 경험,-0.006287736,0.0,0.006287735886871815
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야! (남은 답변: 모든 질문 해결 완료),Loss Function 예시,-0.012095535,0.0,0.012095535174012184
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야! (남은 답변: 모든 질문 해결 완료),Loss Function 정의,-0.0024647878,0.0,0.0024647878017276525
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야! (남은 답변: 모든 질문 해결 완료),MBTI / 좋아하는 아이돌,-0.006790024,0.0,0.0067900242283940315
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야! (남은 답변: 모든 질문 해결 완료),MSE Loss 설명,-0.00061820756,0.0,0.0006182075594551861
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야! (남은 답변: 모든 질문 해결 완료),MSE Loss 용도,-0.008999722,0.0,0.008999722078442574
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야! (남은 답변: 모든 질문 해결 완료),"Multi-Class, Multi-Label 중 BCE 가 좋은 task",-0.0031729536,0.0,0.003172953613102436
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야! (남은 답변: 모든 질문 해결 완료),Multi-Label 에서 CE + Softmax 적용 문제점,-0.0043684333,0.0,0.004368433263152838
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야! (남은 답변: 모든 질문 해결 완료),PEFT 방법 5가지,-0.0050042765,0.0,0.005004276521503925
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야! (남은 답변: 모든 질문 해결 완료),거대 언어 모델 정의,-0.006483619,0.0,0.006483619101345539
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야! (남은 답변: 모든 질문 해결 완료),마지막 할 말,-0.0037906026,0.0,0.0037906025536358356
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야! (남은 답변: 모든 질문 해결 완료),면접 시작 인사,-0.0029648896,0.0,0.002964889630675316
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야! (남은 답변: 모든 질문 해결 완료),면접 종료,-0.0076467018,0.0,0.007646701764315367
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야! (남은 답변: 모든 질문 해결 완료),"인공지능, 머신러닝, 딥러닝 차이",5.6112913e-05,0.0,5.6112912716344e-05
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야! (남은 답변: 모든 질문 해결 완료),잠시 휴식,0.993163,1.0,0.006837010383605957
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야! (남은 답변: 모든 질문 해결 완료),확률 예측에서 MSE Loss 미 사용 이유,0.006279401,0.0,0.0062794010154902935
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해 (남은 답변: 모든 질문 해결 완료),BCE Loss 설명,-0.009687005,0.0,0.00968700461089611
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해 (남은 답변: 모든 질문 해결 완료),LLM Fine-Tuning 의 PEFT,-0.0007354903,0.0,0.0007354902918450534
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해 (남은 답변: 모든 질문 해결 완료),LoRA,-0.0067948597,0.0,0.006794859655201435
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해 (남은 답변: 모든 질문 해결 완료),LoRA 와 QLoRA 의 차이,0.0036766487,0.0,0.0036766487173736095
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해 (남은 답변: 모든 질문 해결 완료),Loss Function 관련 실무 경험,-0.005311933,0.0,0.005311932880431414
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해 (남은 답변: 모든 질문 해결 완료),Loss Function 예시,-0.0065554366,0.0,0.00655543664470315
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해 (남은 답변: 모든 질문 해결 완료),Loss Function 정의,-0.003382066,0.0,0.0033820660319179296
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해 (남은 답변: 모든 질문 해결 완료),MBTI / 좋아하는 아이돌,0.0005837651,0.0,0.0005837650969624519
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해 (남은 답변: 모든 질문 해결 완료),MSE Loss 설명,-0.005666499,0.0,0.005666499026119709
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해 (남은 답변: 모든 질문 해결 완료),MSE Loss 용도,-0.0037480912,0.0,0.003748091170564294
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해 (남은 답변: 모든 질문 해결 완료),"Multi-Class, Multi-Label 중 BCE 가 좋은 task",-0.0035819719,0.0,0.003581971861422062
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해 (남은 답변: 모든 질문 해결 완료),Multi-Label 에서 CE + Softmax 적용 문제점,-0.0014134878,0.0,0.0014134878292679787
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해 (남은 답변: 모든 질문 해결 완료),PEFT 방법 5가지,-0.004694831,0.0,0.004694831091910601
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해 (남은 답변: 모든 질문 해결 완료),거대 언어 모델 정의,-0.005348948,0.0,0.0053489478304982185
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해 (남은 답변: 모든 질문 해결 완료),마지막 할 말,-0.0075353957,0.0,0.0075353956781327724
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해 (남은 답변: 모든 질문 해결 완료),면접 시작 인사,-0.0029987814,0.0,0.0029987813904881477
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해 (남은 답변: 모든 질문 해결 완료),면접 종료,-0.012791397,0.0,0.012791397050023079
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해 (남은 답변: 모든 질문 해결 완료),"인공지능, 머신러닝, 딥러닝 차이",-0.0013312881,0.0,0.0013312881346791983
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해 (남은 답변: 모든 질문 해결 완료),잠시 휴식,0.99385595,1.0,0.006144046783447266
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해 (남은 답변: 모든 질문 해결 완료),확률 예측에서 MSE Loss 미 사용 이유,0.0054862606,0.0,0.005486260633915663
잠시 휴식 -> 재미있는 이야기 해줄래? (남은 답변: 잠시 휴식),BCE Loss 설명,-0.004442715,0.0,0.004442715086042881
잠시 휴식 -> 재미있는 이야기 해줄래? (남은 답변: 잠시 휴식),LLM Fine-Tuning 의 PEFT,0.9981012,1.0,0.0018988251686096191
잠시 휴식 -> 재미있는 이야기 해줄래? (남은 답변: 잠시 휴식),LoRA,-0.0024486668,0.0,0.0024486668407917023
잠시 휴식 -> 재미있는 이야기 해줄래? (남은 답변: 잠시 휴식),LoRA 와 QLoRA 의 차이,-0.006628485,0.0,0.006628484930843115
잠시 휴식 -> 재미있는 이야기 해줄래? (남은 답변: 잠시 휴식),Loss Function 관련 실무 경험,-0.005944041,0.0,0.005944041069597006
잠시 휴식 -> 재미있는 이야기 해줄래? (남은 답변: 잠시 휴식),Loss Function 예시,-0.0054874313,0.0,0.005487431306391954
잠시 휴식 -> 재미있는 이야기 해줄래? (남은 답변: 잠시 휴식),Loss Function 정의,0.004713877,0.0,0.0047138771042227745
잠시 휴식 -> 재미있는 이야기 해줄래? (남은 답변: 잠시 휴식),MBTI / 좋아하는 아이돌,-0.0013662686,0.0,0.0013662686105817556
잠시 휴식 -> 재미있는 이야기 해줄래? (남은 답변: 잠시 휴식),MSE Loss 설명,-0.0026947525,0.0,0.0026947525329887867
잠시 휴식 -> 재미있는 이야기 해줄래? (남은 답변: 잠시 휴식),MSE Loss 용도,0.0008850922,0.0,0.0008850921876728535
잠시 휴식 -> 재미있는 이야기 해줄래? (남은 답변: 잠시 휴식),"Multi-Class, Multi-Label 중 BCE 가 좋은 task",-0.00075614516,0.0,0.0007561451639048755
잠시 휴식 -> 재미있는 이야기 해줄래? (남은 답변: 잠시 휴식),Multi-Label 에서 CE + Softmax 적용 문제점,-0.008743532,0.0,0.008743532001972198
잠시 휴식 -> 재미있는 이야기 해줄래? (남은 답변: 잠시 휴식),PEFT 방법 5가지,-0.0014943723,0.0,0.0014943722635507584
잠시 휴식 -> 재미있는 이야기 해줄래? (남은 답변: 잠시 휴식),거대 언어 모델 정의,-0.007300869,0.0,0.007300869096070528
잠시 휴식 -> 재미있는 이야기 해줄래? (남은 답변: 잠시 휴식),마지막 할 말,-0.0046388623,0.0,0.004638862330466509
잠시 휴식 -> 재미있는 이야기 해줄래? (남은 답변: 잠시 휴식),면접 시작 인사,-0.0018228479,0.0,0.0018228478729724884
잠시 휴식 -> 재미있는 이야기 해줄래? (남은 답변: 잠시 휴식),면접 종료,-0.00013932657,0.0,0.00013932657020632178
잠시 휴식 -> 재미있는 이야기 해줄래? (남은 답변: 잠시 휴식),"인공지능, 머신러닝, 딥러닝 차이",0.0014034243,0.0,0.0014034243067726493
잠시 휴식 -> 재미있는 이야기 해줄래? (남은 답변: 잠시 휴식),잠시 휴식,-0.005836314,0.0,0.005836314056068659
잠시 휴식 -> 재미있는 이야기 해줄래? (남은 답변: 잠시 휴식),확률 예측에서 MSE Loss 미 사용 이유,-0.00025944645,0.0,0.0002594464458525181
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야 (남은 답변: 모든 질문 해결 완료)",BCE Loss 설명,-0.0029052536,0.0,0.002905253553763032
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야 (남은 답변: 모든 질문 해결 완료)",LLM Fine-Tuning 의 PEFT,0.07879832,0.0,0.0787983164191246
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야 (남은 답변: 모든 질문 해결 완료)",LoRA,-0.0147091355,0.0,0.01470913551747799
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야 (남은 답변: 모든 질문 해결 완료)",LoRA 와 QLoRA 의 차이,-0.0059380108,0.0,0.005938010755926371
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야 (남은 답변: 모든 질문 해결 완료)",Loss Function 관련 실무 경험,-0.00558293,0.0,0.0055829300545156
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야 (남은 답변: 모든 질문 해결 완료)",Loss Function 예시,0.014553214,0.0,0.01455321442335844
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야 (남은 답변: 모든 질문 해결 완료)",Loss Function 정의,-0.0049270997,0.0,0.00492709968239069
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야 (남은 답변: 모든 질문 해결 완료)",MBTI / 좋아하는 아이돌,-0.004682118,0.0,0.004682118073105812
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야 (남은 답변: 모든 질문 해결 완료)",MSE Loss 설명,0.009299549,0.0,0.009299549274146557
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야 (남은 답변: 모든 질문 해결 완료)",MSE Loss 용도,-0.0007815656,0.0,0.000781565613579005
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야 (남은 답변: 모든 질문 해결 완료)","Multi-Class, Multi-Label 중 BCE 가 좋은 task",-0.0065387855,0.0,0.0065387855283916
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야 (남은 답변: 모든 질문 해결 완료)",Multi-Label 에서 CE + Softmax 적용 문제점,-0.00050403946,0.0,0.0005040394607931376
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야 (남은 답변: 모든 질문 해결 완료)",PEFT 방법 5가지,0.97213244,1.0,0.027867555618286133
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야 (남은 답변: 모든 질문 해결 완료)",거대 언어 모델 정의,-0.010793506,0.0,0.010793506167829037
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야 (남은 답변: 모든 질문 해결 완료)",마지막 할 말,-0.011416234,0.0,0.011416234076023102
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야 (남은 답변: 모든 질문 해결 완료)",면접 시작 인사,0.00094144675,0.0,0.0009414467494934797
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야 (남은 답변: 모든 질문 해결 완료)",면접 종료,-0.003632291,0.0,0.0036322909872978926
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야 (남은 답변: 모든 질문 해결 완료)","인공지능, 머신러닝, 딥러닝 차이",-0.0007863818,0.0,0.0007863817736506462
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야 (남은 답변: 모든 질문 해결 완료)",잠시 휴식,-0.0028376598,0.0,0.002837659791111946
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야 (남은 답변: 모든 질문 해결 완료)",확률 예측에서 MSE Loss 미 사용 이유,0.005007592,0.0,0.005007592029869556
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지? (남은 답변: LLM Fine-Tuning 의 PEFT),BCE Loss 설명,0.002029776,0.0,0.002029776107519865
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지? (남은 답변: LLM Fine-Tuning 의 PEFT),LLM Fine-Tuning 의 PEFT,0.9976315,1.0,0.0023685097694396973
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지? (남은 답변: LLM Fine-Tuning 의 PEFT),LoRA,-0.0071998364,0.0,0.007199836429208517
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지? (남은 답변: LLM Fine-Tuning 의 PEFT),LoRA 와 QLoRA 의 차이,-0.0037439968,0.0,0.0037439968436956406
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지? (남은 답변: LLM Fine-Tuning 의 PEFT),Loss Function 관련 실무 경험,-0.004396227,0.0,0.004396227188408375
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지? (남은 답변: LLM Fine-Tuning 의 PEFT),Loss Function 예시,-0.005149298,0.0,0.005149297881871462
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지? (남은 답변: LLM Fine-Tuning 의 PEFT),Loss Function 정의,0.001982956,0.0,0.0019829559605568647
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지? (남은 답변: LLM Fine-Tuning 의 PEFT),MBTI / 좋아하는 아이돌,-0.003447918,0.0,0.003447917988523841
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지? (남은 답변: LLM Fine-Tuning 의 PEFT),MSE Loss 설명,-0.004796028,0.0,0.004796028137207031
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지? (남은 답변: LLM Fine-Tuning 의 PEFT),MSE Loss 용도,0.0021012675,0.0,0.0021012674551457167
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지? (남은 답변: LLM Fine-Tuning 의 PEFT),"Multi-Class, Multi-Label 중 BCE 가 좋은 task",-0.0018452717,0.0,0.0018452716758474708
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지? (남은 답변: LLM Fine-Tuning 의 PEFT),Multi-Label 에서 CE + Softmax 적용 문제점,-0.0072268373,0.0,0.007226837333291769
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지? (남은 답변: LLM Fine-Tuning 의 PEFT),PEFT 방법 5가지,-0.004957843,0.0,0.004957843106240034
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지? (남은 답변: LLM Fine-Tuning 의 PEFT),거대 언어 모델 정의,-0.007837986,0.0,0.007837985642254353
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지? (남은 답변: LLM Fine-Tuning 의 PEFT),마지막 할 말,-0.0045115505,0.0,0.004511550534516573
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지? (남은 답변: LLM Fine-Tuning 의 PEFT),면접 시작 인사,0.0023118446,0.0,0.0023118446115404367
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지? (남은 답변: LLM Fine-Tuning 의 PEFT),면접 종료,-0.0012395345,0.0,0.001239534467458725
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지? (남은 답변: LLM Fine-Tuning 의 PEFT),"인공지능, 머신러닝, 딥러닝 차이",-0.0008551547,0.0,0.0008551547070965171
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지? (남은 답변: LLM Fine-Tuning 의 PEFT),잠시 휴식,-0.0047021313,0.0,0.004702131263911724
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지? (남은 답변: LLM Fine-Tuning 의 PEFT),확률 예측에서 MSE Loss 미 사용 이유,-5.80212e-05,0.0,5.802119994768873e-05
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야! (남은 답변: 모든 질문 해결 완료),BCE Loss 설명,-0.0019188669,0.0,0.0019188668811693788
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야! (남은 답변: 모든 질문 해결 완료),LLM Fine-Tuning 의 PEFT,-0.00994836,0.0,0.009948359802365303
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야! (남은 답변: 모든 질문 해결 완료),LoRA,-0.0107323835,0.0,0.010732383467257023
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야! (남은 답변: 모든 질문 해결 완료),LoRA 와 QLoRA 의 차이,-0.0067962077,0.0,0.006796207744628191
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야! (남은 답변: 모든 질문 해결 완료),Loss Function 관련 실무 경험,-0.0016641926,0.0,0.0016641926486045122
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야! (남은 답변: 모든 질문 해결 완료),Loss Function 예시,0.0024357804,0.0,0.0024357803631573915
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야! (남은 답변: 모든 질문 해결 완료),Loss Function 정의,-0.0008003819,0.0,0.0008003818802535534
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야! (남은 답변: 모든 질문 해결 완료),MBTI / 좋아하는 아이돌,0.00018760121,0.0,0.0001876012102002278
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야! (남은 답변: 모든 질문 해결 완료),MSE Loss 설명,-0.0028796608,0.0,0.0028796608094125986
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야! (남은 답변: 모든 질문 해결 완료),MSE Loss 용도,0.003139046,0.0,0.0031390460208058357
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야! (남은 답변: 모든 질문 해결 완료),"Multi-Class, Multi-Label 중 BCE 가 좋은 task",-0.0020277645,0.0,0.0020277644507586956
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야! (남은 답변: 모든 질문 해결 완료),Multi-Label 에서 CE + Softmax 적용 문제점,0.0017513661,0.0,0.0017513660714030266
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야! (남은 답변: 모든 질문 해결 완료),PEFT 방법 5가지,0.99513745,1.0,0.004862546920776367
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야! (남은 답변: 모든 질문 해결 완료),거대 언어 모델 정의,-0.0014096344,0.0,0.0014096343657001853
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야! (남은 답변: 모든 질문 해결 완료),마지막 할 말,-0.014574053,0.0,0.014574052765965462
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야! (남은 답변: 모든 질문 해결 완료),면접 시작 인사,-0.0059893467,0.0,0.005989346653223038
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야! (남은 답변: 모든 질문 해결 완료),면접 종료,0.007596898,0.0,0.007596897892653942
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야! (남은 답변: 모든 질문 해결 완료),"인공지능, 머신러닝, 딥러닝 차이",0.0024433509,0.0,0.002443350851535797
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야! (남은 답변: 모든 질문 해결 완료),잠시 휴식,-0.0024718973,0.0,0.0024718972854316235
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야! (남은 답변: 모든 질문 해결 완료),확률 예측에서 MSE Loss 미 사용 이유,-0.0054642293,0.0,0.005464229267090559
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다 (남은 답변: LLM Fine-Tuning 의 PEFT),BCE Loss 설명,0.00277994,0.0,0.0027799399103969336
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다 (남은 답변: LLM Fine-Tuning 의 PEFT),LLM Fine-Tuning 의 PEFT,0.99725246,1.0,0.0027475357055664062
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다 (남은 답변: LLM Fine-Tuning 의 PEFT),LoRA,-0.0013569777,0.0,0.0013569777365773916
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다 (남은 답변: LLM Fine-Tuning 의 PEFT),LoRA 와 QLoRA 의 차이,-0.0026706152,0.0,0.0026706152129918337
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다 (남은 답변: LLM Fine-Tuning 의 PEFT),Loss Function 관련 실무 경험,-0.0025521559,0.0,0.0025521558709442616
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다 (남은 답변: LLM Fine-Tuning 의 PEFT),Loss Function 예시,-0.0040268432,0.0,0.00402684323489666
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다 (남은 답변: LLM Fine-Tuning 의 PEFT),Loss Function 정의,0.0007914343,0.0,0.000791434315033257
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다 (남은 답변: LLM Fine-Tuning 의 PEFT),MBTI / 좋아하는 아이돌,-0.0035411064,0.0,0.0035411063581705093
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다 (남은 답변: LLM Fine-Tuning 의 PEFT),MSE Loss 설명,-0.006581238,0.0,0.006581238005310297
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다 (남은 답변: LLM Fine-Tuning 의 PEFT),MSE Loss 용도,0.0026715882,0.0,0.002671588212251663
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다 (남은 답변: LLM Fine-Tuning 의 PEFT),"Multi-Class, Multi-Label 중 BCE 가 좋은 task",-0.002124526,0.0,0.00212452607229352
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다 (남은 답변: LLM Fine-Tuning 의 PEFT),Multi-Label 에서 CE + Softmax 적용 문제점,-0.0054260474,0.0,0.005426047369837761
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다 (남은 답변: LLM Fine-Tuning 의 PEFT),PEFT 방법 5가지,-0.007635812,0.0,0.007635811809450388
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다 (남은 답변: LLM Fine-Tuning 의 PEFT),거대 언어 모델 정의,-0.009018062,0.0,0.009018061682581902
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다 (남은 답변: LLM Fine-Tuning 의 PEFT),마지막 할 말,-0.0016763586,0.0,0.001676358631812036
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다 (남은 답변: LLM Fine-Tuning 의 PEFT),면접 시작 인사,0.0015635528,0.0,0.0015635527670383453
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다 (남은 답변: LLM Fine-Tuning 의 PEFT),면접 종료,0.0020967862,0.0,0.0020967861637473106
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다 (남은 답변: LLM Fine-Tuning 의 PEFT),"인공지능, 머신러닝, 딥러닝 차이",-0.00504943,0.0,0.005049429833889008
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다 (남은 답변: LLM Fine-Tuning 의 PEFT),잠시 휴식,-0.0051360778,0.0,0.005136077757924795
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다 (남은 답변: LLM Fine-Tuning 의 PEFT),확률 예측에서 MSE Loss 미 사용 이유,-0.0014455467,0.0,0.0014455467462539673
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지? (남은 답변: 모든 질문 해결 완료)",BCE Loss 설명,-0.005598363,0.0,0.005598363000899553
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지? (남은 답변: 모든 질문 해결 완료)",LLM Fine-Tuning 의 PEFT,-0.0006096122,0.0,0.0006096122087910771
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지? (남은 답변: 모든 질문 해결 완료)",LoRA,0.99303234,1.0,0.006967663764953613
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지? (남은 답변: 모든 질문 해결 완료)",LoRA 와 QLoRA 의 차이,-0.008672605,0.0,0.008672605268657207
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지? (남은 답변: 모든 질문 해결 완료)",Loss Function 관련 실무 경험,0.00047439657,0.0,0.0004743965691886842
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지? (남은 답변: 모든 질문 해결 완료)",Loss Function 예시,-0.004623211,0.0,0.0046232109889388084
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지? (남은 답변: 모든 질문 해결 완료)",Loss Function 정의,-0.0040178895,0.0,0.004017889499664307
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지? (남은 답변: 모든 질문 해결 완료)",MBTI / 좋아하는 아이돌,0.0042716516,0.0,0.004271651618182659
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지? (남은 답변: 모든 질문 해결 완료)",MSE Loss 설명,-0.010268561,0.0,0.010268560610711575
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지? (남은 답변: 모든 질문 해결 완료)",MSE Loss 용도,-0.0087833265,0.0,0.008783326484262943
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지? (남은 답변: 모든 질문 해결 완료)","Multi-Class, Multi-Label 중 BCE 가 좋은 task",-0.0069399355,0.0,0.006939935497939587
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지? (남은 답변: 모든 질문 해결 완료)",Multi-Label 에서 CE + Softmax 적용 문제점,0.006937538,0.0,0.006937537807971239
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지? (남은 답변: 모든 질문 해결 완료)",PEFT 방법 5가지,-0.014251082,0.0,0.014251082204282284
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지? (남은 답변: 모든 질문 해결 완료)",거대 언어 모델 정의,-0.0036832744,0.0,0.0036832743790000677
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지? (남은 답변: 모든 질문 해결 완료)",마지막 할 말,-0.00082144904,0.0,0.0008214490371756256
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지? (남은 답변: 모든 질문 해결 완료)",면접 시작 인사,-0.005549726,0.0,0.005549726076424122
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지? (남은 답변: 모든 질문 해결 완료)",면접 종료,-0.011693981,0.0,0.011693980544805527
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지? (남은 답변: 모든 질문 해결 완료)","인공지능, 머신러닝, 딥러닝 차이",-0.002585497,0.0,0.002585496986284852
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지? (남은 답변: 모든 질문 해결 완료)",잠시 휴식,-0.017489161,0.0,0.01748916134238243
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지? (남은 답변: 모든 질문 해결 완료)",확률 예측에서 MSE Loss 미 사용 이유,-0.01084621,0.0,0.010846209712326527
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야? (남은 답변: PEFT 방법 5가지),BCE Loss 설명,0.0004974221,0.0,0.00049742212286219
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야? (남은 답변: PEFT 방법 5가지),LLM Fine-Tuning 의 PEFT,-0.0042058085,0.0,0.004205808509141207
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야? (남은 답변: PEFT 방법 5가지),LoRA,0.00084563077,0.0,0.0008456307696178555
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야? (남은 답변: PEFT 방법 5가지),LoRA 와 QLoRA 의 차이,-0.006390786,0.0,0.006390785798430443
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야? (남은 답변: PEFT 방법 5가지),Loss Function 관련 실무 경험,-0.008454637,0.0,0.008454636670649052
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야? (남은 답변: PEFT 방법 5가지),Loss Function 예시,0.0067962417,0.0,0.0067962417379021645
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야? (남은 답변: PEFT 방법 5가지),Loss Function 정의,-0.009442476,0.0,0.009442475624382496
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야? (남은 답변: PEFT 방법 5가지),MBTI / 좋아하는 아이돌,-0.0032210562,0.0,0.003221056191250682
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야? (남은 답변: PEFT 방법 5가지),MSE Loss 설명,0.0018526502,0.0,0.0018526501953601837
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야? (남은 답변: PEFT 방법 5가지),MSE Loss 용도,0.0034078732,0.0,0.003407873213291168
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야? (남은 답변: PEFT 방법 5가지),"Multi-Class, Multi-Label 중 BCE 가 좋은 task",-0.007284734,0.0,0.007284733932465315
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야? (남은 답변: PEFT 방법 5가지),Multi-Label 에서 CE + Softmax 적용 문제점,0.0016416761,0.0,0.0016416760627180338
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야? (남은 답변: PEFT 방법 5가지),PEFT 방법 5가지,0.9961342,1.0,0.0038657784461975098
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야? (남은 답변: PEFT 방법 5가지),거대 언어 모델 정의,-0.005036599,0.0,0.00503659900277853
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야? (남은 답변: PEFT 방법 5가지),마지막 할 말,-0.008568675,0.0,0.008568675257265568
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야? (남은 답변: PEFT 방법 5가지),면접 시작 인사,-0.0017323138,0.0,0.001732313772663474
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야? (남은 답변: PEFT 방법 5가지),면접 종료,0.004554775,0.0,0.0045547750778496265
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야? (남은 답변: PEFT 방법 5가지),"인공지능, 머신러닝, 딥러닝 차이",-0.0017097277,0.0,0.001709727686829865
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야? (남은 답변: PEFT 방법 5가지),잠시 휴식,-0.0041238177,0.0,0.004123817663639784
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야? (남은 답변: PEFT 방법 5가지),확률 예측에서 MSE Loss 미 사용 이유,-0.002325842,0.0,0.00232584192417562
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야! (남은 답변: 모든 질문 해결 완료),BCE Loss 설명,0.0026714972,0.0,0.0026714971754699945
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야! (남은 답변: 모든 질문 해결 완료),LLM Fine-Tuning 의 PEFT,-0.0012841666,0.0,0.001284166588447988
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야! (남은 답변: 모든 질문 해결 완료),LoRA,-0.008941523,0.0,0.008941522799432278
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야! (남은 답변: 모든 질문 해결 완료),LoRA 와 QLoRA 의 차이,0.99582845,1.0,0.004171550273895264
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야! (남은 답변: 모든 질문 해결 완료),Loss Function 관련 실무 경험,0.004053204,0.0,0.004053203854709864
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야! (남은 답변: 모든 질문 해결 완료),Loss Function 예시,-0.0035264774,0.0,0.003526477375999093
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야! (남은 답변: 모든 질문 해결 완료),Loss Function 정의,-0.0037271744,0.0,0.003727174364030361
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야! (남은 답변: 모든 질문 해결 완료),MBTI / 좋아하는 아이돌,-0.008872957,0.0,0.008872956968843937
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야! (남은 답변: 모든 질문 해결 완료),MSE Loss 설명,0.0006295691,0.0,0.0006295691127888858
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야! (남은 답변: 모든 질문 해결 완료),MSE Loss 용도,-0.00044892737,0.0,0.0004489273705985397
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야! (남은 답변: 모든 질문 해결 완료),"Multi-Class, Multi-Label 중 BCE 가 좋은 task",-0.0020639733,0.0,0.002063973341137171
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야! (남은 답변: 모든 질문 해결 완료),Multi-Label 에서 CE + Softmax 적용 문제점,-0.0035834033,0.0,0.003583403304219246
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야! (남은 답변: 모든 질문 해결 완료),PEFT 방법 5가지,-0.006627144,0.0,0.0066271438263356686
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야! (남은 답변: 모든 질문 해결 완료),거대 언어 모델 정의,-0.0068137557,0.0,0.006813755724579096
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야! (남은 답변: 모든 질문 해결 완료),마지막 할 말,0.0009375785,0.0,0.0009375785011798143
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야! (남은 답변: 모든 질문 해결 완료),면접 시작 인사,-0.003438135,0.0,0.0034381349105387926
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야! (남은 답변: 모든 질문 해결 완료),면접 종료,5.7401758e-05,0.0,5.7401757658226416e-05
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야! (남은 답변: 모든 질문 해결 완료),"인공지능, 머신러닝, 딥러닝 차이",0.002171699,0.0,0.002171698957681656
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야! (남은 답변: 모든 질문 해결 완료),잠시 휴식,-0.00445489,0.0,0.004454889800399542
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야! (남은 답변: 모든 질문 해결 완료),확률 예측에서 MSE Loss 미 사용 이유,-0.0011089686,0.0,0.0011089686304330826
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아? (남은 답변: LoRA),BCE Loss 설명,-0.0056362776,0.0,0.005636277608573437
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아? (남은 답변: LoRA),LLM Fine-Tuning 의 PEFT,-0.0062750042,0.0,0.006275004241615534
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아? (남은 답변: LoRA),LoRA,0.99591154,1.0,0.004088461399078369
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아? (남은 답변: LoRA),LoRA 와 QLoRA 의 차이,-0.009887035,0.0,0.009887035004794598
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아? (남은 답변: LoRA),Loss Function 관련 실무 경험,-0.0002978547,0.0,0.00029785471269860864
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아? (남은 답변: LoRA),Loss Function 예시,-0.0011008163,0.0,0.001100816298276186
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아? (남은 답변: LoRA),Loss Function 정의,0.00053367,0.0,0.0005336700123734772
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아? (남은 답변: LoRA),MBTI / 좋아하는 아이돌,0.002366421,0.0,0.0023664210457354784
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아? (남은 답변: LoRA),MSE Loss 설명,-0.011532678,0.0,0.01153267826884985
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아? (남은 답변: LoRA),MSE Loss 용도,0.00052343524,0.0,0.0005234352429397404
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아? (남은 답변: LoRA),"Multi-Class, Multi-Label 중 BCE 가 좋은 task",-0.0046823905,0.0,0.004682390484958887
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아? (남은 답변: LoRA),Multi-Label 에서 CE + Softmax 적용 문제점,0.0008930326,0.0,0.0008930325857363641
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아? (남은 답변: LoRA),PEFT 방법 5가지,-0.009574576,0.0,0.009574576281011105
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아? (남은 답변: LoRA),거대 언어 모델 정의,0.00032837206,0.0,0.000328372057992965
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아? (남은 답변: LoRA),마지막 할 말,-0.0130404085,0.0,0.013040408492088318
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아? (남은 답변: LoRA),면접 시작 인사,-0.006001863,0.0,0.0060018631629645824
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아? (남은 답변: LoRA),면접 종료,-0.01072175,0.0,0.010721749626100063
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아? (남은 답변: LoRA),"인공지능, 머신러닝, 딥러닝 차이",-0.00021893134,0.0,0.00021893133816774935
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아? (남은 답변: LoRA),잠시 휴식,-0.013368962,0.0,0.013368962332606316
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아? (남은 답변: LoRA),확률 예측에서 MSE Loss 미 사용 이유,-0.0056431927,0.0,0.005643192678689957
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야! (남은 답변: 모든 질문 해결 완료),BCE Loss 설명,0.0026377582,0.0,0.0026377581525593996
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야! (남은 답변: 모든 질문 해결 완료),LLM Fine-Tuning 의 PEFT,-0.0020789674,0.0,0.0020789674017578363
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야! (남은 답변: 모든 질문 해결 완료),LoRA,-0.004785615,0.0,0.004785615019500256
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야! (남은 답변: 모든 질문 해결 완료),LoRA 와 QLoRA 의 차이,0.9949921,1.0,0.005007922649383545
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야! (남은 답변: 모든 질문 해결 완료),Loss Function 관련 실무 경험,0.0033555434,0.0,0.0033555433619767427
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야! (남은 답변: 모든 질문 해결 완료),Loss Function 예시,0.0028305396,0.0,0.002830539597198367
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야! (남은 답변: 모든 질문 해결 완료),Loss Function 정의,-0.0048042294,0.0,0.004804229363799095
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야! (남은 답변: 모든 질문 해결 완료),MBTI / 좋아하는 아이돌,-0.0032939352,0.0,0.0032939352095127106
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야! (남은 답변: 모든 질문 해결 완료),MSE Loss 설명,-0.0020259793,0.0,0.0020259793382138014
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야! (남은 답변: 모든 질문 해결 완료),MSE Loss 용도,-0.00013207659,0.0,0.00013207658776082098
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야! (남은 답변: 모든 질문 해결 완료),"Multi-Class, Multi-Label 중 BCE 가 좋은 task",-0.0064597395,0.0,0.006459739524871111
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야! (남은 답변: 모든 질문 해결 완료),Multi-Label 에서 CE + Softmax 적용 문제점,-0.00067720556,0.0,0.0006772055639885366
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야! (남은 답변: 모든 질문 해결 완료),PEFT 방법 5가지,-0.008647497,0.0,0.008647496812045574
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야! (남은 답변: 모든 질문 해결 완료),거대 언어 모델 정의,-0.009627675,0.0,0.009627674706280231
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야! (남은 답변: 모든 질문 해결 완료),마지막 할 말,-0.0065940735,0.0,0.0065940734930336475
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야! (남은 답변: 모든 질문 해결 완료),면접 시작 인사,-0.0025602477,0.0,0.002560247667133808
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야! (남은 답변: 모든 질문 해결 완료),면접 종료,-0.003879038,0.0,0.003879037918522954
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야! (남은 답변: 모든 질문 해결 완료),"인공지능, 머신러닝, 딥러닝 차이",-0.0009919389,0.0,0.0009919388685375452
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야! (남은 답변: 모든 질문 해결 완료),잠시 휴식,-0.0017259124,0.0,0.0017259124433621764
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야! (남은 답변: 모든 질문 해결 완료),확률 예측에서 MSE Loss 미 사용 이유,0.0047907084,0.0,0.004790708422660828
LoRA -> 무슨 OOM 없앤다는 것 같은데 (남은 답변: LoRA),BCE Loss 설명,-0.003921388,0.0,0.003921388182789087
LoRA -> 무슨 OOM 없앤다는 것 같은데 (남은 답변: LoRA),LLM Fine-Tuning 의 PEFT,-0.0050954088,0.0,0.0050954087637364864
LoRA -> 무슨 OOM 없앤다는 것 같은데 (남은 답변: LoRA),LoRA,0.99606913,1.0,0.003930866718292236
LoRA -> 무슨 OOM 없앤다는 것 같은데 (남은 답변: LoRA),LoRA 와 QLoRA 의 차이,-0.012919692,0.0,0.012919692322611809
LoRA -> 무슨 OOM 없앤다는 것 같은데 (남은 답변: LoRA),Loss Function 관련 실무 경험,0.00024611477,0.0,0.0002461147669237107
LoRA -> 무슨 OOM 없앤다는 것 같은데 (남은 답변: LoRA),Loss Function 예시,0.0018586505,0.0,0.0018586504738777876
LoRA -> 무슨 OOM 없앤다는 것 같은데 (남은 답변: LoRA),Loss Function 정의,-0.00059474737,0.0,0.0005947473691776395
LoRA -> 무슨 OOM 없앤다는 것 같은데 (남은 답변: LoRA),MBTI / 좋아하는 아이돌,0.0008762826,0.0,0.000876282574608922
LoRA -> 무슨 OOM 없앤다는 것 같은데 (남은 답변: LoRA),MSE Loss 설명,-0.0122745065,0.0,0.012274506501853466
LoRA -> 무슨 OOM 없앤다는 것 같은데 (남은 답변: LoRA),MSE Loss 용도,-5.0477185e-05,0.0,5.0477185141062364e-05
LoRA -> 무슨 OOM 없앤다는 것 같은데 (남은 답변: LoRA),"Multi-Class, Multi-Label 중 BCE 가 좋은 task",-0.0047724526,0.0,0.004772452637553215
LoRA -> 무슨 OOM 없앤다는 것 같은데 (남은 답변: LoRA),Multi-Label 에서 CE + Softmax 적용 문제점,0.0015908643,0.0,0.0015908642672002316
LoRA -> 무슨 OOM 없앤다는 것 같은데 (남은 답변: LoRA),PEFT 방법 5가지,-0.010569113,0.0,0.010569113306701183
LoRA -> 무슨 OOM 없앤다는 것 같은데 (남은 답변: LoRA),거대 언어 모델 정의,-0.0007100853,0.0,0.0007100853254087269
LoRA -> 무슨 OOM 없앤다는 것 같은데 (남은 답변: LoRA),마지막 할 말,-0.013423204,0.0,0.01342320442199707
LoRA -> 무슨 OOM 없앤다는 것 같은데 (남은 답변: LoRA),면접 시작 인사,-0.0047880956,0.0,0.004788095597177744
LoRA -> 무슨 OOM 없앤다는 것 같은데 (남은 답변: LoRA),면접 종료,-0.01191496,0.0,0.011914960108697414
LoRA -> 무슨 OOM 없앤다는 것 같은데 (남은 답변: LoRA),"인공지능, 머신러닝, 딥러닝 차이",-0.002002425,0.0,0.0020024250261485577
LoRA -> 무슨 OOM 없앤다는 것 같은데 (남은 답변: LoRA),잠시 휴식,-0.012930246,0.0,0.012930246070027351
LoRA -> 무슨 OOM 없앤다는 것 같은데 (남은 답변: LoRA),확률 예측에서 MSE Loss 미 사용 이유,-0.00648177,0.0,0.00648176996037364
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야! (남은 답변: 모든 질문 해결 완료),BCE Loss 설명,-0.005324064,0.0,0.0053240638226270676
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야! (남은 답변: 모든 질문 해결 완료),LLM Fine-Tuning 의 PEFT,-0.0037886675,0.0,0.0037886674981564283
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야! (남은 답변: 모든 질문 해결 완료),LoRA,-0.0033014528,0.0,0.0033014528453350067
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야! (남은 답변: 모든 질문 해결 완료),LoRA 와 QLoRA 의 차이,-0.004477692,0.0,0.00447769183665514
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야! (남은 답변: 모든 질문 해결 완료),Loss Function 관련 실무 경험,-0.0021697576,0.0,0.00216975761577487
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야! (남은 답변: 모든 질문 해결 완료),Loss Function 예시,0.00012089663,0.0,0.00012089662777725607
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야! (남은 답변: 모든 질문 해결 완료),Loss Function 정의,-0.0022423973,0.0,0.0022423972841352224
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야! (남은 답변: 모든 질문 해결 완료),MBTI / 좋아하는 아이돌,-0.022010807,0.0,0.02201080694794655
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야! (남은 답변: 모든 질문 해결 완료),MSE Loss 설명,-0.021927135,0.0,0.021927135065197945
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야! (남은 답변: 모든 질문 해결 완료),MSE Loss 용도,-0.00023255333,0.0,0.00023255332780536264
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야! (남은 답변: 모든 질문 해결 완료),"Multi-Class, Multi-Label 중 BCE 가 좋은 task",-0.0031055475,0.0,0.0031055475119501352
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야! (남은 답변: 모든 질문 해결 완료),Multi-Label 에서 CE + Softmax 적용 문제점,0.010697558,0.0,0.010697557590901852
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야! (남은 답변: 모든 질문 해결 완료),PEFT 방법 5가지,-0.01431072,0.0,0.01431072037667036
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야! (남은 답변: 모든 질문 해결 완료),거대 언어 모델 정의,-0.0016103894,0.0,0.001610389444977045
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야! (남은 답변: 모든 질문 해결 완료),마지막 할 말,0.9892572,1.0,0.010742783546447754
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야! (남은 답변: 모든 질문 해결 완료),면접 시작 인사,-0.00681999,0.0,0.006819989997893572
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야! (남은 답변: 모든 질문 해결 완료),면접 종료,-0.0005147971,0.0,0.00051479710964486
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야! (남은 답변: 모든 질문 해결 완료),"인공지능, 머신러닝, 딥러닝 차이",-0.0040213764,0.0,0.004021376371383667
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야! (남은 답변: 모든 질문 해결 완료),잠시 휴식,-0.013639458,0.0,0.013639458455145359
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야! (남은 답변: 모든 질문 해결 완료),확률 예측에서 MSE Loss 미 사용 이유,-0.014387912,0.0,0.01438791211694479
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데 (남은 답변: LoRA 와 QLoRA 의 차이),BCE Loss 설명,-0.0019528602,0.0,0.0019528601551428437
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데 (남은 답변: LoRA 와 QLoRA 의 차이),LLM Fine-Tuning 의 PEFT,-0.0021389762,0.0,0.00213897624053061
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데 (남은 답변: LoRA 와 QLoRA 의 차이),LoRA,-0.010026555,0.0,0.010026554577052593
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데 (남은 답변: LoRA 와 QLoRA 의 차이),LoRA 와 QLoRA 의 차이,0.9972618,1.0,0.00273817777633667
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데 (남은 답변: LoRA 와 QLoRA 의 차이),Loss Function 관련 실무 경험,0.0055852765,0.0,0.005585276521742344
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데 (남은 답변: LoRA 와 QLoRA 의 차이),Loss Function 예시,-0.0021702196,0.0,0.002170219551771879
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데 (남은 답변: LoRA 와 QLoRA 의 차이),Loss Function 정의,0.001576342,0.0,0.0015763420378789306
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데 (남은 답변: LoRA 와 QLoRA 의 차이),MBTI / 좋아하는 아이돌,-0.0003338606,0.0,0.0003338606038596481
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데 (남은 답변: LoRA 와 QLoRA 의 차이),MSE Loss 설명,-0.0009072786,0.0,0.0009072786197066307
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데 (남은 답변: LoRA 와 QLoRA 의 차이),MSE Loss 용도,0.0009725784,0.0,0.0009725784184411168
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데 (남은 답변: LoRA 와 QLoRA 의 차이),"Multi-Class, Multi-Label 중 BCE 가 좋은 task",-0.0007132117,0.0,0.0007132117170840502
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데 (남은 답변: LoRA 와 QLoRA 의 차이),Multi-Label 에서 CE + Softmax 적용 문제점,-0.0016554932,0.0,0.001655493164435029
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데 (남은 답변: LoRA 와 QLoRA 의 차이),PEFT 방법 5가지,-0.00072585366,0.0,0.0007258536643348634
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데 (남은 답변: LoRA 와 QLoRA 의 차이),거대 언어 모델 정의,-0.0036146292,0.0,0.003614629153162241
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데 (남은 답변: LoRA 와 QLoRA 의 차이),마지막 할 말,-0.0034746607,0.0,0.0034746606834232807
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데 (남은 답변: LoRA 와 QLoRA 의 차이),면접 시작 인사,-0.0076530823,0.0,0.007653082255274057
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데 (남은 답변: LoRA 와 QLoRA 의 차이),면접 종료,-0.00027544156,0.0,0.00027544156182557344
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데 (남은 답변: LoRA 와 QLoRA 의 차이),"인공지능, 머신러닝, 딥러닝 차이",0.0011743577,0.0,0.00117435771971941
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데 (남은 답변: LoRA 와 QLoRA 의 차이),잠시 휴식,-0.0036069124,0.0,0.0036069124471396208
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데 (남은 답변: LoRA 와 QLoRA 의 차이),확률 예측에서 MSE Loss 미 사용 이유,0.0034992648,0.0,0.0034992648288607597
마지막 할 말 -> 로라야 정말 고마워! (남은 답변: 마지막 할 말),BCE Loss 설명,0.00057684205,0.0,0.0005768420523963869
마지막 할 말 -> 로라야 정말 고마워! (남은 답변: 마지막 할 말),LLM Fine-Tuning 의 PEFT,-0.00031075525,0.0,0.00031075524748302996
마지막 할 말 -> 로라야 정말 고마워! (남은 답변: 마지막 할 말),LoRA,-0.007866387,0.0,0.007866387255489826
마지막 할 말 -> 로라야 정말 고마워! (남은 답변: 마지막 할 말),LoRA 와 QLoRA 의 차이,-0.0002774251,0.0,0.0002774251042865217
마지막 할 말 -> 로라야 정말 고마워! (남은 답변: 마지막 할 말),Loss Function 관련 실무 경험,0.004993878,0.0,0.004993877839297056
마지막 할 말 -> 로라야 정말 고마워! (남은 답변: 마지막 할 말),Loss Function 예시,-0.0003663687,0.0,0.00036636870936490595
마지막 할 말 -> 로라야 정말 고마워! (남은 답변: 마지막 할 말),Loss Function 정의,0.0014234186,0.0,0.0014234186382964253
마지막 할 말 -> 로라야 정말 고마워! (남은 답변: 마지막 할 말),MBTI / 좋아하는 아이돌,0.0039012623,0.0,0.003901262301951647
마지막 할 말 -> 로라야 정말 고마워! (남은 답변: 마지막 할 말),MSE Loss 설명,-0.005021312,0.0,0.005021311808377504
마지막 할 말 -> 로라야 정말 고마워! (남은 답변: 마지막 할 말),MSE Loss 용도,-0.0038363314,0.0,0.0038363314233720303
마지막 할 말 -> 로라야 정말 고마워! (남은 답변: 마지막 할 말),"Multi-Class, Multi-Label 중 BCE 가 좋은 task",0.00043079662,0.0,0.00043079661554656923
마지막 할 말 -> 로라야 정말 고마워! (남은 답변: 마지막 할 말),Multi-Label 에서 CE + Softmax 적용 문제점,0.00021185422,0.0,0.00021185421792324632
마지막 할 말 -> 로라야 정말 고마워! (남은 답변: 마지막 할 말),PEFT 방법 5가지,0.0006141774,0.0,0.0006141773774288595
마지막 할 말 -> 로라야 정말 고마워! (남은 답변: 마지막 할 말),거대 언어 모델 정의,-0.0038589614,0.0,0.003858961397781968
마지막 할 말 -> 로라야 정말 고마워! (남은 답변: 마지막 할 말),마지막 할 말,-0.01844927,0.0,0.018449269235134125
마지막 할 말 -> 로라야 정말 고마워! (남은 답변: 마지막 할 말),면접 시작 인사,-0.017224472,0.0,0.017224472016096115
마지막 할 말 -> 로라야 정말 고마워! (남은 답변: 마지막 할 말),면접 종료,0.9955037,1.0,0.004496276378631592
마지막 할 말 -> 로라야 정말 고마워! (남은 답변: 마지막 할 말),"인공지능, 머신러닝, 딥러닝 차이",0.0010669056,0.0,0.0010669055627658963
마지막 할 말 -> 로라야 정말 고마워! (남은 답변: 마지막 할 말),잠시 휴식,-0.0029140993,0.0,0.0029140992555767298
마지막 할 말 -> 로라야 정말 고마워! (남은 답변: 마지막 할 말),확률 예측에서 MSE Loss 미 사용 이유,0.0009439243,0.0,0.0009439243003726006
마지막 할 말 -> 로라야 사랑해 (남은 답변: 마지막 할 말),BCE Loss 설명,0.00016926519,0.0,0.00016926518583204597
마지막 할 말 -> 로라야 사랑해 (남은 답변: 마지막 할 말),LLM Fine-Tuning 의 PEFT,0.0017813253,0.0,0.0017813253216445446
마지막 할 말 -> 로라야 사랑해 (남은 답변: 마지막 할 말),LoRA,-0.00832705,0.0,0.008327050134539604
마지막 할 말 -> 로라야 사랑해 (남은 답변: 마지막 할 말),LoRA 와 QLoRA 의 차이,0.00010229626,0.0,0.00010229626059299335
마지막 할 말 -> 로라야 사랑해 (남은 답변: 마지막 할 말),Loss Function 관련 실무 경험,0.0057681487,0.0,0.005768148694187403
마지막 할 말 -> 로라야 사랑해 (남은 답변: 마지막 할 말),Loss Function 예시,-0.002093151,0.0,0.0020931509789079428
마지막 할 말 -> 로라야 사랑해 (남은 답변: 마지막 할 말),Loss Function 정의,0.0035395182,0.0,0.0035395182203501463
마지막 할 말 -> 로라야 사랑해 (남은 답변: 마지막 할 말),MBTI / 좋아하는 아이돌,0.0025216306,0.0,0.002521630609408021
마지막 할 말 -> 로라야 사랑해 (남은 답변: 마지막 할 말),MSE Loss 설명,-0.0072893174,0.0,0.007289317436516285
마지막 할 말 -> 로라야 사랑해 (남은 답변: 마지막 할 말),MSE Loss 용도,-0.0037478434,0.0,0.003747843438759446
마지막 할 말 -> 로라야 사랑해 (남은 답변: 마지막 할 말),"Multi-Class, Multi-Label 중 BCE 가 좋은 task",-5.473274e-05,0.0,5.473273995448835e-05
마지막 할 말 -> 로라야 사랑해 (남은 답변: 마지막 할 말),Multi-Label 에서 CE + Softmax 적용 문제점,0.00092655845,0.0,0.000926558452192694
마지막 할 말 -> 로라야 사랑해 (남은 답변: 마지막 할 말),PEFT 방법 5가지,0.001260648,0.0,0.0012606480158865452
마지막 할 말 -> 로라야 사랑해 (남은 답변: 마지막 할 말),거대 언어 모델 정의,-0.002739161,0.0,0.00273916102014482
마지막 할 말 -> 로라야 사랑해 (남은 답변: 마지막 할 말),마지막 할 말,-0.018977685,0.0,0.018977684900164604
마지막 할 말 -> 로라야 사랑해 (남은 답변: 마지막 할 말),면접 시작 인사,-0.01811383,0.0,0.01811382919549942
마지막 할 말 -> 로라야 사랑해 (남은 답변: 마지막 할 말),면접 종료,0.9955044,1.0,0.0044956207275390625
마지막 할 말 -> 로라야 사랑해 (남은 답변: 마지막 할 말),"인공지능, 머신러닝, 딥러닝 차이",0.0002909958,0.0,0.0002909958129748702
마지막 할 말 -> 로라야 사랑해 (남은 답변: 마지막 할 말),잠시 휴식,-0.0032854292,0.0,0.0032854292076081038
마지막 할 말 -> 로라야 사랑해 (남은 답변: 마지막 할 말),확률 예측에서 MSE Loss 미 사용 이유,0.0017333925,0.0,0.0017333924770355225
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어 (남은 답변: 마지막 할 말),BCE Loss 설명,0.00024002371,0.0,0.00024002371355891228
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어 (남은 답변: 마지막 할 말),LLM Fine-Tuning 의 PEFT,0.0009269872,0.0,0.0009269872098229825
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어 (남은 답변: 마지막 할 말),LoRA,-0.0077924742,0.0,0.007792474236339331
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어 (남은 답변: 마지막 할 말),LoRA 와 QLoRA 의 차이,0.00023298095,0.0,0.00023298095038626343
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어 (남은 답변: 마지막 할 말),Loss Function 관련 실무 경험,0.0056210025,0.0,0.005621002521365881
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어 (남은 답변: 마지막 할 말),Loss Function 예시,-0.0019438448,0.0,0.001943844836205244
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어 (남은 답변: 마지막 할 말),Loss Function 정의,0.0008160494,0.0,0.0008160494035109878
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어 (남은 답변: 마지막 할 말),MBTI / 좋아하는 아이돌,0.004717502,0.0,0.004717501811683178
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어 (남은 답변: 마지막 할 말),MSE Loss 설명,-0.0080007855,0.0,0.008000785484910011
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어 (남은 답변: 마지막 할 말),MSE Loss 용도,-0.0040848115,0.0,0.004084811545908451
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어 (남은 답변: 마지막 할 말),"Multi-Class, Multi-Label 중 BCE 가 좋은 task",0.00023760533,0.0,0.00023760533076711
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어 (남은 답변: 마지막 할 말),Multi-Label 에서 CE + Softmax 적용 문제점,-0.00081365387,0.0,0.000813653867226094
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어 (남은 답변: 마지막 할 말),PEFT 방법 5가지,2.9629655e-05,0.0,2.9629654818563722e-05
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어 (남은 답변: 마지막 할 말),거대 언어 모델 정의,-0.004570204,0.0,0.004570203833281994
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어 (남은 답변: 마지막 할 말),마지막 할 말,-0.019671973,0.0,0.0196719728410244
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어 (남은 답변: 마지막 할 말),면접 시작 인사,-0.01759655,0.0,0.017596550285816193
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어 (남은 답변: 마지막 할 말),면접 종료,0.9951167,1.0,0.004883289337158203
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어 (남은 답변: 마지막 할 말),"인공지능, 머신러닝, 딥러닝 차이",-0.000114774324,0.0,0.00011477432417450473
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어 (남은 답변: 마지막 할 말),잠시 휴식,-0.0048326696,0.0,0.004832669626921415
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어 (남은 답변: 마지막 할 말),확률 예측에서 MSE Loss 미 사용 이유,0.002021931,0.0,0.0020219311118125916
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워 (남은 답변: 마지막 할 말),BCE Loss 설명,-0.00037726393,0.0,0.0003772639320231974
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워 (남은 답변: 마지막 할 말),LLM Fine-Tuning 의 PEFT,0.0020765432,0.0,0.0020765431690961123
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워 (남은 답변: 마지막 할 말),LoRA,-0.010424933,0.0,0.010424933396279812
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워 (남은 답변: 마지막 할 말),LoRA 와 QLoRA 의 차이,-0.0019761822,0.0,0.0019761822186410427
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워 (남은 답변: 마지막 할 말),Loss Function 관련 실무 경험,0.0075680315,0.0,0.007568031549453735
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워 (남은 답변: 마지막 할 말),Loss Function 예시,-0.0014685664,0.0,0.00146856636274606
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워 (남은 답변: 마지막 할 말),Loss Function 정의,-0.0004622604,0.0,0.0004622603883035481
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워 (남은 답변: 마지막 할 말),MBTI / 좋아하는 아이돌,0.004026745,0.0,0.004026744980365038
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워 (남은 답변: 마지막 할 말),MSE Loss 설명,-0.0031813898,0.0,0.0031813897658139467
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워 (남은 답변: 마지막 할 말),MSE Loss 용도,-0.004846223,0.0,0.004846223164349794
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워 (남은 답변: 마지막 할 말),"Multi-Class, Multi-Label 중 BCE 가 좋은 task",-0.0002011612,0.0,0.00020116119412705302
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워 (남은 답변: 마지막 할 말),Multi-Label 에서 CE + Softmax 적용 문제점,-1.3867604e-05,0.0,1.3867604138795286e-05
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워 (남은 답변: 마지막 할 말),PEFT 방법 5가지,0.00026776682,0.0,0.00026776682352647185
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워 (남은 답변: 마지막 할 말),거대 언어 모델 정의,-0.005325039,0.0,0.00532503891736269
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워 (남은 답변: 마지막 할 말),마지막 할 말,-0.018278658,0.0,0.018278658390045166
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워 (남은 답변: 마지막 할 말),면접 시작 인사,-0.01447933,0.0,0.01447932980954647
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워 (남은 답변: 마지막 할 말),면접 종료,0.9949265,1.0,0.005073487758636475
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워 (남은 답변: 마지막 할 말),"인공지능, 머신러닝, 딥러닝 차이",-0.002118146,0.0,0.0021181460469961166
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워 (남은 답변: 마지막 할 말),잠시 휴식,-0.0050818464,0.0,0.0050818463787436485
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워 (남은 답변: 마지막 할 말),확률 예측에서 MSE Loss 미 사용 이유,-0.001818061,0.0,0.0018180609913542867
