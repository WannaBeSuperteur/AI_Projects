input_part,next_question,predicted_similarity,ground_truth_similarity,absolute_error
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서? (남은 답변: 면접 시작 인사),BCE Loss 설명,-0.017159363,0.0,0.017159363254904747
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서? (남은 답변: 면접 시작 인사),LLM Fine-Tuning 의 PEFT,0.0020393475,0.0,0.002039347542449832
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서? (남은 답변: 면접 시작 인사),LoRA,0.013258935,0.0,0.013258934952318668
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서? (남은 답변: 면접 시작 인사),LoRA 와 QLoRA 의 차이,0.019498983,0.0,0.019498983398079872
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서? (남은 답변: 면접 시작 인사),Loss Function 관련 실무 경험,0.052360136,0.0,0.052360136061906815
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서? (남은 답변: 면접 시작 인사),Loss Function 예시,-0.013058124,0.0,0.013058124110102654
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서? (남은 답변: 면접 시작 인사),Loss Function 정의,0.01608363,0.0,0.016083629801869392
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서? (남은 답변: 면접 시작 인사),MBTI / 좋아하는 아이돌,-0.02192526,0.0,0.02192525938153267
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서? (남은 답변: 면접 시작 인사),MSE Loss 설명,0.04364981,0.0,0.043649811297655106
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서? (남은 답변: 면접 시작 인사),MSE Loss 용도,0.0035401336,0.0,0.0035401335917413235
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서? (남은 답변: 면접 시작 인사),"Multi-Class, Multi-Label 중 BCE 가 좋은 task",0.018871505,0.0,0.018871504813432693
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서? (남은 답변: 면접 시작 인사),Multi-Label 에서 CE + Softmax 적용 문제점,-0.012003567,0.0,0.012003567069768906
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서? (남은 답변: 면접 시작 인사),PEFT 방법 5가지,0.010888571,0.0,0.010888570919632912
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서? (남은 답변: 면접 시작 인사),거대 언어 모델 정의,0.02336377,0.0,0.02336377091705799
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서? (남은 답변: 면접 시작 인사),마지막 할 말,0.008763068,0.0,0.008763068355619907
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서? (남은 답변: 면접 시작 인사),면접 시작 인사,0.8366245,1.0,0.16337549686431885
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서? (남은 답변: 면접 시작 인사),면접 종료,-0.050671197,0.0,0.05067119747400284
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서? (남은 답변: 면접 시작 인사),"인공지능, 머신러닝, 딥러닝 차이",0.11287422,0.0,0.11287421733140945
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서? (남은 답변: 면접 시작 인사),잠시 휴식,-0.0066864127,0.0,0.00668641272932291
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서? (남은 답변: 면접 시작 인사),확률 예측에서 MSE Loss 미 사용 이유,0.016314898,0.0,0.016314897686243057
면접 시작 인사 -> 로라야 안녕 정말 반가워 (남은 답변: 면접 시작 인사),BCE Loss 설명,-0.049171302,0.0,0.04917130246758461
면접 시작 인사 -> 로라야 안녕 정말 반가워 (남은 답변: 면접 시작 인사),LLM Fine-Tuning 의 PEFT,-0.04250331,0.0,0.04250330850481987
면접 시작 인사 -> 로라야 안녕 정말 반가워 (남은 답변: 면접 시작 인사),LoRA,0.019456044,0.0,0.01945604383945465
면접 시작 인사 -> 로라야 안녕 정말 반가워 (남은 답변: 면접 시작 인사),LoRA 와 QLoRA 의 차이,0.030728241,0.0,0.030728241428732872
면접 시작 인사 -> 로라야 안녕 정말 반가워 (남은 답변: 면접 시작 인사),Loss Function 관련 실무 경험,0.0232056,0.0,0.023205600678920746
면접 시작 인사 -> 로라야 안녕 정말 반가워 (남은 답변: 면접 시작 인사),Loss Function 예시,0.016543979,0.0,0.016543978825211525
면접 시작 인사 -> 로라야 안녕 정말 반가워 (남은 답변: 면접 시작 인사),Loss Function 정의,-0.022133524,0.0,0.02213352359831333
면접 시작 인사 -> 로라야 안녕 정말 반가워 (남은 답변: 면접 시작 인사),MBTI / 좋아하는 아이돌,0.015022897,0.0,0.01502289716154337
면접 시작 인사 -> 로라야 안녕 정말 반가워 (남은 답변: 면접 시작 인사),MSE Loss 설명,0.02930419,0.0,0.02930418960750103
면접 시작 인사 -> 로라야 안녕 정말 반가워 (남은 답변: 면접 시작 인사),MSE Loss 용도,0.031452857,0.0,0.031452856957912445
면접 시작 인사 -> 로라야 안녕 정말 반가워 (남은 답변: 면접 시작 인사),"Multi-Class, Multi-Label 중 BCE 가 좋은 task",0.026458174,0.0,0.026458173990249634
면접 시작 인사 -> 로라야 안녕 정말 반가워 (남은 답변: 면접 시작 인사),Multi-Label 에서 CE + Softmax 적용 문제점,-0.009015989,0.0,0.009015988558530807
면접 시작 인사 -> 로라야 안녕 정말 반가워 (남은 답변: 면접 시작 인사),PEFT 방법 5가지,-0.019234493,0.0,0.019234493374824524
면접 시작 인사 -> 로라야 안녕 정말 반가워 (남은 답변: 면접 시작 인사),거대 언어 모델 정의,0.043308366,0.0,0.04330836609005928
면접 시작 인사 -> 로라야 안녕 정말 반가워 (남은 답변: 면접 시작 인사),마지막 할 말,-0.03654795,0.0,0.03654795140028
면접 시작 인사 -> 로라야 안녕 정말 반가워 (남은 답변: 면접 시작 인사),면접 시작 인사,0.83162755,1.0,0.16837245225906372
면접 시작 인사 -> 로라야 안녕 정말 반가워 (남은 답변: 면접 시작 인사),면접 종료,0.10022542,0.0,0.10022541880607605
면접 시작 인사 -> 로라야 안녕 정말 반가워 (남은 답변: 면접 시작 인사),"인공지능, 머신러닝, 딥러닝 차이",0.024528276,0.0,0.024528276175260544
면접 시작 인사 -> 로라야 안녕 정말 반가워 (남은 답변: 면접 시작 인사),잠시 휴식,0.0454754,0.0,0.04547540098428726
면접 시작 인사 -> 로라야 안녕 정말 반가워 (남은 답변: 면접 시작 인사),확률 예측에서 MSE Loss 미 사용 이유,-0.021905096,0.0,0.021905096247792244
면접 시작 인사 -> 로라야 그럼 네가 면접관이야? (남은 답변: 면접 시작 인사),BCE Loss 설명,-0.03221653,0.0,0.03221653029322624
면접 시작 인사 -> 로라야 그럼 네가 면접관이야? (남은 답변: 면접 시작 인사),LLM Fine-Tuning 의 PEFT,-0.035512827,0.0,0.03551282733678818
면접 시작 인사 -> 로라야 그럼 네가 면접관이야? (남은 답변: 면접 시작 인사),LoRA,0.010442707,0.0,0.010442706756293774
면접 시작 인사 -> 로라야 그럼 네가 면접관이야? (남은 답변: 면접 시작 인사),LoRA 와 QLoRA 의 차이,-0.0033261888,0.0,0.003326188772916794
면접 시작 인사 -> 로라야 그럼 네가 면접관이야? (남은 답변: 면접 시작 인사),Loss Function 관련 실무 경험,0.03130436,0.0,0.031304359436035156
면접 시작 인사 -> 로라야 그럼 네가 면접관이야? (남은 답변: 면접 시작 인사),Loss Function 예시,-0.007381135,0.0,0.007381135132163763
면접 시작 인사 -> 로라야 그럼 네가 면접관이야? (남은 답변: 면접 시작 인사),Loss Function 정의,0.009366289,0.0,0.009366288781166077
면접 시작 인사 -> 로라야 그럼 네가 면접관이야? (남은 답변: 면접 시작 인사),MBTI / 좋아하는 아이돌,-0.020159699,0.0,0.020159699022769928
면접 시작 인사 -> 로라야 그럼 네가 면접관이야? (남은 답변: 면접 시작 인사),MSE Loss 설명,0.036742657,0.0,0.03674265742301941
면접 시작 인사 -> 로라야 그럼 네가 면접관이야? (남은 답변: 면접 시작 인사),MSE Loss 용도,-0.0024419755,0.0,0.0024419755209237337
면접 시작 인사 -> 로라야 그럼 네가 면접관이야? (남은 답변: 면접 시작 인사),"Multi-Class, Multi-Label 중 BCE 가 좋은 task",0.015020445,0.0,0.015020444989204407
면접 시작 인사 -> 로라야 그럼 네가 면접관이야? (남은 답변: 면접 시작 인사),Multi-Label 에서 CE + Softmax 적용 문제점,-0.0022928154,0.0,0.002292815363034606
면접 시작 인사 -> 로라야 그럼 네가 면접관이야? (남은 답변: 면접 시작 인사),PEFT 방법 5가지,-0.0076964414,0.0,0.007696441374719143
면접 시작 인사 -> 로라야 그럼 네가 면접관이야? (남은 답변: 면접 시작 인사),거대 언어 모델 정의,-0.0008725605,0.0,0.0008725604857318103
면접 시작 인사 -> 로라야 그럼 네가 면접관이야? (남은 답변: 면접 시작 인사),마지막 할 말,0.0022717179,0.0,0.002271717879921198
면접 시작 인사 -> 로라야 그럼 네가 면접관이야? (남은 답변: 면접 시작 인사),면접 시작 인사,0.8227722,1.0,0.17722779512405396
면접 시작 인사 -> 로라야 그럼 네가 면접관이야? (남은 답변: 면접 시작 인사),면접 종료,-0.10278703,0.0,0.10278703272342682
면접 시작 인사 -> 로라야 그럼 네가 면접관이야? (남은 답변: 면접 시작 인사),"인공지능, 머신러닝, 딥러닝 차이",0.21276419,0.0,0.2127641886472702
면접 시작 인사 -> 로라야 그럼 네가 면접관이야? (남은 답변: 면접 시작 인사),잠시 휴식,0.029850563,0.0,0.029850563034415245
면접 시작 인사 -> 로라야 그럼 네가 면접관이야? (남은 답변: 면접 시작 인사),확률 예측에서 MSE Loss 미 사용 이유,-0.007573297,0.0,0.007573296781629324
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까 (남은 답변: 면접 시작 인사),BCE Loss 설명,-0.03555402,0.0,0.03555402159690857
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까 (남은 답변: 면접 시작 인사),LLM Fine-Tuning 의 PEFT,-0.03564328,0.0,0.03564327955245972
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까 (남은 답변: 면접 시작 인사),LoRA,0.06539762,0.0,0.06539762020111084
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까 (남은 답변: 면접 시작 인사),LoRA 와 QLoRA 의 차이,0.048769336,0.0,0.04876933619379997
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까 (남은 답변: 면접 시작 인사),Loss Function 관련 실무 경험,0.02706188,0.0,0.027061879634857178
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까 (남은 답변: 면접 시작 인사),Loss Function 예시,-0.05186279,0.0,0.05186279118061066
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까 (남은 답변: 면접 시작 인사),Loss Function 정의,0.06325829,0.0,0.06325829029083252
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까 (남은 답변: 면접 시작 인사),MBTI / 좋아하는 아이돌,-0.029754799,0.0,0.029754798859357834
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까 (남은 답변: 면접 시작 인사),MSE Loss 설명,0.028265769,0.0,0.02826576866209507
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까 (남은 답변: 면접 시작 인사),MSE Loss 용도,0.027675658,0.0,0.027675658464431763
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까 (남은 답변: 면접 시작 인사),"Multi-Class, Multi-Label 중 BCE 가 좋은 task",-0.017544774,0.0,0.01754477433860302
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까 (남은 답변: 면접 시작 인사),Multi-Label 에서 CE + Softmax 적용 문제점,-0.0016175302,0.0,0.0016175302444025874
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까 (남은 답변: 면접 시작 인사),PEFT 방법 5가지,0.011945231,0.0,0.011945230886340141
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까 (남은 답변: 면접 시작 인사),거대 언어 모델 정의,-0.018997971,0.0,0.01899797096848488
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까 (남은 답변: 면접 시작 인사),마지막 할 말,-0.0070602894,0.0,0.00706028938293457
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까 (남은 답변: 면접 시작 인사),면접 시작 인사,0.7942943,1.0,0.2057057023048401
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까 (남은 답변: 면접 시작 인사),면접 종료,0.017198406,0.0,0.017198406159877777
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까 (남은 답변: 면접 시작 인사),"인공지능, 머신러닝, 딥러닝 차이",0.14746751,0.0,0.1474675089120865
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까 (남은 답변: 면접 시작 인사),잠시 휴식,-0.035185162,0.0,0.03518516197800636
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까 (남은 답변: 면접 시작 인사),확률 예측에서 MSE Loss 미 사용 이유,-0.017162828,0.0,0.017162827774882317
면접 시작 인사 -> 파이팅! 시작하자 (남은 답변: 면접 시작 인사),BCE Loss 설명,0.016674867,0.0,0.016674866899847984
면접 시작 인사 -> 파이팅! 시작하자 (남은 답변: 면접 시작 인사),LLM Fine-Tuning 의 PEFT,0.026031634,0.0,0.026031633839011192
면접 시작 인사 -> 파이팅! 시작하자 (남은 답변: 면접 시작 인사),LoRA,-0.00015022064,0.0,0.0001502206432633102
면접 시작 인사 -> 파이팅! 시작하자 (남은 답변: 면접 시작 인사),LoRA 와 QLoRA 의 차이,0.018108727,0.0,0.018108727410435677
면접 시작 인사 -> 파이팅! 시작하자 (남은 답변: 면접 시작 인사),Loss Function 관련 실무 경험,-0.008406009,0.0,0.008406008593738079
면접 시작 인사 -> 파이팅! 시작하자 (남은 답변: 면접 시작 인사),Loss Function 예시,0.015382265,0.0,0.015382264740765095
면접 시작 인사 -> 파이팅! 시작하자 (남은 답변: 면접 시작 인사),Loss Function 정의,0.012333607,0.0,0.01233360730111599
면접 시작 인사 -> 파이팅! 시작하자 (남은 답변: 면접 시작 인사),MBTI / 좋아하는 아이돌,0.026096644,0.0,0.02609664388000965
면접 시작 인사 -> 파이팅! 시작하자 (남은 답변: 면접 시작 인사),MSE Loss 설명,-0.00820705,0.0,0.008207050152122974
면접 시작 인사 -> 파이팅! 시작하자 (남은 답변: 면접 시작 인사),MSE Loss 용도,0.0050592506,0.0,0.005059250630438328
면접 시작 인사 -> 파이팅! 시작하자 (남은 답변: 면접 시작 인사),"Multi-Class, Multi-Label 중 BCE 가 좋은 task",-0.008831577,0.0,0.008831577375531197
면접 시작 인사 -> 파이팅! 시작하자 (남은 답변: 면접 시작 인사),Multi-Label 에서 CE + Softmax 적용 문제점,0.002701274,0.0,0.002701273886486888
면접 시작 인사 -> 파이팅! 시작하자 (남은 답변: 면접 시작 인사),PEFT 방법 5가지,0.026577817,0.0,0.026577817276120186
면접 시작 인사 -> 파이팅! 시작하자 (남은 답변: 면접 시작 인사),거대 언어 모델 정의,-0.039340228,0.0,0.03934022784233093
면접 시작 인사 -> 파이팅! 시작하자 (남은 답변: 면접 시작 인사),마지막 할 말,-0.0040012435,0.0,0.004001243505626917
면접 시작 인사 -> 파이팅! 시작하자 (남은 답변: 면접 시작 인사),면접 시작 인사,0.023956712,0.0,0.02395671233534813
면접 시작 인사 -> 파이팅! 시작하자 (남은 답변: 면접 시작 인사),면접 종료,-0.009312408,0.0,0.009312408044934273
면접 시작 인사 -> 파이팅! 시작하자 (남은 답변: 면접 시작 인사),"인공지능, 머신러닝, 딥러닝 차이",0.8262202,1.0,0.17377978563308716
면접 시작 인사 -> 파이팅! 시작하자 (남은 답변: 면접 시작 인사),잠시 휴식,-0.055545434,0.0,0.05554543435573578
면접 시작 인사 -> 파이팅! 시작하자 (남은 답변: 면접 시작 인사),확률 예측에서 MSE Loss 미 사용 이유,-0.047683693,0.0,0.04768369346857071
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데 (남은 답변: 인공지능, 머신러닝, 딥러닝)",BCE Loss 설명,-0.00020790749,0.0,0.00020790749113075435
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데 (남은 답변: 인공지능, 머신러닝, 딥러닝)",LLM Fine-Tuning 의 PEFT,-0.0032456673,0.0,0.0032456673216074705
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데 (남은 답변: 인공지능, 머신러닝, 딥러닝)",LoRA,0.0057496284,0.0,0.005749628413468599
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데 (남은 답변: 인공지능, 머신러닝, 딥러닝)",LoRA 와 QLoRA 의 차이,0.0015460934,0.0,0.0015460933791473508
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데 (남은 답변: 인공지능, 머신러닝, 딥러닝)",Loss Function 관련 실무 경험,-0.0020886757,0.0,0.0020886757411062717
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데 (남은 답변: 인공지능, 머신러닝, 딥러닝)",Loss Function 예시,0.012836004,0.0,0.012836003676056862
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데 (남은 답변: 인공지능, 머신러닝, 딥러닝)",Loss Function 정의,0.0025114317,0.0,0.002511431695893407
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데 (남은 답변: 인공지능, 머신러닝, 딥러닝)",MBTI / 좋아하는 아이돌,0.008470885,0.0,0.008470885455608368
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데 (남은 답변: 인공지능, 머신러닝, 딥러닝)",MSE Loss 설명,-0.0020846797,0.0,0.0020846796687692404
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데 (남은 답변: 인공지능, 머신러닝, 딥러닝)",MSE Loss 용도,-0.0001492276,0.0,0.0001492276060162112
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데 (남은 답변: 인공지능, 머신러닝, 딥러닝)","Multi-Class, Multi-Label 중 BCE 가 좋은 task",0.008645873,0.0,0.008645872585475445
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데 (남은 답변: 인공지능, 머신러닝, 딥러닝)",Multi-Label 에서 CE + Softmax 적용 문제점,0.00075826806,0.0,0.0007582680555060506
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데 (남은 답변: 인공지능, 머신러닝, 딥러닝)",PEFT 방법 5가지,-0.0052734725,0.0,0.0052734725177288055
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데 (남은 답변: 인공지능, 머신러닝, 딥러닝)",거대 언어 모델 정의,-0.029232193,0.0,0.029232192784547806
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데 (남은 답변: 인공지능, 머신러닝, 딥러닝)",마지막 할 말,0.0010880284,0.0,0.0010880284244194627
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데 (남은 답변: 인공지능, 머신러닝, 딥러닝)",면접 시작 인사,-0.059061486,0.0,0.05906148627400398
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데 (남은 답변: 인공지능, 머신러닝, 딥러닝)",면접 종료,-0.00867211,0.0,0.008672109805047512
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데 (남은 답변: 인공지능, 머신러닝, 딥러닝)","인공지능, 머신러닝, 딥러닝 차이",0.9794837,1.0,0.020516276359558105
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데 (남은 답변: 인공지능, 머신러닝, 딥러닝)",잠시 휴식,0.012316951,0.0,0.012316950596868992
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데 (남은 답변: 인공지능, 머신러닝, 딥러닝)",확률 예측에서 MSE Loss 미 사용 이유,-0.011296938,0.0,0.011296938173472881
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거! (남은 답변: 머신러닝, 딥러닝)",BCE Loss 설명,0.0059409495,0.0,0.00594094954431057
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거! (남은 답변: 머신러닝, 딥러닝)",LLM Fine-Tuning 의 PEFT,0.0032879184,0.0,0.0032879184000194073
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거! (남은 답변: 머신러닝, 딥러닝)",LoRA,-0.00038194732,0.0,0.000381947320420295
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거! (남은 답변: 머신러닝, 딥러닝)",LoRA 와 QLoRA 의 차이,0.005724917,0.0,0.0057249171659350395
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거! (남은 답변: 머신러닝, 딥러닝)",Loss Function 관련 실무 경험,-0.001989497,0.0,0.0019894971046596766
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거! (남은 답변: 머신러닝, 딥러닝)",Loss Function 예시,0.011255113,0.0,0.011255113407969475
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거! (남은 답변: 머신러닝, 딥러닝)",Loss Function 정의,0.0049722423,0.0,0.004972242284566164
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거! (남은 답변: 머신러닝, 딥러닝)",MBTI / 좋아하는 아이돌,-2.974205e-06,0.0,2.974205017380882e-06
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거! (남은 답변: 머신러닝, 딥러닝)",MSE Loss 설명,0.00031794695,0.0,0.00031794694950804114
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거! (남은 답변: 머신러닝, 딥러닝)",MSE Loss 용도,-0.002819872,0.0,0.0028198719955980778
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거! (남은 답변: 머신러닝, 딥러닝)","Multi-Class, Multi-Label 중 BCE 가 좋은 task",0.015003727,0.0,0.015003726817667484
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거! (남은 답변: 머신러닝, 딥러닝)",Multi-Label 에서 CE + Softmax 적용 문제점,-0.0010240631,0.0,0.0010240630945190787
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거! (남은 답변: 머신러닝, 딥러닝)",PEFT 방법 5가지,-0.011439702,0.0,0.011439701542258263
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거! (남은 답변: 머신러닝, 딥러닝)",거대 언어 모델 정의,-0.023447718,0.0,0.02344771847128868
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거! (남은 답변: 머신러닝, 딥러닝)",마지막 할 말,-0.006026124,0.0,0.006026124116033316
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거! (남은 답변: 머신러닝, 딥러닝)",면접 시작 인사,-0.05226966,0.0,0.05226965993642807
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거! (남은 답변: 머신러닝, 딥러닝)",면접 종료,-0.0065872543,0.0,0.006587254349142313
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거! (남은 답변: 머신러닝, 딥러닝)","인공지능, 머신러닝, 딥러닝 차이",0.982578,1.0,0.017422020435333252
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거! (남은 답변: 머신러닝, 딥러닝)",잠시 휴식,0.00093068654,0.0,0.0009306865395046771
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거! (남은 답변: 머신러닝, 딥러닝)",확률 예측에서 MSE Loss 미 사용 이유,-0.01924985,0.0,0.019249850884079933
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지 (남은 답변: 딥러닝)",BCE Loss 설명,0.008143446,0.0,0.008143446408212185
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지 (남은 답변: 딥러닝)",LLM Fine-Tuning 의 PEFT,-0.0025737751,0.0,0.002573775127530098
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지 (남은 답변: 딥러닝)",LoRA,-0.00060794747,0.0,0.000607947469688952
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지 (남은 답변: 딥러닝)",LoRA 와 QLoRA 의 차이,-0.0045983936,0.0,0.0045983935706317425
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지 (남은 답변: 딥러닝)",Loss Function 관련 실무 경험,-0.0028518734,0.0,0.002851873403415084
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지 (남은 답변: 딥러닝)",Loss Function 예시,0.010965435,0.0,0.010965434834361076
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지 (남은 답변: 딥러닝)",Loss Function 정의,0.0077919536,0.0,0.007791953627020121
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지 (남은 답변: 딥러닝)",MBTI / 좋아하는 아이돌,-0.0020480747,0.0,0.00204807473346591
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지 (남은 답변: 딥러닝)",MSE Loss 설명,-0.00816284,0.0,0.008162840269505978
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지 (남은 답변: 딥러닝)",MSE Loss 용도,-0.00075576035,0.0,0.0007557603530585766
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지 (남은 답변: 딥러닝)","Multi-Class, Multi-Label 중 BCE 가 좋은 task",0.0002455615,0.0,0.0002455615031067282
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지 (남은 답변: 딥러닝)",Multi-Label 에서 CE + Softmax 적용 문제점,-0.003490621,0.0,0.00349062099121511
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지 (남은 답변: 딥러닝)",PEFT 방법 5가지,-0.0065162554,0.0,0.006516255438327789
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지 (남은 답변: 딥러닝)",거대 언어 모델 정의,-0.020525124,0.0,0.020525123924016953
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지 (남은 답변: 딥러닝)",마지막 할 말,0.00074426073,0.0,0.0007442607311531901
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지 (남은 답변: 딥러닝)",면접 시작 인사,-0.03592015,0.0,0.035920150578022
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지 (남은 답변: 딥러닝)",면접 종료,-0.007901288,0.0,0.007901287637650967
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지 (남은 답변: 딥러닝)","인공지능, 머신러닝, 딥러닝 차이",0.9829872,1.0,0.01701277494430542
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지 (남은 답변: 딥러닝)",잠시 휴식,-0.0114486795,0.0,0.011448679491877556
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지 (남은 답변: 딥러닝)",확률 예측에서 MSE Loss 미 사용 이유,9.058848e-05,0.0,9.058848081622273e-05
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지? (남은 답변: 인공지능, 머신러닝)",BCE Loss 설명,0.0053156586,0.0,0.005315658636391163
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지? (남은 답변: 인공지능, 머신러닝)",LLM Fine-Tuning 의 PEFT,0.004808576,0.0,0.004808575846254826
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지? (남은 답변: 인공지능, 머신러닝)",LoRA,-0.0025638219,0.0,0.0025638218503445387
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지? (남은 답변: 인공지능, 머신러닝)",LoRA 와 QLoRA 의 차이,-0.0034905165,0.0,0.0034905164502561092
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지? (남은 답변: 인공지능, 머신러닝)",Loss Function 관련 실무 경험,-0.0066797785,0.0,0.006679778452962637
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지? (남은 답변: 인공지능, 머신러닝)",Loss Function 예시,0.010608483,0.0,0.010608483105897903
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지? (남은 답변: 인공지능, 머신러닝)",Loss Function 정의,0.007192386,0.0,0.007192385848611593
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지? (남은 답변: 인공지능, 머신러닝)",MBTI / 좋아하는 아이돌,0.0020406141,0.0,0.002040614141151309
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지? (남은 답변: 인공지능, 머신러닝)",MSE Loss 설명,0.00060195714,0.0,0.0006019571446813643
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지? (남은 답변: 인공지능, 머신러닝)",MSE Loss 용도,-0.002466061,0.0,0.002466060919687152
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지? (남은 답변: 인공지능, 머신러닝)","Multi-Class, Multi-Label 중 BCE 가 좋은 task",0.0052862423,0.0,0.005286242347210646
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지? (남은 답변: 인공지능, 머신러닝)",Multi-Label 에서 CE + Softmax 적용 문제점,0.005980306,0.0,0.005980305839329958
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지? (남은 답변: 인공지능, 머신러닝)",PEFT 방법 5가지,-0.010716183,0.0,0.010716183111071587
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지? (남은 답변: 인공지능, 머신러닝)",거대 언어 모델 정의,-0.0013194887,0.0,0.0013194887433201075
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지? (남은 답변: 인공지능, 머신러닝)",마지막 할 말,-0.009556075,0.0,0.009556074626743793
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지? (남은 답변: 인공지능, 머신러닝)",면접 시작 인사,-0.042226605,0.0,0.042226605117321014
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지? (남은 답변: 인공지능, 머신러닝)",면접 종료,-0.007964909,0.0,0.007964909076690674
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지? (남은 답변: 인공지능, 머신러닝)","인공지능, 머신러닝, 딥러닝 차이",0.9824867,1.0,0.017513275146484375
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지? (남은 답변: 인공지능, 머신러닝)",잠시 휴식,-0.005896288,0.0,0.0058962879702448845
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지? (남은 답변: 인공지능, 머신러닝)",확률 예측에서 MSE Loss 미 사용 이유,-0.0028928795,0.0,0.0028928795363754034
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야 (남은 답변: 모든 질문 해결 완료)",BCE Loss 설명,-0.00042034144,0.0,0.0004203414428047836
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야 (남은 답변: 모든 질문 해결 완료)",LLM Fine-Tuning 의 PEFT,-0.0022051549,0.0,0.0022051548585295677
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야 (남은 답변: 모든 질문 해결 완료)",LoRA,0.0036358677,0.0,0.0036358677316457033
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야 (남은 답변: 모든 질문 해결 완료)",LoRA 와 QLoRA 의 차이,0.0038221367,0.0,0.0038221366703510284
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야 (남은 답변: 모든 질문 해결 완료)",Loss Function 관련 실무 경험,0.0047349017,0.0,0.004734901711344719
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야 (남은 답변: 모든 질문 해결 완료)",Loss Function 예시,0.0017191152,0.0,0.0017191151855513453
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야 (남은 답변: 모든 질문 해결 완료)",Loss Function 정의,-0.0255922,0.0,0.025592200458049774
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야 (남은 답변: 모든 질문 해결 완료)",MBTI / 좋아하는 아이돌,-0.025060028,0.0,0.025060027837753296
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야 (남은 답변: 모든 질문 해결 완료)",MSE Loss 설명,-0.008294995,0.0,0.008294994942843914
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야 (남은 답변: 모든 질문 해결 완료)",MSE Loss 용도,-0.012972602,0.0,0.012972601689398289
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야 (남은 답변: 모든 질문 해결 완료)","Multi-Class, Multi-Label 중 BCE 가 좋은 task",0.001970049,0.0,0.0019700489938259125
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야 (남은 답변: 모든 질문 해결 완료)",Multi-Label 에서 CE + Softmax 적용 문제점,-0.00063623075,0.0,0.0006362307467497885
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야 (남은 답변: 모든 질문 해결 완료)",PEFT 방법 5가지,-0.0051623364,0.0,0.005162336397916079
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야 (남은 답변: 모든 질문 해결 완료)",거대 언어 모델 정의,0.9603968,1.0,0.03960317373275757
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야 (남은 답변: 모든 질문 해결 완료)",마지막 할 말,-0.024623714,0.0,0.02462371438741684
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야 (남은 답변: 모든 질문 해결 완료)",면접 시작 인사,-0.01140858,0.0,0.011408579535782337
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야 (남은 답변: 모든 질문 해결 완료)",면접 종료,0.008589036,0.0,0.008589035831391811
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야 (남은 답변: 모든 질문 해결 완료)","인공지능, 머신러닝, 딥러닝 차이",-0.024120647,0.0,0.024120647460222244
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야 (남은 답변: 모든 질문 해결 완료)",잠시 휴식,-0.009419561,0.0,0.009419561363756657
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야 (남은 답변: 모든 질문 해결 완료)",확률 예측에서 MSE Loss 미 사용 이유,-0.013420508,0.0,0.013420508243143559
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야? (남은 답변: 머신러닝)",BCE Loss 설명,-0.0011697239,0.0,0.0011697239242494106
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야? (남은 답변: 머신러닝)",LLM Fine-Tuning 의 PEFT,-0.0016333803,0.0,0.0016333803068846464
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야? (남은 답변: 머신러닝)",LoRA,0.0023599074,0.0,0.0023599073756486177
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야? (남은 답변: 머신러닝)",LoRA 와 QLoRA 의 차이,-0.0073349304,0.0,0.00733493035659194
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야? (남은 답변: 머신러닝)",Loss Function 관련 실무 경험,-0.008340251,0.0,0.008340250700712204
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야? (남은 답변: 머신러닝)",Loss Function 예시,0.01662828,0.0,0.01662828028202057
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야? (남은 답변: 머신러닝)",Loss Function 정의,-0.00057681353,0.0,0.0005768135306425393
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야? (남은 답변: 머신러닝)",MBTI / 좋아하는 아이돌,-0.0029546923,0.0,0.0029546923469752073
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야? (남은 답변: 머신러닝)",MSE Loss 설명,0.002129163,0.0,0.002129162894561887
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야? (남은 답변: 머신러닝)",MSE Loss 용도,0.0023415661,0.0,0.002341566141694784
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야? (남은 답변: 머신러닝)","Multi-Class, Multi-Label 중 BCE 가 좋은 task",0.0055514234,0.0,0.0055514234118163586
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야? (남은 답변: 머신러닝)",Multi-Label 에서 CE + Softmax 적용 문제점,-0.00622077,0.0,0.006220770068466663
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야? (남은 답변: 머신러닝)",PEFT 방법 5가지,0.0022836833,0.0,0.002283683279529214
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야? (남은 답변: 머신러닝)",거대 언어 모델 정의,-0.019475978,0.0,0.01947597786784172
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야? (남은 답변: 머신러닝)",마지막 할 말,-0.0036366596,0.0,0.00363665958866477
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야? (남은 답변: 머신러닝)",면접 시작 인사,-0.042972464,0.0,0.042972464114427567
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야? (남은 답변: 머신러닝)",면접 종료,-0.006994429,0.0,0.0069944290444254875
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야? (남은 답변: 머신러닝)","인공지능, 머신러닝, 딥러닝 차이",0.9824606,1.0,0.017539381980895996
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야? (남은 답변: 머신러닝)",잠시 휴식,0.004166924,0.0,0.004166923929005861
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야? (남은 답변: 머신러닝)",확률 예측에서 MSE Loss 미 사용 이유,-0.0044595986,0.0,0.004459598567336798
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아 (남은 답변: 인공지능, 머신러닝)",BCE Loss 설명,0.010520641,0.0,0.010520640760660172
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아 (남은 답변: 인공지능, 머신러닝)",LLM Fine-Tuning 의 PEFT,-0.004442722,0.0,0.004442722070962191
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아 (남은 답변: 인공지능, 머신러닝)",LoRA,0.002892127,0.0,0.002892127027735114
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아 (남은 답변: 인공지능, 머신러닝)",LoRA 와 QLoRA 의 차이,0.00057748763,0.0,0.0005774876335635781
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아 (남은 답변: 인공지능, 머신러닝)",Loss Function 관련 실무 경험,-0.0030261502,0.0,0.003026150166988373
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아 (남은 답변: 인공지능, 머신러닝)",Loss Function 예시,0.015055351,0.0,0.015055350959300995
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아 (남은 답변: 인공지능, 머신러닝)",Loss Function 정의,-6.1673363e-06,0.0,6.167336323414929e-06
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아 (남은 답변: 인공지능, 머신러닝)",MBTI / 좋아하는 아이돌,0.011888077,0.0,0.011888076551258564
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아 (남은 답변: 인공지능, 머신러닝)",MSE Loss 설명,0.010008806,0.0,0.010008806362748146
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아 (남은 답변: 인공지능, 머신러닝)",MSE Loss 용도,-0.0071349717,0.0,0.007134971674531698
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아 (남은 답변: 인공지능, 머신러닝)","Multi-Class, Multi-Label 중 BCE 가 좋은 task",0.006825628,0.0,0.006825628224760294
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아 (남은 답변: 인공지능, 머신러닝)",Multi-Label 에서 CE + Softmax 적용 문제점,-0.011001338,0.0,0.011001338250935078
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아 (남은 답변: 인공지능, 머신러닝)",PEFT 방법 5가지,-0.00244284,0.0,0.0024428400211036205
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아 (남은 답변: 인공지능, 머신러닝)",거대 언어 모델 정의,-0.02679085,0.0,0.02679084986448288
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아 (남은 답변: 인공지능, 머신러닝)",마지막 할 말,-0.0013137831,0.0,0.0013137831119820476
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아 (남은 답변: 인공지능, 머신러닝)",면접 시작 인사,-0.06347854,0.0,0.06347853690385818
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아 (남은 답변: 인공지능, 머신러닝)",면접 종료,-0.0057946104,0.0,0.005794610362499952
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아 (남은 답변: 인공지능, 머신러닝)","인공지능, 머신러닝, 딥러닝 차이",0.97981507,1.0,0.02018493413925171
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아 (남은 답변: 인공지능, 머신러닝)",잠시 휴식,0.025479004,0.0,0.02547900378704071
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아 (남은 답변: 인공지능, 머신러닝)",확률 예측에서 MSE Loss 미 사용 이유,-0.0085879825,0.0,0.008587982505559921
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지? (남은 답변: 머신러닝)",BCE Loss 설명,0.0035829202,0.0,0.003582920180633664
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지? (남은 답변: 머신러닝)",LLM Fine-Tuning 의 PEFT,0.006685555,0.0,0.0066855549812316895
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지? (남은 답변: 머신러닝)",LoRA,-0.0044059283,0.0,0.004405928310006857
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지? (남은 답변: 머신러닝)",LoRA 와 QLoRA 의 차이,-0.001558381,0.0,0.0015583810163661838
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지? (남은 답변: 머신러닝)",Loss Function 관련 실무 경험,0.0045916704,0.0,0.004591670352965593
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지? (남은 답변: 머신러닝)",Loss Function 예시,0.012745343,0.0,0.012745343148708344
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지? (남은 답변: 머신러닝)",Loss Function 정의,-0.00035512258,0.0,0.00035512258182279766
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지? (남은 답변: 머신러닝)",MBTI / 좋아하는 아이돌,-0.00058296055,0.0,0.000582960550673306
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지? (남은 답변: 머신러닝)",MSE Loss 설명,0.007488926,0.0,0.007488925941288471
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지? (남은 답변: 머신러닝)",MSE Loss 용도,-0.0046064374,0.0,0.004606437403708696
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지? (남은 답변: 머신러닝)","Multi-Class, Multi-Label 중 BCE 가 좋은 task",0.006097103,0.0,0.006097103003412485
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지? (남은 답변: 머신러닝)",Multi-Label 에서 CE + Softmax 적용 문제점,-0.011468936,0.0,0.011468935757875443
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지? (남은 답변: 머신러닝)",PEFT 방법 5가지,-0.0053585884,0.0,0.005358588416129351
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지? (남은 답변: 머신러닝)",거대 언어 모델 정의,0.00040459124,0.0,0.00040459123556502163
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지? (남은 답변: 머신러닝)",마지막 할 말,-0.009721699,0.0,0.009721699170768261
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지? (남은 답변: 머신러닝)",면접 시작 인사,-0.046964146,0.0,0.046964146196842194
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지? (남은 답변: 머신러닝)",면접 종료,-0.014921898,0.0,0.014921898022294044
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지? (남은 답변: 머신러닝)","인공지능, 머신러닝, 딥러닝 차이",0.9758851,1.0,0.024114906787872314
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지? (남은 답변: 머신러닝)",잠시 휴식,0.010620544,0.0,0.010620543733239174
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지? (남은 답변: 머신러닝)",확률 예측에서 MSE Loss 미 사용 이유,-0.009244524,0.0,0.00924452394247055
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야! (남은 답변: 모든 질문 해결 완료)",BCE Loss 설명,-0.0059964093,0.0,0.005996409337967634
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야! (남은 답변: 모든 질문 해결 완료)",LLM Fine-Tuning 의 PEFT,0.0014311499,0.0,0.001431149896234274
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야! (남은 답변: 모든 질문 해결 완료)",LoRA,0.0064434656,0.0,0.00644346559420228
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야! (남은 답변: 모든 질문 해결 완료)",LoRA 와 QLoRA 의 차이,-0.008455213,0.0,0.008455213159322739
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야! (남은 답변: 모든 질문 해결 완료)",Loss Function 관련 실무 경험,-0.0026184218,0.0,0.0026184218004345894
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야! (남은 답변: 모든 질문 해결 완료)",Loss Function 예시,-0.00066417595,0.0,0.0006641759537160397
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야! (남은 답변: 모든 질문 해결 완료)",Loss Function 정의,-0.025966672,0.0,0.025966672226786613
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야! (남은 답변: 모든 질문 해결 완료)",MBTI / 좋아하는 아이돌,-0.0033032035,0.0,0.00330320349894464
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야! (남은 답변: 모든 질문 해결 완료)",MSE Loss 설명,-0.0096020745,0.0,0.009602074511349201
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야! (남은 답변: 모든 질문 해결 완료)",MSE Loss 용도,-0.009516413,0.0,0.009516413323581219
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야! (남은 답변: 모든 질문 해결 완료)","Multi-Class, Multi-Label 중 BCE 가 좋은 task",0.01086991,0.0,0.010869910009205341
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야! (남은 답변: 모든 질문 해결 완료)",Multi-Label 에서 CE + Softmax 적용 문제점,-0.0061236215,0.0,0.006123621482402086
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야! (남은 답변: 모든 질문 해결 완료)",PEFT 방법 5가지,-0.008131248,0.0,0.008131247945129871
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야! (남은 답변: 모든 질문 해결 완료)",거대 언어 모델 정의,0.9567785,1.0,0.043221473693847656
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야! (남은 답변: 모든 질문 해결 완료)",마지막 할 말,-0.007373445,0.0,0.007373445201665163
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야! (남은 답변: 모든 질문 해결 완료)",면접 시작 인사,-0.0048578684,0.0,0.004857868421822786
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야! (남은 답변: 모든 질문 해결 완료)",면접 종료,0.011651449,0.0,0.011651448905467987
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야! (남은 답변: 모든 질문 해결 완료)","인공지능, 머신러닝, 딥러닝 차이",-0.022709502,0.0,0.022709501907229424
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야! (남은 답변: 모든 질문 해결 완료)",잠시 휴식,-0.02793207,0.0,0.027932070195674896
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야! (남은 답변: 모든 질문 해결 완료)",확률 예측에서 MSE Loss 미 사용 이유,-0.013037977,0.0,0.013037976808845997
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지 (남은 답변: 모든 질문 해결 완료)",BCE Loss 설명,0.003614836,0.0,0.0036148359067738056
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지 (남은 답변: 모든 질문 해결 완료)",LLM Fine-Tuning 의 PEFT,-0.0025322835,0.0,0.00253228354267776
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지 (남은 답변: 모든 질문 해결 완료)",LoRA,-0.00029777805,0.0,0.0002977780532091856
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지 (남은 답변: 모든 질문 해결 완료)",LoRA 와 QLoRA 의 차이,0.008105003,0.0,0.008105003274977207
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지 (남은 답변: 모든 질문 해결 완료)",Loss Function 관련 실무 경험,0.00773948,0.0,0.007739480119198561
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지 (남은 답변: 모든 질문 해결 완료)",Loss Function 예시,0.0030772134,0.0,0.0030772134196013212
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지 (남은 답변: 모든 질문 해결 완료)",Loss Function 정의,-0.026280968,0.0,0.026280967518687248
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지 (남은 답변: 모든 질문 해결 완료)",MBTI / 좋아하는 아이돌,-0.030594716,0.0,0.0305947158485651
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지 (남은 답변: 모든 질문 해결 완료)",MSE Loss 설명,-0.00020539381,0.0,0.0002053938078461215
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지 (남은 답변: 모든 질문 해결 완료)",MSE Loss 용도,-0.0127036795,0.0,0.012703679502010345
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지 (남은 답변: 모든 질문 해결 완료)","Multi-Class, Multi-Label 중 BCE 가 좋은 task",0.0008719719,0.0,0.0008719718898646533
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지 (남은 답변: 모든 질문 해결 완료)",Multi-Label 에서 CE + Softmax 적용 문제점,-0.003879164,0.0,0.0038791641127318144
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지 (남은 답변: 모든 질문 해결 완료)",PEFT 방법 5가지,0.0020200582,0.0,0.00202005822211504
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지 (남은 답변: 모든 질문 해결 완료)",거대 언어 모델 정의,0.95959836,1.0,0.0404016375541687
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지 (남은 답변: 모든 질문 해결 완료)",마지막 할 말,-0.024030456,0.0,0.024030456319451332
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지 (남은 답변: 모든 질문 해결 완료)",면접 시작 인사,-0.012351982,0.0,0.012351982295513153
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지 (남은 답변: 모든 질문 해결 완료)",면접 종료,0.017983364,0.0,0.017983363941311836
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지 (남은 답변: 모든 질문 해결 완료)","인공지능, 머신러닝, 딥러닝 차이",-0.02474929,0.0,0.024749290198087692
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지 (남은 답변: 모든 질문 해결 완료)",잠시 휴식,-0.016225554,0.0,0.016225554049015045
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지 (남은 답변: 모든 질문 해결 완료)",확률 예측에서 MSE Loss 미 사용 이유,-0.019039856,0.0,0.019039856269955635
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야. (남은 답변: 모든 질문 해결 완료)",BCE Loss 설명,0.0073565408,0.0,0.0073565407656133175
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야. (남은 답변: 모든 질문 해결 완료)",LLM Fine-Tuning 의 PEFT,-0.00521288,0.0,0.005212880205363035
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야. (남은 답변: 모든 질문 해결 완료)",LoRA,0.0020591395,0.0,0.002059139544144273
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야. (남은 답변: 모든 질문 해결 완료)",LoRA 와 QLoRA 의 차이,0.006308655,0.0,0.00630865478888154
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야. (남은 답변: 모든 질문 해결 완료)",Loss Function 관련 실무 경험,0.005442988,0.0,0.0054429881274700165
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야. (남은 답변: 모든 질문 해결 완료)",Loss Function 예시,0.004021064,0.0,0.0040210639126598835
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야. (남은 답변: 모든 질문 해결 완료)",Loss Function 정의,-0.01779694,0.0,0.017796939238905907
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야. (남은 답변: 모든 질문 해결 완료)",MBTI / 좋아하는 아이돌,-0.024610547,0.0,0.024610547348856926
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야. (남은 답변: 모든 질문 해결 완료)",MSE Loss 설명,-0.0052787713,0.0,0.00527877127751708
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야. (남은 답변: 모든 질문 해결 완료)",MSE Loss 용도,-0.015041066,0.0,0.015041066333651543
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야. (남은 답변: 모든 질문 해결 완료)","Multi-Class, Multi-Label 중 BCE 가 좋은 task",-0.0009831055,0.0,0.000983105506747961
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야. (남은 답변: 모든 질문 해결 완료)",Multi-Label 에서 CE + Softmax 적용 문제점,-0.0035016488,0.0,0.0035016487818211317
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야. (남은 답변: 모든 질문 해결 완료)",PEFT 방법 5가지,6.530954e-05,0.0,6.53095412417315e-05
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야. (남은 답변: 모든 질문 해결 완료)",거대 언어 모델 정의,0.9584804,1.0,0.04151958227157593
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야. (남은 답변: 모든 질문 해결 완료)",마지막 할 말,-0.026817571,0.0,0.026817571371793747
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야. (남은 답변: 모든 질문 해결 완료)",면접 시작 인사,-0.012675274,0.0,0.012675274163484573
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야. (남은 답변: 모든 질문 해결 완료)",면접 종료,0.015971577,0.0,0.015971576794981956
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야. (남은 답변: 모든 질문 해결 완료)","인공지능, 머신러닝, 딥러닝 차이",-0.033037633,0.0,0.03303763270378113
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야. (남은 답변: 모든 질문 해결 완료)",잠시 휴식,-0.0128415385,0.0,0.012841538526117802
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야. (남은 답변: 모든 질문 해결 완료)",확률 예측에서 MSE Loss 미 사용 이유,-0.0256074,0.0,0.0256073996424675
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로 (남은 답변: 머신러닝, 딥러닝)",BCE Loss 설명,0.004052582,0.0,0.004052582196891308
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로 (남은 답변: 머신러닝, 딥러닝)",LLM Fine-Tuning 의 PEFT,-0.0021096077,0.0,0.002109607681632042
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로 (남은 답변: 머신러닝, 딥러닝)",LoRA,0.0018047618,0.0,0.0018047618214040995
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로 (남은 답변: 머신러닝, 딥러닝)",LoRA 와 QLoRA 의 차이,-0.0046468666,0.0,0.004646866582334042
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로 (남은 답변: 머신러닝, 딥러닝)",Loss Function 관련 실무 경험,-0.00022801541,0.0,0.00022801541490480304
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로 (남은 답변: 머신러닝, 딥러닝)",Loss Function 예시,0.018541966,0.0,0.018541965633630753
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로 (남은 답변: 머신러닝, 딥러닝)",Loss Function 정의,0.0033094294,0.0,0.0033094293903559446
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로 (남은 답변: 머신러닝, 딥러닝)",MBTI / 좋아하는 아이돌,0.00080687605,0.0,0.000806876050774008
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로 (남은 답변: 머신러닝, 딥러닝)",MSE Loss 설명,-0.002407207,0.0,0.0024072069209069014
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로 (남은 답변: 머신러닝, 딥러닝)",MSE Loss 용도,-0.004514457,0.0,0.004514457192271948
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로 (남은 답변: 머신러닝, 딥러닝)","Multi-Class, Multi-Label 중 BCE 가 좋은 task",0.009111717,0.0,0.009111717343330383
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로 (남은 답변: 머신러닝, 딥러닝)",Multi-Label 에서 CE + Softmax 적용 문제점,-0.00905069,0.0,0.00905068963766098
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로 (남은 답변: 머신러닝, 딥러닝)",PEFT 방법 5가지,0.0016941975,0.0,0.0016941975336521864
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로 (남은 답변: 머신러닝, 딥러닝)",거대 언어 모델 정의,-0.017785916,0.0,0.017785916104912758
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로 (남은 답변: 머신러닝, 딥러닝)",마지막 할 말,0.000996297,0.0,0.0009962969925254583
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로 (남은 답변: 머신러닝, 딥러닝)",면접 시작 인사,-0.050201464,0.0,0.05020146444439888
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로 (남은 답변: 머신러닝, 딥러닝)",면접 종료,-0.0034560345,0.0,0.003456034464761615
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로 (남은 답변: 머신러닝, 딥러닝)","인공지능, 머신러닝, 딥러닝 차이",0.9832917,1.0,0.016708314418792725
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로 (남은 답변: 머신러닝, 딥러닝)",잠시 휴식,0.00096578576,0.0,0.0009657857590354979
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로 (남은 답변: 머신러닝, 딥러닝)",확률 예측에서 MSE Loss 미 사용 이유,-0.009197711,0.0,0.009197711013257504
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야 (남은 답변: 모든 질문 해결 완료),BCE Loss 설명,0.009667772,0.0,0.009667771868407726
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야 (남은 답변: 모든 질문 해결 완료),LLM Fine-Tuning 의 PEFT,8.2415645e-05,0.0,8.241564501076937e-05
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야 (남은 답변: 모든 질문 해결 완료),LoRA,0.00039247298,0.0,0.00039247298263944685
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야 (남은 답변: 모든 질문 해결 완료),LoRA 와 QLoRA 의 차이,0.005676784,0.0,0.005676784086972475
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야 (남은 답변: 모든 질문 해결 완료),Loss Function 관련 실무 경험,-0.01307058,0.0,0.013070579618215561
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야 (남은 답변: 모든 질문 해결 완료),Loss Function 예시,-0.045277264,0.0,0.04527726396918297
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야 (남은 답변: 모든 질문 해결 완료),Loss Function 정의,0.9604602,1.0,0.03953981399536133
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야 (남은 답변: 모든 질문 해결 완료),MBTI / 좋아하는 아이돌,-0.01446677,0.0,0.014466769993305206
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야 (남은 답변: 모든 질문 해결 완료),MSE Loss 설명,-0.01350095,0.0,0.013500950299203396
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야 (남은 답변: 모든 질문 해결 완료),MSE Loss 용도,0.0037966862,0.0,0.0037966861855238676
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야 (남은 답변: 모든 질문 해결 완료),"Multi-Class, Multi-Label 중 BCE 가 좋은 task",0.005129744,0.0,0.005129743833094835
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야 (남은 답변: 모든 질문 해결 완료),Multi-Label 에서 CE + Softmax 적용 문제점,-0.0026999994,0.0,0.0026999993715435266
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야 (남은 답변: 모든 질문 해결 완료),PEFT 방법 5가지,-0.005532464,0.0,0.005532464012503624
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야 (남은 답변: 모든 질문 해결 완료),거대 언어 모델 정의,0.023888787,0.0,0.023888787254691124
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야 (남은 답변: 모든 질문 해결 완료),마지막 할 말,0.0064084074,0.0,0.006408407352864742
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야 (남은 답변: 모든 질문 해결 완료),면접 시작 인사,0.020458754,0.0,0.020458754152059555
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야 (남은 답변: 모든 질문 해결 완료),면접 종료,-0.014180382,0.0,0.014180381782352924
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야 (남은 답변: 모든 질문 해결 완료),"인공지능, 머신러닝, 딥러닝 차이",-0.002039061,0.0,0.002039060927927494
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야 (남은 답변: 모든 질문 해결 완료),잠시 휴식,-0.0438327,0.0,0.043832700699567795
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야 (남은 답변: 모든 질문 해결 완료),확률 예측에서 MSE Loss 미 사용 이유,-0.020009723,0.0,0.02000972256064415
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데 (남은 답변: 거대 언어 모델 정의),BCE Loss 설명,-0.031148693,0.0,0.031148692592978477
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데 (남은 답변: 거대 언어 모델 정의),LLM Fine-Tuning 의 PEFT,-0.009822222,0.0,0.009822222404181957
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데 (남은 답변: 거대 언어 모델 정의),LoRA,-0.012772512,0.0,0.012772511690855026
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데 (남은 답변: 거대 언어 모델 정의),LoRA 와 QLoRA 의 차이,-0.043137062,0.0,0.04313706234097481
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데 (남은 답변: 거대 언어 모델 정의),Loss Function 관련 실무 경험,-0.019428669,0.0,0.019428668543696404
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데 (남은 답변: 거대 언어 모델 정의),Loss Function 예시,-0.012709774,0.0,0.01270977407693863
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데 (남은 답변: 거대 언어 모델 정의),Loss Function 정의,0.30465165,0.0,0.3046516478061676
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데 (남은 답변: 거대 언어 모델 정의),MBTI / 좋아하는 아이돌,-0.011937568,0.0,0.011937567964196205
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데 (남은 답변: 거대 언어 모델 정의),MSE Loss 설명,-0.028858718,0.0,0.028858717530965805
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데 (남은 답변: 거대 언어 모델 정의),MSE Loss 용도,-0.017244983,0.0,0.017244983464479446
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데 (남은 답변: 거대 언어 모델 정의),"Multi-Class, Multi-Label 중 BCE 가 좋은 task",-0.020934124,0.0,0.020934123545885086
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데 (남은 답변: 거대 언어 모델 정의),Multi-Label 에서 CE + Softmax 적용 문제점,-0.0028325464,0.0,0.00283254636451602
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데 (남은 답변: 거대 언어 모델 정의),PEFT 방법 5가지,0.009572595,0.0,0.009572595357894897
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데 (남은 답변: 거대 언어 모델 정의),거대 언어 모델 정의,0.88826066,1.0,0.11173933744430542
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데 (남은 답변: 거대 언어 모델 정의),마지막 할 말,-0.030548152,0.0,0.030548151582479477
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데 (남은 답변: 거대 언어 모델 정의),면접 시작 인사,0.003211391,0.0,0.0032113909255713224
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데 (남은 답변: 거대 언어 모델 정의),면접 종료,-0.004154033,0.0,0.004154033027589321
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데 (남은 답변: 거대 언어 모델 정의),"인공지능, 머신러닝, 딥러닝 차이",-0.026561633,0.0,0.026561632752418518
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데 (남은 답변: 거대 언어 모델 정의),잠시 휴식,-0.050429344,0.0,0.050429344177246094
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데 (남은 답변: 거대 언어 모델 정의),확률 예측에서 MSE Loss 미 사용 이유,-0.029853504,0.0,0.02985350415110588
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델 (남은 답변: 모든 질문 해결 완료),BCE Loss 설명,0.008865862,0.0,0.008865862153470516
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델 (남은 답변: 모든 질문 해결 완료),LLM Fine-Tuning 의 PEFT,0.014997219,0.0,0.014997218735516071
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델 (남은 답변: 모든 질문 해결 완료),LoRA,-0.01723793,0.0,0.01723792962729931
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델 (남은 답변: 모든 질문 해결 완료),LoRA 와 QLoRA 의 차이,0.0045639197,0.0,0.004563919734209776
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델 (남은 답변: 모든 질문 해결 완료),Loss Function 관련 실무 경험,-0.003731574,0.0,0.0037315739318728447
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델 (남은 답변: 모든 질문 해결 완료),Loss Function 예시,-0.053999204,0.0,0.05399920418858528
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델 (남은 답변: 모든 질문 해결 완료),Loss Function 정의,0.95761985,1.0,0.04238015413284302
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델 (남은 답변: 모든 질문 해결 완료),MBTI / 좋아하는 아이돌,0.01752094,0.0,0.01752093993127346
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델 (남은 답변: 모든 질문 해결 완료),MSE Loss 설명,-0.010073829,0.0,0.01007382944226265
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델 (남은 답변: 모든 질문 해결 완료),MSE Loss 용도,-0.012067512,0.0,0.012067511677742004
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델 (남은 답변: 모든 질문 해결 완료),"Multi-Class, Multi-Label 중 BCE 가 좋은 task",0.012598245,0.0,0.012598245404660702
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델 (남은 답변: 모든 질문 해결 완료),Multi-Label 에서 CE + Softmax 적용 문제점,0.002875151,0.0,0.002875151112675667
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델 (남은 답변: 모든 질문 해결 완료),PEFT 방법 5가지,0.014570903,0.0,0.014570903033018112
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델 (남은 답변: 모든 질문 해결 완료),거대 언어 모델 정의,-0.023928901,0.0,0.023928901180624962
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델 (남은 답변: 모든 질문 해결 완료),마지막 할 말,0.026856296,0.0,0.02685629576444626
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델 (남은 답변: 모든 질문 해결 완료),면접 시작 인사,0.021695338,0.0,0.021695338189601898
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델 (남은 답변: 모든 질문 해결 완료),면접 종료,-0.003723638,0.0,0.0037236378993839025
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델 (남은 답변: 모든 질문 해결 완료),"인공지능, 머신러닝, 딥러닝 차이",0.005522819,0.0,0.005522818770259619
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델 (남은 답변: 모든 질문 해결 완료),잠시 휴식,-0.034142625,0.0,0.0341426245868206
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델 (남은 답변: 모든 질문 해결 완료),확률 예측에서 MSE Loss 미 사용 이유,-0.03212932,0.0,0.03212932124733925
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지. (남은 답변: 거대 언어 모델 정의),BCE Loss 설명,-0.013480013,0.0,0.013480013236403465
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지. (남은 답변: 거대 언어 모델 정의),LLM Fine-Tuning 의 PEFT,0.003911891,0.0,0.003911891020834446
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지. (남은 답변: 거대 언어 모델 정의),LoRA,0.007175386,0.0,0.00717538595199585
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지. (남은 답변: 거대 언어 모델 정의),LoRA 와 QLoRA 의 차이,-0.00016946392,0.0,0.00016946392133831978
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지. (남은 답변: 거대 언어 모델 정의),Loss Function 관련 실무 경험,0.009524967,0.0,0.009524966590106487
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지. (남은 답변: 거대 언어 모델 정의),Loss Function 예시,-0.0027088206,0.0,0.002708820626139641
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지. (남은 답변: 거대 언어 모델 정의),Loss Function 정의,-0.04958916,0.0,0.049589160829782486
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지. (남은 답변: 거대 언어 모델 정의),MBTI / 좋아하는 아이돌,0.00019177946,0.0,0.00019177945796400309
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지. (남은 답변: 거대 언어 모델 정의),MSE Loss 설명,0.005620508,0.0,0.00562050798907876
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지. (남은 답변: 거대 언어 모델 정의),MSE Loss 용도,0.0034312988,0.0,0.0034312987700104713
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지. (남은 답변: 거대 언어 모델 정의),"Multi-Class, Multi-Label 중 BCE 가 좋은 task",0.012657112,0.0,0.012657112441956997
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지. (남은 답변: 거대 언어 모델 정의),Multi-Label 에서 CE + Softmax 적용 문제점,0.0035904634,0.0,0.003590463427826762
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지. (남은 답변: 거대 언어 모델 정의),PEFT 방법 5가지,0.0038277109,0.0,0.0038277108687907457
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지. (남은 답변: 거대 언어 모델 정의),거대 언어 모델 정의,0.9787125,1.0,0.021287500858306885
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지. (남은 답변: 거대 언어 모델 정의),마지막 할 말,-0.02270135,0.0,0.02270134910941124
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지. (남은 답변: 거대 언어 모델 정의),면접 시작 인사,-0.0012940374,0.0,0.0012940374435856938
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지. (남은 답변: 거대 언어 모델 정의),면접 종료,0.0063708574,0.0,0.006370857357978821
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지. (남은 답변: 거대 언어 모델 정의),"인공지능, 머신러닝, 딥러닝 차이",-0.013371466,0.0,0.013371465727686882
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지. (남은 답변: 거대 언어 모델 정의),잠시 휴식,-0.01398666,0.0,0.013986660167574883
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지. (남은 답변: 거대 언어 모델 정의),확률 예측에서 MSE Loss 미 사용 이유,-0.011369014,0.0,0.011369014158844948
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야 (남은 답변: Loss Function 정의),BCE Loss 설명,-0.008785819,0.0,0.008785818703472614
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야 (남은 답변: Loss Function 정의),LLM Fine-Tuning 의 PEFT,0.00304644,0.0,0.0030464399605989456
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야 (남은 답변: Loss Function 정의),LoRA,0.0032049785,0.0,0.0032049785368144512
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야 (남은 답변: Loss Function 정의),LoRA 와 QLoRA 의 차이,0.015746469,0.0,0.0157464686781168
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야 (남은 답변: Loss Function 정의),Loss Function 관련 실무 경험,-0.009484106,0.0,0.009484105743467808
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야 (남은 답변: Loss Function 정의),Loss Function 예시,0.010711869,0.0,0.010711869224905968
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야 (남은 답변: Loss Function 정의),Loss Function 정의,0.9746487,1.0,0.025351285934448242
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야 (남은 답변: Loss Function 정의),MBTI / 좋아하는 아이돌,0.0010947236,0.0,0.0010947235859930515
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야 (남은 답변: Loss Function 정의),MSE Loss 설명,-0.022750683,0.0,0.02275068312883377
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야 (남은 답변: Loss Function 정의),MSE Loss 용도,-0.0036772664,0.0,0.0036772664170712233
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야 (남은 답변: Loss Function 정의),"Multi-Class, Multi-Label 중 BCE 가 좋은 task",-0.0076419986,0.0,0.007641998585313559
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야 (남은 답변: Loss Function 정의),Multi-Label 에서 CE + Softmax 적용 문제점,0.006225471,0.0,0.006225470919162035
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야 (남은 답변: Loss Function 정의),PEFT 방법 5가지,-0.008258448,0.0,0.008258447982370853
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야 (남은 답변: Loss Function 정의),거대 언어 모델 정의,-0.033174835,0.0,0.03317483514547348
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야 (남은 답변: Loss Function 정의),마지막 할 말,-0.0099015925,0.0,0.009901592507958412
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야 (남은 답변: Loss Function 정의),면접 시작 인사,0.0093223145,0.0,0.009322314523160458
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야 (남은 답변: Loss Function 정의),면접 종료,0.010981409,0.0,0.010981408879160881
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야 (남은 답변: Loss Function 정의),"인공지능, 머신러닝, 딥러닝 차이",0.017558312,0.0,0.01755831204354763
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야 (남은 답변: Loss Function 정의),잠시 휴식,-0.013380569,0.0,0.013380569405853748
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야 (남은 답변: Loss Function 정의),확률 예측에서 MSE Loss 미 사용 이유,0.013360009,0.0,0.013360008597373962
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지! (남은 답변: 모든 질문 해결 완료)",BCE Loss 설명,-0.011819795,0.0,0.011819794774055481
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지! (남은 답변: 모든 질문 해결 완료)",LLM Fine-Tuning 의 PEFT,-0.017402867,0.0,0.01740286685526371
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지! (남은 답변: 모든 질문 해결 완료)",LoRA,-0.024305383,0.0,0.02430538274347782
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지! (남은 답변: 모든 질문 해결 완료)",LoRA 와 QLoRA 의 차이,0.03326286,0.0,0.03326286002993584
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지! (남은 답변: 모든 질문 해결 완료)",Loss Function 관련 실무 경험,-0.011329867,0.0,0.01132986694574356
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지! (남은 답변: 모든 질문 해결 완료)",Loss Function 예시,0.9345039,1.0,0.06549608707427979
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지! (남은 답변: 모든 질문 해결 완료)",Loss Function 정의,-0.04569809,0.0,0.04569809138774872
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지! (남은 답변: 모든 질문 해결 완료)",MBTI / 좋아하는 아이돌,-0.02541069,0.0,0.025410689413547516
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지! (남은 답변: 모든 질문 해결 완료)",MSE Loss 설명,-0.0048409807,0.0,0.004840980749577284
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지! (남은 답변: 모든 질문 해결 완료)",MSE Loss 용도,0.027688485,0.0,0.027688484638929367
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지! (남은 답변: 모든 질문 해결 완료)","Multi-Class, Multi-Label 중 BCE 가 좋은 task",0.0043971366,0.0,0.004397136624902487
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지! (남은 답변: 모든 질문 해결 완료)",Multi-Label 에서 CE + Softmax 적용 문제점,-0.017221602,0.0,0.01722160167992115
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지! (남은 답변: 모든 질문 해결 완료)",PEFT 방법 5가지,0.03042102,0.0,0.030421020463109016
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지! (남은 답변: 모든 질문 해결 완료)",거대 언어 모델 정의,-0.025312623,0.0,0.025312623009085655
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지! (남은 답변: 모든 질문 해결 완료)",마지막 할 말,-0.016891114,0.0,0.01689111441373825
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지! (남은 답변: 모든 질문 해결 완료)",면접 시작 인사,-0.016388793,0.0,0.016388792544603348
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지! (남은 답변: 모든 질문 해결 완료)",면접 종료,0.014273274,0.0,0.014273273758590221
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지! (남은 답변: 모든 질문 해결 완료)","인공지능, 머신러닝, 딥러닝 차이",-0.003920618,0.0,0.00392061797901988
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지! (남은 답변: 모든 질문 해결 완료)",잠시 휴식,-0.0017177504,0.0,0.0017177504487335682
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지! (남은 답변: 모든 질문 해결 완료)",확률 예측에서 MSE Loss 미 사용 이유,-0.039220177,0.0,0.0392201766371727
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야 (남은 답변: Loss Function 정의),BCE Loss 설명,-0.019412657,0.0,0.019412657245993614
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야 (남은 답변: Loss Function 정의),LLM Fine-Tuning 의 PEFT,0.0033617488,0.0,0.0033617487642914057
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야 (남은 답변: Loss Function 정의),LoRA,0.004062413,0.0,0.0040624127723276615
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야 (남은 답변: Loss Function 정의),LoRA 와 QLoRA 의 차이,0.012050187,0.0,0.012050187215209007
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야 (남은 답변: Loss Function 정의),Loss Function 관련 실무 경험,-0.0062807277,0.0,0.006280727684497833
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야 (남은 답변: Loss Function 정의),Loss Function 예시,0.008027994,0.0,0.008027994073927402
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야 (남은 답변: Loss Function 정의),Loss Function 정의,0.97414094,1.0,0.025859057903289795
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야 (남은 답변: Loss Function 정의),MBTI / 좋아하는 아이돌,-0.005858347,0.0,0.0058583468198776245
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야 (남은 답변: Loss Function 정의),MSE Loss 설명,-0.018747525,0.0,0.018747525289654732
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야 (남은 답변: Loss Function 정의),MSE Loss 용도,-0.0049083335,0.0,0.004908333532512188
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야 (남은 답변: Loss Function 정의),"Multi-Class, Multi-Label 중 BCE 가 좋은 task",-0.003708621,0.0,0.003708621021360159
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야 (남은 답변: Loss Function 정의),Multi-Label 에서 CE + Softmax 적용 문제점,0.009094936,0.0,0.009094935841858387
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야 (남은 답변: Loss Function 정의),PEFT 방법 5가지,-0.014564322,0.0,0.01456432230770588
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야 (남은 답변: Loss Function 정의),거대 언어 모델 정의,-0.037366673,0.0,0.03736667335033417
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야 (남은 답변: Loss Function 정의),마지막 할 말,-0.010748824,0.0,0.01074882410466671
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야 (남은 답변: Loss Function 정의),면접 시작 인사,0.014162515,0.0,0.014162515290081501
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야 (남은 답변: Loss Function 정의),면접 종료,0.006062325,0.0,0.006062325090169907
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야 (남은 답변: Loss Function 정의),"인공지능, 머신러닝, 딥러닝 차이",0.015642693,0.0,0.015642693266272545
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야 (남은 답변: Loss Function 정의),잠시 휴식,-0.017119564,0.0,0.01711956411600113
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야 (남은 답변: Loss Function 정의),확률 예측에서 MSE Loss 미 사용 이유,0.01890271,0.0,0.01890270970761776
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지 (남은 답변: 모든 질문 해결 완료),BCE Loss 설명,0.006028112,0.0,0.006028112024068832
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지 (남은 답변: 모든 질문 해결 완료),LLM Fine-Tuning 의 PEFT,-0.0048853885,0.0,0.004885388538241386
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지 (남은 답변: 모든 질문 해결 완료),LoRA,-0.0058002877,0.0,0.005800287704914808
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지 (남은 답변: 모든 질문 해결 완료),LoRA 와 QLoRA 의 차이,0.021114357,0.0,0.021114356815814972
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지 (남은 답변: 모든 질문 해결 완료),Loss Function 관련 실무 경험,-0.030791745,0.0,0.030791744589805603
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지 (남은 답변: 모든 질문 해결 완료),Loss Function 예시,0.9392034,1.0,0.06079661846160889
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지 (남은 답변: 모든 질문 해결 완료),Loss Function 정의,-0.019968854,0.0,0.019968854263424873
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지 (남은 답변: 모든 질문 해결 완료),MBTI / 좋아하는 아이돌,-0.08079137,0.0,0.08079136908054352
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지 (남은 답변: 모든 질문 해결 완료),MSE Loss 설명,-0.0035126205,0.0,0.003512620460242033
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지 (남은 답변: 모든 질문 해결 완료),MSE Loss 용도,0.004995192,0.0,0.004995191935449839
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지 (남은 답변: 모든 질문 해결 완료),"Multi-Class, Multi-Label 중 BCE 가 좋은 task",0.018721009,0.0,0.01872100867331028
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지 (남은 답변: 모든 질문 해결 완료),Multi-Label 에서 CE + Softmax 적용 문제점,0.013745666,0.0,0.013745665550231934
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지 (남은 답변: 모든 질문 해결 완료),PEFT 방법 5가지,0.0026100953,0.0,0.0026100953109562397
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지 (남은 답변: 모든 질문 해결 완료),거대 언어 모델 정의,-0.03256876,0.0,0.032568760216236115
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지 (남은 답변: 모든 질문 해결 완료),마지막 할 말,-0.034370895,0.0,0.034370895475149155
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지 (남은 답변: 모든 질문 해결 완료),면접 시작 인사,-0.027696105,0.0,0.02769610472023487
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지 (남은 답변: 모든 질문 해결 완료),면접 종료,0.009743295,0.0,0.009743294678628445
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지 (남은 답변: 모든 질문 해결 완료),"인공지능, 머신러닝, 딥러닝 차이",-0.001180603,0.0,0.0011806030524894595
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지 (남은 답변: 모든 질문 해결 완료),잠시 휴식,0.009539496,0.0,0.009539496153593063
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지 (남은 답변: 모든 질문 해결 완료),확률 예측에서 MSE Loss 미 사용 이유,-0.0040444164,0.0,0.004044416360557079
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데 (남은 답변: Loss Function 예시),BCE Loss 설명,-0.009177099,0.0,0.009177098982036114
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데 (남은 답변: Loss Function 예시),LLM Fine-Tuning 의 PEFT,-0.0094736,0.0,0.009473600424826145
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데 (남은 답변: Loss Function 예시),LoRA,-0.019834347,0.0,0.019834347069263458
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데 (남은 답변: Loss Function 예시),LoRA 와 QLoRA 의 차이,0.0030081396,0.0,0.0030081395525485277
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데 (남은 답변: Loss Function 예시),Loss Function 관련 실무 경험,-0.019121533,0.0,0.019121533259749413
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데 (남은 답변: Loss Function 예시),Loss Function 예시,0.94869643,1.0,0.05130356550216675
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데 (남은 답변: Loss Function 예시),Loss Function 정의,-0.039628148,0.0,0.03962814807891846
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데 (남은 답변: Loss Function 예시),MBTI / 좋아하는 아이돌,-0.011813857,0.0,0.011813856661319733
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데 (남은 답변: Loss Function 예시),MSE Loss 설명,-0.027880844,0.0,0.027880843728780746
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데 (남은 답변: Loss Function 예시),MSE Loss 용도,-0.004055398,0.0,0.004055398050695658
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데 (남은 답변: Loss Function 예시),"Multi-Class, Multi-Label 중 BCE 가 좋은 task",-0.0066810264,0.0,0.006681026425212622
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데 (남은 답변: Loss Function 예시),Multi-Label 에서 CE + Softmax 적용 문제점,0.0073377565,0.0,0.007337756454944611
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데 (남은 답변: Loss Function 예시),PEFT 방법 5가지,-0.02185415,0.0,0.02185414917767048
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데 (남은 답변: Loss Function 예시),거대 언어 모델 정의,-0.014197292,0.0,0.014197291806340218
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데 (남은 답변: Loss Function 예시),마지막 할 말,-0.008180064,0.0,0.008180064149200916
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데 (남은 답변: Loss Function 예시),면접 시작 인사,0.0029318416,0.0,0.0029318416491150856
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데 (남은 답변: Loss Function 예시),면접 종료,0.008710634,0.0,0.008710633963346481
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데 (남은 답변: Loss Function 예시),"인공지능, 머신러닝, 딥러닝 차이",0.0023019689,0.0,0.002301968866959214
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데 (남은 답변: Loss Function 예시),잠시 휴식,-0.006187726,0.0,0.006187725812196732
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데 (남은 답변: Loss Function 예시),확률 예측에서 MSE Loss 미 사용 이유,-0.015212982,0.0,0.015212981961667538
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지 (남은 답변: 모든 질문 해결 완료)",BCE Loss 설명,-0.034299225,0.0,0.034299224615097046
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지 (남은 답변: 모든 질문 해결 완료)",LLM Fine-Tuning 의 PEFT,0.0049079126,0.0,0.004907912574708462
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지 (남은 답변: 모든 질문 해결 완료)",LoRA,0.017251048,0.0,0.017251048237085342
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지 (남은 답변: 모든 질문 해결 완료)",LoRA 와 QLoRA 의 차이,0.007262085,0.0,0.007262085098773241
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지 (남은 답변: 모든 질문 해결 완료)",Loss Function 관련 실무 경험,-0.00044825123,0.0,0.00044825123040936887
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지 (남은 답변: 모든 질문 해결 완료)",Loss Function 예시,-0.039303873,0.0,0.03930387273430824
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지 (남은 답변: 모든 질문 해결 완료)",Loss Function 정의,-0.0009593525,0.0,0.0009593524737283587
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지 (남은 답변: 모든 질문 해결 완료)",MBTI / 좋아하는 아이돌,0.020603536,0.0,0.020603535696864128
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지 (남은 답변: 모든 질문 해결 완료)",MSE Loss 설명,0.9451754,1.0,0.0548245906829834
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지 (남은 답변: 모든 질문 해결 완료)",MSE Loss 용도,-0.027045285,0.0,0.02704528532922268
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지 (남은 답변: 모든 질문 해결 완료)","Multi-Class, Multi-Label 중 BCE 가 좋은 task",-0.015276734,0.0,0.015276733785867691
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지 (남은 답변: 모든 질문 해결 완료)",Multi-Label 에서 CE + Softmax 적용 문제점,0.0013709845,0.0,0.001370984478853643
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지 (남은 답변: 모든 질문 해결 완료)",PEFT 방법 5가지,0.003650233,0.0,0.0036502329166978598
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지 (남은 답변: 모든 질문 해결 완료)",거대 언어 모델 정의,-0.01581924,0.0,0.015819240361452103
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지 (남은 답변: 모든 질문 해결 완료)",마지막 할 말,0.022458112,0.0,0.022458111867308617
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지 (남은 답변: 모든 질문 해결 완료)",면접 시작 인사,-0.019269746,0.0,0.01926974579691887
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지 (남은 답변: 모든 질문 해결 완료)",면접 종료,-0.022224393,0.0,0.022224392741918564
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지 (남은 답변: 모든 질문 해결 완료)","인공지능, 머신러닝, 딥러닝 차이",0.0037526935,0.0,0.0037526935338974
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지 (남은 답변: 모든 질문 해결 완료)",잠시 휴식,-0.026763372,0.0,0.026763372123241425
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지 (남은 답변: 모든 질문 해결 완료)",확률 예측에서 MSE Loss 미 사용 이유,0.0061568483,0.0,0.006156848277896643
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어 (남은 답변: 모든 질문 해결 완료)",BCE Loss 설명,0.91500205,1.0,0.08499795198440552
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어 (남은 답변: 모든 질문 해결 완료)",LLM Fine-Tuning 의 PEFT,-0.007521478,0.0,0.007521477993577719
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어 (남은 답변: 모든 질문 해결 완료)",LoRA,0.018347804,0.0,0.018347803503274918
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어 (남은 답변: 모든 질문 해결 완료)",LoRA 와 QLoRA 의 차이,0.002590156,0.0,0.002590155927464366
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어 (남은 답변: 모든 질문 해결 완료)",Loss Function 관련 실무 경험,-0.008977961,0.0,0.008977960795164108
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어 (남은 답변: 모든 질문 해결 완료)",Loss Function 예시,-0.079401806,0.0,0.07940180599689484
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어 (남은 답변: 모든 질문 해결 완료)",Loss Function 정의,-0.010633222,0.0,0.010633221827447414
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어 (남은 답변: 모든 질문 해결 완료)",MBTI / 좋아하는 아이돌,0.018261887,0.0,0.01826188713312149
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어 (남은 답변: 모든 질문 해결 완료)",MSE Loss 설명,0.04687297,0.0,0.04687296971678734
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어 (남은 답변: 모든 질문 해결 완료)",MSE Loss 용도,-0.023949716,0.0,0.023949716240167618
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어 (남은 답변: 모든 질문 해결 완료)","Multi-Class, Multi-Label 중 BCE 가 좋은 task",-0.055118192,0.0,0.05511819198727608
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어 (남은 답변: 모든 질문 해결 완료)",Multi-Label 에서 CE + Softmax 적용 문제점,-0.01683784,0.0,0.016837840899825096
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어 (남은 답변: 모든 질문 해결 완료)",PEFT 방법 5가지,0.013875509,0.0,0.013875508680939674
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어 (남은 답변: 모든 질문 해결 완료)",거대 언어 모델 정의,0.0150213875,0.0,0.015021387487649918
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어 (남은 답변: 모든 질문 해결 완료)",마지막 할 말,-0.025371185,0.0,0.025371184572577477
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어 (남은 답변: 모든 질문 해결 완료)",면접 시작 인사,-0.014771354,0.0,0.014771354384720325
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어 (남은 답변: 모든 질문 해결 완료)",면접 종료,0.00399505,0.0,0.003995050210505724
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어 (남은 답변: 모든 질문 해결 완료)","인공지능, 머신러닝, 딥러닝 차이",0.017113857,0.0,0.017113856971263885
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어 (남은 답변: 모든 질문 해결 완료)",잠시 휴식,-0.035480633,0.0,0.03548063337802887
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어 (남은 답변: 모든 질문 해결 완료)",확률 예측에서 MSE Loss 미 사용 이유,-0.07511775,0.0,0.07511775195598602
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야? (남은 답변: Loss Function 예시)",BCE Loss 설명,-0.028715841,0.0,0.028715841472148895
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야? (남은 답변: Loss Function 예시)",LLM Fine-Tuning 의 PEFT,-0.020390652,0.0,0.020390652120113373
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야? (남은 답변: Loss Function 예시)",LoRA,0.00905865,0.0,0.009058649651706219
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야? (남은 답변: Loss Function 예시)",LoRA 와 QLoRA 의 차이,0.0029604016,0.0,0.002960401587188244
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야? (남은 답변: Loss Function 예시)",Loss Function 관련 실무 경험,-0.034317996,0.0,0.034317996352910995
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야? (남은 답변: Loss Function 예시)",Loss Function 예시,0.92822623,1.0,0.07177376747131348
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야? (남은 답변: Loss Function 예시)",Loss Function 정의,-0.0726643,0.0,0.0726642981171608
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야? (남은 답변: Loss Function 예시)",MBTI / 좋아하는 아이돌,-0.014410614,0.0,0.014410614036023617
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야? (남은 답변: Loss Function 예시)",MSE Loss 설명,0.029328398,0.0,0.029328398406505585
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야? (남은 답변: Loss Function 예시)",MSE Loss 용도,-0.0058466033,0.0,0.005846603307873011
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야? (남은 답변: Loss Function 예시)","Multi-Class, Multi-Label 중 BCE 가 좋은 task",-0.00031289528,0.0,0.00031289528124034405
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야? (남은 답변: Loss Function 예시)",Multi-Label 에서 CE + Softmax 적용 문제점,0.019205494,0.0,0.019205493852496147
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야? (남은 답변: Loss Function 예시)",PEFT 방법 5가지,-0.012708625,0.0,0.012708624824881554
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야? (남은 답변: Loss Function 예시)",거대 언어 모델 정의,0.0030667852,0.0,0.003066785167902708
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야? (남은 답변: Loss Function 예시)",마지막 할 말,-0.023681922,0.0,0.023681921884417534
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야? (남은 답변: Loss Function 예시)",면접 시작 인사,0.03323955,0.0,0.03323955088853836
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야? (남은 답변: Loss Function 예시)",면접 종료,-0.008143617,0.0,0.00814361684024334
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야? (남은 답변: Loss Function 예시)","인공지능, 머신러닝, 딥러닝 차이",-0.014088247,0.0,0.01408824697136879
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야? (남은 답변: Loss Function 예시)",잠시 휴식,-0.023103217,0.0,0.023103216663002968
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야? (남은 답변: Loss Function 예시)",확률 예측에서 MSE Loss 미 사용 이유,-0.009747545,0.0,0.00974754523485899
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어 (남은 답변: 모든 질문 해결 완료)",BCE Loss 설명,0.0007567228,0.0,0.0007567228167317808
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어 (남은 답변: 모든 질문 해결 완료)",LLM Fine-Tuning 의 PEFT,0.017337719,0.0,0.017337718978524208
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어 (남은 답변: 모든 질문 해결 완료)",LoRA,-0.0010321577,0.0,0.0010321576846763492
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어 (남은 답변: 모든 질문 해결 완료)",LoRA 와 QLoRA 의 차이,0.0231598,0.0,0.023159800097346306
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어 (남은 답변: 모든 질문 해결 완료)",Loss Function 관련 실무 경험,0.021036383,0.0,0.021036382764577866
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어 (남은 답변: 모든 질문 해결 완료)",Loss Function 예시,-0.031466167,0.0,0.03146616742014885
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어 (남은 답변: 모든 질문 해결 완료)",Loss Function 정의,-0.019395193,0.0,0.019395193085074425
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어 (남은 답변: 모든 질문 해결 완료)",MBTI / 좋아하는 아이돌,0.029673086,0.0,0.02967308647930622
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어 (남은 답변: 모든 질문 해결 완료)",MSE Loss 설명,0.9200795,1.0,0.07992047071456909
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어 (남은 답변: 모든 질문 해결 완료)",MSE Loss 용도,-0.045417823,0.0,0.045417822897434235
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어 (남은 답변: 모든 질문 해결 완료)","Multi-Class, Multi-Label 중 BCE 가 좋은 task",-0.01181476,0.0,0.01181476004421711
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어 (남은 답변: 모든 질문 해결 완료)",Multi-Label 에서 CE + Softmax 적용 문제점,-0.024912773,0.0,0.024912772700190544
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어 (남은 답변: 모든 질문 해결 완료)",PEFT 방법 5가지,0.0022471035,0.0,0.002247103489935398
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어 (남은 답변: 모든 질문 해결 완료)",거대 언어 모델 정의,0.0067946254,0.0,0.006794625427573919
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어 (남은 답변: 모든 질문 해결 완료)",마지막 할 말,-0.00013711592,0.0,0.00013711591600440443
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어 (남은 답변: 모든 질문 해결 완료)",면접 시작 인사,-0.009415023,0.0,0.009415023028850555
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어 (남은 답변: 모든 질문 해결 완료)",면접 종료,-0.031717382,0.0,0.03171738237142563
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어 (남은 답변: 모든 질문 해결 완료)","인공지능, 머신러닝, 딥러닝 차이",0.012640948,0.0,0.01264094840735197
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어 (남은 답변: 모든 질문 해결 완료)",잠시 휴식,-0.09477535,0.0,0.09477534890174866
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어 (남은 답변: 모든 질문 해결 완료)",확률 예측에서 MSE Loss 미 사용 이유,-0.0360894,0.0,0.03608940169215202
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아 (남은 답변: 모든 질문 해결 완료)",BCE Loss 설명,0.9260642,1.0,0.07393580675125122
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아 (남은 답변: 모든 질문 해결 완료)",LLM Fine-Tuning 의 PEFT,0.0014740935,0.0,0.001474093529395759
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아 (남은 답변: 모든 질문 해결 완료)",LoRA,0.0028974812,0.0,0.0028974812012165785
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아 (남은 답변: 모든 질문 해결 완료)",LoRA 와 QLoRA 의 차이,0.0075937114,0.0,0.007593711372464895
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아 (남은 답변: 모든 질문 해결 완료)",Loss Function 관련 실무 경험,-0.005105849,0.0,0.005105848889797926
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아 (남은 답변: 모든 질문 해결 완료)",Loss Function 예시,-0.0767272,0.0,0.07672719657421112
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아 (남은 답변: 모든 질문 해결 완료)",Loss Function 정의,-0.002134351,0.0,0.002134351059794426
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아 (남은 답변: 모든 질문 해결 완료)",MBTI / 좋아하는 아이돌,0.012561609,0.0,0.012561609037220478
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아 (남은 답변: 모든 질문 해결 완료)",MSE Loss 설명,0.059634067,0.0,0.05963406711816788
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아 (남은 답변: 모든 질문 해결 완료)",MSE Loss 용도,-0.03663074,0.0,0.03663073852658272
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아 (남은 답변: 모든 질문 해결 완료)","Multi-Class, Multi-Label 중 BCE 가 좋은 task",-0.037309367,0.0,0.03730936720967293
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아 (남은 답변: 모든 질문 해결 완료)",Multi-Label 에서 CE + Softmax 적용 문제점,-0.007874128,0.0,0.00787412840873003
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아 (남은 답변: 모든 질문 해결 완료)",PEFT 방법 5가지,0.007098217,0.0,0.007098217029124498
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아 (남은 답변: 모든 질문 해결 완료)",거대 언어 모델 정의,0.0037023146,0.0,0.00370231457054615
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아 (남은 답변: 모든 질문 해결 완료)",마지막 할 말,-0.010309987,0.0,0.010309986770153046
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아 (남은 답변: 모든 질문 해결 완료)",면접 시작 인사,-0.0021328186,0.0,0.002132818568497896
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아 (남은 답변: 모든 질문 해결 완료)",면접 종료,-0.014681538,0.0,0.014681537635624409
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아 (남은 답변: 모든 질문 해결 완료)","인공지능, 머신러닝, 딥러닝 차이",0.0130492905,0.0,0.013049290515482426
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아 (남은 답변: 모든 질문 해결 완료)",잠시 휴식,-0.035610072,0.0,0.03561007231473923
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아 (남은 답변: 모든 질문 해결 완료)",확률 예측에서 MSE Loss 미 사용 이유,-0.083223745,0.0,0.08322374522686005
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야 (남은 답변: 모든 질문 해결 완료),BCE Loss 설명,-0.04499422,0.0,0.04499422013759613
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야 (남은 답변: 모든 질문 해결 완료),LLM Fine-Tuning 의 PEFT,-0.018778924,0.0,0.018778923898935318
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야 (남은 답변: 모든 질문 해결 완료),LoRA,0.0006413382,0.0,0.0006413381779566407
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야 (남은 답변: 모든 질문 해결 완료),LoRA 와 QLoRA 의 차이,0.000838309,0.0,0.0008383090025745332
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야 (남은 답변: 모든 질문 해결 완료),Loss Function 관련 실무 경험,0.0036600519,0.0,0.0036600518506020308
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야 (남은 답변: 모든 질문 해결 완료),Loss Function 예시,-0.038169503,0.0,0.0381695032119751
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야 (남은 답변: 모든 질문 해결 완료),Loss Function 정의,-0.01668249,0.0,0.016682490706443787
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야 (남은 답변: 모든 질문 해결 완료),MBTI / 좋아하는 아이돌,0.0006797309,0.0,0.0006797309033572674
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야 (남은 답변: 모든 질문 해결 완료),MSE Loss 설명,0.07024249,0.0,0.0702424868941307
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야 (남은 답변: 모든 질문 해결 완료),MSE Loss 용도,0.9234418,1.0,0.0765581727027893
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야 (남은 답변: 모든 질문 해결 완료),"Multi-Class, Multi-Label 중 BCE 가 좋은 task",0.011772383,0.0,0.011772383004426956
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야 (남은 답변: 모든 질문 해결 완료),Multi-Label 에서 CE + Softmax 적용 문제점,-0.0092973765,0.0,0.009297376498579979
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야 (남은 답변: 모든 질문 해결 완료),PEFT 방법 5가지,-0.013954325,0.0,0.013954324647784233
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야 (남은 답변: 모든 질문 해결 완료),거대 언어 모델 정의,-0.02765157,0.0,0.027651570737361908
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야 (남은 답변: 모든 질문 해결 완료),마지막 할 말,-0.033044126,0.0,0.03304412588477135
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야 (남은 답변: 모든 질문 해결 완료),면접 시작 인사,-0.0024795614,0.0,0.002479561371728778
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야 (남은 답변: 모든 질문 해결 완료),면접 종료,0.017580422,0.0,0.017580421641469002
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야 (남은 답변: 모든 질문 해결 완료),"인공지능, 머신러닝, 딥러닝 차이",0.001210518,0.0,0.0012105179484933615
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야 (남은 답변: 모든 질문 해결 완료),잠시 휴식,-0.005668405,0.0,0.00566840497776866
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야 (남은 답변: 모든 질문 해결 완료),확률 예측에서 MSE Loss 미 사용 이유,-0.007135204,0.0,0.007135204039514065
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지? (남은 답변: MSE Loss 설명),BCE Loss 설명,-0.00457002,0.0,0.004570019897073507
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지? (남은 답변: MSE Loss 설명),LLM Fine-Tuning 의 PEFT,0.010034235,0.0,0.010034235194325447
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지? (남은 답변: MSE Loss 설명),LoRA,0.014458173,0.0,0.01445817295461893
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지? (남은 답변: MSE Loss 설명),LoRA 와 QLoRA 의 차이,0.011139584,0.0,0.011139583773911
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지? (남은 답변: MSE Loss 설명),Loss Function 관련 실무 경험,-0.0020683089,0.0,0.0020683088805526495
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지? (남은 답변: MSE Loss 설명),Loss Function 예시,-0.041985378,0.0,0.04198537766933441
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지? (남은 답변: MSE Loss 설명),Loss Function 정의,-0.023748364,0.0,0.02374836429953575
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지? (남은 답변: MSE Loss 설명),MBTI / 좋아하는 아이돌,0.0098739145,0.0,0.009873914532363415
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지? (남은 답변: MSE Loss 설명),MSE Loss 설명,0.96276623,1.0,0.03723376989364624
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지? (남은 답변: MSE Loss 설명),MSE Loss 용도,-0.020468354,0.0,0.02046835422515869
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지? (남은 답변: MSE Loss 설명),"Multi-Class, Multi-Label 중 BCE 가 좋은 task",0.01097489,0.0,0.010974889621138573
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지? (남은 답변: MSE Loss 설명),Multi-Label 에서 CE + Softmax 적용 문제점,0.0010513214,0.0,0.00105132139287889
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지? (남은 답변: MSE Loss 설명),PEFT 방법 5가지,0.009808681,0.0,0.009808680973947048
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지? (남은 답변: MSE Loss 설명),거대 언어 모델 정의,-0.013427698,0.0,0.01342769805341959
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지? (남은 답변: MSE Loss 설명),마지막 할 말,-0.0033855904,0.0,0.0033855903893709183
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지? (남은 답변: MSE Loss 설명),면접 시작 인사,-0.0006636991,0.0,0.0006636991165578365
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지? (남은 답변: MSE Loss 설명),면접 종료,-0.008574916,0.0,0.008574916049838066
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지? (남은 답변: MSE Loss 설명),"인공지능, 머신러닝, 딥러닝 차이",0.0008310903,0.0,0.0008310903212986887
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지? (남은 답변: MSE Loss 설명),잠시 휴식,-0.023978613,0.0,0.023978613317012787
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지? (남은 답변: MSE Loss 설명),확률 예측에서 MSE Loss 미 사용 이유,-0.0283406,0.0,0.028340600430965424
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지 (남은 답변: 모든 질문 해결 완료)",BCE Loss 설명,-0.050313797,0.0,0.0503137968480587
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지 (남은 답변: 모든 질문 해결 완료)",LLM Fine-Tuning 의 PEFT,0.012983929,0.0,0.012983929365873337
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지 (남은 답변: 모든 질문 해결 완료)",LoRA,-0.0050520496,0.0,0.005052049644291401
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지 (남은 답변: 모든 질문 해결 완료)",LoRA 와 QLoRA 의 차이,-0.0072709853,0.0,0.007270985282957554
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지 (남은 답변: 모든 질문 해결 완료)",Loss Function 관련 실무 경험,0.018550131,0.0,0.01855013146996498
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지 (남은 답변: 모든 질문 해결 완료)",Loss Function 예시,0.011269602,0.0,0.011269601993262768
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지 (남은 답변: 모든 질문 해결 완료)",Loss Function 정의,-0.0033454294,0.0,0.0033454294316470623
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지 (남은 답변: 모든 질문 해결 완료)",MBTI / 좋아하는 아이돌,-0.020226019,0.0,0.020226018503308296
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지 (남은 답변: 모든 질문 해결 완료)",MSE Loss 설명,-0.029342316,0.0,0.02934231609106064
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지 (남은 답변: 모든 질문 해결 완료)",MSE Loss 용도,-0.022835474,0.0,0.022835474461317062
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지 (남은 답변: 모든 질문 해결 완료)","Multi-Class, Multi-Label 중 BCE 가 좋은 task",0.0024575735,0.0,0.0024575735442340374
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지 (남은 답변: 모든 질문 해결 완료)",Multi-Label 에서 CE + Softmax 적용 문제점,-0.005335577,0.0,0.005335576832294464
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지 (남은 답변: 모든 질문 해결 완료)",PEFT 방법 5가지,-0.03948502,0.0,0.03948501870036125
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지 (남은 답변: 모든 질문 해결 완료)",거대 언어 모델 정의,0.0067880256,0.0,0.006788025610148907
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지 (남은 답변: 모든 질문 해결 완료)",마지막 할 말,-0.012134642,0.0,0.012134642340242863
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지 (남은 답변: 모든 질문 해결 완료)",면접 시작 인사,0.012508726,0.0,0.012508725747466087
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지 (남은 답변: 모든 질문 해결 완료)",면접 종료,-0.013423049,0.0,0.01342304889112711
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지 (남은 답변: 모든 질문 해결 완료)","인공지능, 머신러닝, 딥러닝 차이",-0.011385656,0.0,0.011385655961930752
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지 (남은 답변: 모든 질문 해결 완료)",잠시 휴식,-0.009178658,0.0,0.00917865801602602
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지 (남은 답변: 모든 질문 해결 완료)",확률 예측에서 MSE Loss 미 사용 이유,0.9180768,1.0,0.08192318677902222
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지 (남은 답변: MSE Loss 용도),BCE Loss 설명,0.0072655636,0.0,0.00726556358858943
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지 (남은 답변: MSE Loss 용도),LLM Fine-Tuning 의 PEFT,-0.010284719,0.0,0.010284719057381153
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지 (남은 답변: MSE Loss 용도),LoRA,0.00862365,0.0,0.008623650297522545
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지 (남은 답변: MSE Loss 용도),LoRA 와 QLoRA 의 차이,0.00080702256,0.0,0.0008070225594565272
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지 (남은 답변: MSE Loss 용도),Loss Function 관련 실무 경험,0.011811597,0.0,0.011811597272753716
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지 (남은 답변: MSE Loss 용도),Loss Function 예시,-0.0020549274,0.0,0.0020549274049699306
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지 (남은 답변: MSE Loss 용도),Loss Function 정의,-0.0008929942,0.0,0.0008929942268878222
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지 (남은 답변: MSE Loss 용도),MBTI / 좋아하는 아이돌,0.004353201,0.0,0.004353201016783714
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지 (남은 답변: MSE Loss 용도),MSE Loss 설명,-0.044389565,0.0,0.04438956454396248
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지 (남은 답변: MSE Loss 용도),MSE Loss 용도,0.9680149,1.0,0.03198510408401489
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지 (남은 답변: MSE Loss 용도),"Multi-Class, Multi-Label 중 BCE 가 좋은 task",-0.0056253723,0.0,0.005625372286885977
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지 (남은 답변: MSE Loss 용도),Multi-Label 에서 CE + Softmax 적용 문제점,-0.0010449764,0.0,0.0010449764085933566
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지 (남은 답변: MSE Loss 용도),PEFT 방법 5가지,-0.020670174,0.0,0.020670173689723015
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지 (남은 답변: MSE Loss 용도),거대 언어 모델 정의,-0.008776274,0.0,0.00877627357840538
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지 (남은 답변: MSE Loss 용도),마지막 할 말,-0.005801079,0.0,0.005801078863441944
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지 (남은 답변: MSE Loss 용도),면접 시작 인사,0.0057242415,0.0,0.005724241491407156
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지 (남은 답변: MSE Loss 용도),면접 종료,-0.01641255,0.0,0.01641255058348179
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지 (남은 답변: MSE Loss 용도),"인공지능, 머신러닝, 딥러닝 차이",-0.008708797,0.0,0.00870879739522934
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지 (남은 답변: MSE Loss 용도),잠시 휴식,-0.014381281,0.0,0.014381281100213528
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지 (남은 답변: MSE Loss 용도),확률 예측에서 MSE Loss 미 사용 이유,-0.013510275,0.0,0.013510274700820446
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야? (남은 답변: 확률 예측에서 MSE Loss 미 사용 이유),BCE Loss 설명,-0.0035843318,0.0,0.0035843318328261375
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야? (남은 답변: 확률 예측에서 MSE Loss 미 사용 이유),LLM Fine-Tuning 의 PEFT,-0.010327921,0.0,0.010327921248972416
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야? (남은 답변: 확률 예측에서 MSE Loss 미 사용 이유),LoRA,-0.011749162,0.0,0.011749162338674068
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야? (남은 답변: 확률 예측에서 MSE Loss 미 사용 이유),LoRA 와 QLoRA 의 차이,-0.012152583,0.0,0.012152583338320255
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야? (남은 답변: 확률 예측에서 MSE Loss 미 사용 이유),Loss Function 관련 실무 경험,-0.009569087,0.0,0.009569087065756321
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야? (남은 답변: 확률 예측에서 MSE Loss 미 사용 이유),Loss Function 예시,-0.026508152,0.0,0.0265081524848938
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야? (남은 답변: 확률 예측에서 MSE Loss 미 사용 이유),Loss Function 정의,-0.0053767897,0.0,0.005376789718866348
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야? (남은 답변: 확률 예측에서 MSE Loss 미 사용 이유),MBTI / 좋아하는 아이돌,-0.015796233,0.0,0.015796232968568802
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야? (남은 답변: 확률 예측에서 MSE Loss 미 사용 이유),MSE Loss 설명,-0.038175493,0.0,0.038175493478775024
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야? (남은 답변: 확률 예측에서 MSE Loss 미 사용 이유),MSE Loss 용도,-0.0045863306,0.0,0.0045863306149840355
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야? (남은 답변: 확률 예측에서 MSE Loss 미 사용 이유),"Multi-Class, Multi-Label 중 BCE 가 좋은 task",0.008110648,0.0,0.008110648021101952
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야? (남은 답변: 확률 예측에서 MSE Loss 미 사용 이유),Multi-Label 에서 CE + Softmax 적용 문제점,-0.01607119,0.0,0.01607118919491768
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야? (남은 답변: 확률 예측에서 MSE Loss 미 사용 이유),PEFT 방법 5가지,-0.022316104,0.0,0.02231610380113125
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야? (남은 답변: 확률 예측에서 MSE Loss 미 사용 이유),거대 언어 모델 정의,-0.020243982,0.0,0.02024398185312748
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야? (남은 답변: 확률 예측에서 MSE Loss 미 사용 이유),마지막 할 말,-0.00846362,0.0,0.008463620208203793
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야? (남은 답변: 확률 예측에서 MSE Loss 미 사용 이유),면접 시작 인사,-0.020992184,0.0,0.020992184057831764
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야? (남은 답변: 확률 예측에서 MSE Loss 미 사용 이유),면접 종료,-0.0065360377,0.0,0.006536037661135197
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야? (남은 답변: 확률 예측에서 MSE Loss 미 사용 이유),"인공지능, 머신러닝, 딥러닝 차이",-0.012892784,0.0,0.012892783619463444
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야? (남은 답변: 확률 예측에서 MSE Loss 미 사용 이유),잠시 휴식,0.0030422683,0.0,0.0030422683339565992
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야? (남은 답변: 확률 예측에서 MSE Loss 미 사용 이유),확률 예측에서 MSE Loss 미 사용 이유,0.98286396,1.0,0.017136037349700928
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고? (남은 답변: 모든 질문 해결 완료),BCE Loss 설명,0.87183416,1.0,0.1281658411026001
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고? (남은 답변: 모든 질문 해결 완료),LLM Fine-Tuning 의 PEFT,-0.01813021,0.0,0.018130209296941757
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고? (남은 답변: 모든 질문 해결 완료),LoRA,-0.04562793,0.0,0.04562792927026749
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고? (남은 답변: 모든 질문 해결 완료),LoRA 와 QLoRA 의 차이,0.008507058,0.0,0.008507058024406433
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고? (남은 답변: 모든 질문 해결 완료),Loss Function 관련 실무 경험,-0.017733146,0.0,0.017733145505189896
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고? (남은 답변: 모든 질문 해결 완료),Loss Function 예시,0.022457985,0.0,0.02245798520743847
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고? (남은 답변: 모든 질문 해결 완료),Loss Function 정의,0.009236117,0.0,0.009236116893589497
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고? (남은 답변: 모든 질문 해결 완료),MBTI / 좋아하는 아이돌,0.00019584643,0.0,0.00019584642723202705
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고? (남은 답변: 모든 질문 해결 완료),MSE Loss 설명,-0.056541715,0.0,0.05654171481728554
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고? (남은 답변: 모든 질문 해결 완료),MSE Loss 용도,0.027234491,0.0,0.02723449096083641
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고? (남은 답변: 모든 질문 해결 완료),"Multi-Class, Multi-Label 중 BCE 가 좋은 task",0.024968099,0.0,0.02496809884905815
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고? (남은 답변: 모든 질문 해결 완료),Multi-Label 에서 CE + Softmax 적용 문제점,-0.0036645548,0.0,0.0036645547952502966
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고? (남은 답변: 모든 질문 해결 완료),PEFT 방법 5가지,-0.005002,0.0,0.005001999903470278
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고? (남은 답변: 모든 질문 해결 완료),거대 언어 모델 정의,-0.044545367,0.0,0.04454536736011505
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고? (남은 답변: 모든 질문 해결 완료),마지막 할 말,0.0025269955,0.0,0.0025269954930990934
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고? (남은 답변: 모든 질문 해결 완료),면접 시작 인사,-0.0053778696,0.0,0.005377869587391615
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고? (남은 답변: 모든 질문 해결 완료),면접 종료,-0.03113095,0.0,0.031130950897932053
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고? (남은 답변: 모든 질문 해결 완료),"인공지능, 머신러닝, 딥러닝 차이",0.0021336146,0.0,0.0021336146164685488
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고? (남은 답변: 모든 질문 해결 완료),잠시 휴식,0.005600107,0.0,0.005600106902420521
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고? (남은 답변: 모든 질문 해결 완료),확률 예측에서 MSE Loss 미 사용 이유,0.096487254,0.0,0.09648725390434265
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고 (남은 답변: 핵심 아이디어),BCE Loss 설명,0.96408385,1.0,0.035916149616241455
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고 (남은 답변: 핵심 아이디어),LLM Fine-Tuning 의 PEFT,-0.020325715,0.0,0.020325714722275734
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고 (남은 답변: 핵심 아이디어),LoRA,0.0029301879,0.0,0.002930187853053212
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고 (남은 답변: 핵심 아이디어),LoRA 와 QLoRA 의 차이,0.008791052,0.0,0.008791051805019379
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고 (남은 답변: 핵심 아이디어),Loss Function 관련 실무 경험,-0.0027643468,0.0,0.0027643467765301466
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고 (남은 답변: 핵심 아이디어),Loss Function 예시,-0.004873309,0.0,0.004873308818787336
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고 (남은 답변: 핵심 아이디어),Loss Function 정의,-0.00063767727,0.0,0.0006376772653311491
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고 (남은 답변: 핵심 아이디어),MBTI / 좋아하는 아이돌,0.014575404,0.0,0.014575404115021229
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고 (남은 답변: 핵심 아이디어),MSE Loss 설명,-0.03552086,0.0,0.03552085906267166
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고 (남은 답변: 핵심 아이디어),MSE Loss 용도,-0.007860069,0.0,0.007860069163143635
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고 (남은 답변: 핵심 아이디어),"Multi-Class, Multi-Label 중 BCE 가 좋은 task",0.032082055,0.0,0.032082054764032364
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고 (남은 답변: 핵심 아이디어),Multi-Label 에서 CE + Softmax 적용 문제점,-0.012864528,0.0,0.012864528223872185
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고 (남은 답변: 핵심 아이디어),PEFT 방법 5가지,0.0018077361,0.0,0.0018077361164614558
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고 (남은 답변: 핵심 아이디어),거대 언어 모델 정의,-0.021764787,0.0,0.021764786913990974
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고 (남은 답변: 핵심 아이디어),마지막 할 말,-0.019907871,0.0,0.019907871261239052
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고 (남은 답변: 핵심 아이디어),면접 시작 인사,-0.0016013101,0.0,0.0016013100976124406
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고 (남은 답변: 핵심 아이디어),면접 종료,-0.011498233,0.0,0.011498233303427696
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고 (남은 답변: 핵심 아이디어),"인공지능, 머신러닝, 딥러닝 차이",-0.016362986,0.0,0.016362985596060753
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고 (남은 답변: 핵심 아이디어),잠시 휴식,0.004439434,0.0,0.004439434036612511
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고 (남은 답변: 핵심 아이디어),확률 예측에서 MSE Loss 미 사용 이유,-0.046947412,0.0,0.0469474121928215
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거 (남은 답변: 모든 질문 해결 완료)",BCE Loss 설명,0.017421624,0.0,0.017421623691916466
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거 (남은 답변: 모든 질문 해결 완료)",LLM Fine-Tuning 의 PEFT,-0.022619963,0.0,0.022619962692260742
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거 (남은 답변: 모든 질문 해결 완료)",LoRA,-0.0121032605,0.0,0.01210326049476862
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거 (남은 답변: 모든 질문 해결 완료)",LoRA 와 QLoRA 의 차이,0.021805013,0.0,0.021805012598633766
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거 (남은 답변: 모든 질문 해결 완료)",Loss Function 관련 실무 경험,0.011449751,0.0,0.011449751444160938
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거 (남은 답변: 모든 질문 해결 완료)",Loss Function 예시,0.0008901707,0.0,0.0008901706896722317
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거 (남은 답변: 모든 질문 해결 완료)",Loss Function 정의,0.0143952705,0.0,0.014395270496606827
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거 (남은 답변: 모든 질문 해결 완료)",MBTI / 좋아하는 아이돌,0.0036462867,0.0,0.0036462866701185703
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거 (남은 답변: 모든 질문 해결 완료)",MSE Loss 설명,0.004741716,0.0,0.00474171619862318
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거 (남은 답변: 모든 질문 해결 완료)",MSE Loss 용도,0.015118573,0.0,0.015118572860956192
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거 (남은 답변: 모든 질문 해결 완료)","Multi-Class, Multi-Label 중 BCE 가 좋은 task",0.91586995,1.0,0.08413004875183105
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거 (남은 답변: 모든 질문 해결 완료)",Multi-Label 에서 CE + Softmax 적용 문제점,-0.02633271,0.0,0.026332709938287735
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거 (남은 답변: 모든 질문 해결 완료)",PEFT 방법 5가지,-0.0026562246,0.0,0.002656224649399519
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거 (남은 답변: 모든 질문 해결 완료)",거대 언어 모델 정의,0.004206936,0.0,0.004206935875117779
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거 (남은 답변: 모든 질문 해결 완료)",마지막 할 말,0.016326513,0.0,0.01632651314139366
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거 (남은 답변: 모든 질문 해결 완료)",면접 시작 인사,0.012993168,0.0,0.012993168085813522
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거 (남은 답변: 모든 질문 해결 완료)",면접 종료,-0.019347962,0.0,0.019347961992025375
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거 (남은 답변: 모든 질문 해결 완료)","인공지능, 머신러닝, 딥러닝 차이",0.009970066,0.0,0.009970066137611866
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거 (남은 답변: 모든 질문 해결 완료)",잠시 휴식,-0.021901485,0.0,0.021901484578847885
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거 (남은 답변: 모든 질문 해결 완료)",확률 예측에서 MSE Loss 미 사용 이유,-0.013895439,0.0,0.013895438984036446
"BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야? (남은 답변: 핵심 아이디어, 수식)",BCE Loss 설명,0.8121908,1.0,0.18780922889709473
"BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야? (남은 답변: 핵심 아이디어, 수식)",LLM Fine-Tuning 의 PEFT,-0.01750793,0.0,0.017507929354906082
"BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야? (남은 답변: 핵심 아이디어, 수식)",LoRA,-0.01572719,0.0,0.015727190300822258
"BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야? (남은 답변: 핵심 아이디어, 수식)",LoRA 와 QLoRA 의 차이,0.048783127,0.0,0.04878312721848488
"BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야? (남은 답변: 핵심 아이디어, 수식)",Loss Function 관련 실무 경험,0.17888865,0.0,0.17888864874839783
"BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야? (남은 답변: 핵심 아이디어, 수식)",Loss Function 예시,-0.033123333,0.0,0.033123333007097244
"BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야? (남은 답변: 핵심 아이디어, 수식)",Loss Function 정의,-0.005266021,0.0,0.005266021005809307
"BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야? (남은 답변: 핵심 아이디어, 수식)",MBTI / 좋아하는 아이돌,0.009741883,0.0,0.009741882793605328
"BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야? (남은 답변: 핵심 아이디어, 수식)",MSE Loss 설명,-0.06473783,0.0,0.06473782658576965
"BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야? (남은 답변: 핵심 아이디어, 수식)",MSE Loss 용도,-0.009495189,0.0,0.009495189413428307
"BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야? (남은 답변: 핵심 아이디어, 수식)","Multi-Class, Multi-Label 중 BCE 가 좋은 task",0.20875134,0.0,0.20875133574008942
"BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야? (남은 답변: 핵심 아이디어, 수식)",Multi-Label 에서 CE + Softmax 적용 문제점,-0.0014113048,0.0,0.00141130480915308
"BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야? (남은 답변: 핵심 아이디어, 수식)",PEFT 방법 5가지,-0.020772204,0.0,0.02077220380306244
"BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야? (남은 답변: 핵심 아이디어, 수식)",거대 언어 모델 정의,-0.014924237,0.0,0.014924236573278904
"BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야? (남은 답변: 핵심 아이디어, 수식)",마지막 할 말,-0.041772727,0.0,0.04177272692322731
"BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야? (남은 답변: 핵심 아이디어, 수식)",면접 시작 인사,-0.038679957,0.0,0.03867995738983154
"BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야? (남은 답변: 핵심 아이디어, 수식)",면접 종료,-0.009504609,0.0,0.009504608809947968
"BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야? (남은 답변: 핵심 아이디어, 수식)","인공지능, 머신러닝, 딥러닝 차이",-0.039424416,0.0,0.03942441567778587
"BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야? (남은 답변: 핵심 아이디어, 수식)",잠시 휴식,-0.06320005,0.0,0.06320004910230637
"BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야? (남은 답변: 핵심 아이디어, 수식)",확률 예측에서 MSE Loss 미 사용 이유,0.003638273,0.0,0.0036382731050252914
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고 (남은 답변: 수식)",BCE Loss 설명,0.96643364,1.0,0.03356635570526123
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고 (남은 답변: 수식)",LLM Fine-Tuning 의 PEFT,-0.011078013,0.0,0.011078013107180595
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고 (남은 답변: 수식)",LoRA,0.002180659,0.0,0.0021806589793413877
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고 (남은 답변: 수식)",LoRA 와 QLoRA 의 차이,0.0326065,0.0,0.03260650113224983
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고 (남은 답변: 수식)",Loss Function 관련 실무 경험,0.0034370115,0.0,0.0034370115026831627
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고 (남은 답변: 수식)",Loss Function 예시,-0.021607513,0.0,0.02160751260817051
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고 (남은 답변: 수식)",Loss Function 정의,0.004596863,0.0,0.004596862941980362
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고 (남은 답변: 수식)",MBTI / 좋아하는 아이돌,-0.0066191866,0.0,0.006619186606258154
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고 (남은 답변: 수식)",MSE Loss 설명,-0.022123672,0.0,0.02212367206811905
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고 (남은 답변: 수식)",MSE Loss 용도,-0.01526739,0.0,0.015267389826476574
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고 (남은 답변: 수식)","Multi-Class, Multi-Label 중 BCE 가 좋은 task",-0.023980437,0.0,0.023980436846613884
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고 (남은 답변: 수식)",Multi-Label 에서 CE + Softmax 적용 문제점,-0.0054409835,0.0,0.005440983455628157
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고 (남은 답변: 수식)",PEFT 방법 5가지,-0.038566947,0.0,0.0385669469833374
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고 (남은 답변: 수식)",거대 언어 모델 정의,0.0036119982,0.0,0.0036119981668889523
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고 (남은 답변: 수식)",마지막 할 말,-0.034533117,0.0,0.03453311696648598
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고 (남은 답변: 수식)",면접 시작 인사,-0.01630752,0.0,0.016307519748806953
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고 (남은 답변: 수식)",면접 종료,-0.0023418274,0.0,0.002341827377676964
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고 (남은 답변: 수식)","인공지능, 머신러닝, 딥러닝 차이",0.014964057,0.0,0.014964057132601738
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고 (남은 답변: 수식)",잠시 휴식,-0.014747394,0.0,0.014747394248843193
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고 (남은 답변: 수식)",확률 예측에서 MSE Loss 미 사용 이유,-0.017486248,0.0,0.017486248165369034
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')] (남은 답변: 핵심 아이디어)",BCE Loss 설명,0.9669478,1.0,0.03305220603942871
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')] (남은 답변: 핵심 아이디어)",LLM Fine-Tuning 의 PEFT,-0.010388314,0.0,0.010388313792645931
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')] (남은 답변: 핵심 아이디어)",LoRA,-0.004744619,0.0,0.004744619131088257
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')] (남은 답변: 핵심 아이디어)",LoRA 와 QLoRA 의 차이,0.028610228,0.0,0.02861022762954235
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')] (남은 답변: 핵심 아이디어)",Loss Function 관련 실무 경험,-0.009130989,0.0,0.009130989201366901
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')] (남은 답변: 핵심 아이디어)",Loss Function 예시,-0.010741573,0.0,0.010741572827100754
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')] (남은 답변: 핵심 아이디어)",Loss Function 정의,-0.00054070185,0.0,0.0005407018470577896
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')] (남은 답변: 핵심 아이디어)",MBTI / 좋아하는 아이돌,0.005126325,0.0,0.005126324947923422
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')] (남은 답변: 핵심 아이디어)",MSE Loss 설명,-0.012278637,0.0,0.012278636917471886
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')] (남은 답변: 핵심 아이디어)",MSE Loss 용도,-0.010112485,0.0,0.01011248491704464
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')] (남은 답변: 핵심 아이디어)","Multi-Class, Multi-Label 중 BCE 가 좋은 task",0.024998488,0.0,0.024998487904667854
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')] (남은 답변: 핵심 아이디어)",Multi-Label 에서 CE + Softmax 적용 문제점,-0.005330381,0.0,0.005330380983650684
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')] (남은 답변: 핵심 아이디어)",PEFT 방법 5가지,-0.022757487,0.0,0.02275748737156391
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')] (남은 답변: 핵심 아이디어)",거대 언어 모델 정의,-0.021099875,0.0,0.0210998747497797
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')] (남은 답변: 핵심 아이디어)",마지막 할 말,-0.027822085,0.0,0.027822084724903107
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')] (남은 답변: 핵심 아이디어)",면접 시작 인사,-0.008329265,0.0,0.00832926481962204
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')] (남은 답변: 핵심 아이디어)",면접 종료,-0.013094067,0.0,0.013094066642224789
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')] (남은 답변: 핵심 아이디어)","인공지능, 머신러닝, 딥러닝 차이",0.004977735,0.0,0.004977735225111246
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')] (남은 답변: 핵심 아이디어)",잠시 휴식,-0.024285616,0.0,0.024285616353154182
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')] (남은 답변: 핵심 아이디어)",확률 예측에서 MSE Loss 미 사용 이유,-0.018416597,0.0,0.018416596576571465
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다 (남은 답변: 모든 질문 해결 완료)",BCE Loss 설명,0.03265389,0.0,0.032653890550136566
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다 (남은 답변: 모든 질문 해결 완료)",LLM Fine-Tuning 의 PEFT,-0.037935074,0.0,0.03793507441878319
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다 (남은 답변: 모든 질문 해결 완료)",LoRA,-0.00831166,0.0,0.008311660028994083
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다 (남은 답변: 모든 질문 해결 완료)",LoRA 와 QLoRA 의 차이,0.024613304,0.0,0.024613304063677788
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다 (남은 답변: 모든 질문 해결 완료)",Loss Function 관련 실무 경험,0.016338844,0.0,0.01633884385228157
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다 (남은 답변: 모든 질문 해결 완료)",Loss Function 예시,0.006892639,0.0,0.006892639212310314
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다 (남은 답변: 모든 질문 해결 완료)",Loss Function 정의,-0.0054378407,0.0,0.005437840707600117
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다 (남은 답변: 모든 질문 해결 완료)",MBTI / 좋아하는 아이돌,0.009420192,0.0,0.009420191869139671
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다 (남은 답변: 모든 질문 해결 완료)",MSE Loss 설명,0.018611815,0.0,0.018611814826726913
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다 (남은 답변: 모든 질문 해결 완료)",MSE Loss 용도,0.011430551,0.0,0.011430551297962666
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다 (남은 답변: 모든 질문 해결 완료)","Multi-Class, Multi-Label 중 BCE 가 좋은 task",0.9068603,1.0,0.09313970804214478
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다 (남은 답변: 모든 질문 해결 완료)",Multi-Label 에서 CE + Softmax 적용 문제점,-0.028782481,0.0,0.02878248132765293
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다 (남은 답변: 모든 질문 해결 완료)",PEFT 방법 5가지,0.019156676,0.0,0.019156675785779953
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다 (남은 답변: 모든 질문 해결 완료)",거대 언어 모델 정의,0.0028107634,0.0,0.0028107634279876947
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다 (남은 답변: 모든 질문 해결 완료)",마지막 할 말,0.010525512,0.0,0.01052551157772541
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다 (남은 답변: 모든 질문 해결 완료)",면접 시작 인사,0.0053385603,0.0,0.005338560324162245
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다 (남은 답변: 모든 질문 해결 완료)",면접 종료,-1.7392354e-06,0.0,1.7392353584000375e-06
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다 (남은 답변: 모든 질문 해결 완료)","인공지능, 머신러닝, 딥러닝 차이",0.0032648533,0.0,0.0032648532651364803
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다 (남은 답변: 모든 질문 해결 완료)",잠시 휴식,-0.02605237,0.0,0.02605237066745758
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다 (남은 답변: 모든 질문 해결 완료)",확률 예측에서 MSE Loss 미 사용 이유,-0.018179793,0.0,0.018179792910814285
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거 (남은 답변: 핵심 아이디어),BCE Loss 설명,0.9501263,1.0,0.0498737096786499
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거 (남은 답변: 핵심 아이디어),LLM Fine-Tuning 의 PEFT,-0.014112279,0.0,0.014112278819084167
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거 (남은 답변: 핵심 아이디어),LoRA,0.0063338582,0.0,0.006333858240395784
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거 (남은 답변: 핵심 아이디어),LoRA 와 QLoRA 의 차이,0.025545808,0.0,0.02554580755531788
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거 (남은 답변: 핵심 아이디어),Loss Function 관련 실무 경험,-0.015239695,0.0,0.015239695087075233
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거 (남은 답변: 핵심 아이디어),Loss Function 예시,-0.009337212,0.0,0.009337211959064007
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거 (남은 답변: 핵심 아이디어),Loss Function 정의,-0.008500527,0.0,0.008500526659190655
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거 (남은 답변: 핵심 아이디어),MBTI / 좋아하는 아이돌,0.027117211,0.0,0.027117211371660233
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거 (남은 답변: 핵심 아이디어),MSE Loss 설명,-0.045457345,0.0,0.04545734450221062
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거 (남은 답변: 핵심 아이디어),MSE Loss 용도,0.016398381,0.0,0.01639838144183159
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거 (남은 답변: 핵심 아이디어),"Multi-Class, Multi-Label 중 BCE 가 좋은 task",0.023103744,0.0,0.0231037437915802
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거 (남은 답변: 핵심 아이디어),Multi-Label 에서 CE + Softmax 적용 문제점,-0.011146813,0.0,0.011146812699735165
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거 (남은 답변: 핵심 아이디어),PEFT 방법 5가지,-0.027598318,0.0,0.027598317712545395
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거 (남은 답변: 핵심 아이디어),거대 언어 모델 정의,-0.011373997,0.0,0.01137399673461914
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거 (남은 답변: 핵심 아이디어),마지막 할 말,-0.024791991,0.0,0.024791991338133812
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거 (남은 답변: 핵심 아이디어),면접 시작 인사,-0.025088103,0.0,0.025088103488087654
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거 (남은 답변: 핵심 아이디어),면접 종료,0.0020468666,0.0,0.00204686657525599
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거 (남은 답변: 핵심 아이디어),"인공지능, 머신러닝, 딥러닝 차이",0.019213453,0.0,0.01921345293521881
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거 (남은 답변: 핵심 아이디어),잠시 휴식,0.019497769,0.0,0.019497768953442574
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거 (남은 답변: 핵심 아이디어),확률 예측에서 MSE Loss 미 사용 이유,-0.04215213,0.0,0.042152129113674164
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지 (남은 답변: 모든 질문 해결 완료)",BCE Loss 설명,0.02743828,0.0,0.02743827924132347
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지 (남은 답변: 모든 질문 해결 완료)",LLM Fine-Tuning 의 PEFT,0.0048611057,0.0,0.00486110569909215
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지 (남은 답변: 모든 질문 해결 완료)",LoRA,-0.012041145,0.0,0.012041145004332066
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지 (남은 답변: 모든 질문 해결 완료)",LoRA 와 QLoRA 의 차이,0.03332359,0.0,0.03332358971238136
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지 (남은 답변: 모든 질문 해결 완료)",Loss Function 관련 실무 경험,0.03203118,0.0,0.03203117847442627
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지 (남은 답변: 모든 질문 해결 완료)",Loss Function 예시,-0.0096844155,0.0,0.00968441553413868
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지 (남은 답변: 모든 질문 해결 완료)",Loss Function 정의,0.02385789,0.0,0.02385788969695568
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지 (남은 답변: 모든 질문 해결 완료)",MBTI / 좋아하는 아이돌,-0.013541022,0.0,0.013541022315621376
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지 (남은 답변: 모든 질문 해결 완료)",MSE Loss 설명,0.009916312,0.0,0.00991631206125021
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지 (남은 답변: 모든 질문 해결 완료)",MSE Loss 용도,0.007793198,0.0,0.007793197873979807
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지 (남은 답변: 모든 질문 해결 완료)","Multi-Class, Multi-Label 중 BCE 가 좋은 task",0.91013074,1.0,0.08986926078796387
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지 (남은 답변: 모든 질문 해결 완료)",Multi-Label 에서 CE + Softmax 적용 문제점,-0.019367931,0.0,0.01936793141067028
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지 (남은 답변: 모든 질문 해결 완료)",PEFT 방법 5가지,0.017578976,0.0,0.0175789762288332
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지 (남은 답변: 모든 질문 해결 완료)",거대 언어 모델 정의,0.01072247,0.0,0.010722469538450241
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지 (남은 답변: 모든 질문 해결 완료)",마지막 할 말,-0.008608304,0.0,0.008608303964138031
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지 (남은 답변: 모든 질문 해결 완료)",면접 시작 인사,0.029442681,0.0,0.02944268099963665
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지 (남은 답변: 모든 질문 해결 완료)",면접 종료,-0.031405035,0.0,0.03140503540635109
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지 (남은 답변: 모든 질문 해결 완료)","인공지능, 머신러닝, 딥러닝 차이",-0.004767654,0.0,0.004767653997987509
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지 (남은 답변: 모든 질문 해결 완료)",잠시 휴식,-0.05823409,0.0,0.058234091848134995
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지 (남은 답변: 모든 질문 해결 완료)",확률 예측에서 MSE Loss 미 사용 이유,-0.015297868,0.0,0.01529786828905344
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야? (남은 답변: BCE 가 좋은 task, BCE 가 좋은 이유)",BCE Loss 설명,-0.022521144,0.0,0.022521143779158592
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야? (남은 답변: BCE 가 좋은 task, BCE 가 좋은 이유)",LLM Fine-Tuning 의 PEFT,-0.0028849742,0.0,0.0028849742375314236
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야? (남은 답변: BCE 가 좋은 task, BCE 가 좋은 이유)",LoRA,0.0048613306,0.0,0.004861330613493919
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야? (남은 답변: BCE 가 좋은 task, BCE 가 좋은 이유)",LoRA 와 QLoRA 의 차이,0.0026607092,0.0,0.002660709200426936
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야? (남은 답변: BCE 가 좋은 task, BCE 가 좋은 이유)",Loss Function 관련 실무 경험,-0.0065315557,0.0,0.00653155567124486
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야? (남은 답변: BCE 가 좋은 task, BCE 가 좋은 이유)",Loss Function 예시,0.010439088,0.0,0.010439087636768818
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야? (남은 답변: BCE 가 좋은 task, BCE 가 좋은 이유)",Loss Function 정의,0.0015410922,0.0,0.0015410921769216657
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야? (남은 답변: BCE 가 좋은 task, BCE 가 좋은 이유)",MBTI / 좋아하는 아이돌,0.0066958866,0.0,0.006695886608213186
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야? (남은 답변: BCE 가 좋은 task, BCE 가 좋은 이유)",MSE Loss 설명,0.005921487,0.0,0.005921487230807543
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야? (남은 답변: BCE 가 좋은 task, BCE 가 좋은 이유)",MSE Loss 용도,-0.004453881,0.0,0.004453881178051233
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야? (남은 답변: BCE 가 좋은 task, BCE 가 좋은 이유)","Multi-Class, Multi-Label 중 BCE 가 좋은 task",0.99026334,1.0,0.00973665714263916
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야? (남은 답변: BCE 가 좋은 task, BCE 가 좋은 이유)",Multi-Label 에서 CE + Softmax 적용 문제점,-0.011931868,0.0,0.011931868270039558
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야? (남은 답변: BCE 가 좋은 task, BCE 가 좋은 이유)",PEFT 방법 5가지,-0.0028439227,0.0,0.0028439227025955915
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야? (남은 답변: BCE 가 좋은 task, BCE 가 좋은 이유)",거대 언어 모델 정의,0.0024853982,0.0,0.0024853982031345367
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야? (남은 답변: BCE 가 좋은 task, BCE 가 좋은 이유)",마지막 할 말,0.0010852725,0.0,0.0010852725245058537
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야? (남은 답변: BCE 가 좋은 task, BCE 가 좋은 이유)",면접 시작 인사,-0.0020459055,0.0,0.002045905450358987
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야? (남은 답변: BCE 가 좋은 task, BCE 가 좋은 이유)",면접 종료,-0.008212128,0.0,0.008212127722799778
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야? (남은 답변: BCE 가 좋은 task, BCE 가 좋은 이유)","인공지능, 머신러닝, 딥러닝 차이",0.0014776154,0.0,0.0014776154421269894
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야? (남은 답변: BCE 가 좋은 task, BCE 가 좋은 이유)",잠시 휴식,0.0026051786,0.0,0.002605178626254201
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야? (남은 답변: BCE 가 좋은 task, BCE 가 좋은 이유)",확률 예측에서 MSE Loss 미 사용 이유,0.0040168324,0.0,0.004016832448542118
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!! (남은 답변: BCE 가 좋은 task, BCE 가 좋은 이유)",BCE Loss 설명,-0.02539645,0.0,0.025396449491381645
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!! (남은 답변: BCE 가 좋은 task, BCE 가 좋은 이유)",LLM Fine-Tuning 의 PEFT,-0.0018163784,0.0,0.001816378440707922
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!! (남은 답변: BCE 가 좋은 task, BCE 가 좋은 이유)",LoRA,0.009075964,0.0,0.009075963869690895
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!! (남은 답변: BCE 가 좋은 task, BCE 가 좋은 이유)",LoRA 와 QLoRA 의 차이,-0.0038576273,0.0,0.0038576272781938314
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!! (남은 답변: BCE 가 좋은 task, BCE 가 좋은 이유)",Loss Function 관련 실무 경험,-0.008566418,0.0,0.0085664177313447
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!! (남은 답변: BCE 가 좋은 task, BCE 가 좋은 이유)",Loss Function 예시,0.0046196543,0.0,0.004619654268026352
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!! (남은 답변: BCE 가 좋은 task, BCE 가 좋은 이유)",Loss Function 정의,0.0006739129,0.0,0.0006739128730259836
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!! (남은 답변: BCE 가 좋은 task, BCE 가 좋은 이유)",MBTI / 좋아하는 아이돌,0.007666039,0.0,0.007666038814932108
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!! (남은 답변: BCE 가 좋은 task, BCE 가 좋은 이유)",MSE Loss 설명,0.008005128,0.0,0.008005128242075443
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!! (남은 답변: BCE 가 좋은 task, BCE 가 좋은 이유)",MSE Loss 용도,-0.010297628,0.0,0.010297628119587898
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!! (남은 답변: BCE 가 좋은 task, BCE 가 좋은 이유)","Multi-Class, Multi-Label 중 BCE 가 좋은 task",0.99300176,1.0,0.006998240947723389
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!! (남은 답변: BCE 가 좋은 task, BCE 가 좋은 이유)",Multi-Label 에서 CE + Softmax 적용 문제점,-0.006414401,0.0,0.00641440087929368
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!! (남은 답변: BCE 가 좋은 task, BCE 가 좋은 이유)",PEFT 방법 5가지,-0.0008876787,0.0,0.0008876787032932043
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!! (남은 답변: BCE 가 좋은 task, BCE 가 좋은 이유)",거대 언어 모델 정의,0.0044279965,0.0,0.004427996464073658
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!! (남은 답변: BCE 가 좋은 task, BCE 가 좋은 이유)",마지막 할 말,0.0003254358,0.0,0.0003254358016420156
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!! (남은 답변: BCE 가 좋은 task, BCE 가 좋은 이유)",면접 시작 인사,-0.0038458994,0.0,0.0038458993658423424
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!! (남은 답변: BCE 가 좋은 task, BCE 가 좋은 이유)",면접 종료,-0.0075237923,0.0,0.007523792330175638
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!! (남은 답변: BCE 가 좋은 task, BCE 가 좋은 이유)","인공지능, 머신러닝, 딥러닝 차이",0.010271003,0.0,0.010271002538502216
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!! (남은 답변: BCE 가 좋은 task, BCE 가 좋은 이유)",잠시 휴식,0.012459172,0.0,0.012459171935915947
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!! (남은 답변: BCE 가 좋은 task, BCE 가 좋은 이유)",확률 예측에서 MSE Loss 미 사용 이유,0.009185789,0.0,0.00918578915297985
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어 (남은 답변: BCE 가 좋은 이유)",BCE Loss 설명,-0.014581823,0.0,0.014581822790205479
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어 (남은 답변: BCE 가 좋은 이유)",LLM Fine-Tuning 의 PEFT,-0.009243799,0.0,0.0092437993735075
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어 (남은 답변: BCE 가 좋은 이유)",LoRA,0.0051001008,0.0,0.005100100766867399
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어 (남은 답변: BCE 가 좋은 이유)",LoRA 와 QLoRA 의 차이,-0.01250979,0.0,0.012509790249168873
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어 (남은 답변: BCE 가 좋은 이유)",Loss Function 관련 실무 경험,-0.010325196,0.0,0.010325196199119091
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어 (남은 답변: BCE 가 좋은 이유)",Loss Function 예시,0.013425953,0.0,0.01342595275491476
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어 (남은 답변: BCE 가 좋은 이유)",Loss Function 정의,0.0032532976,0.0,0.003253297647461295
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어 (남은 답변: BCE 가 좋은 이유)",MBTI / 좋아하는 아이돌,0.0051041474,0.0,0.0051041473634541035
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어 (남은 답변: BCE 가 좋은 이유)",MSE Loss 설명,0.0017612318,0.0,0.001761231804266572
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어 (남은 답변: BCE 가 좋은 이유)",MSE Loss 용도,-0.008157336,0.0,0.00815733615309
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어 (남은 답변: BCE 가 좋은 이유)","Multi-Class, Multi-Label 중 BCE 가 좋은 task",0.98696965,1.0,0.01303035020828247
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어 (남은 답변: BCE 가 좋은 이유)",Multi-Label 에서 CE + Softmax 적용 문제점,-0.015939645,0.0,0.01593964546918869
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어 (남은 답변: BCE 가 좋은 이유)",PEFT 방법 5가지,0.006861868,0.0,0.0068618678487837315
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어 (남은 답변: BCE 가 좋은 이유)",거대 언어 모델 정의,-0.004728748,0.0,0.004728747997432947
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어 (남은 답변: BCE 가 좋은 이유)",마지막 할 말,-0.0026584163,0.0,0.002658416284248233
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어 (남은 답변: BCE 가 좋은 이유)",면접 시작 인사,-0.004314708,0.0,0.004314708057790995
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어 (남은 답변: BCE 가 좋은 이유)",면접 종료,-0.0039782044,0.0,0.003978204447776079
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어 (남은 답변: BCE 가 좋은 이유)","인공지능, 머신러닝, 딥러닝 차이",0.008257168,0.0,0.008257168345153332
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어 (남은 답변: BCE 가 좋은 이유)",잠시 휴식,0.0055909012,0.0,0.005590901244431734
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어 (남은 답변: BCE 가 좋은 이유)",확률 예측에서 MSE Loss 미 사용 이유,0.0036637008,0.0,0.003663700772449374
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아 (남은 답변: 모든 질문 해결 완료)",BCE Loss 설명,0.014761833,0.0,0.014761832542717457
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아 (남은 답변: 모든 질문 해결 완료)",LLM Fine-Tuning 의 PEFT,-0.0076671443,0.0,0.0076671442948281765
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아 (남은 답변: 모든 질문 해결 완료)",LoRA,0.0041632853,0.0,0.004163285251706839
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아 (남은 답변: 모든 질문 해결 완료)",LoRA 와 QLoRA 의 차이,0.028665163,0.0,0.02866516262292862
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아 (남은 답변: 모든 질문 해결 완료)",Loss Function 관련 실무 경험,-0.023280056,0.0,0.023280056193470955
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아 (남은 답변: 모든 질문 해결 완료)",Loss Function 예시,-0.017347094,0.0,0.017347093671560287
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아 (남은 답변: 모든 질문 해결 완료)",Loss Function 정의,0.0062127872,0.0,0.006212787237018347
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아 (남은 답변: 모든 질문 해결 완료)",MBTI / 좋아하는 아이돌,-0.03799248,0.0,0.037992481142282486
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아 (남은 답변: 모든 질문 해결 완료)",MSE Loss 설명,0.0022934412,0.0,0.0022934412118047476
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아 (남은 답변: 모든 질문 해결 완료)",MSE Loss 용도,-0.018352779,0.0,0.018352778628468513
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아 (남은 답변: 모든 질문 해결 완료)","Multi-Class, Multi-Label 중 BCE 가 좋은 task",0.07564151,0.0,0.07564151287078857
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아 (남은 답변: 모든 질문 해결 완료)",Multi-Label 에서 CE + Softmax 적용 문제점,0.95134676,1.0,0.048653244972229004
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아 (남은 답변: 모든 질문 해결 완료)",PEFT 방법 5가지,-0.0036689742,0.0,0.0036689741536974907
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아 (남은 답변: 모든 질문 해결 완료)",거대 언어 모델 정의,0.0011895107,0.0,0.0011895106872543693
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아 (남은 답변: 모든 질문 해결 완료)",마지막 할 말,-0.0065545742,0.0,0.006554574239999056
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아 (남은 답변: 모든 질문 해결 완료)",면접 시작 인사,0.0029692417,0.0,0.002969241701066494
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아 (남은 답변: 모든 질문 해결 완료)",면접 종료,-0.012039409,0.0,0.012039409019052982
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아 (남은 답변: 모든 질문 해결 완료)","인공지능, 머신러닝, 딥러닝 차이",-0.005976583,0.0,0.005976582877337933
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아 (남은 답변: 모든 질문 해결 완료)",잠시 휴식,-0.013847991,0.0,0.01384799089282751
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아 (남은 답변: 모든 질문 해결 완료)",확률 예측에서 MSE Loss 미 사용 이유,-0.020163538,0.0,0.020163537934422493
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지 (남은 답변: 모든 질문 해결 완료),BCE Loss 설명,-0.033101913,0.0,0.03310191258788109
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지 (남은 답변: 모든 질문 해결 완료),LLM Fine-Tuning 의 PEFT,0.002305635,0.0,0.002305635018274188
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지 (남은 답변: 모든 질문 해결 완료),LoRA,-0.0007826386,0.0,0.0007826386136002839
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지 (남은 답변: 모든 질문 해결 완료),LoRA 와 QLoRA 의 차이,0.056842107,0.0,0.05684210732579231
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지 (남은 답변: 모든 질문 해결 완료),Loss Function 관련 실무 경험,0.8808736,1.0,0.11912637948989868
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지 (남은 답변: 모든 질문 해결 완료),Loss Function 예시,-0.027295172,0.0,0.027295172214508057
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지 (남은 답변: 모든 질문 해결 완료),Loss Function 정의,0.0011435469,0.0,0.001143546891398728
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지 (남은 답변: 모든 질문 해결 완료),MBTI / 좋아하는 아이돌,-0.026758831,0.0,0.0267588309943676
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지 (남은 답변: 모든 질문 해결 완료),MSE Loss 설명,0.0014538319,0.0,0.0014538319082930684
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지 (남은 답변: 모든 질문 해결 완료),MSE Loss 용도,0.04443624,0.0,0.04443623870611191
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지 (남은 답변: 모든 질문 해결 완료),"Multi-Class, Multi-Label 중 BCE 가 좋은 task",-0.029399589,0.0,0.029399588704109192
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지 (남은 답변: 모든 질문 해결 완료),Multi-Label 에서 CE + Softmax 적용 문제점,0.0056353034,0.0,0.005635303445160389
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지 (남은 답변: 모든 질문 해결 완료),PEFT 방법 5가지,-0.00017477655,0.0,0.00017477654910180718
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지 (남은 답변: 모든 질문 해결 완료),거대 언어 모델 정의,0.017926486,0.0,0.01792648620903492
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지 (남은 답변: 모든 질문 해결 완료),마지막 할 말,-0.01523655,0.0,0.015236550010740757
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지 (남은 답변: 모든 질문 해결 완료),면접 시작 인사,-0.017819026,0.0,0.017819026485085487
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지 (남은 답변: 모든 질문 해결 완료),면접 종료,0.009268343,0.0,0.009268343448638916
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지 (남은 답변: 모든 질문 해결 완료),"인공지능, 머신러닝, 딥러닝 차이",0.025416777,0.0,0.025416776537895203
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지 (남은 답변: 모든 질문 해결 완료),잠시 휴식,-0.056046642,0.0,0.05604664236307144
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지 (남은 답변: 모든 질문 해결 완료),확률 예측에서 MSE Loss 미 사용 이유,0.009889466,0.0,0.009889465756714344
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나 (남은 답변: Multi-Label 에서 CE + Softmax 적용 문제점),BCE Loss 설명,-0.018201116,0.0,0.01820111647248268
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나 (남은 답변: Multi-Label 에서 CE + Softmax 적용 문제점),LLM Fine-Tuning 의 PEFT,-0.00433257,0.0,0.004332569893449545
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나 (남은 답변: Multi-Label 에서 CE + Softmax 적용 문제점),LoRA,0.009907713,0.0,0.009907713159918785
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나 (남은 답변: Multi-Label 에서 CE + Softmax 적용 문제점),LoRA 와 QLoRA 의 차이,0.0064708022,0.0,0.006470802240073681
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나 (남은 답변: Multi-Label 에서 CE + Softmax 적용 문제점),Loss Function 관련 실무 경험,-0.032946613,0.0,0.03294661268591881
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나 (남은 답변: Multi-Label 에서 CE + Softmax 적용 문제점),Loss Function 예시,0.013140919,0.0,0.01314091868698597
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나 (남은 답변: Multi-Label 에서 CE + Softmax 적용 문제점),Loss Function 정의,-0.005866418,0.0,0.0058664181269705296
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나 (남은 답변: Multi-Label 에서 CE + Softmax 적용 문제점),MBTI / 좋아하는 아이돌,-0.010900029,0.0,0.010900028981268406
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나 (남은 답변: Multi-Label 에서 CE + Softmax 적용 문제점),MSE Loss 설명,-0.0006258629,0.0,0.0006258629146032035
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나 (남은 답변: Multi-Label 에서 CE + Softmax 적용 문제점),MSE Loss 용도,-0.000778884,0.0,0.0007788839866407216
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나 (남은 답변: Multi-Label 에서 CE + Softmax 적용 문제점),"Multi-Class, Multi-Label 중 BCE 가 좋은 task",0.0012852242,0.0,0.0012852242216467857
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나 (남은 답변: Multi-Label 에서 CE + Softmax 적용 문제점),Multi-Label 에서 CE + Softmax 적용 문제점,0.9763774,1.0,0.023622572422027588
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나 (남은 답변: Multi-Label 에서 CE + Softmax 적용 문제점),PEFT 방법 5가지,0.011197709,0.0,0.011197708547115326
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나 (남은 답변: Multi-Label 에서 CE + Softmax 적용 문제점),거대 언어 모델 정의,-0.0056370855,0.0,0.005637085530906916
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나 (남은 답변: Multi-Label 에서 CE + Softmax 적용 문제점),마지막 할 말,-0.0096738,0.0,0.009673800319433212
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나 (남은 답변: Multi-Label 에서 CE + Softmax 적용 문제점),면접 시작 인사,0.014703349,0.0,0.014703349210321903
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나 (남은 답변: Multi-Label 에서 CE + Softmax 적용 문제점),면접 종료,-0.007102265,0.0,0.0071022650226950645
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나 (남은 답변: Multi-Label 에서 CE + Softmax 적용 문제점),"인공지능, 머신러닝, 딥러닝 차이",-0.0036156916,0.0,0.0036156915593892336
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나 (남은 답변: Multi-Label 에서 CE + Softmax 적용 문제점),잠시 휴식,0.008604807,0.0,0.00860480684787035
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나 (남은 답변: Multi-Label 에서 CE + Softmax 적용 문제점),확률 예측에서 MSE Loss 미 사용 이유,-0.007584948,0.0,0.007584948092699051
Loss Function 관련 실무 경험 -> [기본 경험] 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어 (남은 답변: 상세 경험),BCE Loss 설명,0.0043456643,0.0,0.0043456642888486385
Loss Function 관련 실무 경험 -> [기본 경험] 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어 (남은 답변: 상세 경험),LLM Fine-Tuning 의 PEFT,-0.019730177,0.0,0.019730176776647568
Loss Function 관련 실무 경험 -> [기본 경험] 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어 (남은 답변: 상세 경험),LoRA,-0.0007621569,0.0,0.0007621569093316793
Loss Function 관련 실무 경험 -> [기본 경험] 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어 (남은 답변: 상세 경험),LoRA 와 QLoRA 의 차이,-0.010008969,0.0,0.010008969344198704
Loss Function 관련 실무 경험 -> [기본 경험] 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어 (남은 답변: 상세 경험),Loss Function 관련 실무 경험,0.9647566,1.0,0.03524339199066162
Loss Function 관련 실무 경험 -> [기본 경험] 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어 (남은 답변: 상세 경험),Loss Function 예시,-0.007250209,0.0,0.0072502088733017445
Loss Function 관련 실무 경험 -> [기본 경험] 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어 (남은 답변: 상세 경험),Loss Function 정의,-0.019213643,0.0,0.019213642925024033
Loss Function 관련 실무 경험 -> [기본 경험] 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어 (남은 답변: 상세 경험),MBTI / 좋아하는 아이돌,-0.023781605,0.0,0.023781605064868927
Loss Function 관련 실무 경험 -> [기본 경험] 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어 (남은 답변: 상세 경험),MSE Loss 설명,-0.007190539,0.0,0.007190539035946131
Loss Function 관련 실무 경험 -> [기본 경험] 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어 (남은 답변: 상세 경험),MSE Loss 용도,0.00013782254,0.0,0.0001378225424559787
Loss Function 관련 실무 경험 -> [기본 경험] 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어 (남은 답변: 상세 경험),"Multi-Class, Multi-Label 중 BCE 가 좋은 task",-0.011339285,0.0,0.011339285410940647
Loss Function 관련 실무 경험 -> [기본 경험] 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어 (남은 답변: 상세 경험),Multi-Label 에서 CE + Softmax 적용 문제점,0.005826083,0.0,0.0058260830119252205
Loss Function 관련 실무 경험 -> [기본 경험] 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어 (남은 답변: 상세 경험),PEFT 방법 5가지,-0.012270658,0.0,0.012270658276975155
Loss Function 관련 실무 경험 -> [기본 경험] 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어 (남은 답변: 상세 경험),거대 언어 모델 정의,-0.005956264,0.0,0.005956264212727547
Loss Function 관련 실무 경험 -> [기본 경험] 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어 (남은 답변: 상세 경험),마지막 할 말,-0.0074904365,0.0,0.0074904365465044975
Loss Function 관련 실무 경험 -> [기본 경험] 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어 (남은 답변: 상세 경험),면접 시작 인사,-0.007324123,0.0,0.007324122823774815
Loss Function 관련 실무 경험 -> [기본 경험] 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어 (남은 답변: 상세 경험),면접 종료,0.013714737,0.0,0.013714737258851528
Loss Function 관련 실무 경험 -> [기본 경험] 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어 (남은 답변: 상세 경험),"인공지능, 머신러닝, 딥러닝 차이",0.010135914,0.0,0.010135914199054241
Loss Function 관련 실무 경험 -> [기본 경험] 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어 (남은 답변: 상세 경험),잠시 휴식,0.044675913,0.0,0.04467591270804405
Loss Function 관련 실무 경험 -> [기본 경험] 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어 (남은 답변: 상세 경험),확률 예측에서 MSE Loss 미 사용 이유,0.01274228,0.0,0.012742280028760433
Loss Function 관련 실무 경험 -> [상세 경험] 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지? (남은 답변: 모든 질문 해결 완료),BCE Loss 설명,-0.016928554,0.0,0.016928553581237793
Loss Function 관련 실무 경험 -> [상세 경험] 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지? (남은 답변: 모든 질문 해결 완료),LLM Fine-Tuning 의 PEFT,-0.013315448,0.0,0.013315447606146336
Loss Function 관련 실무 경험 -> [상세 경험] 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지? (남은 답변: 모든 질문 해결 완료),LoRA,3.6672755e-05,0.0,3.6672754504252225e-05
Loss Function 관련 실무 경험 -> [상세 경험] 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지? (남은 답변: 모든 질문 해결 완료),LoRA 와 QLoRA 의 차이,0.00865431,0.0,0.008654310368001461
Loss Function 관련 실무 경험 -> [상세 경험] 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지? (남은 답변: 모든 질문 해결 완료),Loss Function 관련 실무 경험,-0.073137544,0.0,0.0731375440955162
Loss Function 관련 실무 경험 -> [상세 경험] 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지? (남은 답변: 모든 질문 해결 완료),Loss Function 예시,-0.023031607,0.0,0.023031607270240784
Loss Function 관련 실무 경험 -> [상세 경험] 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지? (남은 답변: 모든 질문 해결 완료),Loss Function 정의,-0.014139986,0.0,0.014139985665678978
Loss Function 관련 실무 경험 -> [상세 경험] 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지? (남은 답변: 모든 질문 해결 완료),MBTI / 좋아하는 아이돌,0.9260101,1.0,0.07398992776870728
Loss Function 관련 실무 경험 -> [상세 경험] 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지? (남은 답변: 모든 질문 해결 완료),MSE Loss 설명,0.0039554853,0.0,0.003955485299229622
Loss Function 관련 실무 경험 -> [상세 경험] 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지? (남은 답변: 모든 질문 해결 완료),MSE Loss 용도,0.033133104,0.0,0.03313310444355011
Loss Function 관련 실무 경험 -> [상세 경험] 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지? (남은 답변: 모든 질문 해결 완료),"Multi-Class, Multi-Label 중 BCE 가 좋은 task",0.014575182,0.0,0.01457518246024847
Loss Function 관련 실무 경험 -> [상세 경험] 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지? (남은 답변: 모든 질문 해결 완료),Multi-Label 에서 CE + Softmax 적용 문제점,0.0077045304,0.0,0.007704530376940966
Loss Function 관련 실무 경험 -> [상세 경험] 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지? (남은 답변: 모든 질문 해결 완료),PEFT 방법 5가지,-0.027482348,0.0,0.027482347562909126
Loss Function 관련 실무 경험 -> [상세 경험] 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지? (남은 답변: 모든 질문 해결 완료),거대 언어 모델 정의,0.006661731,0.0,0.006661730818450451
Loss Function 관련 실무 경험 -> [상세 경험] 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지? (남은 답변: 모든 질문 해결 완료),마지막 할 말,-0.023939777,0.0,0.02393977716565132
Loss Function 관련 실무 경험 -> [상세 경험] 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지? (남은 답변: 모든 질문 해결 완료),면접 시작 인사,0.0016064782,0.0,0.0016064782394096255
Loss Function 관련 실무 경험 -> [상세 경험] 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지? (남은 답변: 모든 질문 해결 완료),면접 종료,0.0034586787,0.0,0.003458678722381592
Loss Function 관련 실무 경험 -> [상세 경험] 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지? (남은 답변: 모든 질문 해결 완료),"인공지능, 머신러닝, 딥러닝 차이",0.004653706,0.0,0.004653706215322018
Loss Function 관련 실무 경험 -> [상세 경험] 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지? (남은 답변: 모든 질문 해결 완료),잠시 휴식,-0.010833331,0.0,0.010833331383764744
Loss Function 관련 실무 경험 -> [상세 경험] 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지? (남은 답변: 모든 질문 해결 완료),확률 예측에서 MSE Loss 미 사용 이유,-0.02061901,0.0,0.02061901055276394
Loss Function 관련 실무 경험 -> [상세 경험] 근데 기억 진짜 안나는데 어떡하지? (남은 답변: 상세 경험),BCE Loss 설명,-0.031102573,0.0,0.03110257349908352
Loss Function 관련 실무 경험 -> [상세 경험] 근데 기억 진짜 안나는데 어떡하지? (남은 답변: 상세 경험),LLM Fine-Tuning 의 PEFT,0.015551925,0.0,0.015551924705505371
Loss Function 관련 실무 경험 -> [상세 경험] 근데 기억 진짜 안나는데 어떡하지? (남은 답변: 상세 경험),LoRA,-0.040805202,0.0,0.04080520197749138
Loss Function 관련 실무 경험 -> [상세 경험] 근데 기억 진짜 안나는데 어떡하지? (남은 답변: 상세 경험),LoRA 와 QLoRA 의 차이,-0.0056603043,0.0,0.005660304334014654
Loss Function 관련 실무 경험 -> [상세 경험] 근데 기억 진짜 안나는데 어떡하지? (남은 답변: 상세 경험),Loss Function 관련 실무 경험,-0.0066149854,0.0,0.0066149854101240635
Loss Function 관련 실무 경험 -> [상세 경험] 근데 기억 진짜 안나는데 어떡하지? (남은 답변: 상세 경험),Loss Function 예시,0.012682371,0.0,0.012682370841503143
Loss Function 관련 실무 경험 -> [상세 경험] 근데 기억 진짜 안나는데 어떡하지? (남은 답변: 상세 경험),Loss Function 정의,0.014052014,0.0,0.014052013866603374
Loss Function 관련 실무 경험 -> [상세 경험] 근데 기억 진짜 안나는데 어떡하지? (남은 답변: 상세 경험),MBTI / 좋아하는 아이돌,-0.022437876,0.0,0.02243787609040737
Loss Function 관련 실무 경험 -> [상세 경험] 근데 기억 진짜 안나는데 어떡하지? (남은 답변: 상세 경험),MSE Loss 설명,0.011623186,0.0,0.011623186059296131
Loss Function 관련 실무 경험 -> [상세 경험] 근데 기억 진짜 안나는데 어떡하지? (남은 답변: 상세 경험),MSE Loss 용도,0.016438529,0.0,0.016438528895378113
Loss Function 관련 실무 경험 -> [상세 경험] 근데 기억 진짜 안나는데 어떡하지? (남은 답변: 상세 경험),"Multi-Class, Multi-Label 중 BCE 가 좋은 task",0.009140358,0.0,0.009140358306467533
Loss Function 관련 실무 경험 -> [상세 경험] 근데 기억 진짜 안나는데 어떡하지? (남은 답변: 상세 경험),Multi-Label 에서 CE + Softmax 적용 문제점,0.011363432,0.0,0.011363431811332703
Loss Function 관련 실무 경험 -> [상세 경험] 근데 기억 진짜 안나는데 어떡하지? (남은 답변: 상세 경험),PEFT 방법 5가지,-0.017872809,0.0,0.017872808501124382
Loss Function 관련 실무 경험 -> [상세 경험] 근데 기억 진짜 안나는데 어떡하지? (남은 답변: 상세 경험),거대 언어 모델 정의,-0.028378189,0.0,0.028378188610076904
Loss Function 관련 실무 경험 -> [상세 경험] 근데 기억 진짜 안나는데 어떡하지? (남은 답변: 상세 경험),마지막 할 말,-0.03613644,0.0,0.0361364409327507
Loss Function 관련 실무 경험 -> [상세 경험] 근데 기억 진짜 안나는데 어떡하지? (남은 답변: 상세 경험),면접 시작 인사,0.014670841,0.0,0.01467084139585495
Loss Function 관련 실무 경험 -> [상세 경험] 근데 기억 진짜 안나는데 어떡하지? (남은 답변: 상세 경험),면접 종료,0.025503775,0.0,0.025503775104880333
Loss Function 관련 실무 경험 -> [상세 경험] 근데 기억 진짜 안나는데 어떡하지? (남은 답변: 상세 경험),"인공지능, 머신러닝, 딥러닝 차이",-0.004827978,0.0,0.00482797808945179
Loss Function 관련 실무 경험 -> [상세 경험] 근데 기억 진짜 안나는데 어떡하지? (남은 답변: 상세 경험),잠시 휴식,0.8549484,1.0,0.14505159854888916
Loss Function 관련 실무 경험 -> [상세 경험] 근데 기억 진짜 안나는데 어떡하지? (남은 답변: 상세 경험),확률 예측에서 MSE Loss 미 사용 이유,0.015828291,0.0,0.015828290954232216
Loss Function 관련 실무 경험 -> [기본 경험] 나 아직 실무 경험 없는데 ㅠㅠ (남은 답변: 모든 질문 해결 완료),BCE Loss 설명,-0.01422643,0.0,0.014226430095732212
Loss Function 관련 실무 경험 -> [기본 경험] 나 아직 실무 경험 없는데 ㅠㅠ (남은 답변: 모든 질문 해결 완료),LLM Fine-Tuning 의 PEFT,-0.016885113,0.0,0.01688511297106743
Loss Function 관련 실무 경험 -> [기본 경험] 나 아직 실무 경험 없는데 ㅠㅠ (남은 답변: 모든 질문 해결 완료),LoRA,-0.008845185,0.0,0.008845184929668903
Loss Function 관련 실무 경험 -> [기본 경험] 나 아직 실무 경험 없는데 ㅠㅠ (남은 답변: 모든 질문 해결 완료),LoRA 와 QLoRA 의 차이,0.0054810094,0.0,0.005481009371578693
Loss Function 관련 실무 경험 -> [기본 경험] 나 아직 실무 경험 없는데 ㅠㅠ (남은 답변: 모든 질문 해결 완료),Loss Function 관련 실무 경험,-0.059425775,0.0,0.059425774961709976
Loss Function 관련 실무 경험 -> [기본 경험] 나 아직 실무 경험 없는데 ㅠㅠ (남은 답변: 모든 질문 해결 완료),Loss Function 예시,-0.0074588982,0.0,0.007458898238837719
Loss Function 관련 실무 경험 -> [기본 경험] 나 아직 실무 경험 없는데 ㅠㅠ (남은 답변: 모든 질문 해결 완료),Loss Function 정의,-0.019249616,0.0,0.01924961619079113
Loss Function 관련 실무 경험 -> [기본 경험] 나 아직 실무 경험 없는데 ㅠㅠ (남은 답변: 모든 질문 해결 완료),MBTI / 좋아하는 아이돌,0.92997444,1.0,0.07002556324005127
Loss Function 관련 실무 경험 -> [기본 경험] 나 아직 실무 경험 없는데 ㅠㅠ (남은 답변: 모든 질문 해결 완료),MSE Loss 설명,0.0105089,0.0,0.010508899576961994
Loss Function 관련 실무 경험 -> [기본 경험] 나 아직 실무 경험 없는데 ㅠㅠ (남은 답변: 모든 질문 해결 완료),MSE Loss 용도,0.036800414,0.0,0.03680041432380676
Loss Function 관련 실무 경험 -> [기본 경험] 나 아직 실무 경험 없는데 ㅠㅠ (남은 답변: 모든 질문 해결 완료),"Multi-Class, Multi-Label 중 BCE 가 좋은 task",0.010456421,0.0,0.010456421412527561
Loss Function 관련 실무 경험 -> [기본 경험] 나 아직 실무 경험 없는데 ㅠㅠ (남은 답변: 모든 질문 해결 완료),Multi-Label 에서 CE + Softmax 적용 문제점,0.0039889403,0.0,0.003988940268754959
Loss Function 관련 실무 경험 -> [기본 경험] 나 아직 실무 경험 없는데 ㅠㅠ (남은 답변: 모든 질문 해결 완료),PEFT 방법 5가지,-0.041126255,0.0,0.04112625494599342
Loss Function 관련 실무 경험 -> [기본 경험] 나 아직 실무 경험 없는데 ㅠㅠ (남은 답변: 모든 질문 해결 완료),거대 언어 모델 정의,0.0029762713,0.0,0.0029762713238596916
Loss Function 관련 실무 경험 -> [기본 경험] 나 아직 실무 경험 없는데 ㅠㅠ (남은 답변: 모든 질문 해결 완료),마지막 할 말,-0.047796607,0.0,0.04779660701751709
Loss Function 관련 실무 경험 -> [기본 경험] 나 아직 실무 경험 없는데 ㅠㅠ (남은 답변: 모든 질문 해결 완료),면접 시작 인사,0.018552475,0.0,0.018552474677562714
Loss Function 관련 실무 경험 -> [기본 경험] 나 아직 실무 경험 없는데 ㅠㅠ (남은 답변: 모든 질문 해결 완료),면접 종료,-0.018143667,0.0,0.01814366690814495
Loss Function 관련 실무 경험 -> [기본 경험] 나 아직 실무 경험 없는데 ㅠㅠ (남은 답변: 모든 질문 해결 완료),"인공지능, 머신러닝, 딥러닝 차이",0.0030013102,0.0,0.0030013101641088724
Loss Function 관련 실무 경험 -> [기본 경험] 나 아직 실무 경험 없는데 ㅠㅠ (남은 답변: 모든 질문 해결 완료),잠시 휴식,0.017721452,0.0,0.017721451818943024
Loss Function 관련 실무 경험 -> [기본 경험] 나 아직 실무 경험 없는데 ㅠㅠ (남은 답변: 모든 질문 해결 완료),확률 예측에서 MSE Loss 미 사용 이유,0.0010182161,0.0,0.001018216134980321
"Loss Function 관련 실무 경험 -> [기본 경험] 대답하기 싫은데 힝 (남은 답변: 기본 경험, 상세 경험)",BCE Loss 설명,-0.010753591,0.0,0.010753590613603592
"Loss Function 관련 실무 경험 -> [기본 경험] 대답하기 싫은데 힝 (남은 답변: 기본 경험, 상세 경험)",LLM Fine-Tuning 의 PEFT,0.025608918,0.0,0.025608917698264122
"Loss Function 관련 실무 경험 -> [기본 경험] 대답하기 싫은데 힝 (남은 답변: 기본 경험, 상세 경험)",LoRA,-0.037611008,0.0,0.03761100769042969
"Loss Function 관련 실무 경험 -> [기본 경험] 대답하기 싫은데 힝 (남은 답변: 기본 경험, 상세 경험)",LoRA 와 QLoRA 의 차이,0.006376916,0.0,0.006376916076987982
"Loss Function 관련 실무 경험 -> [기본 경험] 대답하기 싫은데 힝 (남은 답변: 기본 경험, 상세 경험)",Loss Function 관련 실무 경험,-0.009990461,0.0,0.00999046117067337
"Loss Function 관련 실무 경험 -> [기본 경험] 대답하기 싫은데 힝 (남은 답변: 기본 경험, 상세 경험)",Loss Function 예시,0.0042716144,0.0,0.0042716143652796745
"Loss Function 관련 실무 경험 -> [기본 경험] 대답하기 싫은데 힝 (남은 답변: 기본 경험, 상세 경험)",Loss Function 정의,0.026250878,0.0,0.02625087834894657
"Loss Function 관련 실무 경험 -> [기본 경험] 대답하기 싫은데 힝 (남은 답변: 기본 경험, 상세 경험)",MBTI / 좋아하는 아이돌,-0.039269168,0.0,0.03926916792988777
"Loss Function 관련 실무 경험 -> [기본 경험] 대답하기 싫은데 힝 (남은 답변: 기본 경험, 상세 경험)",MSE Loss 설명,0.0173197,0.0,0.017319699749350548
"Loss Function 관련 실무 경험 -> [기본 경험] 대답하기 싫은데 힝 (남은 답변: 기본 경험, 상세 경험)",MSE Loss 용도,0.00035196968,0.0,0.0003519696765579283
"Loss Function 관련 실무 경험 -> [기본 경험] 대답하기 싫은데 힝 (남은 답변: 기본 경험, 상세 경험)","Multi-Class, Multi-Label 중 BCE 가 좋은 task",0.00515921,0.0,0.005159209948033094
"Loss Function 관련 실무 경험 -> [기본 경험] 대답하기 싫은데 힝 (남은 답변: 기본 경험, 상세 경험)",Multi-Label 에서 CE + Softmax 적용 문제점,-0.00058505934,0.0,0.0005850593443028629
"Loss Function 관련 실무 경험 -> [기본 경험] 대답하기 싫은데 힝 (남은 답변: 기본 경험, 상세 경험)",PEFT 방법 5가지,-0.027778272,0.0,0.027778271585702896
"Loss Function 관련 실무 경험 -> [기본 경험] 대답하기 싫은데 힝 (남은 답변: 기본 경험, 상세 경험)",거대 언어 모델 정의,-0.022423986,0.0,0.022423986345529556
"Loss Function 관련 실무 경험 -> [기본 경험] 대답하기 싫은데 힝 (남은 답변: 기본 경험, 상세 경험)",마지막 할 말,-0.042296488,0.0,0.04229648783802986
"Loss Function 관련 실무 경험 -> [기본 경험] 대답하기 싫은데 힝 (남은 답변: 기본 경험, 상세 경험)",면접 시작 인사,0.022980383,0.0,0.022980382665991783
"Loss Function 관련 실무 경험 -> [기본 경험] 대답하기 싫은데 힝 (남은 답변: 기본 경험, 상세 경험)",면접 종료,-0.0020782596,0.0,0.0020782595966011286
"Loss Function 관련 실무 경험 -> [기본 경험] 대답하기 싫은데 힝 (남은 답변: 기본 경험, 상세 경험)","인공지능, 머신러닝, 딥러닝 차이",0.0012960649,0.0,0.0012960649328306317
"Loss Function 관련 실무 경험 -> [기본 경험] 대답하기 싫은데 힝 (남은 답변: 기본 경험, 상세 경험)",잠시 휴식,0.8595596,1.0,0.14044040441513062
"Loss Function 관련 실무 경험 -> [기본 경험] 대답하기 싫은데 힝 (남은 답변: 기본 경험, 상세 경험)",확률 예측에서 MSE Loss 미 사용 이유,0.017217876,0.0,0.01721787638962269
Loss Function 관련 실무 경험 -> [기본 경험] 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ (남은 답변: 상세 경험),BCE Loss 설명,0.0061043547,0.0,0.0061043547466397285
Loss Function 관련 실무 경험 -> [기본 경험] 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ (남은 답변: 상세 경험),LLM Fine-Tuning 의 PEFT,-0.008711717,0.0,0.008711717091500759
Loss Function 관련 실무 경험 -> [기본 경험] 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ (남은 답변: 상세 경험),LoRA,0.0035346178,0.0,0.0035346178337931633
Loss Function 관련 실무 경험 -> [기본 경험] 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ (남은 답변: 상세 경험),LoRA 와 QLoRA 의 차이,-0.013321644,0.0,0.013321643695235252
Loss Function 관련 실무 경험 -> [기본 경험] 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ (남은 답변: 상세 경험),Loss Function 관련 실무 경험,0.967618,1.0,0.03238201141357422
Loss Function 관련 실무 경험 -> [기본 경험] 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ (남은 답변: 상세 경험),Loss Function 예시,0.0024503418,0.0,0.002450341824442148
Loss Function 관련 실무 경험 -> [기본 경험] 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ (남은 답변: 상세 경험),Loss Function 정의,-0.020449052,0.0,0.02044905163347721
Loss Function 관련 실무 경험 -> [기본 경험] 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ (남은 답변: 상세 경험),MBTI / 좋아하는 아이돌,-0.027293352,0.0,0.027293352410197258
Loss Function 관련 실무 경험 -> [기본 경험] 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ (남은 답변: 상세 경험),MSE Loss 설명,-0.0067297546,0.0,0.0067297546193003654
Loss Function 관련 실무 경험 -> [기본 경험] 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ (남은 답변: 상세 경험),MSE Loss 용도,-0.0034950448,0.0,0.0034950447734445333
Loss Function 관련 실무 경험 -> [기본 경험] 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ (남은 답변: 상세 경험),"Multi-Class, Multi-Label 중 BCE 가 좋은 task",-0.013257672,0.0,0.01325767207890749
Loss Function 관련 실무 경험 -> [기본 경험] 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ (남은 답변: 상세 경험),Multi-Label 에서 CE + Softmax 적용 문제점,-0.0038880934,0.0,0.003888093400746584
Loss Function 관련 실무 경험 -> [기본 경험] 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ (남은 답변: 상세 경험),PEFT 방법 5가지,-0.00011808789,0.0,0.00011808788985945284
Loss Function 관련 실무 경험 -> [기본 경험] 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ (남은 답변: 상세 경험),거대 언어 모델 정의,-0.007093857,0.0,0.007093857042491436
Loss Function 관련 실무 경험 -> [기본 경험] 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ (남은 답변: 상세 경험),마지막 할 말,-0.0001576364,0.0,0.00015763640112709254
Loss Function 관련 실무 경험 -> [기본 경험] 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ (남은 답변: 상세 경험),면접 시작 인사,-0.0050461926,0.0,0.005046192556619644
Loss Function 관련 실무 경험 -> [기본 경험] 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ (남은 답변: 상세 경험),면접 종료,0.0069431695,0.0,0.006943169515579939
Loss Function 관련 실무 경험 -> [기본 경험] 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ (남은 답변: 상세 경험),"인공지능, 머신러닝, 딥러닝 차이",0.0028765008,0.0,0.0028765008319169283
Loss Function 관련 실무 경험 -> [기본 경험] 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ (남은 답변: 상세 경험),잠시 휴식,0.059900917,0.0,0.0599009171128273
Loss Function 관련 실무 경험 -> [기본 경험] 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ (남은 답변: 상세 경험),확률 예측에서 MSE Loss 미 사용 이유,-0.008015472,0.0,0.008015472441911697
Loss Function 관련 실무 경험 -> [상세 경험] 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어 (남은 답변: 모든 질문 해결 완료),BCE Loss 설명,0.05837086,0.0,0.05837085843086243
Loss Function 관련 실무 경험 -> [상세 경험] 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어 (남은 답변: 모든 질문 해결 완료),LLM Fine-Tuning 의 PEFT,0.0023856538,0.0,0.0023856537882238626
Loss Function 관련 실무 경험 -> [상세 경험] 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어 (남은 답변: 모든 질문 해결 완료),LoRA,-0.025823034,0.0,0.025823034346103668
Loss Function 관련 실무 경험 -> [상세 경험] 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어 (남은 답변: 모든 질문 해결 완료),LoRA 와 QLoRA 의 차이,0.0041818283,0.0,0.00418182834982872
Loss Function 관련 실무 경험 -> [상세 경험] 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어 (남은 답변: 모든 질문 해결 완료),Loss Function 관련 실무 경험,0.00826483,0.0,0.008264830335974693
Loss Function 관련 실무 경험 -> [상세 경험] 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어 (남은 답변: 모든 질문 해결 완료),Loss Function 예시,-0.020486815,0.0,0.02048681490123272
Loss Function 관련 실무 경험 -> [상세 경험] 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어 (남은 답변: 모든 질문 해결 완료),Loss Function 정의,-0.025722954,0.0,0.02572295442223549
Loss Function 관련 실무 경험 -> [상세 경험] 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어 (남은 답변: 모든 질문 해결 완료),MBTI / 좋아하는 아이돌,0.8948963,1.0,0.10510367155075073
Loss Function 관련 실무 경험 -> [상세 경험] 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어 (남은 답변: 모든 질문 해결 완료),MSE Loss 설명,-0.047317665,0.0,0.047317665070295334
Loss Function 관련 실무 경험 -> [상세 경험] 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어 (남은 답변: 모든 질문 해결 완료),MSE Loss 용도,0.044043235,0.0,0.04404323548078537
Loss Function 관련 실무 경험 -> [상세 경험] 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어 (남은 답변: 모든 질문 해결 완료),"Multi-Class, Multi-Label 중 BCE 가 좋은 task",0.0039324784,0.0,0.003932478372007608
Loss Function 관련 실무 경험 -> [상세 경험] 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어 (남은 답변: 모든 질문 해결 완료),Multi-Label 에서 CE + Softmax 적용 문제점,0.002252825,0.0,0.0022528250701725483
Loss Function 관련 실무 경험 -> [상세 경험] 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어 (남은 답변: 모든 질문 해결 완료),PEFT 방법 5가지,0.015590198,0.0,0.015590198338031769
Loss Function 관련 실무 경험 -> [상세 경험] 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어 (남은 답변: 모든 질문 해결 완료),거대 언어 모델 정의,-0.022958828,0.0,0.022958828136324883
Loss Function 관련 실무 경험 -> [상세 경험] 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어 (남은 답변: 모든 질문 해결 완료),마지막 할 말,0.013454161,0.0,0.013454160653054714
Loss Function 관련 실무 경험 -> [상세 경험] 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어 (남은 답변: 모든 질문 해결 완료),면접 시작 인사,0.018902501,0.0,0.018902501091361046
Loss Function 관련 실무 경험 -> [상세 경험] 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어 (남은 답변: 모든 질문 해결 완료),면접 종료,-0.018609723,0.0,0.018609723076224327
Loss Function 관련 실무 경험 -> [상세 경험] 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어 (남은 답변: 모든 질문 해결 완료),"인공지능, 머신러닝, 딥러닝 차이",-0.02700208,0.0,0.027002079412341118
Loss Function 관련 실무 경험 -> [상세 경험] 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어 (남은 답변: 모든 질문 해결 완료),잠시 휴식,-0.12502594,0.0,0.1250259429216385
Loss Function 관련 실무 경험 -> [상세 경험] 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어 (남은 답변: 모든 질문 해결 완료),확률 예측에서 MSE Loss 미 사용 이유,-0.038262334,0.0,0.03826233372092247
Loss Function 관련 실무 경험 -> [상세 경험] 기억 진짜 안 나는데 가물가물하다 (남은 답변: 상세 경험),BCE Loss 설명,-0.027362946,0.0,0.027362946420907974
Loss Function 관련 실무 경험 -> [상세 경험] 기억 진짜 안 나는데 가물가물하다 (남은 답변: 상세 경험),LLM Fine-Tuning 의 PEFT,0.028713644,0.0,0.028713643550872803
Loss Function 관련 실무 경험 -> [상세 경험] 기억 진짜 안 나는데 가물가물하다 (남은 답변: 상세 경험),LoRA,-0.018686932,0.0,0.0186869315803051
Loss Function 관련 실무 경험 -> [상세 경험] 기억 진짜 안 나는데 가물가물하다 (남은 답변: 상세 경험),LoRA 와 QLoRA 의 차이,-0.01079814,0.0,0.010798140428960323
Loss Function 관련 실무 경험 -> [상세 경험] 기억 진짜 안 나는데 가물가물하다 (남은 답변: 상세 경험),Loss Function 관련 실무 경험,0.009572362,0.0,0.009572361595928669
Loss Function 관련 실무 경험 -> [상세 경험] 기억 진짜 안 나는데 가물가물하다 (남은 답변: 상세 경험),Loss Function 예시,0.0042656767,0.0,0.0042656767182052135
Loss Function 관련 실무 경험 -> [상세 경험] 기억 진짜 안 나는데 가물가물하다 (남은 답변: 상세 경험),Loss Function 정의,0.014573447,0.0,0.014573447406291962
Loss Function 관련 실무 경험 -> [상세 경험] 기억 진짜 안 나는데 가물가물하다 (남은 답변: 상세 경험),MBTI / 좋아하는 아이돌,-0.025847344,0.0,0.02584734372794628
Loss Function 관련 실무 경험 -> [상세 경험] 기억 진짜 안 나는데 가물가물하다 (남은 답변: 상세 경험),MSE Loss 설명,0.02321802,0.0,0.023218020796775818
Loss Function 관련 실무 경험 -> [상세 경험] 기억 진짜 안 나는데 가물가물하다 (남은 답변: 상세 경험),MSE Loss 용도,0.007479695,0.0,0.00747969513759017
Loss Function 관련 실무 경험 -> [상세 경험] 기억 진짜 안 나는데 가물가물하다 (남은 답변: 상세 경험),"Multi-Class, Multi-Label 중 BCE 가 좋은 task",0.015279301,0.0,0.015279301442205906
Loss Function 관련 실무 경험 -> [상세 경험] 기억 진짜 안 나는데 가물가물하다 (남은 답변: 상세 경험),Multi-Label 에서 CE + Softmax 적용 문제점,0.0068325344,0.0,0.006832534447312355
Loss Function 관련 실무 경험 -> [상세 경험] 기억 진짜 안 나는데 가물가물하다 (남은 답변: 상세 경험),PEFT 방법 5가지,-0.023161042,0.0,0.023161042481660843
Loss Function 관련 실무 경험 -> [상세 경험] 기억 진짜 안 나는데 가물가물하다 (남은 답변: 상세 경험),거대 언어 모델 정의,-0.025306053,0.0,0.025306053459644318
Loss Function 관련 실무 경험 -> [상세 경험] 기억 진짜 안 나는데 가물가물하다 (남은 답변: 상세 경험),마지막 할 말,-0.03722101,0.0,0.03722101077437401
Loss Function 관련 실무 경험 -> [상세 경험] 기억 진짜 안 나는데 가물가물하다 (남은 답변: 상세 경험),면접 시작 인사,0.017515074,0.0,0.017515074461698532
Loss Function 관련 실무 경험 -> [상세 경험] 기억 진짜 안 나는데 가물가물하다 (남은 답변: 상세 경험),면접 종료,0.00527984,0.0,0.005279839970171452
Loss Function 관련 실무 경험 -> [상세 경험] 기억 진짜 안 나는데 가물가물하다 (남은 답변: 상세 경험),"인공지능, 머신러닝, 딥러닝 차이",-0.009517139,0.0,0.009517138823866844
Loss Function 관련 실무 경험 -> [상세 경험] 기억 진짜 안 나는데 가물가물하다 (남은 답변: 상세 경험),잠시 휴식,0.8469544,1.0,0.15304559469223022
Loss Function 관련 실무 경험 -> [상세 경험] 기억 진짜 안 나는데 가물가물하다 (남은 답변: 상세 경험),확률 예측에서 MSE Loss 미 사용 이유,0.014172712,0.0,0.014172712340950966
Loss Function 관련 실무 경험 -> [기본 경험] 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ (남은 답변: 모든 질문 해결 완료),BCE Loss 설명,-0.0127273295,0.0,0.012727329507470131
Loss Function 관련 실무 경험 -> [기본 경험] 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ (남은 답변: 모든 질문 해결 완료),LLM Fine-Tuning 의 PEFT,-0.021520417,0.0,0.02152041718363762
Loss Function 관련 실무 경험 -> [기본 경험] 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ (남은 답변: 모든 질문 해결 완료),LoRA,-0.013612802,0.0,0.013612802140414715
Loss Function 관련 실무 경험 -> [기본 경험] 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ (남은 답변: 모든 질문 해결 완료),LoRA 와 QLoRA 의 차이,0.010352234,0.0,0.010352234356105328
Loss Function 관련 실무 경험 -> [기본 경험] 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ (남은 답변: 모든 질문 해결 완료),Loss Function 관련 실무 경험,-0.05363921,0.0,0.053639210760593414
Loss Function 관련 실무 경험 -> [기본 경험] 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ (남은 답변: 모든 질문 해결 완료),Loss Function 예시,-0.011453805,0.0,0.011453804560005665
Loss Function 관련 실무 경험 -> [기본 경험] 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ (남은 답변: 모든 질문 해결 완료),Loss Function 정의,-0.016064085,0.0,0.016064085066318512
Loss Function 관련 실무 경험 -> [기본 경험] 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ (남은 답변: 모든 질문 해결 완료),MBTI / 좋아하는 아이돌,0.9301758,1.0,0.06982421875
Loss Function 관련 실무 경험 -> [기본 경험] 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ (남은 답변: 모든 질문 해결 완료),MSE Loss 설명,-0.0035995292,0.0,0.003599529154598713
Loss Function 관련 실무 경험 -> [기본 경험] 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ (남은 답변: 모든 질문 해결 완료),MSE Loss 용도,0.032886975,0.0,0.03288697451353073
Loss Function 관련 실무 경험 -> [기본 경험] 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ (남은 답변: 모든 질문 해결 완료),"Multi-Class, Multi-Label 중 BCE 가 좋은 task",0.010676134,0.0,0.010676134377717972
Loss Function 관련 실무 경험 -> [기본 경험] 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ (남은 답변: 모든 질문 해결 완료),Multi-Label 에서 CE + Softmax 적용 문제점,0.0043634246,0.0,0.004363424610346556
Loss Function 관련 실무 경험 -> [기본 경험] 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ (남은 답변: 모든 질문 해결 완료),PEFT 방법 5가지,-0.047551252,0.0,0.04755125194787979
Loss Function 관련 실무 경험 -> [기본 경험] 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ (남은 답변: 모든 질문 해결 완료),거대 언어 모델 정의,-0.0024768468,0.0,0.0024768467992544174
Loss Function 관련 실무 경험 -> [기본 경험] 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ (남은 답변: 모든 질문 해결 완료),마지막 할 말,-0.045810804,0.0,0.04581080377101898
Loss Function 관련 실무 경험 -> [기본 경험] 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ (남은 답변: 모든 질문 해결 완료),면접 시작 인사,0.01813063,0.0,0.018130630254745483
Loss Function 관련 실무 경험 -> [기본 경험] 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ (남은 답변: 모든 질문 해결 완료),면접 종료,-0.009810364,0.0,0.009810363873839378
Loss Function 관련 실무 경험 -> [기본 경험] 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ (남은 답변: 모든 질문 해결 완료),"인공지능, 머신러닝, 딥러닝 차이",0.0029647856,0.0,0.0029647855553776026
Loss Function 관련 실무 경험 -> [기본 경험] 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ (남은 답변: 모든 질문 해결 완료),잠시 휴식,0.009374006,0.0,0.00937400572001934
Loss Function 관련 실무 경험 -> [기본 경험] 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ (남은 답변: 모든 질문 해결 완료),확률 예측에서 MSE Loss 미 사용 이유,-0.0047019264,0.0,0.004701926372945309
"Loss Function 관련 실무 경험 -> [기본 경험] 이걸 왜 알려줘야 하지 (남은 답변: 기본 경험, 상세 경험)",BCE Loss 설명,-0.024687272,0.0,0.0246872715651989
"Loss Function 관련 실무 경험 -> [기본 경험] 이걸 왜 알려줘야 하지 (남은 답변: 기본 경험, 상세 경험)",LLM Fine-Tuning 의 PEFT,0.026224013,0.0,0.026224013417959213
"Loss Function 관련 실무 경험 -> [기본 경험] 이걸 왜 알려줘야 하지 (남은 답변: 기본 경험, 상세 경험)",LoRA,-0.04562927,0.0,0.04562927037477493
"Loss Function 관련 실무 경험 -> [기본 경험] 이걸 왜 알려줘야 하지 (남은 답변: 기본 경험, 상세 경험)",LoRA 와 QLoRA 의 차이,0.002145858,0.0,0.0021458580158650875
"Loss Function 관련 실무 경험 -> [기본 경험] 이걸 왜 알려줘야 하지 (남은 답변: 기본 경험, 상세 경험)",Loss Function 관련 실무 경험,0.051608168,0.0,0.051608167588710785
"Loss Function 관련 실무 경험 -> [기본 경험] 이걸 왜 알려줘야 하지 (남은 답변: 기본 경험, 상세 경험)",Loss Function 예시,-0.012368194,0.0,0.012368193827569485
"Loss Function 관련 실무 경험 -> [기본 경험] 이걸 왜 알려줘야 하지 (남은 답변: 기본 경험, 상세 경험)",Loss Function 정의,0.018248294,0.0,0.018248293548822403
"Loss Function 관련 실무 경험 -> [기본 경험] 이걸 왜 알려줘야 하지 (남은 답변: 기본 경험, 상세 경험)",MBTI / 좋아하는 아이돌,0.008147619,0.0,0.008147618733346462
"Loss Function 관련 실무 경험 -> [기본 경험] 이걸 왜 알려줘야 하지 (남은 답변: 기본 경험, 상세 경험)",MSE Loss 설명,-0.0020138707,0.0,0.0020138707477599382
"Loss Function 관련 실무 경험 -> [기본 경험] 이걸 왜 알려줘야 하지 (남은 답변: 기본 경험, 상세 경험)",MSE Loss 용도,-0.008159533,0.0,0.008159533143043518
"Loss Function 관련 실무 경험 -> [기본 경험] 이걸 왜 알려줘야 하지 (남은 답변: 기본 경험, 상세 경험)","Multi-Class, Multi-Label 중 BCE 가 좋은 task",-0.0045883874,0.0,0.004588387440890074
"Loss Function 관련 실무 경험 -> [기본 경험] 이걸 왜 알려줘야 하지 (남은 답변: 기본 경험, 상세 경험)",Multi-Label 에서 CE + Softmax 적용 문제점,-0.004492841,0.0,0.004492841195315123
"Loss Function 관련 실무 경험 -> [기본 경험] 이걸 왜 알려줘야 하지 (남은 답변: 기본 경험, 상세 경험)",PEFT 방법 5가지,-0.009637921,0.0,0.009637921117246151
"Loss Function 관련 실무 경험 -> [기본 경험] 이걸 왜 알려줘야 하지 (남은 답변: 기본 경험, 상세 경험)",거대 언어 모델 정의,-0.026302796,0.0,0.026302795857191086
"Loss Function 관련 실무 경험 -> [기본 경험] 이걸 왜 알려줘야 하지 (남은 답변: 기본 경험, 상세 경험)",마지막 할 말,-0.03419604,0.0,0.03419604152441025
"Loss Function 관련 실무 경험 -> [기본 경험] 이걸 왜 알려줘야 하지 (남은 답변: 기본 경험, 상세 경험)",면접 시작 인사,0.042803526,0.0,0.04280352592468262
"Loss Function 관련 실무 경험 -> [기본 경험] 이걸 왜 알려줘야 하지 (남은 답변: 기본 경험, 상세 경험)",면접 종료,-0.0025010572,0.0,0.0025010572280734777
"Loss Function 관련 실무 경험 -> [기본 경험] 이걸 왜 알려줘야 하지 (남은 답변: 기본 경험, 상세 경험)","인공지능, 머신러닝, 딥러닝 차이",-0.007924006,0.0,0.007924006320536137
"Loss Function 관련 실무 경험 -> [기본 경험] 이걸 왜 알려줘야 하지 (남은 답변: 기본 경험, 상세 경험)",잠시 휴식,0.83639854,1.0,0.16360145807266235
"Loss Function 관련 실무 경험 -> [기본 경험] 이걸 왜 알려줘야 하지 (남은 답변: 기본 경험, 상세 경험)",확률 예측에서 MSE Loss 미 사용 이유,0.021261368,0.0,0.021261367946863174
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야! (남은 답변: 모든 질문 해결 완료),BCE Loss 설명,-0.02173595,0.0,0.02173594944179058
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야! (남은 답변: 모든 질문 해결 완료),LLM Fine-Tuning 의 PEFT,-0.0014315683,0.0,0.00143156829290092
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야! (남은 답변: 모든 질문 해결 완료),LoRA,-0.046320226,0.0,0.04632022604346275
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야! (남은 답변: 모든 질문 해결 완료),LoRA 와 QLoRA 의 차이,0.016231535,0.0,0.016231535002589226
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야! (남은 답변: 모든 질문 해결 완료),Loss Function 관련 실무 경험,-0.017555837,0.0,0.017555836588144302
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야! (남은 답변: 모든 질문 해결 완료),Loss Function 예시,-0.008328466,0.0,0.00832846574485302
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야! (남은 답변: 모든 질문 해결 완료),Loss Function 정의,-0.011064347,0.0,0.011064346879720688
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야! (남은 답변: 모든 질문 해결 완료),MBTI / 좋아하는 아이돌,-0.004069067,0.0,0.004069067072123289
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야! (남은 답변: 모든 질문 해결 완료),MSE Loss 설명,-0.018586198,0.0,0.01858619786798954
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야! (남은 답변: 모든 질문 해결 완료),MSE Loss 용도,0.036797933,0.0,0.03679793328046799
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야! (남은 답변: 모든 질문 해결 완료),"Multi-Class, Multi-Label 중 BCE 가 좋은 task",-0.004861779,0.0,0.004861779045313597
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야! (남은 답변: 모든 질문 해결 완료),Multi-Label 에서 CE + Softmax 적용 문제점,0.023370085,0.0,0.023370085284113884
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야! (남은 답변: 모든 질문 해결 완료),PEFT 방법 5가지,0.009982134,0.0,0.009982134215533733
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야! (남은 답변: 모든 질문 해결 완료),거대 언어 모델 정의,-0.003224124,0.0,0.0032241239678114653
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야! (남은 답변: 모든 질문 해결 완료),마지막 할 말,-0.027505165,0.0,0.027505164965987206
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야! (남은 답변: 모든 질문 해결 완료),면접 시작 인사,0.0027867837,0.0,0.0027867837343364954
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야! (남은 답변: 모든 질문 해결 완료),면접 종료,0.041210152,0.0,0.041210152208805084
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야! (남은 답변: 모든 질문 해결 완료),"인공지능, 머신러닝, 딥러닝 차이",-0.0012410608,0.0,0.001241060788743198
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야! (남은 답변: 모든 질문 해결 완료),잠시 휴식,0.9024542,1.0,0.0975458025932312
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야! (남은 답변: 모든 질문 해결 완료),확률 예측에서 MSE Loss 미 사용 이유,0.024912862,0.0,0.024912862107157707
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해 (남은 답변: 모든 질문 해결 완료),BCE Loss 설명,-0.013427203,0.0,0.013427202589809895
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해 (남은 답변: 모든 질문 해결 완료),LLM Fine-Tuning 의 PEFT,0.0077016405,0.0,0.007701640482991934
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해 (남은 답변: 모든 질문 해결 완료),LoRA,-0.025047638,0.0,0.02504763752222061
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해 (남은 답변: 모든 질문 해결 완료),LoRA 와 QLoRA 의 차이,0.05020199,0.0,0.05020198971033096
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해 (남은 답변: 모든 질문 해결 완료),Loss Function 관련 실무 경험,0.0007310429,0.0,0.000731042877305299
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해 (남은 답변: 모든 질문 해결 완료),Loss Function 예시,0.013323169,0.0,0.013323169201612473
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해 (남은 답변: 모든 질문 해결 완료),Loss Function 정의,0.00021498861,0.0,0.00021498861315194517
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해 (남은 답변: 모든 질문 해결 완료),MBTI / 좋아하는 아이돌,-0.058493365,0.0,0.05849336460232735
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해 (남은 답변: 모든 질문 해결 완료),MSE Loss 설명,-0.015641442,0.0,0.01564144156873226
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해 (남은 답변: 모든 질문 해결 완료),MSE Loss 용도,0.018165896,0.0,0.018165895715355873
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해 (남은 답변: 모든 질문 해결 완료),"Multi-Class, Multi-Label 중 BCE 가 좋은 task",0.0015454395,0.0,0.001545439474284649
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해 (남은 답변: 모든 질문 해결 완료),Multi-Label 에서 CE + Softmax 적용 문제점,0.012032807,0.0,0.012032806873321533
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해 (남은 답변: 모든 질문 해결 완료),PEFT 방법 5가지,-0.012226547,0.0,0.012226547114551067
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해 (남은 답변: 모든 질문 해결 완료),거대 언어 모델 정의,0.018074472,0.0,0.01807447150349617
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해 (남은 답변: 모든 질문 해결 완료),마지막 할 말,-0.028420908,0.0,0.028420908376574516
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해 (남은 답변: 모든 질문 해결 완료),면접 시작 인사,-0.0020488803,0.0,0.0020488803274929523
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해 (남은 답변: 모든 질문 해결 완료),면접 종료,0.01453341,0.0,0.014533409848809242
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해 (남은 답변: 모든 질문 해결 완료),"인공지능, 머신러닝, 딥러닝 차이",0.0017386366,0.0,0.0017386366380378604
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해 (남은 답변: 모든 질문 해결 완료),잠시 휴식,0.90660167,1.0,0.0933983325958252
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해 (남은 답변: 모든 질문 해결 완료),확률 예측에서 MSE Loss 미 사용 이유,0.0036630325,0.0,0.0036630325485020876
잠시 휴식 -> 재미있는 이야기 해줄래? (남은 답변: 잠시 휴식),BCE Loss 설명,-0.014535161,0.0,0.01453516073524952
잠시 휴식 -> 재미있는 이야기 해줄래? (남은 답변: 잠시 휴식),LLM Fine-Tuning 의 PEFT,0.97948015,1.0,0.02051985263824463
잠시 휴식 -> 재미있는 이야기 해줄래? (남은 답변: 잠시 휴식),LoRA,0.0021724752,0.0,0.002172475215047598
잠시 휴식 -> 재미있는 이야기 해줄래? (남은 답변: 잠시 휴식),LoRA 와 QLoRA 의 차이,0.0022975365,0.0,0.0022975364699959755
잠시 휴식 -> 재미있는 이야기 해줄래? (남은 답변: 잠시 휴식),Loss Function 관련 실무 경험,-0.0040879524,0.0,0.004087952431291342
잠시 휴식 -> 재미있는 이야기 해줄래? (남은 답변: 잠시 휴식),Loss Function 예시,-0.021895718,0.0,0.021895717829465866
잠시 휴식 -> 재미있는 이야기 해줄래? (남은 답변: 잠시 휴식),Loss Function 정의,0.002011592,0.0,0.002011592034250498
잠시 휴식 -> 재미있는 이야기 해줄래? (남은 답변: 잠시 휴식),MBTI / 좋아하는 아이돌,-0.014932967,0.0,0.01493296679109335
잠시 휴식 -> 재미있는 이야기 해줄래? (남은 답변: 잠시 휴식),MSE Loss 설명,-0.00033095948,0.0,0.00033095947583206
잠시 휴식 -> 재미있는 이야기 해줄래? (남은 답변: 잠시 휴식),MSE Loss 용도,-0.005467486,0.0,0.005467486102133989
잠시 휴식 -> 재미있는 이야기 해줄래? (남은 답변: 잠시 휴식),"Multi-Class, Multi-Label 중 BCE 가 좋은 task",-0.0058201337,0.0,0.005820133723318577
잠시 휴식 -> 재미있는 이야기 해줄래? (남은 답변: 잠시 휴식),Multi-Label 에서 CE + Softmax 적용 문제점,-0.015332458,0.0,0.015332457609474659
잠시 휴식 -> 재미있는 이야기 해줄래? (남은 답변: 잠시 휴식),PEFT 방법 5가지,-0.00014937985,0.0,0.00014937984815333039
잠시 휴식 -> 재미있는 이야기 해줄래? (남은 답변: 잠시 휴식),거대 언어 모델 정의,-0.011366303,0.0,0.011366303078830242
잠시 휴식 -> 재미있는 이야기 해줄래? (남은 답변: 잠시 휴식),마지막 할 말,-0.009549048,0.0,0.00954904779791832
잠시 휴식 -> 재미있는 이야기 해줄래? (남은 답변: 잠시 휴식),면접 시작 인사,-0.03778484,0.0,0.037784840911626816
잠시 휴식 -> 재미있는 이야기 해줄래? (남은 답변: 잠시 휴식),면접 종료,0.028407406,0.0,0.02840740606188774
잠시 휴식 -> 재미있는 이야기 해줄래? (남은 답변: 잠시 휴식),"인공지능, 머신러닝, 딥러닝 차이",0.023011992,0.0,0.023011991754174232
잠시 휴식 -> 재미있는 이야기 해줄래? (남은 답변: 잠시 휴식),잠시 휴식,-0.015047975,0.0,0.01504797488451004
잠시 휴식 -> 재미있는 이야기 해줄래? (남은 답변: 잠시 휴식),확률 예측에서 MSE Loss 미 사용 이유,-0.019348849,0.0,0.01934884861111641
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야 (남은 답변: 모든 질문 해결 완료)",BCE Loss 설명,-0.0030441284,0.0,0.00304412841796875
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야 (남은 답변: 모든 질문 해결 완료)",LLM Fine-Tuning 의 PEFT,0.07447932,0.0,0.07447931915521622
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야 (남은 답변: 모든 질문 해결 완료)",LoRA,-0.023843003,0.0,0.023843003436923027
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야 (남은 답변: 모든 질문 해결 완료)",LoRA 와 QLoRA 의 차이,-0.0150602115,0.0,0.015060211531817913
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야 (남은 답변: 모든 질문 해결 완료)",Loss Function 관련 실무 경험,-0.0028537842,0.0,0.002853784244507551
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야 (남은 답변: 모든 질문 해결 완료)",Loss Function 예시,0.012363494,0.0,0.012363494373857975
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야 (남은 답변: 모든 질문 해결 완료)",Loss Function 정의,0.026395,0.0,0.026395000517368317
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야 (남은 답변: 모든 질문 해결 완료)",MBTI / 좋아하는 아이돌,-0.03388363,0.0,0.033883631229400635
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야 (남은 답변: 모든 질문 해결 완료)",MSE Loss 설명,0.008264168,0.0,0.008264168165624142
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야 (남은 답변: 모든 질문 해결 완료)",MSE Loss 용도,-0.020589814,0.0,0.020589813590049744
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야 (남은 답변: 모든 질문 해결 완료)","Multi-Class, Multi-Label 중 BCE 가 좋은 task",0.0035968018,0.0,0.0035968017764389515
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야 (남은 답변: 모든 질문 해결 완료)",Multi-Label 에서 CE + Softmax 적용 문제점,0.009313255,0.0,0.009313254617154598
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야 (남은 답변: 모든 질문 해결 완료)",PEFT 방법 5가지,0.9237993,1.0,0.07620072364807129
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야 (남은 답변: 모든 질문 해결 완료)",거대 언어 모델 정의,0.004346063,0.0,0.004346062894910574
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야 (남은 답변: 모든 질문 해결 완료)",마지막 할 말,0.01434852,0.0,0.014348519966006279
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야 (남은 답변: 모든 질문 해결 완료)",면접 시작 인사,0.02659547,0.0,0.026595469564199448
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야 (남은 답변: 모든 질문 해결 완료)",면접 종료,-0.008967697,0.0,0.008967696689069271
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야 (남은 답변: 모든 질문 해결 완료)","인공지능, 머신러닝, 딥러닝 차이",-0.027383061,0.0,0.02738306112587452
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야 (남은 답변: 모든 질문 해결 완료)",잠시 휴식,-0.020909585,0.0,0.020909585058689117
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야 (남은 답변: 모든 질문 해결 완료)",확률 예측에서 MSE Loss 미 사용 이유,-0.016713157,0.0,0.016713157296180725
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지? (남은 답변: LLM Fine-Tuning 의 PEFT),BCE Loss 설명,0.002510659,0.0,0.0025106589309871197
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지? (남은 답변: LLM Fine-Tuning 의 PEFT),LLM Fine-Tuning 의 PEFT,0.9827905,1.0,0.01720947027206421
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지? (남은 답변: LLM Fine-Tuning 의 PEFT),LoRA,0.0035518578,0.0,0.003551857778802514
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지? (남은 답변: LLM Fine-Tuning 의 PEFT),LoRA 와 QLoRA 의 차이,0.0077081793,0.0,0.007708179298788309
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지? (남은 답변: LLM Fine-Tuning 의 PEFT),Loss Function 관련 실무 경험,-0.007427833,0.0,0.007427833043038845
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지? (남은 답변: LLM Fine-Tuning 의 PEFT),Loss Function 예시,-0.0036226688,0.0,0.003622668795287609
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지? (남은 답변: LLM Fine-Tuning 의 PEFT),Loss Function 정의,-0.009759263,0.0,0.009759263135492802
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지? (남은 답변: LLM Fine-Tuning 의 PEFT),MBTI / 좋아하는 아이돌,-0.00026913488,0.0,0.0002691348781809211
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지? (남은 답변: LLM Fine-Tuning 의 PEFT),MSE Loss 설명,-0.005930365,0.0,0.005930365063250065
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지? (남은 답변: LLM Fine-Tuning 의 PEFT),MSE Loss 용도,-0.015400637,0.0,0.015400636941194534
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지? (남은 답변: LLM Fine-Tuning 의 PEFT),"Multi-Class, Multi-Label 중 BCE 가 좋은 task",-0.0076700374,0.0,0.0076700374484062195
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지? (남은 답변: LLM Fine-Tuning 의 PEFT),Multi-Label 에서 CE + Softmax 적용 문제점,-0.013371873,0.0,0.013371872715651989
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지? (남은 답변: LLM Fine-Tuning 의 PEFT),PEFT 방법 5가지,-0.00028269857,0.0,0.00028269857284612954
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지? (남은 답변: LLM Fine-Tuning 의 PEFT),거대 언어 모델 정의,0.0006271925,0.0,0.000627192493993789
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지? (남은 답변: LLM Fine-Tuning 의 PEFT),마지막 할 말,-0.0074481945,0.0,0.007448194548487663
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지? (남은 답변: LLM Fine-Tuning 의 PEFT),면접 시작 인사,-0.013946201,0.0,0.013946200720965862
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지? (남은 답변: LLM Fine-Tuning 의 PEFT),면접 종료,-0.010743832,0.0,0.010743832215666771
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지? (남은 답변: LLM Fine-Tuning 의 PEFT),"인공지능, 머신러닝, 딥러닝 차이",-0.010106573,0.0,0.01010657288134098
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지? (남은 답변: LLM Fine-Tuning 의 PEFT),잠시 휴식,-0.018754425,0.0,0.018754424527287483
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지? (남은 답변: LLM Fine-Tuning 의 PEFT),확률 예측에서 MSE Loss 미 사용 이유,-0.008591339,0.0,0.008591338992118835
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야! (남은 답변: 모든 질문 해결 완료),BCE Loss 설명,-0.008585858,0.0,0.008585858158767223
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야! (남은 답변: 모든 질문 해결 완료),LLM Fine-Tuning 의 PEFT,-0.019485135,0.0,0.01948513463139534
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야! (남은 답변: 모든 질문 해결 완료),LoRA,-0.011511913,0.0,0.011511912569403648
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야! (남은 답변: 모든 질문 해결 완료),LoRA 와 QLoRA 의 차이,0.00048678124,0.0,0.00048678123857825994
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야! (남은 답변: 모든 질문 해결 완료),Loss Function 관련 실무 경험,-0.0033451589,0.0,0.0033451588824391365
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야! (남은 답변: 모든 질문 해결 완료),Loss Function 예시,-0.02796511,0.0,0.027965109795331955
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야! (남은 답변: 모든 질문 해결 완료),Loss Function 정의,-0.013348509,0.0,0.01334850862622261
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야! (남은 답변: 모든 질문 해결 완료),MBTI / 좋아하는 아이돌,0.004359801,0.0,0.004359800834208727
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야! (남은 답변: 모든 질문 해결 완료),MSE Loss 설명,-0.005540532,0.0,0.005540532059967518
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야! (남은 답변: 모든 질문 해결 완료),MSE Loss 용도,0.0043839896,0.0,0.004383989609777927
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야! (남은 답변: 모든 질문 해결 완료),"Multi-Class, Multi-Label 중 BCE 가 좋은 task",0.0041155764,0.0,0.0041155763901770115
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야! (남은 답변: 모든 질문 해결 완료),Multi-Label 에서 CE + Softmax 적용 문제점,0.011308017,0.0,0.011308017186820507
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야! (남은 답변: 모든 질문 해결 완료),PEFT 방법 5가지,0.9669456,1.0,0.0330544114112854
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야! (남은 답변: 모든 질문 해결 완료),거대 언어 모델 정의,0.0026886268,0.0,0.0026886267587542534
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야! (남은 답변: 모든 질문 해결 완료),마지막 할 말,-0.0018085894,0.0,0.0018085894407704473
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야! (남은 답변: 모든 질문 해결 완료),면접 시작 인사,0.008989679,0.0,0.00898967869579792
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야! (남은 답변: 모든 질문 해결 완료),면접 종료,0.015856126,0.0,0.015856126323342323
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야! (남은 답변: 모든 질문 해결 완료),"인공지능, 머신러닝, 딥러닝 차이",0.0012690616,0.0,0.0012690615840256214
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야! (남은 답변: 모든 질문 해결 완료),잠시 휴식,-0.018186636,0.0,0.01818663626909256
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야! (남은 답변: 모든 질문 해결 완료),확률 예측에서 MSE Loss 미 사용 이유,-0.032357726,0.0,0.032357726246118546
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다 (남은 답변: LLM Fine-Tuning 의 PEFT),BCE Loss 설명,-0.0011922613,0.0,0.001192261348478496
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다 (남은 답변: LLM Fine-Tuning 의 PEFT),LLM Fine-Tuning 의 PEFT,0.9815332,1.0,0.0184667706489563
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다 (남은 답변: LLM Fine-Tuning 의 PEFT),LoRA,0.022432895,0.0,0.022432895377278328
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다 (남은 답변: LLM Fine-Tuning 의 PEFT),LoRA 와 QLoRA 의 차이,0.009833913,0.0,0.009833913296461105
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다 (남은 답변: LLM Fine-Tuning 의 PEFT),Loss Function 관련 실무 경험,0.0013076442,0.0,0.0013076441828161478
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다 (남은 답변: LLM Fine-Tuning 의 PEFT),Loss Function 예시,0.0003361504,0.0,0.00033615040592849255
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다 (남은 답변: LLM Fine-Tuning 의 PEFT),Loss Function 정의,-0.010292365,0.0,0.010292365215718746
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다 (남은 답변: LLM Fine-Tuning 의 PEFT),MBTI / 좋아하는 아이돌,-0.002662304,0.0,0.002662304090335965
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다 (남은 답변: LLM Fine-Tuning 의 PEFT),MSE Loss 설명,-0.009905511,0.0,0.00990551058202982
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다 (남은 답변: LLM Fine-Tuning 의 PEFT),MSE Loss 용도,-0.012594213,0.0,0.012594212777912617
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다 (남은 답변: LLM Fine-Tuning 의 PEFT),"Multi-Class, Multi-Label 중 BCE 가 좋은 task",-0.004898995,0.0,0.0048989951610565186
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다 (남은 답변: LLM Fine-Tuning 의 PEFT),Multi-Label 에서 CE + Softmax 적용 문제점,-0.019857746,0.0,0.019857745617628098
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다 (남은 답변: LLM Fine-Tuning 의 PEFT),PEFT 방법 5가지,-0.0029816008,0.0,0.0029816008172929287
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다 (남은 답변: LLM Fine-Tuning 의 PEFT),거대 언어 모델 정의,-0.0025086366,0.0,0.002508636564016342
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다 (남은 답변: LLM Fine-Tuning 의 PEFT),마지막 할 말,-0.006157754,0.0,0.006157753989100456
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다 (남은 답변: LLM Fine-Tuning 의 PEFT),면접 시작 인사,-0.0048872265,0.0,0.00488722650334239
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다 (남은 답변: LLM Fine-Tuning 의 PEFT),면접 종료,-0.009263865,0.0,0.00926386471837759
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다 (남은 답변: LLM Fine-Tuning 의 PEFT),"인공지능, 머신러닝, 딥러닝 차이",-0.017363554,0.0,0.01736355386674404
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다 (남은 답변: LLM Fine-Tuning 의 PEFT),잠시 휴식,-0.009150508,0.0,0.009150507859885693
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다 (남은 답변: LLM Fine-Tuning 의 PEFT),확률 예측에서 MSE Loss 미 사용 이유,-0.023639701,0.0,0.023639701306819916
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지? (남은 답변: 모든 질문 해결 완료)",BCE Loss 설명,-0.022065751,0.0,0.022065751254558563
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지? (남은 답변: 모든 질문 해결 완료)",LLM Fine-Tuning 의 PEFT,-0.0010450938,0.0,0.0010450937552377582
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지? (남은 답변: 모든 질문 해결 완료)",LoRA,0.95209676,1.0,0.047903239727020264
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지? (남은 답변: 모든 질문 해결 완료)",LoRA 와 QLoRA 의 차이,0.036490902,0.0,0.03649090230464935
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지? (남은 답변: 모든 질문 해결 완료)",Loss Function 관련 실무 경험,0.01448447,0.0,0.014484469778835773
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지? (남은 답변: 모든 질문 해결 완료)",Loss Function 예시,-0.00971601,0.0,0.009716009721159935
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지? (남은 답변: 모든 질문 해결 완료)",Loss Function 정의,-0.013782809,0.0,0.013782808557152748
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지? (남은 답변: 모든 질문 해결 완료)",MBTI / 좋아하는 아이돌,-0.012350216,0.0,0.012350215576589108
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지? (남은 답변: 모든 질문 해결 완료)",MSE Loss 설명,0.021602202,0.0,0.021602202206850052
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지? (남은 답변: 모든 질문 해결 완료)",MSE Loss 용도,-0.0189769,0.0,0.018976900726556778
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지? (남은 답변: 모든 질문 해결 완료)","Multi-Class, Multi-Label 중 BCE 가 좋은 task",0.0041351793,0.0,0.004135179333388805
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지? (남은 답변: 모든 질문 해결 완료)",Multi-Label 에서 CE + Softmax 적용 문제점,0.0062933457,0.0,0.006293345708400011
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지? (남은 답변: 모든 질문 해결 완료)",PEFT 방법 5가지,-0.007398678,0.0,0.0073986779898405075
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지? (남은 답변: 모든 질문 해결 완료)",거대 언어 모델 정의,-0.023061091,0.0,0.02306109108030796
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지? (남은 답변: 모든 질문 해결 완료)",마지막 할 말,0.012272042,0.0,0.012272042222321033
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지? (남은 답변: 모든 질문 해결 완료)",면접 시작 인사,-0.01865229,0.0,0.018652290105819702
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지? (남은 답변: 모든 질문 해결 완료)",면접 종료,-0.002506682,0.0,0.002506681950762868
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지? (남은 답변: 모든 질문 해결 완료)","인공지능, 머신러닝, 딥러닝 차이",-0.006681215,0.0,0.006681215018033981
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지? (남은 답변: 모든 질문 해결 완료)",잠시 휴식,-0.025714554,0.0,0.025714553892612457
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지? (남은 답변: 모든 질문 해결 완료)",확률 예측에서 MSE Loss 미 사용 이유,-0.017416112,0.0,0.01741611212491989
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야? (남은 답변: PEFT 방법 5가지),BCE Loss 설명,-0.02339382,0.0,0.023393819108605385
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야? (남은 답변: PEFT 방법 5가지),LLM Fine-Tuning 의 PEFT,0.0055453028,0.0,0.005545302759855986
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야? (남은 답변: PEFT 방법 5가지),LoRA,0.046728324,0.0,0.04672832414507866
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야? (남은 답변: PEFT 방법 5가지),LoRA 와 QLoRA 의 차이,-0.013709205,0.0,0.013709205202758312
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야? (남은 답변: PEFT 방법 5가지),Loss Function 관련 실무 경험,-0.019098824,0.0,0.01909882389008999
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야? (남은 답변: PEFT 방법 5가지),Loss Function 예시,-0.010846724,0.0,0.010846723802387714
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야? (남은 답변: PEFT 방법 5가지),Loss Function 정의,-0.021173675,0.0,0.02117367461323738
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야? (남은 답변: PEFT 방법 5가지),MBTI / 좋아하는 아이돌,-0.018905388,0.0,0.018905388191342354
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야? (남은 답변: PEFT 방법 5가지),MSE Loss 설명,-0.009581012,0.0,0.009581011720001698
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야? (남은 답변: PEFT 방법 5가지),MSE Loss 용도,0.00018354286,0.0,0.00018354285566601902
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야? (남은 답변: PEFT 방법 5가지),"Multi-Class, Multi-Label 중 BCE 가 좋은 task",0.023839427,0.0,0.023839427158236504
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야? (남은 답변: PEFT 방법 5가지),Multi-Label 에서 CE + Softmax 적용 문제점,-0.0006085495,0.0,0.00060854951152578
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야? (남은 답변: PEFT 방법 5가지),PEFT 방법 5가지,0.97035074,1.0,0.02964925765991211
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야? (남은 답변: PEFT 방법 5가지),거대 언어 모델 정의,0.006062472,0.0,0.0060624717734754086
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야? (남은 답변: PEFT 방법 5가지),마지막 할 말,-0.026253518,0.0,0.02625351771712303
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야? (남은 답변: PEFT 방법 5가지),면접 시작 인사,0.0076982807,0.0,0.0076982807368040085
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야? (남은 답변: PEFT 방법 5가지),면접 종료,0.007980049,0.0,0.007980048656463623
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야? (남은 답변: PEFT 방법 5가지),"인공지능, 머신러닝, 딥러닝 차이",-0.0017452677,0.0,0.0017452676547691226
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야? (남은 답변: PEFT 방법 5가지),잠시 휴식,-0.01182232,0.0,0.011822319589555264
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야? (남은 답변: PEFT 방법 5가지),확률 예측에서 MSE Loss 미 사용 이유,-0.019515388,0.0,0.01951538771390915
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야! (남은 답변: 모든 질문 해결 완료),BCE Loss 설명,0.014906952,0.0,0.014906952157616615
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야! (남은 답변: 모든 질문 해결 완료),LLM Fine-Tuning 의 PEFT,0.00018682219,0.0,0.0001868221879703924
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야! (남은 답변: 모든 질문 해결 완료),LoRA,0.08041899,0.0,0.08041898906230927
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야! (남은 답변: 모든 질문 해결 완료),LoRA 와 QLoRA 의 차이,0.9370798,1.0,0.0629202127456665
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야! (남은 답변: 모든 질문 해결 완료),Loss Function 관련 실무 경험,0.020356908,0.0,0.020356908440589905
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야! (남은 답변: 모든 질문 해결 완료),Loss Function 예시,-0.012032662,0.0,0.012032661586999893
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야! (남은 답변: 모든 질문 해결 완료),Loss Function 정의,-0.007486474,0.0,0.007486473768949509
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야! (남은 답변: 모든 질문 해결 완료),MBTI / 좋아하는 아이돌,0.0055378363,0.0,0.005537836346775293
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야! (남은 답변: 모든 질문 해결 완료),MSE Loss 설명,0.019212019,0.0,0.019212018698453903
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야! (남은 답변: 모든 질문 해결 완료),MSE Loss 용도,0.012984109,0.0,0.012984109111130238
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야! (남은 답변: 모든 질문 해결 완료),"Multi-Class, Multi-Label 중 BCE 가 좋은 task",-0.012596943,0.0,0.01259694341570139
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야! (남은 답변: 모든 질문 해결 완료),Multi-Label 에서 CE + Softmax 적용 문제점,-0.012070245,0.0,0.0120702451094985
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야! (남은 답변: 모든 질문 해결 완료),PEFT 방법 5가지,-0.02244285,0.0,0.022442849352955818
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야! (남은 답변: 모든 질문 해결 완료),거대 언어 모델 정의,-0.020649623,0.0,0.02064962312579155
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야! (남은 답변: 모든 질문 해결 완료),마지막 할 말,-0.011503366,0.0,0.011503365822136402
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야! (남은 답변: 모든 질문 해결 완료),면접 시작 인사,-0.0030808237,0.0,0.003080823691561818
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야! (남은 답변: 모든 질문 해결 완료),면접 종료,0.0064213052,0.0,0.006421305239200592
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야! (남은 답변: 모든 질문 해결 완료),"인공지능, 머신러닝, 딥러닝 차이",-0.0073888665,0.0,0.0073888665065169334
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야! (남은 답변: 모든 질문 해결 완료),잠시 휴식,-0.00961855,0.0,0.009618549607694149
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야! (남은 답변: 모든 질문 해결 완료),확률 예측에서 MSE Loss 미 사용 이유,-0.014580477,0.0,0.01458047702908516
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아? (남은 답변: LoRA),BCE Loss 설명,0.0037364867,0.0,0.0037364866584539413
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아? (남은 답변: LoRA),LLM Fine-Tuning 의 PEFT,-0.0035969678,0.0,0.0035969677846878767
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아? (남은 답변: LoRA),LoRA,0.968196,1.0,0.031804025173187256
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아? (남은 답변: LoRA),LoRA 와 QLoRA 의 차이,0.008660521,0.0,0.008660521358251572
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아? (남은 답변: LoRA),Loss Function 관련 실무 경험,0.005531166,0.0,0.005531166214495897
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아? (남은 답변: LoRA),Loss Function 예시,-0.001785588,0.0,0.0017855879850685596
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아? (남은 답변: LoRA),Loss Function 정의,0.0060691587,0.0,0.006069158669561148
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아? (남은 답변: LoRA),MBTI / 좋아하는 아이돌,-0.011809002,0.0,0.011809001676738262
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아? (남은 답변: LoRA),MSE Loss 설명,0.011103322,0.0,0.011103321798145771
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아? (남은 답변: LoRA),MSE Loss 용도,-0.024771042,0.0,0.02477104216814041
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아? (남은 답변: LoRA),"Multi-Class, Multi-Label 중 BCE 가 좋은 task",-0.0030305404,0.0,0.00303054042160511
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아? (남은 답변: LoRA),Multi-Label 에서 CE + Softmax 적용 문제점,0.0053151613,0.0,0.005315161310136318
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아? (남은 답변: LoRA),PEFT 방법 5가지,-0.016480705,0.0,0.01648070476949215
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아? (남은 답변: LoRA),거대 언어 모델 정의,0.0044334303,0.0,0.004433430265635252
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아? (남은 답변: LoRA),마지막 할 말,-0.00829944,0.0,0.008299440145492554
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아? (남은 답변: LoRA),면접 시작 인사,-0.013834292,0.0,0.013834292069077492
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아? (남은 답변: LoRA),면접 종료,-0.0014330953,0.0,0.0014330953126773238
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아? (남은 답변: LoRA),"인공지능, 머신러닝, 딥러닝 차이",0.011771478,0.0,0.01177147775888443
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아? (남은 답변: LoRA),잠시 휴식,-0.0045868605,0.0,0.004586860537528992
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아? (남은 답변: LoRA),확률 예측에서 MSE Loss 미 사용 이유,0.0054645934,0.0,0.005464593414217234
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야! (남은 답변: 모든 질문 해결 완료),BCE Loss 설명,0.009318904,0.0,0.009318904019892216
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야! (남은 답변: 모든 질문 해결 완료),LLM Fine-Tuning 의 PEFT,-0.00097420765,0.0,0.0009742076508700848
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야! (남은 답변: 모든 질문 해결 완료),LoRA,-0.02027016,0.0,0.02027015946805477
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야! (남은 답변: 모든 질문 해결 완료),LoRA 와 QLoRA 의 차이,0.9430488,1.0,0.05695122480392456
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야! (남은 답변: 모든 질문 해결 완료),Loss Function 관련 실무 경험,-0.003173886,0.0,0.00317388609983027
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야! (남은 답변: 모든 질문 해결 완료),Loss Function 예시,-0.00658632,0.0,0.006586320232599974
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야! (남은 답변: 모든 질문 해결 완료),Loss Function 정의,-0.01316782,0.0,0.013167819939553738
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야! (남은 답변: 모든 질문 해결 완료),MBTI / 좋아하는 아이돌,0.008164711,0.0,0.00816471129655838
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야! (남은 답변: 모든 질문 해결 완료),MSE Loss 설명,0.011772482,0.0,0.011772481724619865
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야! (남은 답변: 모든 질문 해결 완료),MSE Loss 용도,-0.028720733,0.0,0.028720732778310776
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야! (남은 답변: 모든 질문 해결 완료),"Multi-Class, Multi-Label 중 BCE 가 좋은 task",0.011818468,0.0,0.011818467639386654
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야! (남은 답변: 모든 질문 해결 완료),Multi-Label 에서 CE + Softmax 적용 문제점,-0.0056825764,0.0,0.005682576447725296
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야! (남은 답변: 모든 질문 해결 완료),PEFT 방법 5가지,0.026831623,0.0,0.026831623166799545
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야! (남은 답변: 모든 질문 해결 완료),거대 언어 모델 정의,-0.0015652413,0.0,0.0015652412548661232
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야! (남은 답변: 모든 질문 해결 완료),마지막 할 말,-0.019872384,0.0,0.019872384145855904
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야! (남은 답변: 모든 질문 해결 완료),면접 시작 인사,-0.003643038,0.0,0.003643037984147668
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야! (남은 답변: 모든 질문 해결 완료),면접 종료,0.009559412,0.0,0.00955941155552864
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야! (남은 답변: 모든 질문 해결 완료),"인공지능, 머신러닝, 딥러닝 차이",-0.009582766,0.0,0.009582766331732273
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야! (남은 답변: 모든 질문 해결 완료),잠시 휴식,0.01656654,0.0,0.01656653918325901
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야! (남은 답변: 모든 질문 해결 완료),확률 예측에서 MSE Loss 미 사용 이유,0.011498559,0.0,0.011498559266328812
LoRA -> 무슨 OOM 없앤다는 것 같은데 (남은 답변: LoRA),BCE Loss 설명,0.008736002,0.0,0.008736002258956432
LoRA -> 무슨 OOM 없앤다는 것 같은데 (남은 답변: LoRA),LLM Fine-Tuning 의 PEFT,0.0014966055,0.0,0.0014966054586693645
LoRA -> 무슨 OOM 없앤다는 것 같은데 (남은 답변: LoRA),LoRA,0.972262,1.0,0.027737975120544434
LoRA -> 무슨 OOM 없앤다는 것 같은데 (남은 답변: LoRA),LoRA 와 QLoRA 의 차이,-0.028975394,0.0,0.028975393623113632
LoRA -> 무슨 OOM 없앤다는 것 같은데 (남은 답변: LoRA),Loss Function 관련 실무 경험,0.014977432,0.0,0.01497743185609579
LoRA -> 무슨 OOM 없앤다는 것 같은데 (남은 답변: LoRA),Loss Function 예시,0.0025392366,0.0,0.0025392365641891956
LoRA -> 무슨 OOM 없앤다는 것 같은데 (남은 답변: LoRA),Loss Function 정의,0.0012642063,0.0,0.0012642062501981854
LoRA -> 무슨 OOM 없앤다는 것 같은데 (남은 답변: LoRA),MBTI / 좋아하는 아이돌,-0.015968656,0.0,0.015968656167387962
LoRA -> 무슨 OOM 없앤다는 것 같은데 (남은 답변: LoRA),MSE Loss 설명,0.008252766,0.0,0.008252765983343124
LoRA -> 무슨 OOM 없앤다는 것 같은데 (남은 답변: LoRA),MSE Loss 용도,-0.022530971,0.0,0.022530971094965935
LoRA -> 무슨 OOM 없앤다는 것 같은데 (남은 답변: LoRA),"Multi-Class, Multi-Label 중 BCE 가 좋은 task",0.0045329817,0.0,0.004532981663942337
LoRA -> 무슨 OOM 없앤다는 것 같은데 (남은 답변: LoRA),Multi-Label 에서 CE + Softmax 적용 문제점,0.0059997686,0.0,0.005999768618494272
LoRA -> 무슨 OOM 없앤다는 것 같은데 (남은 답변: LoRA),PEFT 방법 5가지,-0.011123025,0.0,0.011123024858534336
LoRA -> 무슨 OOM 없앤다는 것 같은데 (남은 답변: LoRA),거대 언어 모델 정의,0.009033818,0.0,0.009033817797899246
LoRA -> 무슨 OOM 없앤다는 것 같은데 (남은 답변: LoRA),마지막 할 말,-0.011146645,0.0,0.011146645061671734
LoRA -> 무슨 OOM 없앤다는 것 같은데 (남은 답변: LoRA),면접 시작 인사,-0.0071534985,0.0,0.007153498474508524
LoRA -> 무슨 OOM 없앤다는 것 같은데 (남은 답변: LoRA),면접 종료,-0.0045247613,0.0,0.0045247613452374935
LoRA -> 무슨 OOM 없앤다는 것 같은데 (남은 답변: LoRA),"인공지능, 머신러닝, 딥러닝 차이",0.005619685,0.0,0.005619685165584087
LoRA -> 무슨 OOM 없앤다는 것 같은데 (남은 답변: LoRA),잠시 휴식,-0.0039526275,0.0,0.003952627535909414
LoRA -> 무슨 OOM 없앤다는 것 같은데 (남은 답변: LoRA),확률 예측에서 MSE Loss 미 사용 이유,-0.0077561405,0.0,0.007756140548735857
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야! (남은 답변: 모든 질문 해결 완료),BCE Loss 설명,-0.012206559,0.0,0.01220655906945467
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야! (남은 답변: 모든 질문 해결 완료),LLM Fine-Tuning 의 PEFT,-0.00043918059,0.0,0.00043918058509007096
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야! (남은 답변: 모든 질문 해결 완료),LoRA,-0.033085722,0.0,0.03308572247624397
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야! (남은 답변: 모든 질문 해결 완료),LoRA 와 QLoRA 의 차이,-0.008374535,0.0,0.008374534547328949
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야! (남은 답변: 모든 질문 해결 완료),Loss Function 관련 실무 경험,0.024569297,0.0,0.024569297209382057
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야! (남은 답변: 모든 질문 해결 완료),Loss Function 예시,-0.0001946985,0.0,0.00019469849939923733
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야! (남은 답변: 모든 질문 해결 완료),Loss Function 정의,0.0044149715,0.0,0.004414971452206373
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야! (남은 답변: 모든 질문 해결 완료),MBTI / 좋아하는 아이돌,-0.028123643,0.0,0.0281236432492733
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야! (남은 답변: 모든 질문 해결 완료),MSE Loss 설명,-0.014056106,0.0,0.014056106097996235
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야! (남은 답변: 모든 질문 해결 완료),MSE Loss 용도,-0.021292517,0.0,0.021292516961693764
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야! (남은 답변: 모든 질문 해결 완료),"Multi-Class, Multi-Label 중 BCE 가 좋은 task",0.008022498,0.0,0.008022498339414597
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야! (남은 답변: 모든 질문 해결 완료),Multi-Label 에서 CE + Softmax 적용 문제점,0.0020395878,0.0,0.0020395878236740828
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야! (남은 답변: 모든 질문 해결 완료),PEFT 방법 5가지,0.023376185,0.0,0.023376185446977615
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야! (남은 답변: 모든 질문 해결 완료),거대 언어 모델 정의,-0.009706838,0.0,0.009706838056445122
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야! (남은 답변: 모든 질문 해결 완료),마지막 할 말,0.87421715,1.0,0.12578284740447998
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야! (남은 답변: 모든 질문 해결 완료),면접 시작 인사,-0.00762449,0.0,0.007624490186572075
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야! (남은 답변: 모든 질문 해결 완료),면접 종료,0.016036611,0.0,0.016036611050367355
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야! (남은 답변: 모든 질문 해결 완료),"인공지능, 머신러닝, 딥러닝 차이",-0.0012835447,0.0,0.0012835446977987885
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야! (남은 답변: 모든 질문 해결 완료),잠시 휴식,-0.02422153,0.0,0.024221530184149742
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야! (남은 답변: 모든 질문 해결 완료),확률 예측에서 MSE Loss 미 사용 이유,-0.030806612,0.0,0.030806612223386765
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데 (남은 답변: LoRA 와 QLoRA 의 차이),BCE Loss 설명,0.021358913,0.0,0.02135891281068325
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데 (남은 답변: LoRA 와 QLoRA 의 차이),LLM Fine-Tuning 의 PEFT,-0.015307801,0.0,0.015307800844311714
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데 (남은 답변: LoRA 와 QLoRA 의 차이),LoRA,-0.0374616,0.0,0.037461601197719574
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데 (남은 답변: LoRA 와 QLoRA 의 차이),LoRA 와 QLoRA 의 차이,0.96710294,1.0,0.03289705514907837
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데 (남은 답변: LoRA 와 QLoRA 의 차이),Loss Function 관련 실무 경험,-0.01514645,0.0,0.015146450139582157
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데 (남은 답변: LoRA 와 QLoRA 의 차이),Loss Function 예시,0.008123626,0.0,0.008123626001179218
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데 (남은 답변: LoRA 와 QLoRA 의 차이),Loss Function 정의,-0.02071253,0.0,0.020712530240416527
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데 (남은 답변: LoRA 와 QLoRA 의 차이),MBTI / 좋아하는 아이돌,0.0029203056,0.0,0.0029203055892139673
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데 (남은 답변: LoRA 와 QLoRA 의 차이),MSE Loss 설명,0.0074917804,0.0,0.007491780444979668
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데 (남은 답변: LoRA 와 QLoRA 의 차이),MSE Loss 용도,0.030742439,0.0,0.03074243851006031
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데 (남은 답변: LoRA 와 QLoRA 의 차이),"Multi-Class, Multi-Label 중 BCE 가 좋은 task",-0.018813698,0.0,0.01881369762122631
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데 (남은 답변: LoRA 와 QLoRA 의 차이),Multi-Label 에서 CE + Softmax 적용 문제점,0.0026168157,0.0,0.002616815734654665
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데 (남은 답변: LoRA 와 QLoRA 의 차이),PEFT 방법 5가지,-0.012169741,0.0,0.012169741094112396
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데 (남은 답변: LoRA 와 QLoRA 의 차이),거대 언어 모델 정의,-3.866286e-05,0.0,3.866285987896845e-05
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데 (남은 답변: LoRA 와 QLoRA 의 차이),마지막 할 말,-0.02269887,0.0,0.022698869928717613
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데 (남은 답변: LoRA 와 QLoRA 의 차이),면접 시작 인사,-0.0039796936,0.0,0.003979693632572889
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데 (남은 답변: LoRA 와 QLoRA 의 차이),면접 종료,0.005236081,0.0,0.005236080847680569
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데 (남은 답변: LoRA 와 QLoRA 의 차이),"인공지능, 머신러닝, 딥러닝 차이",-0.008387512,0.0,0.008387511596083641
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데 (남은 답변: LoRA 와 QLoRA 의 차이),잠시 휴식,0.02737034,0.0,0.027370339259505272
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데 (남은 답변: LoRA 와 QLoRA 의 차이),확률 예측에서 MSE Loss 미 사용 이유,-0.00487019,0.0,0.004870189819484949
마지막 할 말 -> 로라야 정말 고마워! (남은 답변: 마지막 할 말),BCE Loss 설명,-0.017354516,0.0,0.017354516312479973
마지막 할 말 -> 로라야 정말 고마워! (남은 답변: 마지막 할 말),LLM Fine-Tuning 의 PEFT,0.005920369,0.0,0.005920369178056717
마지막 할 말 -> 로라야 정말 고마워! (남은 답변: 마지막 할 말),LoRA,-0.015760982,0.0,0.015760982409119606
마지막 할 말 -> 로라야 정말 고마워! (남은 답변: 마지막 할 말),LoRA 와 QLoRA 의 차이,0.018252635,0.0,0.01825263537466526
마지막 할 말 -> 로라야 정말 고마워! (남은 답변: 마지막 할 말),Loss Function 관련 실무 경험,-0.0047824965,0.0,0.004782496485859156
마지막 할 말 -> 로라야 정말 고마워! (남은 답변: 마지막 할 말),Loss Function 예시,0.016673973,0.0,0.016673972830176353
마지막 할 말 -> 로라야 정말 고마워! (남은 답변: 마지막 할 말),Loss Function 정의,-0.009679125,0.0,0.009679124690592289
마지막 할 말 -> 로라야 정말 고마워! (남은 답변: 마지막 할 말),MBTI / 좋아하는 아이돌,-0.007713424,0.0,0.007713424041867256
마지막 할 말 -> 로라야 정말 고마워! (남은 답변: 마지막 할 말),MSE Loss 설명,-0.005146874,0.0,0.005146874114871025
마지막 할 말 -> 로라야 정말 고마워! (남은 답변: 마지막 할 말),MSE Loss 용도,0.0077419947,0.0,0.0077419946901500225
마지막 할 말 -> 로라야 정말 고마워! (남은 답변: 마지막 할 말),"Multi-Class, Multi-Label 중 BCE 가 좋은 task",-0.006349631,0.0,0.006349631119519472
마지막 할 말 -> 로라야 정말 고마워! (남은 답변: 마지막 할 말),Multi-Label 에서 CE + Softmax 적용 문제점,0.013103128,0.0,0.013103128410875797
마지막 할 말 -> 로라야 정말 고마워! (남은 답변: 마지막 할 말),PEFT 방법 5가지,-0.0002871089,0.0,0.00028710890910588205
마지막 할 말 -> 로라야 정말 고마워! (남은 답변: 마지막 할 말),거대 언어 모델 정의,-0.007712233,0.0,0.007712232880294323
마지막 할 말 -> 로라야 정말 고마워! (남은 답변: 마지막 할 말),마지막 할 말,-0.013459691,0.0,0.013459690846502781
마지막 할 말 -> 로라야 정말 고마워! (남은 답변: 마지막 할 말),면접 시작 인사,-0.019397542,0.0,0.019397541880607605
마지막 할 말 -> 로라야 정말 고마워! (남은 답변: 마지막 할 말),면접 종료,0.96477157,1.0,0.035228431224823
마지막 할 말 -> 로라야 정말 고마워! (남은 답변: 마지막 할 말),"인공지능, 머신러닝, 딥러닝 차이",-0.011581823,0.0,0.011581823229789734
마지막 할 말 -> 로라야 정말 고마워! (남은 답변: 마지막 할 말),잠시 휴식,0.020195773,0.0,0.020195772871375084
마지막 할 말 -> 로라야 정말 고마워! (남은 답변: 마지막 할 말),확률 예측에서 MSE Loss 미 사용 이유,0.011366937,0.0,0.011366937309503555
마지막 할 말 -> 로라야 사랑해 (남은 답변: 마지막 할 말),BCE Loss 설명,-0.019889073,0.0,0.019889073446393013
마지막 할 말 -> 로라야 사랑해 (남은 답변: 마지막 할 말),LLM Fine-Tuning 의 PEFT,0.027936693,0.0,0.027936693280935287
마지막 할 말 -> 로라야 사랑해 (남은 답변: 마지막 할 말),LoRA,-0.013318855,0.0,0.013318855315446854
마지막 할 말 -> 로라야 사랑해 (남은 답변: 마지막 할 말),LoRA 와 QLoRA 의 차이,0.019510793,0.0,0.019510792568325996
마지막 할 말 -> 로라야 사랑해 (남은 답변: 마지막 할 말),Loss Function 관련 실무 경험,-0.0025000614,0.0,0.00250006141141057
마지막 할 말 -> 로라야 사랑해 (남은 답변: 마지막 할 말),Loss Function 예시,0.012960793,0.0,0.012960793450474739
마지막 할 말 -> 로라야 사랑해 (남은 답변: 마지막 할 말),Loss Function 정의,-0.009548967,0.0,0.009548966772854328
마지막 할 말 -> 로라야 사랑해 (남은 답변: 마지막 할 말),MBTI / 좋아하는 아이돌,-0.00878855,0.0,0.008788550272583961
마지막 할 말 -> 로라야 사랑해 (남은 답변: 마지막 할 말),MSE Loss 설명,-0.006256522,0.0,0.00625652214512229
마지막 할 말 -> 로라야 사랑해 (남은 답변: 마지막 할 말),MSE Loss 용도,0.0051704855,0.0,0.005170485470443964
마지막 할 말 -> 로라야 사랑해 (남은 답변: 마지막 할 말),"Multi-Class, Multi-Label 중 BCE 가 좋은 task",-0.018286167,0.0,0.018286166712641716
마지막 할 말 -> 로라야 사랑해 (남은 답변: 마지막 할 말),Multi-Label 에서 CE + Softmax 적용 문제점,0.009254467,0.0,0.009254466742277145
마지막 할 말 -> 로라야 사랑해 (남은 답변: 마지막 할 말),PEFT 방법 5가지,-0.00042699592,0.0,0.0004269959172233939
마지막 할 말 -> 로라야 사랑해 (남은 답변: 마지막 할 말),거대 언어 모델 정의,-0.010582171,0.0,0.010582171380519867
마지막 할 말 -> 로라야 사랑해 (남은 답변: 마지막 할 말),마지막 할 말,-0.023422606,0.0,0.02342260628938675
마지막 할 말 -> 로라야 사랑해 (남은 답변: 마지막 할 말),면접 시작 인사,-0.023630401,0.0,0.023630401119589806
마지막 할 말 -> 로라야 사랑해 (남은 답변: 마지막 할 말),면접 종료,0.9592453,1.0,0.04075467586517334
마지막 할 말 -> 로라야 사랑해 (남은 답변: 마지막 할 말),"인공지능, 머신러닝, 딥러닝 차이",-0.016503252,0.0,0.01650325208902359
마지막 할 말 -> 로라야 사랑해 (남은 답변: 마지막 할 말),잠시 휴식,0.03842869,0.0,0.038428690284490585
마지막 할 말 -> 로라야 사랑해 (남은 답변: 마지막 할 말),확률 예측에서 MSE Loss 미 사용 이유,0.016877754,0.0,0.016877753660082817
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어 (남은 답변: 마지막 할 말),BCE Loss 설명,-0.013555492,0.0,0.013555492274463177
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어 (남은 답변: 마지막 할 말),LLM Fine-Tuning 의 PEFT,0.0064381817,0.0,0.006438181735575199
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어 (남은 답변: 마지막 할 말),LoRA,-0.0046056155,0.0,0.004605615511536598
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어 (남은 답변: 마지막 할 말),LoRA 와 QLoRA 의 차이,0.011052706,0.0,0.011052706278860569
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어 (남은 답변: 마지막 할 말),Loss Function 관련 실무 경험,-0.002330423,0.0,0.0023304230999201536
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어 (남은 답변: 마지막 할 말),Loss Function 예시,0.011929208,0.0,0.011929208412766457
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어 (남은 답변: 마지막 할 말),Loss Function 정의,0.0025623888,0.0,0.002562388777732849
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어 (남은 답변: 마지막 할 말),MBTI / 좋아하는 아이돌,-0.0058821915,0.0,0.005882191471755505
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어 (남은 답변: 마지막 할 말),MSE Loss 설명,-0.0021660344,0.0,0.002166034420952201
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어 (남은 답변: 마지막 할 말),MSE Loss 용도,0.006416841,0.0,0.006416840944439173
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어 (남은 답변: 마지막 할 말),"Multi-Class, Multi-Label 중 BCE 가 좋은 task",-0.0014684662,0.0,0.0014684662455692887
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어 (남은 답변: 마지막 할 말),Multi-Label 에서 CE + Softmax 적용 문제점,0.0042487886,0.0,0.004248788580298424
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어 (남은 답변: 마지막 할 말),PEFT 방법 5가지,-0.0039735287,0.0,0.003973528742790222
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어 (남은 답변: 마지막 할 말),거대 언어 모델 정의,-0.013016705,0.0,0.01301670540124178
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어 (남은 답변: 마지막 할 말),마지막 할 말,-0.01553085,0.0,0.015530849806964397
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어 (남은 답변: 마지막 할 말),면접 시작 인사,-0.02339232,0.0,0.023392319679260254
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어 (남은 답변: 마지막 할 말),면접 종료,0.96373445,1.0,0.036265552043914795
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어 (남은 답변: 마지막 할 말),"인공지능, 머신러닝, 딥러닝 차이",-0.011094602,0.0,0.011094601824879646
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어 (남은 답변: 마지막 할 말),잠시 휴식,0.008652839,0.0,0.008652838878333569
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어 (남은 답변: 마지막 할 말),확률 예측에서 MSE Loss 미 사용 이유,0.013878749,0.0,0.013878748752176762
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워 (남은 답변: 마지막 할 말),BCE Loss 설명,-0.009144878,0.0,0.009144878014922142
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워 (남은 답변: 마지막 할 말),LLM Fine-Tuning 의 PEFT,0.0056404457,0.0,0.005640445742756128
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워 (남은 답변: 마지막 할 말),LoRA,-0.031009039,0.0,0.031009038910269737
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워 (남은 답변: 마지막 할 말),LoRA 와 QLoRA 의 차이,0.024845324,0.0,0.024845324456691742
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워 (남은 답변: 마지막 할 말),Loss Function 관련 실무 경험,0.0026708352,0.0,0.0026708352379500866
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워 (남은 답변: 마지막 할 말),Loss Function 예시,0.0073134806,0.0,0.0073134806007146835
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워 (남은 답변: 마지막 할 말),Loss Function 정의,-0.009122826,0.0,0.009122826159000397
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워 (남은 답변: 마지막 할 말),MBTI / 좋아하는 아이돌,-0.012465472,0.0,0.01246547233313322
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워 (남은 답변: 마지막 할 말),MSE Loss 설명,0.001209672,0.0,0.0012096719583496451
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워 (남은 답변: 마지막 할 말),MSE Loss 용도,0.013859063,0.0,0.013859063386917114
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워 (남은 답변: 마지막 할 말),"Multi-Class, Multi-Label 중 BCE 가 좋은 task",-0.004271887,0.0,0.0042718867771327496
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워 (남은 답변: 마지막 할 말),Multi-Label 에서 CE + Softmax 적용 문제점,0.013033831,0.0,0.013033830560743809
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워 (남은 답변: 마지막 할 말),PEFT 방법 5가지,0.00055982533,0.0,0.0005598253337666392
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워 (남은 답변: 마지막 할 말),거대 언어 모델 정의,-0.0110672265,0.0,0.011067226529121399
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워 (남은 답변: 마지막 할 말),마지막 할 말,-0.018149715,0.0,0.018149714916944504
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워 (남은 답변: 마지막 할 말),면접 시작 인사,-0.00396567,0.0,0.0039656697772443295
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워 (남은 답변: 마지막 할 말),면접 종료,0.96385175,1.0,0.036148250102996826
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워 (남은 답변: 마지막 할 말),"인공지능, 머신러닝, 딥러닝 차이",-0.0037934736,0.0,0.0037934735883027315
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워 (남은 답변: 마지막 할 말),잠시 휴식,0.008320778,0.0,0.008320777676999569
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워 (남은 답변: 마지막 할 말),확률 예측에서 MSE Loss 미 사용 이유,-0.009975757,0.0,0.009975757449865341
