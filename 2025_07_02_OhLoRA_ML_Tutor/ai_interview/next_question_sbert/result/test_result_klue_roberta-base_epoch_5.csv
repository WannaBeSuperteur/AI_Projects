input_part,next_question,predicted_similarity,ground_truth_similarity,absolute_error
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서? (남은 답변: 면접 시작 인사),BCE Loss 설명,-0.008849996,0.0,0.008849996142089367
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서? (남은 답변: 면접 시작 인사),LLM Fine-Tuning 의 PEFT,0.009027159,0.0,0.009027158841490746
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서? (남은 답변: 면접 시작 인사),LoRA,-0.01563856,0.0,0.0156385600566864
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서? (남은 답변: 면접 시작 인사),LoRA 와 QLoRA 의 차이,0.019584216,0.0,0.01958421617746353
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서? (남은 답변: 면접 시작 인사),Loss Function 관련 실무 경험,-0.034031957,0.0,0.034031957387924194
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서? (남은 답변: 면접 시작 인사),Loss Function 예시,0.0043501533,0.0,0.004350153263658285
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서? (남은 답변: 면접 시작 인사),Loss Function 정의,-0.012042697,0.0,0.012042696587741375
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서? (남은 답변: 면접 시작 인사),MBTI / 좋아하는 아이돌,-0.023227235,0.0,0.023227235302329063
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서? (남은 답변: 면접 시작 인사),MSE Loss 설명,-0.013026686,0.0,0.013026686385273933
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서? (남은 답변: 면접 시작 인사),MSE Loss 용도,-0.016231457,0.0,0.016231456771492958
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서? (남은 답변: 면접 시작 인사),"Multi-Class, Multi-Label 중 BCE 가 좋은 task",-0.009246661,0.0,0.009246661327779293
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서? (남은 답변: 면접 시작 인사),Multi-Label 에서 CE + Softmax 적용 문제점,-0.00068406394,0.0,0.0006840639398433268
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서? (남은 답변: 면접 시작 인사),PEFT 방법 5가지,-0.021626314,0.0,0.021626314148306847
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서? (남은 답변: 면접 시작 인사),거대 언어 모델 정의,0.030799134,0.0,0.030799133703112602
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서? (남은 답변: 면접 시작 인사),마지막 할 말,0.0067777764,0.0,0.006777776405215263
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서? (남은 답변: 면접 시작 인사),면접 시작 인사,0.53912646,1.0,0.460873544216156
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서? (남은 답변: 면접 시작 인사),면접 종료,-0.053979993,0.0,0.05397999286651611
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서? (남은 답변: 면접 시작 인사),"인공지능, 머신러닝, 딥러닝 차이",0.59144247,0.0,0.5914424657821655
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서? (남은 답변: 면접 시작 인사),잠시 휴식,0.0074058673,0.0,0.007405867334455252
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서? (남은 답변: 면접 시작 인사),확률 예측에서 MSE Loss 미 사용 이유,0.023657372,0.0,0.02365737222135067
면접 시작 인사 -> 로라야 안녕 정말 반가워 (남은 답변: 면접 시작 인사),BCE Loss 설명,0.017926041,0.0,0.017926041036844254
면접 시작 인사 -> 로라야 안녕 정말 반가워 (남은 답변: 면접 시작 인사),LLM Fine-Tuning 의 PEFT,-0.047577914,0.0,0.04757791385054588
면접 시작 인사 -> 로라야 안녕 정말 반가워 (남은 답변: 면접 시작 인사),LoRA,-0.026522497,0.0,0.026522496715188026
면접 시작 인사 -> 로라야 안녕 정말 반가워 (남은 답변: 면접 시작 인사),LoRA 와 QLoRA 의 차이,0.004164955,0.0,0.004164955113083124
면접 시작 인사 -> 로라야 안녕 정말 반가워 (남은 답변: 면접 시작 인사),Loss Function 관련 실무 경험,0.0143132415,0.0,0.014313241466879845
면접 시작 인사 -> 로라야 안녕 정말 반가워 (남은 답변: 면접 시작 인사),Loss Function 예시,0.018315662,0.0,0.018315661698579788
면접 시작 인사 -> 로라야 안녕 정말 반가워 (남은 답변: 면접 시작 인사),Loss Function 정의,0.027233152,0.0,0.027233151718974113
면접 시작 인사 -> 로라야 안녕 정말 반가워 (남은 답변: 면접 시작 인사),MBTI / 좋아하는 아이돌,-0.0022737486,0.0,0.002273748628795147
면접 시작 인사 -> 로라야 안녕 정말 반가워 (남은 답변: 면접 시작 인사),MSE Loss 설명,-0.003915034,0.0,0.003915033768862486
면접 시작 인사 -> 로라야 안녕 정말 반가워 (남은 답변: 면접 시작 인사),MSE Loss 용도,-0.008210307,0.0,0.008210306987166405
면접 시작 인사 -> 로라야 안녕 정말 반가워 (남은 답변: 면접 시작 인사),"Multi-Class, Multi-Label 중 BCE 가 좋은 task",0.0052831466,0.0,0.005283146630972624
면접 시작 인사 -> 로라야 안녕 정말 반가워 (남은 답변: 면접 시작 인사),Multi-Label 에서 CE + Softmax 적용 문제점,-0.0035535553,0.0,0.0035535553470253944
면접 시작 인사 -> 로라야 안녕 정말 반가워 (남은 답변: 면접 시작 인사),PEFT 방법 5가지,-0.036563963,0.0,0.03656396269798279
면접 시작 인사 -> 로라야 안녕 정말 반가워 (남은 답변: 면접 시작 인사),거대 언어 모델 정의,0.0037832842,0.0,0.0037832842208445072
면접 시작 인사 -> 로라야 안녕 정말 반가워 (남은 답변: 면접 시작 인사),마지막 할 말,0.030337729,0.0,0.030337728559970856
면접 시작 인사 -> 로라야 안녕 정말 반가워 (남은 답변: 면접 시작 인사),면접 시작 인사,0.70081323,1.0,0.2991867661476135
면접 시작 인사 -> 로라야 안녕 정말 반가워 (남은 답변: 면접 시작 인사),면접 종료,0.024979368,0.0,0.024979367852211
면접 시작 인사 -> 로라야 안녕 정말 반가워 (남은 답변: 면접 시작 인사),"인공지능, 머신러닝, 딥러닝 차이",0.3792462,0.0,0.37924620509147644
면접 시작 인사 -> 로라야 안녕 정말 반가워 (남은 답변: 면접 시작 인사),잠시 휴식,0.031597894,0.0,0.03159789368510246
면접 시작 인사 -> 로라야 안녕 정말 반가워 (남은 답변: 면접 시작 인사),확률 예측에서 MSE Loss 미 사용 이유,0.008851446,0.0,0.008851446211338043
면접 시작 인사 -> 로라야 그럼 네가 면접관이야? (남은 답변: 면접 시작 인사),BCE Loss 설명,0.024592785,0.0,0.02459278516471386
면접 시작 인사 -> 로라야 그럼 네가 면접관이야? (남은 답변: 면접 시작 인사),LLM Fine-Tuning 의 PEFT,-0.022261344,0.0,0.022261343896389008
면접 시작 인사 -> 로라야 그럼 네가 면접관이야? (남은 답변: 면접 시작 인사),LoRA,0.008506256,0.0,0.00850625615566969
면접 시작 인사 -> 로라야 그럼 네가 면접관이야? (남은 답변: 면접 시작 인사),LoRA 와 QLoRA 의 차이,-0.010368904,0.0,0.01036890409886837
면접 시작 인사 -> 로라야 그럼 네가 면접관이야? (남은 답변: 면접 시작 인사),Loss Function 관련 실무 경험,-0.039851945,0.0,0.039851944893598557
면접 시작 인사 -> 로라야 그럼 네가 면접관이야? (남은 답변: 면접 시작 인사),Loss Function 예시,0.031825546,0.0,0.03182554617524147
면접 시작 인사 -> 로라야 그럼 네가 면접관이야? (남은 답변: 면접 시작 인사),Loss Function 정의,0.021840677,0.0,0.02184067666530609
면접 시작 인사 -> 로라야 그럼 네가 면접관이야? (남은 답변: 면접 시작 인사),MBTI / 좋아하는 아이돌,-0.043494634,0.0,0.043494634330272675
면접 시작 인사 -> 로라야 그럼 네가 면접관이야? (남은 답변: 면접 시작 인사),MSE Loss 설명,-0.0029110762,0.0,0.002911076182499528
면접 시작 인사 -> 로라야 그럼 네가 면접관이야? (남은 답변: 면접 시작 인사),MSE Loss 용도,-0.017055461,0.0,0.017055461183190346
면접 시작 인사 -> 로라야 그럼 네가 면접관이야? (남은 답변: 면접 시작 인사),"Multi-Class, Multi-Label 중 BCE 가 좋은 task",-0.018150024,0.0,0.018150024116039276
면접 시작 인사 -> 로라야 그럼 네가 면접관이야? (남은 답변: 면접 시작 인사),Multi-Label 에서 CE + Softmax 적용 문제점,0.015810926,0.0,0.015810925513505936
면접 시작 인사 -> 로라야 그럼 네가 면접관이야? (남은 답변: 면접 시작 인사),PEFT 방법 5가지,-0.07776076,0.0,0.07776076346635818
면접 시작 인사 -> 로라야 그럼 네가 면접관이야? (남은 답변: 면접 시작 인사),거대 언어 모델 정의,0.026051836,0.0,0.02605183608829975
면접 시작 인사 -> 로라야 그럼 네가 면접관이야? (남은 답변: 면접 시작 인사),마지막 할 말,-0.015036483,0.0,0.01503648329526186
면접 시작 인사 -> 로라야 그럼 네가 면접관이야? (남은 답변: 면접 시작 인사),면접 시작 인사,0.4682932,1.0,0.5317068099975586
면접 시작 인사 -> 로라야 그럼 네가 면접관이야? (남은 답변: 면접 시작 인사),면접 종료,-0.058854192,0.0,0.05885419249534607
면접 시작 인사 -> 로라야 그럼 네가 면접관이야? (남은 답변: 면접 시작 인사),"인공지능, 머신러닝, 딥러닝 차이",0.62337947,0.0,0.6233794689178467
면접 시작 인사 -> 로라야 그럼 네가 면접관이야? (남은 답변: 면접 시작 인사),잠시 휴식,0.103152916,0.0,0.10315291583538055
면접 시작 인사 -> 로라야 그럼 네가 면접관이야? (남은 답변: 면접 시작 인사),확률 예측에서 MSE Loss 미 사용 이유,-0.028702173,0.0,0.02870217338204384
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까 (남은 답변: 면접 시작 인사),BCE Loss 설명,0.002799706,0.0,0.002799706067889929
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까 (남은 답변: 면접 시작 인사),LLM Fine-Tuning 의 PEFT,-0.012749837,0.0,0.012749836780130863
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까 (남은 답변: 면접 시작 인사),LoRA,0.016668756,0.0,0.016668755561113358
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까 (남은 답변: 면접 시작 인사),LoRA 와 QLoRA 의 차이,0.031250153,0.0,0.03125015273690224
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까 (남은 답변: 면접 시작 인사),Loss Function 관련 실무 경험,-0.02664035,0.0,0.026640349999070168
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까 (남은 답변: 면접 시작 인사),Loss Function 예시,0.008273565,0.0,0.008273565210402012
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까 (남은 답변: 면접 시작 인사),Loss Function 정의,0.027769586,0.0,0.027769586071372032
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까 (남은 답변: 면접 시작 인사),MBTI / 좋아하는 아이돌,0.0030717927,0.0,0.003071792656555772
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까 (남은 답변: 면접 시작 인사),MSE Loss 설명,-0.014758529,0.0,0.014758529141545296
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까 (남은 답변: 면접 시작 인사),MSE Loss 용도,-0.019070284,0.0,0.019070284441113472
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까 (남은 답변: 면접 시작 인사),"Multi-Class, Multi-Label 중 BCE 가 좋은 task",-0.009973446,0.0,0.009973445907235146
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까 (남은 답변: 면접 시작 인사),Multi-Label 에서 CE + Softmax 적용 문제점,0.015395761,0.0,0.015395760536193848
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까 (남은 답변: 면접 시작 인사),PEFT 방법 5가지,-0.05271285,0.0,0.05271285027265549
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까 (남은 답변: 면접 시작 인사),거대 언어 모델 정의,0.05251871,0.0,0.05251871049404144
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까 (남은 답변: 면접 시작 인사),마지막 할 말,0.010359808,0.0,0.0103598078712821
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까 (남은 답변: 면접 시작 인사),면접 시작 인사,0.6671903,1.0,0.3328096866607666
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까 (남은 답변: 면접 시작 인사),면접 종료,-0.029172363,0.0,0.029172362759709358
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까 (남은 답변: 면접 시작 인사),"인공지능, 머신러닝, 딥러닝 차이",0.40037313,0.0,0.4003731310367584
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까 (남은 답변: 면접 시작 인사),잠시 휴식,0.030198293,0.0,0.030198292806744576
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까 (남은 답변: 면접 시작 인사),확률 예측에서 MSE Loss 미 사용 이유,0.01460466,0.0,0.014604659751057625
면접 시작 인사 -> 파이팅! 시작하자 (남은 답변: 면접 시작 인사),BCE Loss 설명,-0.023441527,0.0,0.023441527038812637
면접 시작 인사 -> 파이팅! 시작하자 (남은 답변: 면접 시작 인사),LLM Fine-Tuning 의 PEFT,0.015421721,0.0,0.015421721152961254
면접 시작 인사 -> 파이팅! 시작하자 (남은 답변: 면접 시작 인사),LoRA,-0.0020223525,0.0,0.002022352535277605
면접 시작 인사 -> 파이팅! 시작하자 (남은 답변: 면접 시작 인사),LoRA 와 QLoRA 의 차이,0.023808429,0.0,0.023808429017663002
면접 시작 인사 -> 파이팅! 시작하자 (남은 답변: 면접 시작 인사),Loss Function 관련 실무 경험,-0.029200856,0.0,0.029200855642557144
면접 시작 인사 -> 파이팅! 시작하자 (남은 답변: 면접 시작 인사),Loss Function 예시,0.00984138,0.0,0.009841379709541798
면접 시작 인사 -> 파이팅! 시작하자 (남은 답변: 면접 시작 인사),Loss Function 정의,-0.007818102,0.0,0.007818101905286312
면접 시작 인사 -> 파이팅! 시작하자 (남은 답변: 면접 시작 인사),MBTI / 좋아하는 아이돌,-0.015053998,0.0,0.015053997747600079
면접 시작 인사 -> 파이팅! 시작하자 (남은 답변: 면접 시작 인사),MSE Loss 설명,0.0005198403,0.0,0.0005198402795940638
면접 시작 인사 -> 파이팅! 시작하자 (남은 답변: 면접 시작 인사),MSE Loss 용도,-0.0019314898,0.0,0.0019314897945150733
면접 시작 인사 -> 파이팅! 시작하자 (남은 답변: 면접 시작 인사),"Multi-Class, Multi-Label 중 BCE 가 좋은 task",-0.024687152,0.0,0.024687152355909348
면접 시작 인사 -> 파이팅! 시작하자 (남은 답변: 면접 시작 인사),Multi-Label 에서 CE + Softmax 적용 문제점,0.008988518,0.0,0.00898851826786995
면접 시작 인사 -> 파이팅! 시작하자 (남은 답변: 면접 시작 인사),PEFT 방법 5가지,-0.016853541,0.0,0.016853541135787964
면접 시작 인사 -> 파이팅! 시작하자 (남은 답변: 면접 시작 인사),거대 언어 모델 정의,0.009562123,0.0,0.009562122635543346
면접 시작 인사 -> 파이팅! 시작하자 (남은 답변: 면접 시작 인사),마지막 할 말,-0.019468183,0.0,0.01946818269789219
면접 시작 인사 -> 파이팅! 시작하자 (남은 답변: 면접 시작 인사),면접 시작 인사,0.26353994,0.0,0.2635399401187897
면접 시작 인사 -> 파이팅! 시작하자 (남은 답변: 면접 시작 인사),면접 종료,-0.0107456,0.0,0.010745599865913391
면접 시작 인사 -> 파이팅! 시작하자 (남은 답변: 면접 시작 인사),"인공지능, 머신러닝, 딥러닝 차이",0.7327563,1.0,0.2672436833381653
면접 시작 인사 -> 파이팅! 시작하자 (남은 답변: 면접 시작 인사),잠시 휴식,-0.02146136,0.0,0.021461360156536102
면접 시작 인사 -> 파이팅! 시작하자 (남은 답변: 면접 시작 인사),확률 예측에서 MSE Loss 미 사용 이유,-0.006505164,0.0,0.006505163852125406
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데 (남은 답변: 인공지능, 머신러닝, 딥러닝)",BCE Loss 설명,-0.0042703575,0.0,0.004270357545465231
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데 (남은 답변: 인공지능, 머신러닝, 딥러닝)",LLM Fine-Tuning 의 PEFT,-0.009554427,0.0,0.009554427117109299
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데 (남은 답변: 인공지능, 머신러닝, 딥러닝)",LoRA,0.0073772594,0.0,0.007377259433269501
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데 (남은 답변: 인공지능, 머신러닝, 딥러닝)",LoRA 와 QLoRA 의 차이,-0.0011535065,0.0,0.0011535064550116658
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데 (남은 답변: 인공지능, 머신러닝, 딥러닝)",Loss Function 관련 실무 경험,0.0073914933,0.0,0.007391493301838636
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데 (남은 답변: 인공지능, 머신러닝, 딥러닝)",Loss Function 예시,0.014273279,0.0,0.014273279346525669
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데 (남은 답변: 인공지능, 머신러닝, 딥러닝)",Loss Function 정의,-0.032107066,0.0,0.03210706636309624
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데 (남은 답변: 인공지능, 머신러닝, 딥러닝)",MBTI / 좋아하는 아이돌,-0.00815523,0.0,0.008155230432748795
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데 (남은 답변: 인공지능, 머신러닝, 딥러닝)",MSE Loss 설명,-0.0012114241,0.0,0.0012114241253584623
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데 (남은 답변: 인공지능, 머신러닝, 딥러닝)",MSE Loss 용도,-0.005743756,0.0,0.005743755958974361
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데 (남은 답변: 인공지능, 머신러닝, 딥러닝)","Multi-Class, Multi-Label 중 BCE 가 좋은 task",0.010887065,0.0,0.010887064971029758
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데 (남은 답변: 인공지능, 머신러닝, 딥러닝)",Multi-Label 에서 CE + Softmax 적용 문제점,-0.004390849,0.0,0.00439084880053997
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데 (남은 답변: 인공지능, 머신러닝, 딥러닝)",PEFT 방법 5가지,0.008986224,0.0,0.008986224420368671
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데 (남은 답변: 인공지능, 머신러닝, 딥러닝)",거대 언어 모델 정의,-0.024802936,0.0,0.024802936241030693
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데 (남은 답변: 인공지능, 머신러닝, 딥러닝)",마지막 할 말,-0.004951587,0.0,0.004951586946845055
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데 (남은 답변: 인공지능, 머신러닝, 딥러닝)",면접 시작 인사,-0.06758079,0.0,0.06758078932762146
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데 (남은 답변: 인공지능, 머신러닝, 딥러닝)",면접 종료,-0.004031496,0.0,0.004031496122479439
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데 (남은 답변: 인공지능, 머신러닝, 딥러닝)","인공지능, 머신러닝, 딥러닝 차이",0.9658518,1.0,0.034148216247558594
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데 (남은 답변: 인공지능, 머신러닝, 딥러닝)",잠시 휴식,0.015132171,0.0,0.015132171101868153
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데 (남은 답변: 인공지능, 머신러닝, 딥러닝)",확률 예측에서 MSE Loss 미 사용 이유,-0.0038403822,0.0,0.0038403822109103203
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거! (남은 답변: 머신러닝, 딥러닝)",BCE Loss 설명,0.0010329367,0.0,0.001032936736010015
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거! (남은 답변: 머신러닝, 딥러닝)",LLM Fine-Tuning 의 PEFT,-0.008654732,0.0,0.008654732257127762
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거! (남은 답변: 머신러닝, 딥러닝)",LoRA,0.00092204293,0.0,0.0009220429346896708
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거! (남은 답변: 머신러닝, 딥러닝)",LoRA 와 QLoRA 의 차이,-0.00033769268,0.0,0.0003376926761120558
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거! (남은 답변: 머신러닝, 딥러닝)",Loss Function 관련 실무 경험,0.0031309857,0.0,0.0031309856567531824
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거! (남은 답변: 머신러닝, 딥러닝)",Loss Function 예시,0.014579894,0.0,0.01457989402115345
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거! (남은 답변: 머신러닝, 딥러닝)",Loss Function 정의,-0.027437555,0.0,0.02743755467236042
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거! (남은 답변: 머신러닝, 딥러닝)",MBTI / 좋아하는 아이돌,-0.0113421185,0.0,0.011342118494212627
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거! (남은 답변: 머신러닝, 딥러닝)",MSE Loss 설명,0.008215419,0.0,0.008215419016778469
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거! (남은 답변: 머신러닝, 딥러닝)",MSE Loss 용도,0.0041696355,0.0,0.004169635474681854
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거! (남은 답변: 머신러닝, 딥러닝)","Multi-Class, Multi-Label 중 BCE 가 좋은 task",0.011489113,0.0,0.011489112861454487
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거! (남은 답변: 머신러닝, 딥러닝)",Multi-Label 에서 CE + Softmax 적용 문제점,0.0034373528,0.0,0.0034373528324067593
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거! (남은 답변: 머신러닝, 딥러닝)",PEFT 방법 5가지,-0.0048873317,0.0,0.004887331742793322
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거! (남은 답변: 머신러닝, 딥러닝)",거대 언어 모델 정의,-0.031280845,0.0,0.031280845403671265
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거! (남은 답변: 머신러닝, 딥러닝)",마지막 할 말,-0.0009315889,0.0,0.0009315888746641576
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거! (남은 답변: 머신러닝, 딥러닝)",면접 시작 인사,-0.06399417,0.0,0.06399416923522949
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거! (남은 답변: 머신러닝, 딥러닝)",면접 종료,-0.006352648,0.0,0.006352648138999939
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거! (남은 답변: 머신러닝, 딥러닝)","인공지능, 머신러닝, 딥러닝 차이",0.9712511,1.0,0.028748929500579834
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거! (남은 답변: 머신러닝, 딥러닝)",잠시 휴식,-0.0045054555,0.0,0.004505455493927002
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거! (남은 답변: 머신러닝, 딥러닝)",확률 예측에서 MSE Loss 미 사용 이유,-0.0045496123,0.0,0.004549612291157246
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지 (남은 답변: 딥러닝)",BCE Loss 설명,-0.00166434,0.0,0.0016643400304019451
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지 (남은 답변: 딥러닝)",LLM Fine-Tuning 의 PEFT,-0.008306317,0.0,0.008306317031383514
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지 (남은 답변: 딥러닝)",LoRA,0.0060921363,0.0,0.006092136260122061
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지 (남은 답변: 딥러닝)",LoRA 와 QLoRA 의 차이,-0.0012987601,0.0,0.001298760063946247
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지 (남은 답변: 딥러닝)",Loss Function 관련 실무 경험,0.0070175636,0.0,0.007017563562840223
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지 (남은 답변: 딥러닝)",Loss Function 예시,0.017478628,0.0,0.01747862808406353
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지 (남은 답변: 딥러닝)",Loss Function 정의,-0.024050912,0.0,0.024050911888480186
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지 (남은 답변: 딥러닝)",MBTI / 좋아하는 아이돌,-0.010932705,0.0,0.010932705365121365
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지 (남은 답변: 딥러닝)",MSE Loss 설명,0.009937769,0.0,0.009937768802046776
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지 (남은 답변: 딥러닝)",MSE Loss 용도,0.006884743,0.0,0.006884742993861437
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지 (남은 답변: 딥러닝)","Multi-Class, Multi-Label 중 BCE 가 좋은 task",0.00590158,0.0,0.005901580210775137
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지 (남은 답변: 딥러닝)",Multi-Label 에서 CE + Softmax 적용 문제점,0.006101617,0.0,0.006101617123931646
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지 (남은 답변: 딥러닝)",PEFT 방법 5가지,-0.00094146625,0.0,0.0009414662490598857
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지 (남은 답변: 딥러닝)",거대 언어 모델 정의,-0.024775993,0.0,0.024775993078947067
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지 (남은 답변: 딥러닝)",마지막 할 말,0.002388271,0.0,0.002388271037489176
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지 (남은 답변: 딥러닝)",면접 시작 인사,-0.06436013,0.0,0.0643601268529892
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지 (남은 답변: 딥러닝)",면접 종료,-0.008641317,0.0,0.008641316555440426
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지 (남은 답변: 딥러닝)","인공지능, 머신러닝, 딥러닝 차이",0.97297347,1.0,0.02702653408050537
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지 (남은 답변: 딥러닝)",잠시 휴식,-0.015898207,0.0,0.01589820720255375
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지 (남은 답변: 딥러닝)",확률 예측에서 MSE Loss 미 사용 이유,0.011514678,0.0,0.011514677666127682
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지? (남은 답변: 인공지능, 머신러닝)",BCE Loss 설명,-0.0019105803,0.0,0.0019105803221464157
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지? (남은 답변: 인공지능, 머신러닝)",LLM Fine-Tuning 의 PEFT,-0.009175627,0.0,0.009175626561045647
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지? (남은 답변: 인공지능, 머신러닝)",LoRA,0.0029281045,0.0,0.0029281044844537973
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지? (남은 답변: 인공지능, 머신러닝)",LoRA 와 QLoRA 의 차이,-0.0036297634,0.0,0.003629763377830386
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지? (남은 답변: 인공지능, 머신러닝)",Loss Function 관련 실무 경험,0.0022603425,0.0,0.0022603424731642008
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지? (남은 답변: 인공지능, 머신러닝)",Loss Function 예시,0.017328434,0.0,0.017328433692455292
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지? (남은 답변: 인공지능, 머신러닝)",Loss Function 정의,-0.02274795,0.0,0.02274795062839985
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지? (남은 답변: 인공지능, 머신러닝)",MBTI / 좋아하는 아이돌,-0.014046571,0.0,0.014046571217477322
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지? (남은 답변: 인공지능, 머신러닝)",MSE Loss 설명,0.0043111835,0.0,0.004311183467507362
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지? (남은 답변: 인공지능, 머신러닝)",MSE Loss 용도,-0.00020895494,0.0,0.0002089549379888922
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지? (남은 답변: 인공지능, 머신러닝)","Multi-Class, Multi-Label 중 BCE 가 좋은 task",0.011329262,0.0,0.01132926158607006
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지? (남은 답변: 인공지능, 머신러닝)",Multi-Label 에서 CE + Softmax 적용 문제점,0.001980343,0.0,0.0019803429022431374
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지? (남은 답변: 인공지능, 머신러닝)",PEFT 방법 5가지,-0.013572269,0.0,0.0135722691193223
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지? (남은 답변: 인공지능, 머신러닝)",거대 언어 모델 정의,-0.01980237,0.0,0.01980236917734146
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지? (남은 답변: 인공지능, 머신러닝)",마지막 할 말,0.004939832,0.0,0.004939831793308258
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지? (남은 답변: 인공지능, 머신러닝)",면접 시작 인사,-0.060260694,0.0,0.06026069447398186
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지? (남은 답변: 인공지능, 머신러닝)",면접 종료,-0.009065775,0.0,0.009065775200724602
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지? (남은 답변: 인공지능, 머신러닝)","인공지능, 머신러닝, 딥러닝 차이",0.97204775,1.0,0.027952253818511963
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지? (남은 답변: 인공지능, 머신러닝)",잠시 휴식,-0.004667345,0.0,0.004667344968765974
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지? (남은 답변: 인공지능, 머신러닝)",확률 예측에서 MSE Loss 미 사용 이유,0.010140662,0.0,0.010140662081539631
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야 (남은 답변: 모든 질문 해결 완료)",BCE Loss 설명,0.02545307,0.0,0.025453070178627968
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야 (남은 답변: 모든 질문 해결 완료)",LLM Fine-Tuning 의 PEFT,0.009741387,0.0,0.009741387329995632
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야 (남은 답변: 모든 질문 해결 완료)",LoRA,-0.009284425,0.0,0.009284424595534801
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야 (남은 답변: 모든 질문 해결 완료)",LoRA 와 QLoRA 의 차이,0.005700243,0.0,0.005700243171304464
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야 (남은 답변: 모든 질문 해결 완료)",Loss Function 관련 실무 경험,-0.0048424196,0.0,0.004842419642955065
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야 (남은 답변: 모든 질문 해결 완료)",Loss Function 예시,-0.030737491,0.0,0.030737491324543953
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야 (남은 답변: 모든 질문 해결 완료)",Loss Function 정의,0.027202066,0.0,0.027202066034078598
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야 (남은 답변: 모든 질문 해결 완료)",MBTI / 좋아하는 아이돌,-0.03477703,0.0,0.03477703034877777
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야 (남은 답변: 모든 질문 해결 완료)",MSE Loss 설명,0.005023929,0.0,0.005023928824812174
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야 (남은 답변: 모든 질문 해결 완료)",MSE Loss 용도,-0.011524988,0.0,0.01152498833835125
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야 (남은 답변: 모든 질문 해결 완료)","Multi-Class, Multi-Label 중 BCE 가 좋은 task",-0.0036011299,0.0,0.0036011298652738333
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야 (남은 답변: 모든 질문 해결 완료)",Multi-Label 에서 CE + Softmax 적용 문제점,0.0007308002,0.0,0.0007308002095669508
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야 (남은 답변: 모든 질문 해결 완료)",PEFT 방법 5가지,-0.008057106,0.0,0.008057106286287308
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야 (남은 답변: 모든 질문 해결 완료)",거대 언어 모델 정의,0.91842335,1.0,0.0815766453742981
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야 (남은 답변: 모든 질문 해결 완료)",마지막 할 말,-0.016962688,0.0,0.0169626884162426
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야 (남은 답변: 모든 질문 해결 완료)",면접 시작 인사,-0.0036222788,0.0,0.003622278803959489
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야 (남은 답변: 모든 질문 해결 완료)",면접 종료,0.014070269,0.0,0.014070268720388412
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야 (남은 답변: 모든 질문 해결 완료)","인공지능, 머신러닝, 딥러닝 차이",-0.0024840333,0.0,0.0024840333499014378
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야 (남은 답변: 모든 질문 해결 완료)",잠시 휴식,-0.028098622,0.0,0.02809862233698368
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야 (남은 답변: 모든 질문 해결 완료)",확률 예측에서 MSE Loss 미 사용 이유,-0.012156155,0.0,0.012156154960393906
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야? (남은 답변: 머신러닝)",BCE Loss 설명,-0.0038503683,0.0,0.0038503683172166348
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야? (남은 답변: 머신러닝)",LLM Fine-Tuning 의 PEFT,-0.005243727,0.0,0.005243727006018162
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야? (남은 답변: 머신러닝)",LoRA,-0.0027907786,0.0,0.0027907786425203085
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야? (남은 답변: 머신러닝)",LoRA 와 QLoRA 의 차이,-0.005438072,0.0,0.005438072141259909
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야? (남은 답변: 머신러닝)",Loss Function 관련 실무 경험,0.0011435358,0.0,0.0011435358319431543
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야? (남은 답변: 머신러닝)",Loss Function 예시,0.015590691,0.0,0.01559069100767374
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야? (남은 답변: 머신러닝)",Loss Function 정의,-0.025617838,0.0,0.02561783790588379
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야? (남은 답변: 머신러닝)",MBTI / 좋아하는 아이돌,-0.018358096,0.0,0.018358096480369568
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야? (남은 답변: 머신러닝)",MSE Loss 설명,0.015216688,0.0,0.015216687694191933
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야? (남은 답변: 머신러닝)",MSE Loss 용도,0.011222985,0.0,0.01122298464179039
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야? (남은 답변: 머신러닝)","Multi-Class, Multi-Label 중 BCE 가 좋은 task",0.0047730324,0.0,0.004773032385855913
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야? (남은 답변: 머신러닝)",Multi-Label 에서 CE + Softmax 적용 문제점,-0.0039789053,0.0,0.003978905268013477
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야? (남은 답변: 머신러닝)",PEFT 방법 5가지,-0.001642613,0.0,0.001642612973228097
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야? (남은 답변: 머신러닝)",거대 언어 모델 정의,-0.012648549,0.0,0.012648548930883408
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야? (남은 답변: 머신러닝)",마지막 할 말,0.0045802663,0.0,0.004580266308039427
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야? (남은 답변: 머신러닝)",면접 시작 인사,-0.06224849,0.0,0.06224849075078964
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야? (남은 답변: 머신러닝)",면접 종료,-0.007478227,0.0,0.007478226907551289
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야? (남은 답변: 머신러닝)","인공지능, 머신러닝, 딥러닝 차이",0.96873933,1.0,0.031260669231414795
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야? (남은 답변: 머신러닝)",잠시 휴식,-0.0031190407,0.0,0.003119040746241808
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야? (남은 답변: 머신러닝)",확률 예측에서 MSE Loss 미 사용 이유,-0.0017615089,0.0,0.00176150887273252
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아 (남은 답변: 인공지능, 머신러닝)",BCE Loss 설명,0.0039896914,0.0,0.0039896913804113865
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아 (남은 답변: 인공지능, 머신러닝)",LLM Fine-Tuning 의 PEFT,-0.017404152,0.0,0.01740415208041668
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아 (남은 답변: 인공지능, 머신러닝)",LoRA,0.014776692,0.0,0.014776691794395447
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아 (남은 답변: 인공지능, 머신러닝)",LoRA 와 QLoRA 의 차이,0.0053473418,0.0,0.005347341764718294
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아 (남은 답변: 인공지능, 머신러닝)",Loss Function 관련 실무 경험,0.0024839654,0.0,0.002483965363353491
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아 (남은 답변: 인공지능, 머신러닝)",Loss Function 예시,0.009284037,0.0,0.009284037165343761
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아 (남은 답변: 인공지능, 머신러닝)",Loss Function 정의,-0.04366863,0.0,0.043668631464242935
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아 (남은 답변: 인공지능, 머신러닝)",MBTI / 좋아하는 아이돌,0.0012870327,0.0,0.0012870327336713672
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아 (남은 답변: 인공지능, 머신러닝)",MSE Loss 설명,0.006047636,0.0,0.006047635804861784
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아 (남은 답변: 인공지능, 머신러닝)",MSE Loss 용도,0.002036518,0.0,0.0020365179516375065
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아 (남은 답변: 인공지능, 머신러닝)","Multi-Class, Multi-Label 중 BCE 가 좋은 task",0.0082673235,0.0,0.008267323486506939
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아 (남은 답변: 인공지능, 머신러닝)",Multi-Label 에서 CE + Softmax 적용 문제점,-0.010854329,0.0,0.010854328982532024
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아 (남은 답변: 인공지능, 머신러닝)",PEFT 방법 5가지,0.019639539,0.0,0.01963953860104084
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아 (남은 답변: 인공지능, 머신러닝)",거대 언어 모델 정의,-0.029497249,0.0,0.02949724905192852
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아 (남은 답변: 인공지능, 머신러닝)",마지막 할 말,0.0007069706,0.0,0.0007069705752655864
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아 (남은 답변: 인공지능, 머신러닝)",면접 시작 인사,-0.05871246,0.0,0.05871246010065079
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아 (남은 답변: 인공지능, 머신러닝)",면접 종료,-0.009365886,0.0,0.009365886449813843
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아 (남은 답변: 인공지능, 머신러닝)","인공지능, 머신러닝, 딥러닝 차이",0.9632934,1.0,0.036706626415252686
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아 (남은 답변: 인공지능, 머신러닝)",잠시 휴식,0.01575367,0.0,0.015753669664263725
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아 (남은 답변: 인공지능, 머신러닝)",확률 예측에서 MSE Loss 미 사용 이유,0.019301113,0.0,0.01930111274123192
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지? (남은 답변: 머신러닝)",BCE Loss 설명,0.007161168,0.0,0.007161167915910482
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지? (남은 답변: 머신러닝)",LLM Fine-Tuning 의 PEFT,0.00050528476,0.0,0.0005052847554907203
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지? (남은 답변: 머신러닝)",LoRA,-0.0054594525,0.0,0.005459452513605356
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지? (남은 답변: 머신러닝)",LoRA 와 QLoRA 의 차이,-0.007448038,0.0,0.007448038086295128
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지? (남은 답변: 머신러닝)",Loss Function 관련 실무 경험,-0.0026622454,0.0,0.0026622454170137644
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지? (남은 답변: 머신러닝)",Loss Function 예시,0.016309842,0.0,0.016309842467308044
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지? (남은 답변: 머신러닝)",Loss Function 정의,-0.024212403,0.0,0.02421240322291851
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지? (남은 답변: 머신러닝)",MBTI / 좋아하는 아이돌,-0.014016006,0.0,0.014016006141901016
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지? (남은 답변: 머신러닝)",MSE Loss 설명,0.006496637,0.0,0.006496637128293514
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지? (남은 답변: 머신러닝)",MSE Loss 용도,0.0027865276,0.0,0.002786527620628476
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지? (남은 답변: 머신러닝)","Multi-Class, Multi-Label 중 BCE 가 좋은 task",-0.0024102882,0.0,0.0024102882016450167
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지? (남은 답변: 머신러닝)",Multi-Label 에서 CE + Softmax 적용 문제점,-0.00752874,0.0,0.007528739981353283
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지? (남은 답변: 머신러닝)",PEFT 방법 5가지,-0.003545652,0.0,0.003545651910826564
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지? (남은 답변: 머신러닝)",거대 언어 모델 정의,0.007402867,0.0,0.007402867078781128
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지? (남은 답변: 머신러닝)",마지막 할 말,0.009680634,0.0,0.00968063436448574
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지? (남은 답변: 머신러닝)",면접 시작 인사,-0.06903735,0.0,0.06903734803199768
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지? (남은 답변: 머신러닝)",면접 종료,-0.011216198,0.0,0.011216198094189167
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지? (남은 답변: 머신러닝)","인공지능, 머신러닝, 딥러닝 차이",0.9605526,1.0,0.03944742679595947
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지? (남은 답변: 머신러닝)",잠시 휴식,-0.0015469369,0.0,0.0015469369245693088
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지? (남은 답변: 머신러닝)",확률 예측에서 MSE Loss 미 사용 이유,-0.011983443,0.0,0.011983443051576614
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야! (남은 답변: 모든 질문 해결 완료)",BCE Loss 설명,0.023382721,0.0,0.023382721468806267
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야! (남은 답변: 모든 질문 해결 완료)",LLM Fine-Tuning 의 PEFT,0.015819652,0.0,0.015819652006030083
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야! (남은 답변: 모든 질문 해결 완료)",LoRA,-0.0066995923,0.0,0.006699592340737581
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야! (남은 답변: 모든 질문 해결 완료)",LoRA 와 QLoRA 의 차이,0.0064427652,0.0,0.006442765239626169
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야! (남은 답변: 모든 질문 해결 완료)",Loss Function 관련 실무 경험,-0.0022159074,0.0,0.0022159074433147907
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야! (남은 답변: 모든 질문 해결 완료)",Loss Function 예시,-0.033555213,0.0,0.03355521336197853
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야! (남은 답변: 모든 질문 해결 완료)",Loss Function 정의,0.024093589,0.0,0.024093588814139366
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야! (남은 답변: 모든 질문 해결 완료)",MBTI / 좋아하는 아이돌,-0.0054111774,0.0,0.005411177407950163
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야! (남은 답변: 모든 질문 해결 완료)",MSE Loss 설명,0.0029422252,0.0,0.0029422251973301172
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야! (남은 답변: 모든 질문 해결 완료)",MSE Loss 용도,-0.013008172,0.0,0.013008171692490578
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야! (남은 답변: 모든 질문 해결 완료)","Multi-Class, Multi-Label 중 BCE 가 좋은 task",0.00018378433,0.0,0.00018378433014731854
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야! (남은 답변: 모든 질문 해결 완료)",Multi-Label 에서 CE + Softmax 적용 문제점,-0.0022255937,0.0,0.002225593663752079
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야! (남은 답변: 모든 질문 해결 완료)",PEFT 방법 5가지,-0.0024519959,0.0,0.0024519958533346653
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야! (남은 답변: 모든 질문 해결 완료)",거대 언어 모델 정의,0.9201461,1.0,0.07985389232635498
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야! (남은 답변: 모든 질문 해결 완료)",마지막 할 말,-0.011379786,0.0,0.01137978583574295
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야! (남은 답변: 모든 질문 해결 완료)",면접 시작 인사,-0.0091236,0.0,0.009123600088059902
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야! (남은 답변: 모든 질문 해결 완료)",면접 종료,0.0011683287,0.0,0.0011683286866173148
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야! (남은 답변: 모든 질문 해결 완료)","인공지능, 머신러닝, 딥러닝 차이",-0.020739643,0.0,0.020739642903208733
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야! (남은 답변: 모든 질문 해결 완료)",잠시 휴식,-0.033834442,0.0,0.033834442496299744
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야! (남은 답변: 모든 질문 해결 완료)",확률 예측에서 MSE Loss 미 사용 이유,-0.016351925,0.0,0.01635192520916462
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지 (남은 답변: 모든 질문 해결 완료)",BCE Loss 설명,0.026933566,0.0,0.026933565735816956
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지 (남은 답변: 모든 질문 해결 완료)",LLM Fine-Tuning 의 PEFT,0.0076686596,0.0,0.007668659556657076
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지 (남은 답변: 모든 질문 해결 완료)",LoRA,-0.00830439,0.0,0.008304390124976635
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지 (남은 답변: 모든 질문 해결 완료)",LoRA 와 QLoRA 의 차이,0.013840502,0.0,0.013840502128005028
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지 (남은 답변: 모든 질문 해결 완료)",Loss Function 관련 실무 경험,-0.0056037717,0.0,0.005603771656751633
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지 (남은 답변: 모든 질문 해결 완료)",Loss Function 예시,-0.031112405,0.0,0.03111240454018116
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지 (남은 답변: 모든 질문 해결 완료)",Loss Function 정의,0.026041359,0.0,0.026041358709335327
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지 (남은 답변: 모든 질문 해결 완료)",MBTI / 좋아하는 아이돌,-0.011145492,0.0,0.01114549208432436
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지 (남은 답변: 모든 질문 해결 완료)",MSE Loss 설명,0.009642303,0.0,0.009642302989959717
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지 (남은 답변: 모든 질문 해결 완료)",MSE Loss 용도,-0.00614482,0.0,0.0061448197811841965
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지 (남은 답변: 모든 질문 해결 완료)","Multi-Class, Multi-Label 중 BCE 가 좋은 task",0.0045773797,0.0,0.004577379673719406
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지 (남은 답변: 모든 질문 해결 완료)",Multi-Label 에서 CE + Softmax 적용 문제점,-0.007631594,0.0,0.0076315938495099545
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지 (남은 답변: 모든 질문 해결 완료)",PEFT 방법 5가지,0.0051636947,0.0,0.005163694731891155
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지 (남은 답변: 모든 질문 해결 완료)",거대 언어 모델 정의,0.92016715,1.0,0.07983285188674927
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지 (남은 답변: 모든 질문 해결 완료)",마지막 할 말,-0.00881679,0.0,0.008816789835691452
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지 (남은 답변: 모든 질문 해결 완료)",면접 시작 인사,-0.0026531622,0.0,0.0026531622279435396
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지 (남은 답변: 모든 질문 해결 완료)",면접 종료,0.014402971,0.0,0.014402970671653748
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지 (남은 답변: 모든 질문 해결 완료)","인공지능, 머신러닝, 딥러닝 차이",-0.03712828,0.0,0.037128280848264694
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지 (남은 답변: 모든 질문 해결 완료)",잠시 휴식,-0.029561903,0.0,0.029561903327703476
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지 (남은 답변: 모든 질문 해결 완료)",확률 예측에서 MSE Loss 미 사용 이유,-0.013865449,0.0,0.013865448534488678
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야. (남은 답변: 모든 질문 해결 완료)",BCE Loss 설명,0.024105184,0.0,0.02410518378019333
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야. (남은 답변: 모든 질문 해결 완료)",LLM Fine-Tuning 의 PEFT,0.008753932,0.0,0.00875393208116293
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야. (남은 답변: 모든 질문 해결 완료)",LoRA,-0.013742071,0.0,0.013742070645093918
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야. (남은 답변: 모든 질문 해결 완료)",LoRA 와 QLoRA 의 차이,0.016146742,0.0,0.016146741807460785
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야. (남은 답변: 모든 질문 해결 완료)",Loss Function 관련 실무 경험,-0.0023790945,0.0,0.002379094483330846
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야. (남은 답변: 모든 질문 해결 완료)",Loss Function 예시,-0.029308153,0.0,0.029308153316378593
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야. (남은 답변: 모든 질문 해결 완료)",Loss Function 정의,0.028467828,0.0,0.028467828407883644
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야. (남은 답변: 모든 질문 해결 완료)",MBTI / 좋아하는 아이돌,-0.021966621,0.0,0.02196662127971649
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야. (남은 답변: 모든 질문 해결 완료)",MSE Loss 설명,0.0062761544,0.0,0.006276154424995184
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야. (남은 답변: 모든 질문 해결 완료)",MSE Loss 용도,-0.008767037,0.0,0.008767036721110344
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야. (남은 답변: 모든 질문 해결 완료)","Multi-Class, Multi-Label 중 BCE 가 좋은 task",0.0050284667,0.0,0.005028466694056988
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야. (남은 답변: 모든 질문 해결 완료)",Multi-Label 에서 CE + Softmax 적용 문제점,-0.0022369565,0.0,0.002236956497654319
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야. (남은 답변: 모든 질문 해결 완료)",PEFT 방법 5가지,0.0017852612,0.0,0.0017852612072601914
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야. (남은 답변: 모든 질문 해결 완료)",거대 언어 모델 정의,0.9201021,1.0,0.07989788055419922
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야. (남은 답변: 모든 질문 해결 완료)",마지막 할 말,-0.01172735,0.0,0.011727349832654
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야. (남은 답변: 모든 질문 해결 완료)",면접 시작 인사,-0.0038408136,0.0,0.003840813646093011
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야. (남은 답변: 모든 질문 해결 완료)",면접 종료,0.011332076,0.0,0.011332076042890549
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야. (남은 답변: 모든 질문 해결 완료)","인공지능, 머신러닝, 딥러닝 차이",-0.024308223,0.0,0.0243082232773304
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야. (남은 답변: 모든 질문 해결 완료)",잠시 휴식,-0.03930544,0.0,0.039305441081523895
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야. (남은 답변: 모든 질문 해결 완료)",확률 예측에서 MSE Loss 미 사용 이유,-0.011974916,0.0,0.011974915862083435
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로 (남은 답변: 머신러닝, 딥러닝)",BCE Loss 설명,-0.00328337,0.0,0.003283370053395629
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로 (남은 답변: 머신러닝, 딥러닝)",LLM Fine-Tuning 의 PEFT,-0.004315202,0.0,0.004315202124416828
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로 (남은 답변: 머신러닝, 딥러닝)",LoRA,0.0036651264,0.0,0.003665126394480467
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로 (남은 답변: 머신러닝, 딥러닝)",LoRA 와 QLoRA 의 차이,-0.0044056373,0.0,0.00440563727170229
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로 (남은 답변: 머신러닝, 딥러닝)",Loss Function 관련 실무 경험,0.0036002325,0.0,0.0036002325359731913
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로 (남은 답변: 머신러닝, 딥러닝)",Loss Function 예시,0.015343946,0.0,0.015343946404755116
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로 (남은 답변: 머신러닝, 딥러닝)",Loss Function 정의,-0.028215868,0.0,0.028215868398547173
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로 (남은 답변: 머신러닝, 딥러닝)",MBTI / 좋아하는 아이돌,-0.014612906,0.0,0.01461290568113327
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로 (남은 답변: 머신러닝, 딥러닝)",MSE Loss 설명,0.010275336,0.0,0.010275335982441902
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로 (남은 답변: 머신러닝, 딥러닝)",MSE Loss 용도,0.0077050263,0.0,0.007705026306211948
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로 (남은 답변: 머신러닝, 딥러닝)","Multi-Class, Multi-Label 중 BCE 가 좋은 task",0.015307457,0.0,0.015307457186281681
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로 (남은 답변: 머신러닝, 딥러닝)",Multi-Label 에서 CE + Softmax 적용 문제점,-0.011073136,0.0,0.011073135770857334
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로 (남은 답변: 머신러닝, 딥러닝)",PEFT 방법 5가지,0.002877322,0.0,0.0028773220255970955
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로 (남은 답변: 머신러닝, 딥러닝)",거대 언어 모델 정의,-0.015811771,0.0,0.015811771154403687
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로 (남은 답변: 머신러닝, 딥러닝)",마지막 할 말,0.009362264,0.0,0.009362263604998589
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로 (남은 답변: 머신러닝, 딥러닝)",면접 시작 인사,-0.06315384,0.0,0.06315384060144424
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로 (남은 답변: 머신러닝, 딥러닝)",면접 종료,-0.0063773505,0.0,0.00637735053896904
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로 (남은 답변: 머신러닝, 딥러닝)","인공지능, 머신러닝, 딥러닝 차이",0.9680695,1.0,0.031930506229400635
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로 (남은 답변: 머신러닝, 딥러닝)",잠시 휴식,-0.01865362,0.0,0.018653620034456253
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로 (남은 답변: 머신러닝, 딥러닝)",확률 예측에서 MSE Loss 미 사용 이유,-0.0028345892,0.0,0.002834589220583439
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야 (남은 답변: 모든 질문 해결 완료),BCE Loss 설명,-0.04733154,0.0,0.047331541776657104
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야 (남은 답변: 모든 질문 해결 완료),LLM Fine-Tuning 의 PEFT,-0.00991294,0.0,0.009912939742207527
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야 (남은 답변: 모든 질문 해결 완료),LoRA,-0.046595708,0.0,0.04659570753574371
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야 (남은 답변: 모든 질문 해결 완료),LoRA 와 QLoRA 의 차이,0.019182023,0.0,0.019182022660970688
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야 (남은 답변: 모든 질문 해결 완료),Loss Function 관련 실무 경험,0.015867686,0.0,0.01586768589913845
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야 (남은 답변: 모든 질문 해결 완료),Loss Function 예시,0.42269358,0.0,0.4226935803890228
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야 (남은 답변: 모든 질문 해결 완료),Loss Function 정의,0.68954074,1.0,0.3104592561721802
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야 (남은 답변: 모든 질문 해결 완료),MBTI / 좋아하는 아이돌,-0.04190511,0.0,0.04190510883927345
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야 (남은 답변: 모든 질문 해결 완료),MSE Loss 설명,-0.006982759,0.0,0.006982759106904268
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야 (남은 답변: 모든 질문 해결 완료),MSE Loss 용도,0.0053237285,0.0,0.005323728546500206
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야 (남은 답변: 모든 질문 해결 완료),"Multi-Class, Multi-Label 중 BCE 가 좋은 task",-0.0038023621,0.0,0.0038023621309548616
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야 (남은 답변: 모든 질문 해결 완료),Multi-Label 에서 CE + Softmax 적용 문제점,-0.0018951214,0.0,0.0018951214151456952
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야 (남은 답변: 모든 질문 해결 완료),PEFT 방법 5가지,0.00018682049,0.0,0.0001868204853963107
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야 (남은 답변: 모든 질문 해결 완료),거대 언어 모델 정의,0.053492557,0.0,0.053492557257413864
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야 (남은 답변: 모든 질문 해결 완료),마지막 할 말,0.0057199537,0.0,0.005719953682273626
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야 (남은 답변: 모든 질문 해결 완료),면접 시작 인사,0.022289433,0.0,0.02228943258523941
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야 (남은 답변: 모든 질문 해결 완료),면접 종료,-0.0624364,0.0,0.06243640184402466
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야 (남은 답변: 모든 질문 해결 완료),"인공지능, 머신러닝, 딥러닝 차이",-0.02071672,0.0,0.020716719329357147
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야 (남은 답변: 모든 질문 해결 완료),잠시 휴식,-0.03606027,0.0,0.03606026992201805
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야 (남은 답변: 모든 질문 해결 완료),확률 예측에서 MSE Loss 미 사용 이유,0.03331736,0.0,0.033317361027002335
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데 (남은 답변: 거대 언어 모델 정의),BCE Loss 설명,-0.050988138,0.0,0.05098813772201538
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데 (남은 답변: 거대 언어 모델 정의),LLM Fine-Tuning 의 PEFT,-0.017025998,0.0,0.01702599786221981
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데 (남은 답변: 거대 언어 모델 정의),LoRA,-0.016355108,0.0,0.016355108469724655
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데 (남은 답변: 거대 언어 모델 정의),LoRA 와 QLoRA 의 차이,-0.019775398,0.0,0.019775398075580597
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데 (남은 답변: 거대 언어 모델 정의),Loss Function 관련 실무 경험,-0.009500368,0.0,0.009500367566943169
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데 (남은 답변: 거대 언어 모델 정의),Loss Function 예시,0.16078913,0.0,0.1607891321182251
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데 (남은 답변: 거대 언어 모델 정의),Loss Function 정의,0.41154817,0.0,0.4115481674671173
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데 (남은 답변: 거대 언어 모델 정의),MBTI / 좋아하는 아이돌,-0.024206571,0.0,0.02420657128095627
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데 (남은 답변: 거대 언어 모델 정의),MSE Loss 설명,-0.02288958,0.0,0.022889580577611923
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데 (남은 답변: 거대 언어 모델 정의),MSE Loss 용도,-0.033333983,0.0,0.03333398327231407
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데 (남은 답변: 거대 언어 모델 정의),"Multi-Class, Multi-Label 중 BCE 가 좋은 task",-0.038939573,0.0,0.038939572870731354
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데 (남은 답변: 거대 언어 모델 정의),Multi-Label 에서 CE + Softmax 적용 문제점,-0.023526862,0.0,0.023526862263679504
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데 (남은 답변: 거대 언어 모델 정의),PEFT 방법 5가지,0.01603475,0.0,0.016034750267863274
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데 (남은 답변: 거대 언어 모델 정의),거대 언어 모델 정의,0.783029,1.0,0.21697098016738892
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데 (남은 답변: 거대 언어 모델 정의),마지막 할 말,-0.014571153,0.0,0.014571152627468109
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데 (남은 답변: 거대 언어 모델 정의),면접 시작 인사,-0.035858884,0.0,0.0358588844537735
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데 (남은 답변: 거대 언어 모델 정의),면접 종료,-0.043959953,0.0,0.043959952890872955
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데 (남은 답변: 거대 언어 모델 정의),"인공지능, 머신러닝, 딥러닝 차이",-0.06630409,0.0,0.06630408763885498
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데 (남은 답변: 거대 언어 모델 정의),잠시 휴식,-0.04184253,0.0,0.04184253141283989
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데 (남은 답변: 거대 언어 모델 정의),확률 예측에서 MSE Loss 미 사용 이유,-0.026043179,0.0,0.026043178513646126
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델 (남은 답변: 모든 질문 해결 완료),BCE Loss 설명,-0.036715306,0.0,0.0367153063416481
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델 (남은 답변: 모든 질문 해결 완료),LLM Fine-Tuning 의 PEFT,-0.0036249517,0.0,0.0036249516997486353
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델 (남은 답변: 모든 질문 해결 완료),LoRA,-0.06229263,0.0,0.06229263171553612
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델 (남은 답변: 모든 질문 해결 완료),LoRA 와 QLoRA 의 차이,0.011650671,0.0,0.011650671251118183
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델 (남은 답변: 모든 질문 해결 완료),Loss Function 관련 실무 경험,0.011582929,0.0,0.011582928709685802
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델 (남은 답변: 모든 질문 해결 완료),Loss Function 예시,0.46391267,0.0,0.4639126658439636
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델 (남은 답변: 모든 질문 해결 완료),Loss Function 정의,0.6740896,1.0,0.32591038942337036
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델 (남은 답변: 모든 질문 해결 완료),MBTI / 좋아하는 아이돌,-0.018456906,0.0,0.01845690608024597
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델 (남은 답변: 모든 질문 해결 완료),MSE Loss 설명,-0.0042316043,0.0,0.004231604281812906
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델 (남은 답변: 모든 질문 해결 완료),MSE Loss 용도,0.0037382664,0.0,0.0037382664158940315
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델 (남은 답변: 모든 질문 해결 완료),"Multi-Class, Multi-Label 중 BCE 가 좋은 task",-0.004241772,0.0,0.004241771996021271
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델 (남은 답변: 모든 질문 해결 완료),Multi-Label 에서 CE + Softmax 적용 문제점,-0.002272738,0.0,0.0022727379109710455
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델 (남은 답변: 모든 질문 해결 완료),PEFT 방법 5가지,0.023756666,0.0,0.023756666108965874
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델 (남은 답변: 모든 질문 해결 완료),거대 언어 모델 정의,-0.0032941075,0.0,0.0032941075041890144
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델 (남은 답변: 모든 질문 해결 완료),마지막 할 말,0.0025310535,0.0,0.0025310534983873367
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델 (남은 답변: 모든 질문 해결 완료),면접 시작 인사,0.013892038,0.0,0.01389203779399395
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델 (남은 답변: 모든 질문 해결 완료),면접 종료,-0.029070096,0.0,0.029070096090435982
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델 (남은 답변: 모든 질문 해결 완료),"인공지능, 머신러닝, 딥러닝 차이",-0.011184346,0.0,0.011184345930814743
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델 (남은 답변: 모든 질문 해결 완료),잠시 휴식,-0.014948489,0.0,0.014948489144444466
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델 (남은 답변: 모든 질문 해결 완료),확률 예측에서 MSE Loss 미 사용 이유,-0.041488063,0.0,0.04148806259036064
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지. (남은 답변: 거대 언어 모델 정의),BCE Loss 설명,0.0067055677,0.0,0.006705567706376314
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지. (남은 답변: 거대 언어 모델 정의),LLM Fine-Tuning 의 PEFT,0.011827115,0.0,0.011827114969491959
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지. (남은 답변: 거대 언어 모델 정의),LoRA,-0.0166306,0.0,0.01663059927523136
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지. (남은 답변: 거대 언어 모델 정의),LoRA 와 QLoRA 의 차이,-0.0024442102,0.0,0.0024442102294415236
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지. (남은 답변: 거대 언어 모델 정의),Loss Function 관련 실무 경험,0.014313106,0.0,0.014313106425106525
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지. (남은 답변: 거대 언어 모델 정의),Loss Function 예시,-0.04957809,0.0,0.04957808926701546
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지. (남은 답변: 거대 언어 모델 정의),Loss Function 정의,0.04285133,0.0,0.04285132884979248
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지. (남은 답변: 거대 언어 모델 정의),MBTI / 좋아하는 아이돌,-0.023997318,0.0,0.023997317999601364
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지. (남은 답변: 거대 언어 모델 정의),MSE Loss 설명,-0.008939225,0.0,0.008939225226640701
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지. (남은 답변: 거대 언어 모델 정의),MSE Loss 용도,-0.028970504,0.0,0.0289705041795969
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지. (남은 답변: 거대 언어 모델 정의),"Multi-Class, Multi-Label 중 BCE 가 좋은 task",-6.9046466e-05,0.0,6.904646579641849e-05
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지. (남은 답변: 거대 언어 모델 정의),Multi-Label 에서 CE + Softmax 적용 문제점,0.00019121895,0.0,0.00019121894729323685
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지. (남은 답변: 거대 언어 모델 정의),PEFT 방법 5가지,-0.0028789171,0.0,0.002878917148336768
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지. (남은 답변: 거대 언어 모델 정의),거대 언어 모델 정의,0.9634733,1.0,0.03652667999267578
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지. (남은 답변: 거대 언어 모델 정의),마지막 할 말,-0.0064738635,0.0,0.006473863497376442
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지. (남은 답변: 거대 언어 모델 정의),면접 시작 인사,0.0035267558,0.0,0.003526755841448903
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지. (남은 답변: 거대 언어 모델 정의),면접 종료,-0.015123985,0.0,0.015123984776437283
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지. (남은 답변: 거대 언어 모델 정의),"인공지능, 머신러닝, 딥러닝 차이",-0.029694244,0.0,0.029694244265556335
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지. (남은 답변: 거대 언어 모델 정의),잠시 휴식,-0.009049693,0.0,0.009049693122506142
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지. (남은 답변: 거대 언어 모델 정의),확률 예측에서 MSE Loss 미 사용 이유,-0.015765643,0.0,0.015765642747282982
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야 (남은 답변: Loss Function 정의),BCE Loss 설명,-0.054558035,0.0,0.05455803498625755
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야 (남은 답변: Loss Function 정의),LLM Fine-Tuning 의 PEFT,-0.011336986,0.0,0.011336985975503922
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야 (남은 답변: Loss Function 정의),LoRA,-0.0060090516,0.0,0.006009051576256752
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야 (남은 답변: Loss Function 정의),LoRA 와 QLoRA 의 차이,0.005525447,0.0,0.005525446962565184
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야 (남은 답변: Loss Function 정의),Loss Function 관련 실무 경험,-0.023630928,0.0,0.023630928248167038
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야 (남은 답변: Loss Function 정의),Loss Function 예시,0.4855789,0.0,0.48557889461517334
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야 (남은 답변: Loss Function 정의),Loss Function 정의,0.7126201,1.0,0.2873799204826355
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야 (남은 답변: Loss Function 정의),MBTI / 좋아하는 아이돌,-0.04181825,0.0,0.041818249970674515
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야 (남은 답변: Loss Function 정의),MSE Loss 설명,-0.057134673,0.0,0.05713467299938202
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야 (남은 답변: Loss Function 정의),MSE Loss 용도,-0.0517176,0.0,0.0517176017165184
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야 (남은 답변: Loss Function 정의),"Multi-Class, Multi-Label 중 BCE 가 좋은 task",0.008027928,0.0,0.008027927950024605
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야 (남은 답변: Loss Function 정의),Multi-Label 에서 CE + Softmax 적용 문제점,-0.0017973756,0.0,0.001797375618480146
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야 (남은 답변: Loss Function 정의),PEFT 방법 5가지,0.012329528,0.0,0.012329528108239174
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야 (남은 답변: Loss Function 정의),거대 언어 모델 정의,0.008805424,0.0,0.008805423974990845
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야 (남은 답변: Loss Function 정의),마지막 할 말,-0.0036560663,0.0,0.003656066255643964
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야 (남은 답변: Loss Function 정의),면접 시작 인사,-0.0040284614,0.0,0.004028461407870054
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야 (남은 답변: Loss Function 정의),면접 종료,-0.024814522,0.0,0.02481452189385891
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야 (남은 답변: Loss Function 정의),"인공지능, 머신러닝, 딥러닝 차이",-0.015632786,0.0,0.015632785856723785
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야 (남은 답변: Loss Function 정의),잠시 휴식,-0.010901396,0.0,0.010901396162807941
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야 (남은 답변: Loss Function 정의),확률 예측에서 MSE Loss 미 사용 이유,0.026134513,0.0,0.026134513318538666
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지! (남은 답변: 모든 질문 해결 완료)",BCE Loss 설명,0.006175437,0.0,0.00617543701082468
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지! (남은 답변: 모든 질문 해결 완료)",LLM Fine-Tuning 의 PEFT,-0.03163302,0.0,0.03163301944732666
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지! (남은 답변: 모든 질문 해결 완료)",LoRA,-0.012155223,0.0,0.012155222706496716
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지! (남은 답변: 모든 질문 해결 완료)",LoRA 와 QLoRA 의 차이,-0.00064734166,0.0,0.0006473416578955948
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지! (남은 답변: 모든 질문 해결 완료)",Loss Function 관련 실무 경험,-0.038177747,0.0,0.038177747279405594
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지! (남은 답변: 모든 질문 해결 완료)",Loss Function 예시,0.5316725,1.0,0.46832752227783203
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지! (남은 답변: 모든 질문 해결 완료)",Loss Function 정의,0.6568007,0.0,0.6568006873130798
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지! (남은 답변: 모든 질문 해결 완료)",MBTI / 좋아하는 아이돌,0.0012559381,0.0,0.001255938084796071
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지! (남은 답변: 모든 질문 해결 완료)",MSE Loss 설명,-0.015039509,0.0,0.015039509162306786
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지! (남은 답변: 모든 질문 해결 완료)",MSE Loss 용도,-0.004334002,0.0,0.004334001801908016
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지! (남은 답변: 모든 질문 해결 완료)","Multi-Class, Multi-Label 중 BCE 가 좋은 task",-0.018919887,0.0,0.018919887021183968
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지! (남은 답변: 모든 질문 해결 완료)",Multi-Label 에서 CE + Softmax 적용 문제점,-0.010466991,0.0,0.010466990992426872
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지! (남은 답변: 모든 질문 해결 완료)",PEFT 방법 5가지,0.022232266,0.0,0.022232266142964363
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지! (남은 답변: 모든 질문 해결 완료)",거대 언어 모델 정의,-0.07766491,0.0,0.07766491174697876
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지! (남은 답변: 모든 질문 해결 완료)",마지막 할 말,0.007459438,0.0,0.007459437940269709
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지! (남은 답변: 모든 질문 해결 완료)",면접 시작 인사,-0.002101137,0.0,0.0021011370699852705
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지! (남은 답변: 모든 질문 해결 완료)",면접 종료,-0.025233246,0.0,0.025233246386051178
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지! (남은 답변: 모든 질문 해결 완료)","인공지능, 머신러닝, 딥러닝 차이",-0.0050002635,0.0,0.005000263452529907
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지! (남은 답변: 모든 질문 해결 완료)",잠시 휴식,-0.044773795,0.0,0.04477379471063614
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지! (남은 답변: 모든 질문 해결 완료)",확률 예측에서 MSE Loss 미 사용 이유,-0.0252608,0.0,0.02526080049574375
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야 (남은 답변: Loss Function 정의),BCE Loss 설명,-0.07106645,0.0,0.07106644660234451
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야 (남은 답변: Loss Function 정의),LLM Fine-Tuning 의 PEFT,-0.0118526155,0.0,0.011852615512907505
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야 (남은 답변: Loss Function 정의),LoRA,0.005513224,0.0,0.005513223819434643
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야 (남은 답변: Loss Function 정의),LoRA 와 QLoRA 의 차이,-0.000513003,0.0,0.0005130029749125242
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야 (남은 답변: Loss Function 정의),Loss Function 관련 실무 경험,-0.032792155,0.0,0.03279215469956398
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야 (남은 답변: Loss Function 정의),Loss Function 예시,0.48796055,0.0,0.487960547208786
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야 (남은 답변: Loss Function 정의),Loss Function 정의,0.7177146,1.0,0.2822853922843933
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야 (남은 답변: Loss Function 정의),MBTI / 좋아하는 아이돌,-0.02559947,0.0,0.025599470362067223
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야 (남은 답변: Loss Function 정의),MSE Loss 설명,-0.049649995,0.0,0.04964999482035637
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야 (남은 답변: Loss Function 정의),MSE Loss 용도,-0.04299538,0.0,0.042995378375053406
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야 (남은 답변: Loss Function 정의),"Multi-Class, Multi-Label 중 BCE 가 좋은 task",0.002385254,0.0,0.002385254018008709
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야 (남은 답변: Loss Function 정의),Multi-Label 에서 CE + Softmax 적용 문제점,0.010188987,0.0,0.010188986547291279
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야 (남은 답변: Loss Function 정의),PEFT 방법 5가지,0.015007915,0.0,0.01500791497528553
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야 (남은 답변: Loss Function 정의),거대 언어 모델 정의,-0.003571549,0.0,0.003571548964828253
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야 (남은 답변: Loss Function 정의),마지막 할 말,-0.014584747,0.0,0.014584747143089771
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야 (남은 답변: Loss Function 정의),면접 시작 인사,0.0014632579,0.0,0.0014632579404860735
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야 (남은 답변: Loss Function 정의),면접 종료,-0.033939134,0.0,0.03393913432955742
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야 (남은 답변: Loss Function 정의),"인공지능, 머신러닝, 딥러닝 차이",-0.017747784,0.0,0.0177477840334177
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야 (남은 답변: Loss Function 정의),잠시 휴식,-0.010133994,0.0,0.010133993811905384
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야 (남은 답변: Loss Function 정의),확률 예측에서 MSE Loss 미 사용 이유,0.030211056,0.0,0.030211055651307106
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지 (남은 답변: 모든 질문 해결 완료),BCE Loss 설명,0.0072050123,0.0,0.007205012254416943
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지 (남은 답변: 모든 질문 해결 완료),LLM Fine-Tuning 의 PEFT,-0.009929226,0.0,0.009929225780069828
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지 (남은 답변: 모든 질문 해결 완료),LoRA,-0.027182912,0.0,0.027182912454009056
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지 (남은 답변: 모든 질문 해결 완료),LoRA 와 QLoRA 의 차이,0.005451862,0.0,0.005451861768960953
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지 (남은 답변: 모든 질문 해결 완료),Loss Function 관련 실무 경험,-0.03089862,0.0,0.030898619443178177
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지 (남은 답변: 모든 질문 해결 완료),Loss Function 예시,0.5049203,1.0,0.4950796961784363
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지 (남은 답변: 모든 질문 해결 완료),Loss Function 정의,0.6774001,0.0,0.6774001121520996
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지 (남은 답변: 모든 질문 해결 완료),MBTI / 좋아하는 아이돌,-0.03640781,0.0,0.03640780970454216
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지 (남은 답변: 모든 질문 해결 완료),MSE Loss 설명,-0.0071302797,0.0,0.0071302796714007854
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지 (남은 답변: 모든 질문 해결 완료),MSE Loss 용도,-0.00020993,0.0,0.00020993000362068415
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지 (남은 답변: 모든 질문 해결 완료),"Multi-Class, Multi-Label 중 BCE 가 좋은 task",0.0027122835,0.0,0.002712283516302705
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지 (남은 답변: 모든 질문 해결 완료),Multi-Label 에서 CE + Softmax 적용 문제점,0.020716082,0.0,0.02071608230471611
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지 (남은 답변: 모든 질문 해결 완료),PEFT 방법 5가지,-0.019597493,0.0,0.01959749311208725
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지 (남은 답변: 모든 질문 해결 완료),거대 언어 모델 정의,-0.045859504,0.0,0.045859504491090775
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지 (남은 답변: 모든 질문 해결 완료),마지막 할 말,0.0027580096,0.0,0.0027580095920711756
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지 (남은 답변: 모든 질문 해결 완료),면접 시작 인사,0.00028969464,0.0,0.00028969463892281055
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지 (남은 답변: 모든 질문 해결 완료),면접 종료,-0.03274879,0.0,0.032748788595199585
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지 (남은 답변: 모든 질문 해결 완료),"인공지능, 머신러닝, 딥러닝 차이",-0.0053840876,0.0,0.005384087562561035
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지 (남은 답변: 모든 질문 해결 완료),잠시 휴식,-0.04107454,0.0,0.041074540466070175
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지 (남은 답변: 모든 질문 해결 완료),확률 예측에서 MSE Loss 미 사용 이유,-0.015140123,0.0,0.01514012273401022
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데 (남은 답변: Loss Function 예시),BCE Loss 설명,0.0066058175,0.0,0.006605817470699549
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데 (남은 답변: Loss Function 예시),LLM Fine-Tuning 의 PEFT,-0.021399893,0.0,0.02139989286661148
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데 (남은 답변: Loss Function 예시),LoRA,-0.044932663,0.0,0.044932663440704346
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데 (남은 답변: Loss Function 예시),LoRA 와 QLoRA 의 차이,0.049520202,0.0,0.04952020198106766
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데 (남은 답변: Loss Function 예시),Loss Function 관련 실무 경험,-0.039802387,0.0,0.03980238735675812
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데 (남은 답변: Loss Function 예시),Loss Function 예시,0.5463938,1.0,0.45360618829727173
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데 (남은 답변: Loss Function 예시),Loss Function 정의,0.501945,0.0,0.5019450187683105
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데 (남은 답변: Loss Function 예시),MBTI / 좋아하는 아이돌,0.011149582,0.0,0.011149582453072071
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데 (남은 답변: Loss Function 예시),MSE Loss 설명,-0.028510222,0.0,0.02851022221148014
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데 (남은 답변: Loss Function 예시),MSE Loss 용도,-0.042104162,0.0,0.04210416227579117
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데 (남은 답변: Loss Function 예시),"Multi-Class, Multi-Label 중 BCE 가 좋은 task",-0.012708636,0.0,0.012708636000752449
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데 (남은 답변: Loss Function 예시),Multi-Label 에서 CE + Softmax 적용 문제점,-0.001735247,0.0,0.0017352469731122255
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데 (남은 답변: Loss Function 예시),PEFT 방법 5가지,-0.0059205755,0.0,0.005920575466006994
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데 (남은 답변: Loss Function 예시),거대 언어 모델 정의,0.021143163,0.0,0.02114316262304783
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데 (남은 답변: Loss Function 예시),마지막 할 말,-0.06163275,0.0,0.06163274869322777
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데 (남은 답변: Loss Function 예시),면접 시작 인사,0.0057274685,0.0,0.005727468524128199
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데 (남은 답변: Loss Function 예시),면접 종료,0.024798943,0.0,0.024798942729830742
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데 (남은 답변: Loss Function 예시),"인공지능, 머신러닝, 딥러닝 차이",0.028111137,0.0,0.028111137449741364
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데 (남은 답변: Loss Function 예시),잠시 휴식,0.04209465,0.0,0.042094651609659195
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데 (남은 답변: Loss Function 예시),확률 예측에서 MSE Loss 미 사용 이유,-0.033281136,0.0,0.03328113630414009
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지 (남은 답변: 모든 질문 해결 완료)",BCE Loss 설명,0.013903412,0.0,0.013903412036597729
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지 (남은 답변: 모든 질문 해결 완료)",LLM Fine-Tuning 의 PEFT,-0.0037408902,0.0,0.003740890184417367
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지 (남은 답변: 모든 질문 해결 완료)",LoRA,-0.018856589,0.0,0.018856588751077652
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지 (남은 답변: 모든 질문 해결 완료)",LoRA 와 QLoRA 의 차이,0.035259753,0.0,0.035259753465652466
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지 (남은 답변: 모든 질문 해결 완료)",Loss Function 관련 실무 경험,-0.012426732,0.0,0.01242673210799694
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지 (남은 답변: 모든 질문 해결 완료)",Loss Function 예시,-0.008428071,0.0,0.008428070694208145
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지 (남은 답변: 모든 질문 해결 완료)",Loss Function 정의,-0.034139685,0.0,0.034139685332775116
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지 (남은 답변: 모든 질문 해결 완료)",MBTI / 좋아하는 아이돌,-0.018445754,0.0,0.018445754423737526
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지 (남은 답변: 모든 질문 해결 완료)",MSE Loss 설명,0.5780383,1.0,0.4219617247581482
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지 (남은 답변: 모든 질문 해결 완료)",MSE Loss 용도,0.5784348,0.0,0.5784348249435425
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지 (남은 답변: 모든 질문 해결 완료)","Multi-Class, Multi-Label 중 BCE 가 좋은 task",-0.065670684,0.0,0.0656706839799881
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지 (남은 답변: 모든 질문 해결 완료)",Multi-Label 에서 CE + Softmax 적용 문제점,0.009870645,0.0,0.00987064465880394
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지 (남은 답변: 모든 질문 해결 완료)",PEFT 방법 5가지,0.0020068646,0.0,0.0020068646408617496
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지 (남은 답변: 모든 질문 해결 완료)",거대 언어 모델 정의,-0.014019143,0.0,0.014019142836332321
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지 (남은 답변: 모든 질문 해결 완료)",마지막 할 말,-0.055245154,0.0,0.05524515360593796
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지 (남은 답변: 모든 질문 해결 완료)",면접 시작 인사,-0.011684362,0.0,0.011684361845254898
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지 (남은 답변: 모든 질문 해결 완료)",면접 종료,-0.060686775,0.0,0.06068677455186844
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지 (남은 답변: 모든 질문 해결 완료)","인공지능, 머신러닝, 딥러닝 차이",0.008800623,0.0,0.008800623007118702
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지 (남은 답변: 모든 질문 해결 완료)",잠시 휴식,0.009179717,0.0,0.009179716929793358
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지 (남은 답변: 모든 질문 해결 완료)",확률 예측에서 MSE Loss 미 사용 이유,-0.056348916,0.0,0.05634891614317894
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어 (남은 답변: 모든 질문 해결 완료)",BCE Loss 설명,0.92799133,1.0,0.07200866937637329
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어 (남은 답변: 모든 질문 해결 완료)",LLM Fine-Tuning 의 PEFT,-0.005773978,0.0,0.005773977842181921
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어 (남은 답변: 모든 질문 해결 완료)",LoRA,0.033412002,0.0,0.03341200202703476
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어 (남은 답변: 모든 질문 해결 완료)",LoRA 와 QLoRA 의 차이,-0.0018848578,0.0,0.0018848577747121453
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어 (남은 답변: 모든 질문 해결 완료)",Loss Function 관련 실무 경험,-0.022161078,0.0,0.022161077708005905
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어 (남은 답변: 모든 질문 해결 완료)",Loss Function 예시,0.019826194,0.0,0.019826194271445274
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어 (남은 답변: 모든 질문 해결 완료)",Loss Function 정의,-0.04352216,0.0,0.04352216050028801
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어 (남은 답변: 모든 질문 해결 완료)",MBTI / 좋아하는 아이돌,0.00031486733,0.0,0.00031486732768826187
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어 (남은 답변: 모든 질문 해결 완료)",MSE Loss 설명,0.08156311,0.0,0.08156310766935349
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어 (남은 답변: 모든 질문 해결 완료)",MSE Loss 용도,0.0408721,0.0,0.04087210074067116
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어 (남은 답변: 모든 질문 해결 완료)","Multi-Class, Multi-Label 중 BCE 가 좋은 task",-0.028571831,0.0,0.028571831062436104
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어 (남은 답변: 모든 질문 해결 완료)",Multi-Label 에서 CE + Softmax 적용 문제점,-0.020807281,0.0,0.020807281136512756
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어 (남은 답변: 모든 질문 해결 완료)",PEFT 방법 5가지,0.0052670767,0.0,0.005267076659947634
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어 (남은 답변: 모든 질문 해결 완료)",거대 언어 모델 정의,0.0030559718,0.0,0.0030559718143194914
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어 (남은 답변: 모든 질문 해결 완료)",마지막 할 말,-0.011661607,0.0,0.011661606840789318
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어 (남은 답변: 모든 질문 해결 완료)",면접 시작 인사,0.0047941054,0.0,0.004794105421751738
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어 (남은 답변: 모든 질문 해결 완료)",면접 종료,-0.05150706,0.0,0.05150705948472023
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어 (남은 답변: 모든 질문 해결 완료)","인공지능, 머신러닝, 딥러닝 차이",-0.009706223,0.0,0.009706223383545876
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어 (남은 답변: 모든 질문 해결 완료)",잠시 휴식,0.0070313453,0.0,0.007031345274299383
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어 (남은 답변: 모든 질문 해결 완료)",확률 예측에서 MSE Loss 미 사용 이유,-0.045890875,0.0,0.04589087516069412
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야? (남은 답변: Loss Function 예시)",BCE Loss 설명,-0.0147436345,0.0,0.01474363449960947
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야? (남은 답변: Loss Function 예시)",LLM Fine-Tuning 의 PEFT,-0.020777563,0.0,0.020777562633156776
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야? (남은 답변: Loss Function 예시)",LoRA,-0.03987089,0.0,0.039870891720056534
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야? (남은 답변: Loss Function 예시)",LoRA 와 QLoRA 의 차이,0.030339375,0.0,0.030339375138282776
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야? (남은 답변: Loss Function 예시)",Loss Function 관련 실무 경험,-0.04641984,0.0,0.046419840306043625
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야? (남은 답변: Loss Function 예시)",Loss Function 예시,0.54036784,1.0,0.45963215827941895
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야? (남은 답변: Loss Function 예시)",Loss Function 정의,0.50431865,0.0,0.5043186545372009
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야? (남은 답변: Loss Function 예시)",MBTI / 좋아하는 아이돌,-0.001658029,0.0,0.0016580290393903852
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야? (남은 답변: Loss Function 예시)",MSE Loss 설명,-0.033211492,0.0,0.033211492002010345
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야? (남은 답변: Loss Function 예시)",MSE Loss 용도,-0.04404138,0.0,0.044041380286216736
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야? (남은 답변: Loss Function 예시)","Multi-Class, Multi-Label 중 BCE 가 좋은 task",-0.021005444,0.0,0.02100544422864914
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야? (남은 답변: Loss Function 예시)",Multi-Label 에서 CE + Softmax 적용 문제점,0.0011828324,0.0,0.0011828324059024453
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야? (남은 답변: Loss Function 예시)",PEFT 방법 5가지,-0.0093524605,0.0,0.009352460503578186
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야? (남은 답변: Loss Function 예시)",거대 언어 모델 정의,0.021883467,0.0,0.021883467212319374
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야? (남은 답변: Loss Function 예시)",마지막 할 말,-0.08331582,0.0,0.08331581950187683
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야? (남은 답변: Loss Function 예시)",면접 시작 인사,0.0025481666,0.0,0.002548166550695896
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야? (남은 답변: Loss Function 예시)",면접 종료,0.0063771997,0.0,0.006377199664711952
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야? (남은 답변: Loss Function 예시)","인공지능, 머신러닝, 딥러닝 차이",0.04950381,0.0,0.049503810703754425
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야? (남은 답변: Loss Function 예시)",잠시 휴식,0.018951343,0.0,0.01895134337246418
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야? (남은 답변: Loss Function 예시)",확률 예측에서 MSE Loss 미 사용 이유,0.001003031,0.0,0.0010030310368165374
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어 (남은 답변: 모든 질문 해결 완료)",BCE Loss 설명,0.005476922,0.0,0.005476921796798706
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어 (남은 답변: 모든 질문 해결 완료)",LLM Fine-Tuning 의 PEFT,-0.0019968627,0.0,0.0019968627020716667
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어 (남은 답변: 모든 질문 해결 완료)",LoRA,-0.0384899,0.0,0.03848990052938461
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어 (남은 답변: 모든 질문 해결 완료)",LoRA 와 QLoRA 의 차이,0.04261639,0.0,0.04261638969182968
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어 (남은 답변: 모든 질문 해결 완료)",Loss Function 관련 실무 경험,0.0046217856,0.0,0.0046217855997383595
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어 (남은 답변: 모든 질문 해결 완료)",Loss Function 예시,-0.014624286,0.0,0.014624286442995071
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어 (남은 답변: 모든 질문 해결 완료)",Loss Function 정의,-0.023051511,0.0,0.023051511496305466
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어 (남은 답변: 모든 질문 해결 완료)",MBTI / 좋아하는 아이돌,-0.010011315,0.0,0.01001131534576416
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어 (남은 답변: 모든 질문 해결 완료)",MSE Loss 설명,0.5948088,1.0,0.40519118309020996
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어 (남은 답변: 모든 질문 해결 완료)",MSE Loss 용도,0.5877738,0.0,0.5877737998962402
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어 (남은 답변: 모든 질문 해결 완료)","Multi-Class, Multi-Label 중 BCE 가 좋은 task",-0.06388002,0.0,0.06388001888990402
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어 (남은 답변: 모든 질문 해결 완료)",Multi-Label 에서 CE + Softmax 적용 문제점,-0.01965549,0.0,0.019655490294098854
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어 (남은 답변: 모든 질문 해결 완료)",PEFT 방법 5가지,-0.0053056036,0.0,0.005305603612214327
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어 (남은 답변: 모든 질문 해결 완료)",거대 언어 모델 정의,0.009355495,0.0,0.009355494752526283
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어 (남은 답변: 모든 질문 해결 완료)",마지막 할 말,-0.05229774,0.0,0.052297741174697876
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어 (남은 답변: 모든 질문 해결 완료)",면접 시작 인사,-0.016584812,0.0,0.016584811732172966
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어 (남은 답변: 모든 질문 해결 완료)",면접 종료,-0.026447888,0.0,0.02644788846373558
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어 (남은 답변: 모든 질문 해결 완료)","인공지능, 머신러닝, 딥러닝 차이",0.010703584,0.0,0.010703584179282188
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어 (남은 답변: 모든 질문 해결 완료)",잠시 휴식,-0.026063683,0.0,0.02606368251144886
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어 (남은 답변: 모든 질문 해결 완료)",확률 예측에서 MSE Loss 미 사용 이유,-0.0055788225,0.0,0.005578822456300259
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아 (남은 답변: 모든 질문 해결 완료)",BCE Loss 설명,0.9289161,1.0,0.0710839033126831
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아 (남은 답변: 모든 질문 해결 완료)",LLM Fine-Tuning 의 PEFT,-0.008811135,0.0,0.008811134845018387
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아 (남은 답변: 모든 질문 해결 완료)",LoRA,0.021740127,0.0,0.021740127354860306
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아 (남은 답변: 모든 질문 해결 완료)",LoRA 와 QLoRA 의 차이,0.008382584,0.0,0.00838258396834135
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아 (남은 답변: 모든 질문 해결 완료)",Loss Function 관련 실무 경험,-0.017202962,0.0,0.017202962189912796
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아 (남은 답변: 모든 질문 해결 완료)",Loss Function 예시,0.008593054,0.0,0.008593053556978703
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아 (남은 답변: 모든 질문 해결 완료)",Loss Function 정의,-0.06154504,0.0,0.06154504045844078
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아 (남은 답변: 모든 질문 해결 완료)",MBTI / 좋아하는 아이돌,-0.002573392,0.0,0.0025733918882906437
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아 (남은 답변: 모든 질문 해결 완료)",MSE Loss 설명,0.070725225,0.0,0.0707252249121666
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아 (남은 답변: 모든 질문 해결 완료)",MSE Loss 용도,0.03267108,0.0,0.03267107903957367
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아 (남은 답변: 모든 질문 해결 완료)","Multi-Class, Multi-Label 중 BCE 가 좋은 task",-0.014766266,0.0,0.014766265638172626
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아 (남은 답변: 모든 질문 해결 완료)",Multi-Label 에서 CE + Softmax 적용 문제점,-0.0010173909,0.0,0.0010173908667638898
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아 (남은 답변: 모든 질문 해결 완료)",PEFT 방법 5가지,0.011168012,0.0,0.011168012395501137
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아 (남은 답변: 모든 질문 해결 완료)",거대 언어 모델 정의,-0.003047229,0.0,0.0030472290236502886
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아 (남은 답변: 모든 질문 해결 완료)",마지막 할 말,-0.0076603596,0.0,0.007660359609872103
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아 (남은 답변: 모든 질문 해결 완료)",면접 시작 인사,0.006869142,0.0,0.006869141943752766
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아 (남은 답변: 모든 질문 해결 완료)",면접 종료,-0.040407788,0.0,0.04040778800845146
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아 (남은 답변: 모든 질문 해결 완료)","인공지능, 머신러닝, 딥러닝 차이",-0.003058296,0.0,0.0030582959298044443
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아 (남은 답변: 모든 질문 해결 완료)",잠시 휴식,-0.0035254464,0.0,0.0035254464019089937
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아 (남은 답변: 모든 질문 해결 완료)",확률 예측에서 MSE Loss 미 사용 이유,-0.062746406,0.0,0.06274640560150146
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야 (남은 답변: 모든 질문 해결 완료),BCE Loss 설명,-0.107339986,0.0,0.10733998566865921
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야 (남은 답변: 모든 질문 해결 완료),LLM Fine-Tuning 의 PEFT,-0.023401177,0.0,0.023401176556944847
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야 (남은 답변: 모든 질문 해결 완료),LoRA,-0.03430483,0.0,0.03430483117699623
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야 (남은 답변: 모든 질문 해결 완료),LoRA 와 QLoRA 의 차이,0.0020827625,0.0,0.0020827625412493944
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야 (남은 답변: 모든 질문 해결 완료),Loss Function 관련 실무 경험,-0.01150872,0.0,0.011508719995617867
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야 (남은 답변: 모든 질문 해결 완료),Loss Function 예시,-0.024595,0.0,0.024594999849796295
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야 (남은 답변: 모든 질문 해결 완료),Loss Function 정의,-0.024307441,0.0,0.02430744096636772
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야 (남은 답변: 모든 질문 해결 완료),MBTI / 좋아하는 아이돌,0.009885012,0.0,0.009885012172162533
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야 (남은 답변: 모든 질문 해결 완료),MSE Loss 설명,0.50024354,0.0,0.5002435445785522
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야 (남은 답변: 모든 질문 해결 완료),MSE Loss 용도,0.53723305,1.0,0.46276694536209106
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야 (남은 답변: 모든 질문 해결 완료),"Multi-Class, Multi-Label 중 BCE 가 좋은 task",0.074176975,0.0,0.07417697459459305
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야 (남은 답변: 모든 질문 해결 완료),Multi-Label 에서 CE + Softmax 적용 문제점,-0.013224902,0.0,0.013224901631474495
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야 (남은 답변: 모든 질문 해결 완료),PEFT 방법 5가지,-0.013446508,0.0,0.013446507975459099
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야 (남은 답변: 모든 질문 해결 완료),거대 언어 모델 정의,-0.03559061,0.0,0.03559061139822006
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야 (남은 답변: 모든 질문 해결 완료),마지막 할 말,0.043459132,0.0,0.04345913231372833
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야 (남은 답변: 모든 질문 해결 완료),면접 시작 인사,-0.0070690047,0.0,0.007069004699587822
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야 (남은 답변: 모든 질문 해결 완료),면접 종료,-0.014073398,0.0,0.01407339796423912
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야 (남은 답변: 모든 질문 해결 완료),"인공지능, 머신러닝, 딥러닝 차이",-0.013168644,0.0,0.013168644160032272
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야 (남은 답변: 모든 질문 해결 완료),잠시 휴식,-0.03144116,0.0,0.031441159546375275
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야 (남은 답변: 모든 질문 해결 완료),확률 예측에서 MSE Loss 미 사용 이유,-0.025230417,0.0,0.025230417028069496
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지? (남은 답변: MSE Loss 설명),BCE Loss 설명,0.023787953,0.0,0.023787952959537506
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지? (남은 답변: MSE Loss 설명),LLM Fine-Tuning 의 PEFT,0.000979293,0.0,0.0009792930213734508
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지? (남은 답변: MSE Loss 설명),LoRA,0.009372019,0.0,0.009372019208967686
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지? (남은 답변: MSE Loss 설명),LoRA 와 QLoRA 의 차이,-0.03292552,0.0,0.03292552009224892
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지? (남은 답변: MSE Loss 설명),Loss Function 관련 실무 경험,0.010845907,0.0,0.010845907032489777
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지? (남은 답변: MSE Loss 설명),Loss Function 예시,-0.03257551,0.0,0.03257551044225693
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지? (남은 답변: MSE Loss 설명),Loss Function 정의,-0.032725047,0.0,0.03272504732012749
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지? (남은 답변: MSE Loss 설명),MBTI / 좋아하는 아이돌,-0.02336227,0.0,0.02336226962506771
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지? (남은 답변: MSE Loss 설명),MSE Loss 설명,0.5277434,1.0,0.472256600856781
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지? (남은 답변: MSE Loss 설명),MSE Loss 용도,0.52940726,0.0,0.529407262802124
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지? (남은 답변: MSE Loss 설명),"Multi-Class, Multi-Label 중 BCE 가 좋은 task",0.0012541534,0.0,0.0012541534379124641
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지? (남은 답변: MSE Loss 설명),Multi-Label 에서 CE + Softmax 적용 문제점,-0.009009065,0.0,0.009009065106511116
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지? (남은 답변: MSE Loss 설명),PEFT 방법 5가지,-0.0071249832,0.0,0.007124983239918947
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지? (남은 답변: MSE Loss 설명),거대 언어 모델 정의,0.031189308,0.0,0.03118930757045746
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지? (남은 답변: MSE Loss 설명),마지막 할 말,-0.051034395,0.0,0.05103439465165138
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지? (남은 답변: MSE Loss 설명),면접 시작 인사,-0.0045248866,0.0,0.004524886608123779
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지? (남은 답변: MSE Loss 설명),면접 종료,-0.03638962,0.0,0.03638961911201477
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지? (남은 답변: MSE Loss 설명),"인공지능, 머신러닝, 딥러닝 차이",-0.025106868,0.0,0.025106867775321007
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지? (남은 답변: MSE Loss 설명),잠시 휴식,-0.028017765,0.0,0.028017764911055565
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지? (남은 답변: MSE Loss 설명),확률 예측에서 MSE Loss 미 사용 이유,-0.01561419,0.0,0.015614190138876438
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지 (남은 답변: 모든 질문 해결 완료)",BCE Loss 설명,-0.038681936,0.0,0.038681935518980026
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지 (남은 답변: 모든 질문 해결 완료)",LLM Fine-Tuning 의 PEFT,0.02172744,0.0,0.021727440878748894
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지 (남은 답변: 모든 질문 해결 완료)",LoRA,-0.02872901,0.0,0.028729010373353958
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지 (남은 답변: 모든 질문 해결 완료)",LoRA 와 QLoRA 의 차이,0.012973552,0.0,0.012973551638424397
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지 (남은 답변: 모든 질문 해결 완료)",Loss Function 관련 실무 경험,-0.0028714982,0.0,0.0028714982327073812
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지 (남은 답변: 모든 질문 해결 완료)",Loss Function 예시,-0.024138136,0.0,0.024138135835528374
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지 (남은 답변: 모든 질문 해결 완료)",Loss Function 정의,0.048452057,0.0,0.04845205694437027
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지 (남은 답변: 모든 질문 해결 완료)",MBTI / 좋아하는 아이돌,-0.015341945,0.0,0.015341944992542267
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지 (남은 답변: 모든 질문 해결 완료)",MSE Loss 설명,-0.034856435,0.0,0.03485643491148949
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지 (남은 답변: 모든 질문 해결 완료)",MSE Loss 용도,-0.017387372,0.0,0.017387371510267258
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지 (남은 답변: 모든 질문 해결 완료)","Multi-Class, Multi-Label 중 BCE 가 좋은 task",-0.025128724,0.0,0.025128724053502083
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지 (남은 답변: 모든 질문 해결 완료)",Multi-Label 에서 CE + Softmax 적용 문제점,0.0024322981,0.0,0.0024322981480509043
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지 (남은 답변: 모든 질문 해결 완료)",PEFT 방법 5가지,-0.03412579,0.0,0.03412578999996185
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지 (남은 답변: 모든 질문 해결 완료)",거대 언어 모델 정의,0.037636768,0.0,0.03763676807284355
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지 (남은 답변: 모든 질문 해결 완료)",마지막 할 말,0.011319281,0.0,0.011319280602037907
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지 (남은 답변: 모든 질문 해결 완료)",면접 시작 인사,-0.0020897337,0.0,0.002089733723551035
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지 (남은 답변: 모든 질문 해결 완료)",면접 종료,-0.00046429413,0.0,0.0004642941348720342
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지 (남은 답변: 모든 질문 해결 완료)","인공지능, 머신러닝, 딥러닝 차이",0.008902944,0.0,0.008902943693101406
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지 (남은 답변: 모든 질문 해결 완료)",잠시 휴식,0.0067245616,0.0,0.0067245615646243095
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지 (남은 답변: 모든 질문 해결 완료)",확률 예측에서 MSE Loss 미 사용 이유,0.9463572,1.0,0.05364280939102173
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지 (남은 답변: MSE Loss 용도),BCE Loss 설명,-0.017616289,0.0,0.017616288736462593
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지 (남은 답변: MSE Loss 용도),LLM Fine-Tuning 의 PEFT,-0.0014983203,0.0,0.0014983202563598752
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지 (남은 답변: MSE Loss 용도),LoRA,-0.011142251,0.0,0.011142251081764698
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지 (남은 답변: MSE Loss 용도),LoRA 와 QLoRA 의 차이,0.022855097,0.0,0.02285509742796421
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지 (남은 답변: MSE Loss 용도),Loss Function 관련 실무 경험,0.016847698,0.0,0.016847698017954826
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지 (남은 답변: MSE Loss 용도),Loss Function 예시,-0.01645438,0.0,0.016454380005598068
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지 (남은 답변: MSE Loss 용도),Loss Function 정의,-0.02174002,0.0,0.02174001932144165
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지 (남은 답변: MSE Loss 용도),MBTI / 좋아하는 아이돌,-0.017594693,0.0,0.01759469322860241
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지 (남은 답변: MSE Loss 용도),MSE Loss 설명,0.6134606,0.0,0.6134606003761292
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지 (남은 답변: MSE Loss 용도),MSE Loss 용도,0.6245577,1.0,0.3754423260688782
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지 (남은 답변: MSE Loss 용도),"Multi-Class, Multi-Label 중 BCE 가 좋은 task",0.005594787,0.0,0.005594787187874317
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지 (남은 답변: MSE Loss 용도),Multi-Label 에서 CE + Softmax 적용 문제점,-0.021370323,0.0,0.02137032337486744
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지 (남은 답변: MSE Loss 용도),PEFT 방법 5가지,0.0008151889,0.0,0.000815188919659704
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지 (남은 답변: MSE Loss 용도),거대 언어 모델 정의,-0.0026080376,0.0,0.002608037553727627
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지 (남은 답변: MSE Loss 용도),마지막 할 말,-0.04825567,0.0,0.04825567081570625
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지 (남은 답변: MSE Loss 용도),면접 시작 인사,-0.010392784,0.0,0.010392784141004086
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지 (남은 답변: MSE Loss 용도),면접 종료,-0.03155817,0.0,0.03155817091464996
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지 (남은 답변: MSE Loss 용도),"인공지능, 머신러닝, 딥러닝 차이",-0.019969035,0.0,0.01996903494000435
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지 (남은 답변: MSE Loss 용도),잠시 휴식,-0.0053717513,0.0,0.0053717512637376785
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지 (남은 답변: MSE Loss 용도),확률 예측에서 MSE Loss 미 사용 이유,-0.0004687934,0.0,0.0004687934124376625
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야? (남은 답변: 확률 예측에서 MSE Loss 미 사용 이유),BCE Loss 설명,-0.030841995,0.0,0.030841995030641556
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야? (남은 답변: 확률 예측에서 MSE Loss 미 사용 이유),LLM Fine-Tuning 의 PEFT,0.01961484,0.0,0.019614839926362038
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야? (남은 답변: 확률 예측에서 MSE Loss 미 사용 이유),LoRA,-0.0046424684,0.0,0.00464246841147542
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야? (남은 답변: 확률 예측에서 MSE Loss 미 사용 이유),LoRA 와 QLoRA 의 차이,0.023955137,0.0,0.02395513653755188
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야? (남은 답변: 확률 예측에서 MSE Loss 미 사용 이유),Loss Function 관련 실무 경험,-0.018977597,0.0,0.01897759735584259
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야? (남은 답변: 확률 예측에서 MSE Loss 미 사용 이유),Loss Function 예시,-0.04408851,0.0,0.04408850893378258
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야? (남은 답변: 확률 예측에서 MSE Loss 미 사용 이유),Loss Function 정의,0.022171093,0.0,0.02217109315097332
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야? (남은 답변: 확률 예측에서 MSE Loss 미 사용 이유),MBTI / 좋아하는 아이돌,-0.0150585845,0.0,0.01505858451128006
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야? (남은 답변: 확률 예측에서 MSE Loss 미 사용 이유),MSE Loss 설명,-0.03297177,0.0,0.03297176957130432
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야? (남은 답변: 확률 예측에서 MSE Loss 미 사용 이유),MSE Loss 용도,-0.014511523,0.0,0.014511522836983204
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야? (남은 답변: 확률 예측에서 MSE Loss 미 사용 이유),"Multi-Class, Multi-Label 중 BCE 가 좋은 task",-0.008445871,0.0,0.00844587106257677
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야? (남은 답변: 확률 예측에서 MSE Loss 미 사용 이유),Multi-Label 에서 CE + Softmax 적용 문제점,-0.0026523767,0.0,0.0026523766573518515
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야? (남은 답변: 확률 예측에서 MSE Loss 미 사용 이유),PEFT 방법 5가지,0.0097777685,0.0,0.009777768515050411
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야? (남은 답변: 확률 예측에서 MSE Loss 미 사용 이유),거대 언어 모델 정의,0.0057435147,0.0,0.005743514746427536
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야? (남은 답변: 확률 예측에서 MSE Loss 미 사용 이유),마지막 할 말,-0.011255187,0.0,0.01125518698245287
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야? (남은 답변: 확률 예측에서 MSE Loss 미 사용 이유),면접 시작 인사,-0.017041113,0.0,0.01704111322760582
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야? (남은 답변: 확률 예측에서 MSE Loss 미 사용 이유),면접 종료,-0.020965504,0.0,0.02096550352871418
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야? (남은 답변: 확률 예측에서 MSE Loss 미 사용 이유),"인공지능, 머신러닝, 딥러닝 차이",-0.0031687473,0.0,0.0031687472946941853
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야? (남은 답변: 확률 예측에서 MSE Loss 미 사용 이유),잠시 휴식,0.023271298,0.0,0.02327129803597927
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야? (남은 답변: 확률 예측에서 MSE Loss 미 사용 이유),확률 예측에서 MSE Loss 미 사용 이유,0.96702266,1.0,0.03297734260559082
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고? (남은 답변: 모든 질문 해결 완료),BCE Loss 설명,0.8244295,1.0,0.17557048797607422
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고? (남은 답변: 모든 질문 해결 완료),LLM Fine-Tuning 의 PEFT,-0.03263615,0.0,0.03263615071773529
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고? (남은 답변: 모든 질문 해결 완료),LoRA,-0.015056944,0.0,0.015056944452226162
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고? (남은 답변: 모든 질문 해결 완료),LoRA 와 QLoRA 의 차이,0.03644789,0.0,0.03644789010286331
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고? (남은 답변: 모든 질문 해결 완료),Loss Function 관련 실무 경험,-0.042272013,0.0,0.04227201268076897
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고? (남은 답변: 모든 질문 해결 완료),Loss Function 예시,0.022606406,0.0,0.02260640636086464
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고? (남은 답변: 모든 질문 해결 완료),Loss Function 정의,-0.028164841,0.0,0.02816484123468399
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고? (남은 답변: 모든 질문 해결 완료),MBTI / 좋아하는 아이돌,-0.063366264,0.0,0.06336626410484314
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고? (남은 답변: 모든 질문 해결 완료),MSE Loss 설명,0.03777883,0.0,0.0377788282930851
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고? (남은 답변: 모든 질문 해결 완료),MSE Loss 용도,0.0036291734,0.0,0.0036291733849793673
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고? (남은 답변: 모든 질문 해결 완료),"Multi-Class, Multi-Label 중 BCE 가 좋은 task",-0.014713191,0.0,0.01471319142729044
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고? (남은 답변: 모든 질문 해결 완료),Multi-Label 에서 CE + Softmax 적용 문제점,-0.020557456,0.0,0.020557455718517303
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고? (남은 답변: 모든 질문 해결 완료),PEFT 방법 5가지,-0.023540497,0.0,0.023540496826171875
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고? (남은 답변: 모든 질문 해결 완료),거대 언어 모델 정의,0.052533343,0.0,0.0525333434343338
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고? (남은 답변: 모든 질문 해결 완료),마지막 할 말,0.027251536,0.0,0.027251536026597023
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고? (남은 답변: 모든 질문 해결 완료),면접 시작 인사,0.013451624,0.0,0.013451623730361462
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고? (남은 답변: 모든 질문 해결 완료),면접 종료,-0.015172242,0.0,0.015172242186963558
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고? (남은 답변: 모든 질문 해결 완료),"인공지능, 머신러닝, 딥러닝 차이",0.0024339869,0.0,0.002433986868709326
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고? (남은 답변: 모든 질문 해결 완료),잠시 휴식,0.04113451,0.0,0.041134510189294815
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고? (남은 답변: 모든 질문 해결 완료),확률 예측에서 MSE Loss 미 사용 이유,0.069389135,0.0,0.06938913464546204
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고 (남은 답변: 핵심 아이디어),BCE Loss 설명,0.9202142,1.0,0.07978582382202148
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고 (남은 답변: 핵심 아이디어),LLM Fine-Tuning 의 PEFT,0.00077938376,0.0,0.0007793837576173246
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고 (남은 답변: 핵심 아이디어),LoRA,-0.010638416,0.0,0.010638415813446045
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고 (남은 답변: 핵심 아이디어),LoRA 와 QLoRA 의 차이,0.025880417,0.0,0.025880416855216026
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고 (남은 답변: 핵심 아이디어),Loss Function 관련 실무 경험,-0.0028673206,0.0,0.0028673205524683
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고 (남은 답변: 핵심 아이디어),Loss Function 예시,0.017441504,0.0,0.017441503703594208
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고 (남은 답변: 핵심 아이디어),Loss Function 정의,-0.04827164,0.0,0.04827164113521576
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고 (남은 답변: 핵심 아이디어),MBTI / 좋아하는 아이돌,0.0012819692,0.0,0.0012819692492485046
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고 (남은 답변: 핵심 아이디어),MSE Loss 설명,0.0144733945,0.0,0.014473394490778446
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고 (남은 답변: 핵심 아이디어),MSE Loss 용도,-0.021280326,0.0,0.021280325949192047
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고 (남은 답변: 핵심 아이디어),"Multi-Class, Multi-Label 중 BCE 가 좋은 task",0.041800745,0.0,0.04180074483156204
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고 (남은 답변: 핵심 아이디어),Multi-Label 에서 CE + Softmax 적용 문제점,0.0054777153,0.0,0.0054777152836322784
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고 (남은 답변: 핵심 아이디어),PEFT 방법 5가지,0.026308993,0.0,0.026308992877602577
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고 (남은 답변: 핵심 아이디어),거대 언어 모델 정의,-0.010440381,0.0,0.010440381243824959
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고 (남은 답변: 핵심 아이디어),마지막 할 말,-0.016538251,0.0,0.01653825119137764
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고 (남은 답변: 핵심 아이디어),면접 시작 인사,0.0033278258,0.0,0.003327825805172324
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고 (남은 답변: 핵심 아이디어),면접 종료,-0.013632199,0.0,0.013632198795676231
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고 (남은 답변: 핵심 아이디어),"인공지능, 머신러닝, 딥러닝 차이",-0.008824244,0.0,0.008824244141578674
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고 (남은 답변: 핵심 아이디어),잠시 휴식,0.021511525,0.0,0.02151152491569519
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고 (남은 답변: 핵심 아이디어),확률 예측에서 MSE Loss 미 사용 이유,-0.028333735,0.0,0.02833373472094536
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거 (남은 답변: 모든 질문 해결 완료)",BCE Loss 설명,-0.009822363,0.0,0.009822363033890724
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거 (남은 답변: 모든 질문 해결 완료)",LLM Fine-Tuning 의 PEFT,0.0025073043,0.0,0.0025073043070733547
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거 (남은 답변: 모든 질문 해결 완료)",LoRA,-0.004341173,0.0,0.004341172985732555
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거 (남은 답변: 모든 질문 해결 완료)",LoRA 와 QLoRA 의 차이,0.018282935,0.0,0.0182829350233078
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거 (남은 답변: 모든 질문 해결 완료)",Loss Function 관련 실무 경험,0.023032011,0.0,0.023032011464238167
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거 (남은 답변: 모든 질문 해결 완료)",Loss Function 예시,-0.0017950458,0.0,0.0017950457986444235
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거 (남은 답변: 모든 질문 해결 완료)",Loss Function 정의,0.0050617647,0.0,0.005061764735728502
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거 (남은 답변: 모든 질문 해결 완료)",MBTI / 좋아하는 아이돌,0.011307259,0.0,0.01130725909024477
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거 (남은 답변: 모든 질문 해결 완료)",MSE Loss 설명,-0.021043893,0.0,0.021043892949819565
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거 (남은 답변: 모든 질문 해결 완료)",MSE Loss 용도,-0.008373512,0.0,0.008373511955142021
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거 (남은 답변: 모든 질문 해결 완료)","Multi-Class, Multi-Label 중 BCE 가 좋은 task",0.79401475,1.0,0.20598524808883667
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거 (남은 답변: 모든 질문 해결 완료)",Multi-Label 에서 CE + Softmax 적용 문제점,-0.001303596,0.0,0.001303595956414938
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거 (남은 답변: 모든 질문 해결 완료)",PEFT 방법 5가지,-0.0023028136,0.0,0.0023028135765343904
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거 (남은 답변: 모든 질문 해결 완료)",거대 언어 모델 정의,-0.0029072526,0.0,0.002907252637669444
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거 (남은 답변: 모든 질문 해결 완료)",마지막 할 말,0.0581233,0.0,0.05812330171465874
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거 (남은 답변: 모든 질문 해결 완료)",면접 시작 인사,-0.041252892,0.0,0.04125289246439934
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거 (남은 답변: 모든 질문 해결 완료)",면접 종료,-0.0031198477,0.0,0.0031198477372527122
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거 (남은 답변: 모든 질문 해결 완료)","인공지능, 머신러닝, 딥러닝 차이",0.020593865,0.0,0.02059386484324932
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거 (남은 답변: 모든 질문 해결 완료)",잠시 휴식,0.009977028,0.0,0.009977027773857117
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거 (남은 답변: 모든 질문 해결 완료)",확률 예측에서 MSE Loss 미 사용 이유,-0.02299152,0.0,0.022991519421339035
"BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야? (남은 답변: 핵심 아이디어, 수식)",BCE Loss 설명,0.9069955,1.0,0.09300452470779419
"BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야? (남은 답변: 핵심 아이디어, 수식)",LLM Fine-Tuning 의 PEFT,-0.0072881137,0.0,0.0072881137020885944
"BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야? (남은 답변: 핵심 아이디어, 수식)",LoRA,-0.016821643,0.0,0.016821643337607384
"BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야? (남은 답변: 핵심 아이디어, 수식)",LoRA 와 QLoRA 의 차이,0.04488705,0.0,0.04488705098628998
"BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야? (남은 답변: 핵심 아이디어, 수식)",Loss Function 관련 실무 경험,0.07462876,0.0,0.07462876290082932
"BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야? (남은 답변: 핵심 아이디어, 수식)",Loss Function 예시,0.003129259,0.0,0.0031292589846998453
"BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야? (남은 답변: 핵심 아이디어, 수식)",Loss Function 정의,-0.04614075,0.0,0.046140749007463455
"BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야? (남은 답변: 핵심 아이디어, 수식)",MBTI / 좋아하는 아이돌,0.0017515616,0.0,0.0017515616491436958
"BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야? (남은 답변: 핵심 아이디어, 수식)",MSE Loss 설명,0.0063546915,0.0,0.006354691460728645
"BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야? (남은 답변: 핵심 아이디어, 수식)",MSE Loss 용도,-0.02818021,0.0,0.028180209919810295
"BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야? (남은 답변: 핵심 아이디어, 수식)","Multi-Class, Multi-Label 중 BCE 가 좋은 task",0.0133309355,0.0,0.013330935500562191
"BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야? (남은 답변: 핵심 아이디어, 수식)",Multi-Label 에서 CE + Softmax 적용 문제점,-0.022271754,0.0,0.02227175422012806
"BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야? (남은 답변: 핵심 아이디어, 수식)",PEFT 방법 5가지,0.052276365,0.0,0.0522763654589653
"BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야? (남은 답변: 핵심 아이디어, 수식)",거대 언어 모델 정의,-0.008510915,0.0,0.008510914631187916
"BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야? (남은 답변: 핵심 아이디어, 수식)",마지막 할 말,-0.026855934,0.0,0.026855934411287308
"BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야? (남은 답변: 핵심 아이디어, 수식)",면접 시작 인사,-0.012040024,0.0,0.012040023691952229
"BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야? (남은 답변: 핵심 아이디어, 수식)",면접 종료,-0.013607502,0.0,0.013607501983642578
"BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야? (남은 답변: 핵심 아이디어, 수식)","인공지능, 머신러닝, 딥러닝 차이",-0.023778258,0.0,0.02377825789153576
"BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야? (남은 답변: 핵심 아이디어, 수식)",잠시 휴식,-0.015220053,0.0,0.015220052562654018
"BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야? (남은 답변: 핵심 아이디어, 수식)",확률 예측에서 MSE Loss 미 사용 이유,-0.020847818,0.0,0.02084781788289547
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고 (남은 답변: 수식)",BCE Loss 설명,0.89874524,1.0,0.10125476121902466
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고 (남은 답변: 수식)",LLM Fine-Tuning 의 PEFT,-0.0074838935,0.0,0.0074838935397565365
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고 (남은 답변: 수식)",LoRA,0.013458494,0.0,0.0134584940969944
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고 (남은 답변: 수식)",LoRA 와 QLoRA 의 차이,-0.0066176965,0.0,0.006617696490138769
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고 (남은 답변: 수식)",Loss Function 관련 실무 경험,0.025573019,0.0,0.025573018938302994
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고 (남은 답변: 수식)",Loss Function 예시,0.010824915,0.0,0.010824915021657944
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고 (남은 답변: 수식)",Loss Function 정의,-0.035262,0.0,0.03526199981570244
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고 (남은 답변: 수식)",MBTI / 좋아하는 아이돌,-0.04055345,0.0,0.04055345058441162
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고 (남은 답변: 수식)",MSE Loss 설명,0.0037106222,0.0,0.003710622200742364
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고 (남은 답변: 수식)",MSE Loss 용도,-0.03172362,0.0,0.031723618507385254
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고 (남은 답변: 수식)","Multi-Class, Multi-Label 중 BCE 가 좋은 task",0.055768214,0.0,0.0557682141661644
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고 (남은 답변: 수식)",Multi-Label 에서 CE + Softmax 적용 문제점,-0.005390238,0.0,0.005390238016843796
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고 (남은 답변: 수식)",PEFT 방법 5가지,-0.041502107,0.0,0.04150210693478584
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고 (남은 답변: 수식)",거대 언어 모델 정의,0.018736484,0.0,0.01873648352921009
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고 (남은 답변: 수식)",마지막 할 말,-0.026867162,0.0,0.026867162436246872
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고 (남은 답변: 수식)",면접 시작 인사,-0.02149853,0.0,0.021498529240489006
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고 (남은 답변: 수식)",면접 종료,-0.01628908,0.0,0.016289079561829567
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고 (남은 답변: 수식)","인공지능, 머신러닝, 딥러닝 차이",-0.0038473578,0.0,0.00384735781699419
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고 (남은 답변: 수식)",잠시 휴식,-0.027724044,0.0,0.027724044397473335
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고 (남은 답변: 수식)",확률 예측에서 MSE Loss 미 사용 이유,0.019525895,0.0,0.01952589489519596
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')] (남은 답변: 핵심 아이디어)",BCE Loss 설명,0.92006993,1.0,0.07993006706237793
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')] (남은 답변: 핵심 아이디어)",LLM Fine-Tuning 의 PEFT,0.006633114,0.0,0.006633114069700241
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')] (남은 답변: 핵심 아이디어)",LoRA,-0.0016099125,0.0,0.00160991249140352
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')] (남은 답변: 핵심 아이디어)",LoRA 와 QLoRA 의 차이,0.0010382994,0.0,0.0010382994078099728
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')] (남은 답변: 핵심 아이디어)",Loss Function 관련 실무 경험,-0.010272445,0.0,0.010272445157170296
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')] (남은 답변: 핵심 아이디어)",Loss Function 예시,0.017010435,0.0,0.017010435461997986
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')] (남은 답변: 핵심 아이디어)",Loss Function 정의,-0.04847067,0.0,0.048470668494701385
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')] (남은 답변: 핵심 아이디어)",MBTI / 좋아하는 아이돌,0.00018288716,0.0,0.00018288716091774404
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')] (남은 답변: 핵심 아이디어)",MSE Loss 설명,0.03245858,0.0,0.032458581030368805
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')] (남은 답변: 핵심 아이디어)",MSE Loss 용도,-0.004497811,0.0,0.004497811198234558
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')] (남은 답변: 핵심 아이디어)","Multi-Class, Multi-Label 중 BCE 가 좋은 task",0.010398746,0.0,0.0103987455368042
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')] (남은 답변: 핵심 아이디어)",Multi-Label 에서 CE + Softmax 적용 문제점,0.0028793833,0.0,0.002879383275285363
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')] (남은 답변: 핵심 아이디어)",PEFT 방법 5가지,0.008654215,0.0,0.00865421537309885
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')] (남은 답변: 핵심 아이디어)",거대 언어 모델 정의,-0.004725296,0.0,0.004725296050310135
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')] (남은 답변: 핵심 아이디어)",마지막 할 말,-0.042279202,0.0,0.042279202491045
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')] (남은 답변: 핵심 아이디어)",면접 시작 인사,-0.008814228,0.0,0.008814227767288685
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')] (남은 답변: 핵심 아이디어)",면접 종료,-0.004205263,0.0,0.004205263219773769
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')] (남은 답변: 핵심 아이디어)","인공지능, 머신러닝, 딥러닝 차이",4.222947e-05,0.0,4.2229468817822635e-05
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')] (남은 답변: 핵심 아이디어)",잠시 휴식,0.0072788377,0.0,0.007278837729245424
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')] (남은 답변: 핵심 아이디어)",확률 예측에서 MSE Loss 미 사용 이유,-0.0013893226,0.0,0.0013893225695937872
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다 (남은 답변: 모든 질문 해결 완료)",BCE Loss 설명,-0.0006646632,0.0,0.0006646632100455463
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다 (남은 답변: 모든 질문 해결 완료)",LLM Fine-Tuning 의 PEFT,0.0024643245,0.0,0.0024643244687467813
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다 (남은 답변: 모든 질문 해결 완료)",LoRA,-0.009121157,0.0,0.009121157228946686
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다 (남은 답변: 모든 질문 해결 완료)",LoRA 와 QLoRA 의 차이,0.01660675,0.0,0.016606749966740608
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다 (남은 답변: 모든 질문 해결 완료)",Loss Function 관련 실무 경험,0.022615496,0.0,0.022615496069192886
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다 (남은 답변: 모든 질문 해결 완료)",Loss Function 예시,-0.0012963231,0.0,0.0012963231420144439
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다 (남은 답변: 모든 질문 해결 완료)",Loss Function 정의,0.0073447363,0.0,0.007344736251980066
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다 (남은 답변: 모든 질문 해결 완료)",MBTI / 좋아하는 아이돌,0.003165284,0.0,0.003165283938869834
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다 (남은 답변: 모든 질문 해결 완료)",MSE Loss 설명,-0.0162061,0.0,0.016206100583076477
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다 (남은 답변: 모든 질문 해결 완료)",MSE Loss 용도,-0.0051708743,0.0,0.005170874297618866
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다 (남은 답변: 모든 질문 해결 완료)","Multi-Class, Multi-Label 중 BCE 가 좋은 task",0.7957088,1.0,0.2042912244796753
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다 (남은 답변: 모든 질문 해결 완료)",Multi-Label 에서 CE + Softmax 적용 문제점,0.0050519314,0.0,0.005051931366324425
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다 (남은 답변: 모든 질문 해결 완료)",PEFT 방법 5가지,0.00354278,0.0,0.0035427799448370934
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다 (남은 답변: 모든 질문 해결 완료)",거대 언어 모델 정의,0.00083532615,0.0,0.0008353261509910226
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다 (남은 답변: 모든 질문 해결 완료)",마지막 할 말,0.048637502,0.0,0.048637501895427704
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다 (남은 답변: 모든 질문 해결 완료)",면접 시작 인사,-0.030156447,0.0,0.030156446620821953
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다 (남은 답변: 모든 질문 해결 완료)",면접 종료,-0.005736634,0.0,0.005736634135246277
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다 (남은 답변: 모든 질문 해결 완료)","인공지능, 머신러닝, 딥러닝 차이",0.00825978,0.0,0.008259779773652554
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다 (남은 답변: 모든 질문 해결 완료)",잠시 휴식,0.022374224,0.0,0.022374223917722702
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다 (남은 답변: 모든 질문 해결 완료)",확률 예측에서 MSE Loss 미 사용 이유,-0.028986908,0.0,0.028986908495426178
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거 (남은 답변: 핵심 아이디어),BCE Loss 설명,0.92294866,1.0,0.07705134153366089
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거 (남은 답변: 핵심 아이디어),LLM Fine-Tuning 의 PEFT,-0.01366536,0.0,0.013665360398590565
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거 (남은 답변: 핵심 아이디어),LoRA,-0.009719628,0.0,0.009719627909362316
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거 (남은 답변: 핵심 아이디어),LoRA 와 QLoRA 의 차이,0.03950418,0.0,0.03950418159365654
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거 (남은 답변: 핵심 아이디어),Loss Function 관련 실무 경험,-0.00023230138,0.0,0.0002323013759450987
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거 (남은 답변: 핵심 아이디어),Loss Function 예시,0.0052846703,0.0,0.005284670274704695
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거 (남은 답변: 핵심 아이디어),Loss Function 정의,-0.057421885,0.0,0.057421885430812836
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거 (남은 답변: 핵심 아이디어),MBTI / 좋아하는 아이돌,-0.009135297,0.0,0.009135296568274498
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거 (남은 답변: 핵심 아이디어),MSE Loss 설명,0.015029368,0.0,0.015029367990791798
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거 (남은 답변: 핵심 아이디어),MSE Loss 용도,-0.01993045,0.0,0.01993045024573803
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거 (남은 답변: 핵심 아이디어),"Multi-Class, Multi-Label 중 BCE 가 좋은 task",-0.00070374866,0.0,0.0007037486648187041
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거 (남은 답변: 핵심 아이디어),Multi-Label 에서 CE + Softmax 적용 문제점,0.004258921,0.0,0.004258920904248953
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거 (남은 답변: 핵심 아이디어),PEFT 방법 5가지,0.002589211,0.0,0.0025892111007124186
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거 (남은 답변: 핵심 아이디어),거대 언어 모델 정의,0.008987649,0.0,0.008987649343907833
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거 (남은 답변: 핵심 아이디어),마지막 할 말,-0.014073754,0.0,0.014073753729462624
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거 (남은 답변: 핵심 아이디어),면접 시작 인사,-0.0030231106,0.0,0.0030231105629354715
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거 (남은 답변: 핵심 아이디어),면접 종료,0.0048684343,0.0,0.004868434276431799
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거 (남은 답변: 핵심 아이디어),"인공지능, 머신러닝, 딥러닝 차이",-0.0088352,0.0,0.00883520022034645
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거 (남은 답변: 핵심 아이디어),잠시 휴식,0.021283315,0.0,0.021283315494656563
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거 (남은 답변: 핵심 아이디어),확률 예측에서 MSE Loss 미 사용 이유,-0.040174983,0.0,0.04017498344182968
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지 (남은 답변: 모든 질문 해결 완료)",BCE Loss 설명,-0.035322122,0.0,0.035322122275829315
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지 (남은 답변: 모든 질문 해결 완료)",LLM Fine-Tuning 의 PEFT,0.0010981989,0.0,0.001098198932595551
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지 (남은 답변: 모든 질문 해결 완료)",LoRA,0.009788814,0.0,0.00978881400078535
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지 (남은 답변: 모든 질문 해결 완료)",LoRA 와 QLoRA 의 차이,0.018394094,0.0,0.018394093960523605
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지 (남은 답변: 모든 질문 해결 완료)",Loss Function 관련 실무 경험,0.027267167,0.0,0.02726716734468937
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지 (남은 답변: 모든 질문 해결 완료)",Loss Function 예시,-0.027964707,0.0,0.02796470746397972
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지 (남은 답변: 모든 질문 해결 완료)",Loss Function 정의,-0.01467881,0.0,0.01467880979180336
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지 (남은 답변: 모든 질문 해결 완료)",MBTI / 좋아하는 아이돌,0.01575145,0.0,0.015751449391245842
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지 (남은 답변: 모든 질문 해결 완료)",MSE Loss 설명,-0.020971442,0.0,0.02097144164144993
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지 (남은 답변: 모든 질문 해결 완료)",MSE Loss 용도,-0.004358496,0.0,0.004358496051281691
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지 (남은 답변: 모든 질문 해결 완료)","Multi-Class, Multi-Label 중 BCE 가 좋은 task",0.7949503,1.0,0.20504969358444214
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지 (남은 답변: 모든 질문 해결 완료)",Multi-Label 에서 CE + Softmax 적용 문제점,0.004125488,0.0,0.004125487990677357
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지 (남은 답변: 모든 질문 해결 완료)",PEFT 방법 5가지,-0.0038469823,0.0,0.0038469822611659765
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지 (남은 답변: 모든 질문 해결 완료)",거대 언어 모델 정의,-0.007670293,0.0,0.007670293096452951
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지 (남은 답변: 모든 질문 해결 완료)",마지막 할 말,0.051997423,0.0,0.05199742317199707
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지 (남은 답변: 모든 질문 해결 완료)",면접 시작 인사,-0.038167495,0.0,0.03816749528050423
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지 (남은 답변: 모든 질문 해결 완료)",면접 종료,-0.0124759665,0.0,0.012475966475903988
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지 (남은 답변: 모든 질문 해결 완료)","인공지능, 머신러닝, 딥러닝 차이",0.022585133,0.0,0.022585133090615273
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지 (남은 답변: 모든 질문 해결 완료)",잠시 휴식,-0.011366641,0.0,0.011366641148924828
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지 (남은 답변: 모든 질문 해결 완료)",확률 예측에서 MSE Loss 미 사용 이유,-0.015308615,0.0,0.015308614820241928
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야? (남은 답변: BCE 가 좋은 task, BCE 가 좋은 이유)",BCE Loss 설명,-0.0503967,0.0,0.05039669945836067
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야? (남은 답변: BCE 가 좋은 task, BCE 가 좋은 이유)",LLM Fine-Tuning 의 PEFT,0.0030262135,0.0,0.0030262134969234467
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야? (남은 답변: BCE 가 좋은 task, BCE 가 좋은 이유)",LoRA,0.009128167,0.0,0.009128167293965816
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야? (남은 답변: BCE 가 좋은 task, BCE 가 좋은 이유)",LoRA 와 QLoRA 의 차이,0.014183488,0.0,0.014183487743139267
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야? (남은 답변: BCE 가 좋은 task, BCE 가 좋은 이유)",Loss Function 관련 실무 경험,-0.0157932,0.0,0.015793200582265854
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야? (남은 답변: BCE 가 좋은 task, BCE 가 좋은 이유)",Loss Function 예시,0.006027585,0.0,0.0060275848954916
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야? (남은 답변: BCE 가 좋은 task, BCE 가 좋은 이유)",Loss Function 정의,0.011844604,0.0,0.011844604276120663
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야? (남은 답변: BCE 가 좋은 task, BCE 가 좋은 이유)",MBTI / 좋아하는 아이돌,0.0020075939,0.0,0.0020075938664376736
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야? (남은 답변: BCE 가 좋은 task, BCE 가 좋은 이유)",MSE Loss 설명,0.00042400273,0.0,0.00042400273378007114
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야? (남은 답변: BCE 가 좋은 task, BCE 가 좋은 이유)",MSE Loss 용도,0.004333223,0.0,0.004333223216235638
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야? (남은 답변: BCE 가 좋은 task, BCE 가 좋은 이유)","Multi-Class, Multi-Label 중 BCE 가 좋은 task",0.9827628,1.0,0.017237186431884766
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야? (남은 답변: BCE 가 좋은 task, BCE 가 좋은 이유)",Multi-Label 에서 CE + Softmax 적용 문제점,-0.01817933,0.0,0.018179329112172127
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야? (남은 답변: BCE 가 좋은 task, BCE 가 좋은 이유)",PEFT 방법 5가지,-0.00413754,0.0,0.004137539770454168
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야? (남은 답변: BCE 가 좋은 task, BCE 가 좋은 이유)",거대 언어 모델 정의,-0.004853522,0.0,0.004853521939367056
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야? (남은 답변: BCE 가 좋은 task, BCE 가 좋은 이유)",마지막 할 말,-0.016990952,0.0,0.01699095219373703
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야? (남은 답변: BCE 가 좋은 task, BCE 가 좋은 이유)",면접 시작 인사,-0.006894504,0.0,0.006894504185765982
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야? (남은 답변: BCE 가 좋은 task, BCE 가 좋은 이유)",면접 종료,0.0015042883,0.0,0.001504288287833333
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야? (남은 답변: BCE 가 좋은 task, BCE 가 좋은 이유)","인공지능, 머신러닝, 딥러닝 차이",0.0048166346,0.0,0.004816634580492973
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야? (남은 답변: BCE 가 좋은 task, BCE 가 좋은 이유)",잠시 휴식,-0.0009892023,0.0,0.0009892022935673594
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야? (남은 답변: BCE 가 좋은 task, BCE 가 좋은 이유)",확률 예측에서 MSE Loss 미 사용 이유,0.007557152,0.0,0.007557151839137077
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!! (남은 답변: BCE 가 좋은 task, BCE 가 좋은 이유)",BCE Loss 설명,-0.053564962,0.0,0.05356496199965477
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!! (남은 답변: BCE 가 좋은 task, BCE 가 좋은 이유)",LLM Fine-Tuning 의 PEFT,0.0005582363,0.0,0.0005582363228313625
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!! (남은 답변: BCE 가 좋은 task, BCE 가 좋은 이유)",LoRA,0.009740393,0.0,0.009740392677485943
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!! (남은 답변: BCE 가 좋은 task, BCE 가 좋은 이유)",LoRA 와 QLoRA 의 차이,0.014471528,0.0,0.014471528120338917
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!! (남은 답변: BCE 가 좋은 task, BCE 가 좋은 이유)",Loss Function 관련 실무 경험,-0.01159008,0.0,0.011590080335736275
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!! (남은 답변: BCE 가 좋은 task, BCE 가 좋은 이유)",Loss Function 예시,0.0026388331,0.0,0.0026388331316411495
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!! (남은 답변: BCE 가 좋은 task, BCE 가 좋은 이유)",Loss Function 정의,0.008308597,0.0,0.008308596909046173
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!! (남은 답변: BCE 가 좋은 task, BCE 가 좋은 이유)",MBTI / 좋아하는 아이돌,-0.0014362225,0.0,0.001436222461052239
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!! (남은 답변: BCE 가 좋은 task, BCE 가 좋은 이유)",MSE Loss 설명,0.0006219985,0.0,0.0006219985079951584
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!! (남은 답변: BCE 가 좋은 task, BCE 가 좋은 이유)",MSE Loss 용도,0.0042703934,0.0,0.004270393401384354
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!! (남은 답변: BCE 가 좋은 task, BCE 가 좋은 이유)","Multi-Class, Multi-Label 중 BCE 가 좋은 task",0.98816794,1.0,0.011832058429718018
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!! (남은 답변: BCE 가 좋은 task, BCE 가 좋은 이유)",Multi-Label 에서 CE + Softmax 적용 문제점,-0.01759194,0.0,0.017591940239071846
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!! (남은 답변: BCE 가 좋은 task, BCE 가 좋은 이유)",PEFT 방법 5가지,-0.0030892738,0.0,0.003089273814111948
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!! (남은 답변: BCE 가 좋은 task, BCE 가 좋은 이유)",거대 언어 모델 정의,-0.004270734,0.0,0.004270733799785376
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!! (남은 답변: BCE 가 좋은 task, BCE 가 좋은 이유)",마지막 할 말,-0.026688159,0.0,0.02668815851211548
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!! (남은 답변: BCE 가 좋은 task, BCE 가 좋은 이유)",면접 시작 인사,-0.012843752,0.0,0.012843752279877663
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!! (남은 답변: BCE 가 좋은 task, BCE 가 좋은 이유)",면접 종료,0.0013021015,0.0,0.0013021015329286456
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!! (남은 답변: BCE 가 좋은 task, BCE 가 좋은 이유)","인공지능, 머신러닝, 딥러닝 차이",0.0038249728,0.0,0.0038249727804213762
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!! (남은 답변: BCE 가 좋은 task, BCE 가 좋은 이유)",잠시 휴식,-0.00067840394,0.0,0.0006784039433114231
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!! (남은 답변: BCE 가 좋은 task, BCE 가 좋은 이유)",확률 예측에서 MSE Loss 미 사용 이유,0.006215328,0.0,0.006215327885001898
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어 (남은 답변: BCE 가 좋은 이유)",BCE Loss 설명,-0.045061763,0.0,0.04506176337599754
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어 (남은 답변: BCE 가 좋은 이유)",LLM Fine-Tuning 의 PEFT,-0.0053775376,0.0,0.0053775375708937645
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어 (남은 답변: BCE 가 좋은 이유)",LoRA,0.00789331,0.0,0.00789330992847681
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어 (남은 답변: BCE 가 좋은 이유)",LoRA 와 QLoRA 의 차이,0.0029153412,0.0,0.0029153411742299795
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어 (남은 답변: BCE 가 좋은 이유)",Loss Function 관련 실무 경험,-0.022419106,0.0,0.02241910621523857
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어 (남은 답변: BCE 가 좋은 이유)",Loss Function 예시,-0.0004637187,0.0,0.00046371869393624365
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어 (남은 답변: BCE 가 좋은 이유)",Loss Function 정의,0.0060206805,0.0,0.006020680535584688
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어 (남은 답변: BCE 가 좋은 이유)",MBTI / 좋아하는 아이돌,-0.00036459434,0.0,0.00036459433613345027
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어 (남은 답변: BCE 가 좋은 이유)",MSE Loss 설명,-0.009363585,0.0,0.009363585151731968
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어 (남은 답변: BCE 가 좋은 이유)",MSE Loss 용도,-0.0062858155,0.0,0.006285815499722958
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어 (남은 답변: BCE 가 좋은 이유)","Multi-Class, Multi-Label 중 BCE 가 좋은 task",0.97795075,1.0,0.022049248218536377
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어 (남은 답변: BCE 가 좋은 이유)",Multi-Label 에서 CE + Softmax 적용 문제점,-0.020372659,0.0,0.020372658967971802
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어 (남은 답변: BCE 가 좋은 이유)",PEFT 방법 5가지,0.008186297,0.0,0.008186296559870243
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어 (남은 답변: BCE 가 좋은 이유)",거대 언어 모델 정의,-0.0121471025,0.0,0.012147102504968643
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어 (남은 답변: BCE 가 좋은 이유)",마지막 할 말,-0.016912801,0.0,0.016912801191210747
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어 (남은 답변: BCE 가 좋은 이유)",면접 시작 인사,-0.017758308,0.0,0.017758307978510857
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어 (남은 답변: BCE 가 좋은 이유)",면접 종료,0.0017207391,0.0,0.0017207390628755093
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어 (남은 답변: BCE 가 좋은 이유)","인공지능, 머신러닝, 딥러닝 차이",0.0048184986,0.0,0.004818498622626066
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어 (남은 답변: BCE 가 좋은 이유)",잠시 휴식,0.0050910804,0.0,0.005091080442070961
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어 (남은 답변: BCE 가 좋은 이유)",확률 예측에서 MSE Loss 미 사용 이유,0.0062511354,0.0,0.006251135375350714
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아 (남은 답변: 모든 질문 해결 완료)",BCE Loss 설명,-0.0034170593,0.0,0.003417059313505888
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아 (남은 답변: 모든 질문 해결 완료)",LLM Fine-Tuning 의 PEFT,0.0142654255,0.0,0.014265425503253937
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아 (남은 답변: 모든 질문 해결 완료)",LoRA,0.0009351612,0.0,0.000935161195229739
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아 (남은 답변: 모든 질문 해결 완료)",LoRA 와 QLoRA 의 차이,-0.02318678,0.0,0.023186780512332916
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아 (남은 답변: 모든 질문 해결 완료)",Loss Function 관련 실무 경험,-0.012823681,0.0,0.012823681347072124
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아 (남은 답변: 모든 질문 해결 완료)",Loss Function 예시,0.0103059085,0.0,0.010305908508598804
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아 (남은 답변: 모든 질문 해결 완료)",Loss Function 정의,0.0033598538,0.0,0.003359853755682707
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아 (남은 답변: 모든 질문 해결 완료)",MBTI / 좋아하는 아이돌,-0.03888476,0.0,0.038884758949279785
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아 (남은 답변: 모든 질문 해결 완료)",MSE Loss 설명,0.014971706,0.0,0.014971706084907055
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아 (남은 답변: 모든 질문 해결 완료)",MSE Loss 용도,0.0088992575,0.0,0.008899257518351078
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아 (남은 답변: 모든 질문 해결 완료)","Multi-Class, Multi-Label 중 BCE 가 좋은 task",0.18099739,0.0,0.18099738657474518
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아 (남은 답변: 모든 질문 해결 완료)",Multi-Label 에서 CE + Softmax 적용 문제점,0.861841,1.0,0.13815897703170776
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아 (남은 답변: 모든 질문 해결 완료)",PEFT 방법 5가지,-0.015982095,0.0,0.015982095152139664
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아 (남은 답변: 모든 질문 해결 완료)",거대 언어 모델 정의,0.008331406,0.0,0.00833140593022108
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아 (남은 답변: 모든 질문 해결 완료)",마지막 할 말,0.015288182,0.0,0.015288181602954865
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아 (남은 답변: 모든 질문 해결 완료)",면접 시작 인사,0.0001632042,0.0,0.00016320419672410935
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아 (남은 답변: 모든 질문 해결 완료)",면접 종료,-0.0021136145,0.0,0.0021136144641786814
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아 (남은 답변: 모든 질문 해결 완료)","인공지능, 머신러닝, 딥러닝 차이",-0.008573033,0.0,0.008573032915592194
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아 (남은 답변: 모든 질문 해결 완료)",잠시 휴식,-0.007446264,0.0,0.007446263916790485
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아 (남은 답변: 모든 질문 해결 완료)",확률 예측에서 MSE Loss 미 사용 이유,-0.03426611,0.0,0.03426611050963402
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지 (남은 답변: 모든 질문 해결 완료),BCE Loss 설명,-0.036256604,0.0,0.03625660389661789
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지 (남은 답변: 모든 질문 해결 완료),LLM Fine-Tuning 의 PEFT,0.013907451,0.0,0.013907451182603836
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지 (남은 답변: 모든 질문 해결 완료),LoRA,-0.047401745,0.0,0.04740174487233162
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지 (남은 답변: 모든 질문 해결 완료),LoRA 와 QLoRA 의 차이,0.122024,0.0,0.12202399969100952
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지 (남은 답변: 모든 질문 해결 완료),Loss Function 관련 실무 경험,0.87351704,1.0,0.12648296356201172
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지 (남은 답변: 모든 질문 해결 완료),Loss Function 예시,-0.04330761,0.0,0.04330760985612869
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지 (남은 답변: 모든 질문 해결 완료),Loss Function 정의,-0.010151123,0.0,0.010151122696697712
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지 (남은 답변: 모든 질문 해결 완료),MBTI / 좋아하는 아이돌,-0.01297037,0.0,0.01297037024050951
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지 (남은 답변: 모든 질문 해결 완료),MSE Loss 설명,-0.00018167219,0.0,0.00018167219241149724
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지 (남은 답변: 모든 질문 해결 완료),MSE Loss 용도,-0.002899525,0.0,0.002899524988606572
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지 (남은 답변: 모든 질문 해결 완료),"Multi-Class, Multi-Label 중 BCE 가 좋은 task",-0.012783181,0.0,0.012783180922269821
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지 (남은 답변: 모든 질문 해결 완료),Multi-Label 에서 CE + Softmax 적용 문제점,0.0169679,0.0,0.016967900097370148
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지 (남은 답변: 모든 질문 해결 완료),PEFT 방법 5가지,0.02754575,0.0,0.0275457501411438
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지 (남은 답변: 모든 질문 해결 완료),거대 언어 모델 정의,-0.0018510353,0.0,0.0018510352820158005
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지 (남은 답변: 모든 질문 해결 완료),마지막 할 말,-0.015771987,0.0,0.015771986916661263
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지 (남은 답변: 모든 질문 해결 완료),면접 시작 인사,-0.017326964,0.0,0.01732696406543255
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지 (남은 답변: 모든 질문 해결 완료),면접 종료,0.018194348,0.0,0.018194347620010376
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지 (남은 답변: 모든 질문 해결 완료),"인공지능, 머신러닝, 딥러닝 차이",0.007672946,0.0,0.007672945968806744
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지 (남은 답변: 모든 질문 해결 완료),잠시 휴식,-0.027711937,0.0,0.027711937204003334
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지 (남은 답변: 모든 질문 해결 완료),확률 예측에서 MSE Loss 미 사용 이유,0.009091343,0.0,0.00909134279936552
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나 (남은 답변: Multi-Label 에서 CE + Softmax 적용 문제점),BCE Loss 설명,-0.006781421,0.0,0.006781421136111021
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나 (남은 답변: Multi-Label 에서 CE + Softmax 적용 문제점),LLM Fine-Tuning 의 PEFT,0.009392567,0.0,0.009392566978931427
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나 (남은 답변: Multi-Label 에서 CE + Softmax 적용 문제점),LoRA,0.003034682,0.0,0.003034682013094425
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나 (남은 답변: Multi-Label 에서 CE + Softmax 적용 문제점),LoRA 와 QLoRA 의 차이,0.0002139112,0.0,0.00021391120390035212
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나 (남은 답변: Multi-Label 에서 CE + Softmax 적용 문제점),Loss Function 관련 실무 경험,-0.03824278,0.0,0.038242779672145844
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나 (남은 답변: Multi-Label 에서 CE + Softmax 적용 문제점),Loss Function 예시,0.007994138,0.0,0.007994137704372406
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나 (남은 답변: Multi-Label 에서 CE + Softmax 적용 문제점),Loss Function 정의,0.0060575064,0.0,0.006057506427168846
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나 (남은 답변: Multi-Label 에서 CE + Softmax 적용 문제점),MBTI / 좋아하는 아이돌,-0.011086827,0.0,0.011086827144026756
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나 (남은 답변: Multi-Label 에서 CE + Softmax 적용 문제점),MSE Loss 설명,-0.012080196,0.0,0.012080196291208267
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나 (남은 답변: Multi-Label 에서 CE + Softmax 적용 문제점),MSE Loss 용도,-0.017275523,0.0,0.017275523394346237
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나 (남은 답변: Multi-Label 에서 CE + Softmax 적용 문제점),"Multi-Class, Multi-Label 중 BCE 가 좋은 task",0.0070273816,0.0,0.00702738156542182
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나 (남은 답변: Multi-Label 에서 CE + Softmax 적용 문제점),Multi-Label 에서 CE + Softmax 적용 문제점,0.9726143,1.0,0.027385711669921875
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나 (남은 답변: Multi-Label 에서 CE + Softmax 적용 문제점),PEFT 방법 5가지,-0.0026008426,0.0,0.002600842621177435
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나 (남은 답변: Multi-Label 에서 CE + Softmax 적용 문제점),거대 언어 모델 정의,0.002803623,0.0,0.002803622977808118
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나 (남은 답변: Multi-Label 에서 CE + Softmax 적용 문제점),마지막 할 말,0.009192917,0.0,0.009192916564643383
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나 (남은 답변: Multi-Label 에서 CE + Softmax 적용 문제점),면접 시작 인사,-0.00016128134,0.0,0.0001612813357496634
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나 (남은 답변: Multi-Label 에서 CE + Softmax 적용 문제점),면접 종료,0.004596257,0.0,0.004596257116645575
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나 (남은 답변: Multi-Label 에서 CE + Softmax 적용 문제점),"인공지능, 머신러닝, 딥러닝 차이",-0.004858066,0.0,0.004858065862208605
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나 (남은 답변: Multi-Label 에서 CE + Softmax 적용 문제점),잠시 휴식,-0.00074026827,0.0,0.0007402682676911354
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나 (남은 답변: Multi-Label 에서 CE + Softmax 적용 문제점),확률 예측에서 MSE Loss 미 사용 이유,-0.009657896,0.0,0.009657896123826504
Loss Function 관련 실무 경험 -> 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어 (남은 답변: 상세 경험),BCE Loss 설명,-0.024767132,0.0,0.024767132475972176
Loss Function 관련 실무 경험 -> 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어 (남은 답변: 상세 경험),LLM Fine-Tuning 의 PEFT,-0.00061748055,0.0,0.0006174805457703769
Loss Function 관련 실무 경험 -> 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어 (남은 답변: 상세 경험),LoRA,0.0025300013,0.0,0.002530001336708665
Loss Function 관련 실무 경험 -> 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어 (남은 답변: 상세 경험),LoRA 와 QLoRA 의 차이,-0.001517663,0.0,0.001517663011327386
Loss Function 관련 실무 경험 -> 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어 (남은 답변: 상세 경험),Loss Function 관련 실무 경험,0.9404515,1.0,0.05954849720001221
Loss Function 관련 실무 경험 -> 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어 (남은 답변: 상세 경험),Loss Function 예시,-0.035698548,0.0,0.0356985479593277
Loss Function 관련 실무 경험 -> 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어 (남은 답변: 상세 경험),Loss Function 정의,-0.022265175,0.0,0.022265175357460976
Loss Function 관련 실무 경험 -> 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어 (남은 답변: 상세 경험),MBTI / 좋아하는 아이돌,0.009005615,0.0,0.009005614556372166
Loss Function 관련 실무 경험 -> 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어 (남은 답변: 상세 경험),MSE Loss 설명,-0.0041026273,0.0,0.004102627281099558
Loss Function 관련 실무 경험 -> 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어 (남은 답변: 상세 경험),MSE Loss 용도,-0.014649052,0.0,0.014649052172899246
Loss Function 관련 실무 경험 -> 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어 (남은 답변: 상세 경험),"Multi-Class, Multi-Label 중 BCE 가 좋은 task",-0.014500932,0.0,0.014500931836664677
Loss Function 관련 실무 경험 -> 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어 (남은 답변: 상세 경험),Multi-Label 에서 CE + Softmax 적용 문제점,-0.005385219,0.0,0.005385219119489193
Loss Function 관련 실무 경험 -> 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어 (남은 답변: 상세 경험),PEFT 방법 5가지,-0.018692102,0.0,0.018692102283239365
Loss Function 관련 실무 경험 -> 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어 (남은 답변: 상세 경험),거대 언어 모델 정의,-0.01875816,0.0,0.01875815913081169
Loss Function 관련 실무 경험 -> 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어 (남은 답변: 상세 경험),마지막 할 말,-0.02076123,0.0,0.02076122909784317
Loss Function 관련 실무 경험 -> 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어 (남은 답변: 상세 경험),면접 시작 인사,-0.0076094936,0.0,0.007609493564814329
Loss Function 관련 실무 경험 -> 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어 (남은 답변: 상세 경험),면접 종료,-0.011229812,0.0,0.011229812167584896
Loss Function 관련 실무 경험 -> 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어 (남은 답변: 상세 경험),"인공지능, 머신러닝, 딥러닝 차이",0.0058897943,0.0,0.005889794323593378
Loss Function 관련 실무 경험 -> 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어 (남은 답변: 상세 경험),잠시 휴식,0.0059557552,0.0,0.005955755244940519
Loss Function 관련 실무 경험 -> 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어 (남은 답변: 상세 경험),확률 예측에서 MSE Loss 미 사용 이유,-0.0016978435,0.0,0.0016978435451164842
Loss Function 관련 실무 경험 -> 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지? (남은 답변: 모든 질문 해결 완료),BCE Loss 설명,-0.036852807,0.0,0.03685280680656433
Loss Function 관련 실무 경험 -> 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지? (남은 답변: 모든 질문 해결 완료),LLM Fine-Tuning 의 PEFT,0.011754235,0.0,0.011754235252737999
Loss Function 관련 실무 경험 -> 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지? (남은 답변: 모든 질문 해결 완료),LoRA,-0.00038752364,0.0,0.00038752364343963563
Loss Function 관련 실무 경험 -> 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지? (남은 답변: 모든 질문 해결 완료),LoRA 와 QLoRA 의 차이,0.0073983683,0.0,0.007398368325084448
Loss Function 관련 실무 경험 -> 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지? (남은 답변: 모든 질문 해결 완료),Loss Function 관련 실무 경험,-0.06171905,0.0,0.06171904876828194
Loss Function 관련 실무 경험 -> 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지? (남은 답변: 모든 질문 해결 완료),Loss Function 예시,-0.021840813,0.0,0.021840812638401985
Loss Function 관련 실무 경험 -> 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지? (남은 답변: 모든 질문 해결 완료),Loss Function 정의,-0.052459538,0.0,0.052459537982940674
Loss Function 관련 실무 경험 -> 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지? (남은 답변: 모든 질문 해결 완료),MBTI / 좋아하는 아이돌,0.92801064,1.0,0.07198935747146606
Loss Function 관련 실무 경험 -> 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지? (남은 답변: 모든 질문 해결 완료),MSE Loss 설명,-0.0106183365,0.0,0.010618336498737335
Loss Function 관련 실무 경험 -> 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지? (남은 답변: 모든 질문 해결 완료),MSE Loss 용도,-0.005635796,0.0,0.0056357961148023605
Loss Function 관련 실무 경험 -> 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지? (남은 답변: 모든 질문 해결 완료),"Multi-Class, Multi-Label 중 BCE 가 좋은 task",-0.009516586,0.0,0.009516585618257523
Loss Function 관련 실무 경험 -> 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지? (남은 답변: 모든 질문 해결 완료),Multi-Label 에서 CE + Softmax 적용 문제점,-0.008605421,0.0,0.008605420589447021
Loss Function 관련 실무 경험 -> 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지? (남은 답변: 모든 질문 해결 완료),PEFT 방법 5가지,0.001439146,0.0,0.0014391459990292788
Loss Function 관련 실무 경험 -> 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지? (남은 답변: 모든 질문 해결 완료),거대 언어 모델 정의,0.02556619,0.0,0.025566190481185913
Loss Function 관련 실무 경험 -> 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지? (남은 답변: 모든 질문 해결 완료),마지막 할 말,-0.004139338,0.0,0.004139338154345751
Loss Function 관련 실무 경험 -> 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지? (남은 답변: 모든 질문 해결 완료),면접 시작 인사,0.013098385,0.0,0.01309838518500328
Loss Function 관련 실무 경험 -> 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지? (남은 답변: 모든 질문 해결 완료),면접 종료,0.006044922,0.0,0.0060449219308793545
Loss Function 관련 실무 경험 -> 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지? (남은 답변: 모든 질문 해결 완료),"인공지능, 머신러닝, 딥러닝 차이",-0.01344105,0.0,0.013441050425171852
Loss Function 관련 실무 경험 -> 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지? (남은 답변: 모든 질문 해결 완료),잠시 휴식,0.008305786,0.0,0.008305786177515984
Loss Function 관련 실무 경험 -> 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지? (남은 답변: 모든 질문 해결 완료),확률 예측에서 MSE Loss 미 사용 이유,-0.0083827535,0.0,0.00838275346904993
Loss Function 관련 실무 경험 -> 근데 기억 진짜 안나는데 어떡하지? (남은 답변: 상세 경험),BCE Loss 설명,0.018407363,0.0,0.018407363444566727
Loss Function 관련 실무 경험 -> 근데 기억 진짜 안나는데 어떡하지? (남은 답변: 상세 경험),LLM Fine-Tuning 의 PEFT,0.018900352,0.0,0.018900351598858833
Loss Function 관련 실무 경험 -> 근데 기억 진짜 안나는데 어떡하지? (남은 답변: 상세 경험),LoRA,-0.007293329,0.0,0.007293329108506441
Loss Function 관련 실무 경험 -> 근데 기억 진짜 안나는데 어떡하지? (남은 답변: 상세 경험),LoRA 와 QLoRA 의 차이,-0.010865654,0.0,0.010865653865039349
Loss Function 관련 실무 경험 -> 근데 기억 진짜 안나는데 어떡하지? (남은 답변: 상세 경험),Loss Function 관련 실무 경험,0.16530438,0.0,0.16530437767505646
Loss Function 관련 실무 경험 -> 근데 기억 진짜 안나는데 어떡하지? (남은 답변: 상세 경험),Loss Function 예시,-0.00414342,0.0,0.0041434201411902905
Loss Function 관련 실무 경험 -> 근데 기억 진짜 안나는데 어떡하지? (남은 답변: 상세 경험),Loss Function 정의,0.0088172415,0.0,0.00881724152714014
Loss Function 관련 실무 경험 -> 근데 기억 진짜 안나는데 어떡하지? (남은 답변: 상세 경험),MBTI / 좋아하는 아이돌,0.052101254,0.0,0.0521012544631958
Loss Function 관련 실무 경험 -> 근데 기억 진짜 안나는데 어떡하지? (남은 답변: 상세 경험),MSE Loss 설명,0.0049896045,0.0,0.004989604465663433
Loss Function 관련 실무 경험 -> 근데 기억 진짜 안나는데 어떡하지? (남은 답변: 상세 경험),MSE Loss 용도,-0.024243949,0.0,0.024243948981165886
Loss Function 관련 실무 경험 -> 근데 기억 진짜 안나는데 어떡하지? (남은 답변: 상세 경험),"Multi-Class, Multi-Label 중 BCE 가 좋은 task",-0.0033157442,0.0,0.003315744223073125
Loss Function 관련 실무 경험 -> 근데 기억 진짜 안나는데 어떡하지? (남은 답변: 상세 경험),Multi-Label 에서 CE + Softmax 적용 문제점,0.015065709,0.0,0.015065709128975868
Loss Function 관련 실무 경험 -> 근데 기억 진짜 안나는데 어떡하지? (남은 답변: 상세 경험),PEFT 방법 5가지,0.017353617,0.0,0.017353616654872894
Loss Function 관련 실무 경험 -> 근데 기억 진짜 안나는데 어떡하지? (남은 답변: 상세 경험),거대 언어 모델 정의,-0.04017724,0.0,0.04017724096775055
Loss Function 관련 실무 경험 -> 근데 기억 진짜 안나는데 어떡하지? (남은 답변: 상세 경험),마지막 할 말,-0.015874455,0.0,0.015874454751610756
Loss Function 관련 실무 경험 -> 근데 기억 진짜 안나는데 어떡하지? (남은 답변: 상세 경험),면접 시작 인사,0.01288843,0.0,0.012888429686427116
Loss Function 관련 실무 경험 -> 근데 기억 진짜 안나는데 어떡하지? (남은 답변: 상세 경험),면접 종료,0.00678779,0.0,0.006787789985537529
Loss Function 관련 실무 경험 -> 근데 기억 진짜 안나는데 어떡하지? (남은 답변: 상세 경험),"인공지능, 머신러닝, 딥러닝 차이",-0.017841341,0.0,0.017841340973973274
Loss Function 관련 실무 경험 -> 근데 기억 진짜 안나는데 어떡하지? (남은 답변: 상세 경험),잠시 휴식,0.6830067,1.0,0.3169932961463928
Loss Function 관련 실무 경험 -> 근데 기억 진짜 안나는데 어떡하지? (남은 답변: 상세 경험),확률 예측에서 MSE Loss 미 사용 이유,0.032661058,0.0,0.03266105800867081
Loss Function 관련 실무 경험 -> 나 아직 실무 경험 없는데 ㅠㅠ (남은 답변: 모든 질문 해결 완료),BCE Loss 설명,-0.016909394,0.0,0.016909394413232803
Loss Function 관련 실무 경험 -> 나 아직 실무 경험 없는데 ㅠㅠ (남은 답변: 모든 질문 해결 완료),LLM Fine-Tuning 의 PEFT,-0.014859089,0.0,0.014859088696539402
Loss Function 관련 실무 경험 -> 나 아직 실무 경험 없는데 ㅠㅠ (남은 답변: 모든 질문 해결 완료),LoRA,-0.025472349,0.0,0.025472348555922508
Loss Function 관련 실무 경험 -> 나 아직 실무 경험 없는데 ㅠㅠ (남은 답변: 모든 질문 해결 완료),LoRA 와 QLoRA 의 차이,-0.007560437,0.0,0.007560437079519033
Loss Function 관련 실무 경험 -> 나 아직 실무 경험 없는데 ㅠㅠ (남은 답변: 모든 질문 해결 완료),Loss Function 관련 실무 경험,-0.050352886,0.0,0.05035288631916046
Loss Function 관련 실무 경험 -> 나 아직 실무 경험 없는데 ㅠㅠ (남은 답변: 모든 질문 해결 완료),Loss Function 예시,-0.017397104,0.0,0.01739710383117199
Loss Function 관련 실무 경험 -> 나 아직 실무 경험 없는데 ㅠㅠ (남은 답변: 모든 질문 해결 완료),Loss Function 정의,-0.055147197,0.0,0.0551471970975399
Loss Function 관련 실무 경험 -> 나 아직 실무 경험 없는데 ㅠㅠ (남은 답변: 모든 질문 해결 완료),MBTI / 좋아하는 아이돌,0.92545676,1.0,0.07454323768615723
Loss Function 관련 실무 경험 -> 나 아직 실무 경험 없는데 ㅠㅠ (남은 답변: 모든 질문 해결 완료),MSE Loss 설명,-0.002779641,0.0,0.002779640955850482
Loss Function 관련 실무 경험 -> 나 아직 실무 경험 없는데 ㅠㅠ (남은 답변: 모든 질문 해결 완료),MSE Loss 용도,-0.0013184096,0.0,0.0013184095732867718
Loss Function 관련 실무 경험 -> 나 아직 실무 경험 없는데 ㅠㅠ (남은 답변: 모든 질문 해결 완료),"Multi-Class, Multi-Label 중 BCE 가 좋은 task",-0.011876193,0.0,0.01187619287520647
Loss Function 관련 실무 경험 -> 나 아직 실무 경험 없는데 ㅠㅠ (남은 답변: 모든 질문 해결 완료),Multi-Label 에서 CE + Softmax 적용 문제점,-0.013532054,0.0,0.013532053679227829
Loss Function 관련 실무 경험 -> 나 아직 실무 경험 없는데 ㅠㅠ (남은 답변: 모든 질문 해결 완료),PEFT 방법 5가지,-0.025954144,0.0,0.025954144075512886
Loss Function 관련 실무 경험 -> 나 아직 실무 경험 없는데 ㅠㅠ (남은 답변: 모든 질문 해결 완료),거대 언어 모델 정의,-0.018488282,0.0,0.018488282337784767
Loss Function 관련 실무 경험 -> 나 아직 실무 경험 없는데 ㅠㅠ (남은 답변: 모든 질문 해결 완료),마지막 할 말,-0.021964412,0.0,0.021964412182569504
Loss Function 관련 실무 경험 -> 나 아직 실무 경험 없는데 ㅠㅠ (남은 답변: 모든 질문 해결 완료),면접 시작 인사,0.0013419748,0.0,0.0013419748283922672
Loss Function 관련 실무 경험 -> 나 아직 실무 경험 없는데 ㅠㅠ (남은 답변: 모든 질문 해결 완료),면접 종료,0.017378422,0.0,0.017378421500325203
Loss Function 관련 실무 경험 -> 나 아직 실무 경험 없는데 ㅠㅠ (남은 답변: 모든 질문 해결 완료),"인공지능, 머신러닝, 딥러닝 차이",0.004917171,0.0,0.004917170852422714
Loss Function 관련 실무 경험 -> 나 아직 실무 경험 없는데 ㅠㅠ (남은 답변: 모든 질문 해결 완료),잠시 휴식,0.06705874,0.0,0.0670587420463562
Loss Function 관련 실무 경험 -> 나 아직 실무 경험 없는데 ㅠㅠ (남은 답변: 모든 질문 해결 완료),확률 예측에서 MSE Loss 미 사용 이유,0.01630741,0.0,0.01630740985274315
"Loss Function 관련 실무 경험 -> 대답하기 싫은데 힝 (남은 답변: 기본 경험, 상세 경험)",BCE Loss 설명,-0.00061614637,0.0,0.0006161463679745793
"Loss Function 관련 실무 경험 -> 대답하기 싫은데 힝 (남은 답변: 기본 경험, 상세 경험)",LLM Fine-Tuning 의 PEFT,0.057420287,0.0,0.057420287281274796
"Loss Function 관련 실무 경험 -> 대답하기 싫은데 힝 (남은 답변: 기본 경험, 상세 경험)",LoRA,0.005189659,0.0,0.00518965907394886
"Loss Function 관련 실무 경험 -> 대답하기 싫은데 힝 (남은 답변: 기본 경험, 상세 경험)",LoRA 와 QLoRA 의 차이,-0.0012232696,0.0,0.0012232696171849966
"Loss Function 관련 실무 경험 -> 대답하기 싫은데 힝 (남은 답변: 기본 경험, 상세 경험)",Loss Function 관련 실무 경험,-0.008772785,0.0,0.00877278484404087
"Loss Function 관련 실무 경험 -> 대답하기 싫은데 힝 (남은 답변: 기본 경험, 상세 경험)",Loss Function 예시,0.010112106,0.0,0.010112105868756771
"Loss Function 관련 실무 경험 -> 대답하기 싫은데 힝 (남은 답변: 기본 경험, 상세 경험)",Loss Function 정의,0.02095661,0.0,0.020956609398126602
"Loss Function 관련 실무 경험 -> 대답하기 싫은데 힝 (남은 답변: 기본 경험, 상세 경험)",MBTI / 좋아하는 아이돌,0.02448007,0.0,0.024480070918798447
"Loss Function 관련 실무 경험 -> 대답하기 싫은데 힝 (남은 답변: 기본 경험, 상세 경험)",MSE Loss 설명,0.014966262,0.0,0.014966261573135853
"Loss Function 관련 실무 경험 -> 대답하기 싫은데 힝 (남은 답변: 기본 경험, 상세 경험)",MSE Loss 용도,-0.019414231,0.0,0.019414231181144714
"Loss Function 관련 실무 경험 -> 대답하기 싫은데 힝 (남은 답변: 기본 경험, 상세 경험)","Multi-Class, Multi-Label 중 BCE 가 좋은 task",0.016164921,0.0,0.01616492122411728
"Loss Function 관련 실무 경험 -> 대답하기 싫은데 힝 (남은 답변: 기본 경험, 상세 경험)",Multi-Label 에서 CE + Softmax 적용 문제점,0.00349221,0.0,0.0034922100603580475
"Loss Function 관련 실무 경험 -> 대답하기 싫은데 힝 (남은 답변: 기본 경험, 상세 경험)",PEFT 방법 5가지,-0.0044563855,0.0,0.004456385504454374
"Loss Function 관련 실무 경험 -> 대답하기 싫은데 힝 (남은 답변: 기본 경험, 상세 경험)",거대 언어 모델 정의,0.0055077453,0.0,0.005507745314389467
"Loss Function 관련 실무 경험 -> 대답하기 싫은데 힝 (남은 답변: 기본 경험, 상세 경험)",마지막 할 말,-0.042109724,0.0,0.04210972413420677
"Loss Function 관련 실무 경험 -> 대답하기 싫은데 힝 (남은 답변: 기본 경험, 상세 경험)",면접 시작 인사,0.043128077,0.0,0.04312807694077492
"Loss Function 관련 실무 경험 -> 대답하기 싫은데 힝 (남은 답변: 기본 경험, 상세 경험)",면접 종료,0.03030247,0.0,0.030302470549941063
"Loss Function 관련 실무 경험 -> 대답하기 싫은데 힝 (남은 답변: 기본 경험, 상세 경험)","인공지능, 머신러닝, 딥러닝 차이",0.05644375,0.0,0.056443750858306885
"Loss Function 관련 실무 경험 -> 대답하기 싫은데 힝 (남은 답변: 기본 경험, 상세 경험)",잠시 휴식,0.7362295,1.0,0.26377052068710327
"Loss Function 관련 실무 경험 -> 대답하기 싫은데 힝 (남은 답변: 기본 경험, 상세 경험)",확률 예측에서 MSE Loss 미 사용 이유,0.009962205,0.0,0.009962204843759537
Loss Function 관련 실무 경험 -> 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ (남은 답변: 상세 경험),BCE Loss 설명,0.0038663065,0.0,0.0038663065060973167
Loss Function 관련 실무 경험 -> 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ (남은 답변: 상세 경험),LLM Fine-Tuning 의 PEFT,0.020835485,0.0,0.02083548530936241
Loss Function 관련 실무 경험 -> 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ (남은 답변: 상세 경험),LoRA,0.008101277,0.0,0.00810127705335617
Loss Function 관련 실무 경험 -> 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ (남은 답변: 상세 경험),LoRA 와 QLoRA 의 차이,-0.009992508,0.0,0.009992508217692375
Loss Function 관련 실무 경험 -> 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ (남은 답변: 상세 경험),Loss Function 관련 실무 경험,0.9497362,1.0,0.050263822078704834
Loss Function 관련 실무 경험 -> 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ (남은 답변: 상세 경험),Loss Function 예시,-0.031385716,0.0,0.031385716050863266
Loss Function 관련 실무 경험 -> 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ (남은 답변: 상세 경험),Loss Function 정의,-0.021011941,0.0,0.021011941134929657
Loss Function 관련 실무 경험 -> 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ (남은 답변: 상세 경험),MBTI / 좋아하는 아이돌,-0.040815823,0.0,0.040815822780132294
Loss Function 관련 실무 경험 -> 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ (남은 답변: 상세 경험),MSE Loss 설명,0.0076575745,0.0,0.007657574489712715
Loss Function 관련 실무 경험 -> 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ (남은 답변: 상세 경험),MSE Loss 용도,-0.006685825,0.0,0.006685825064778328
Loss Function 관련 실무 경험 -> 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ (남은 답변: 상세 경험),"Multi-Class, Multi-Label 중 BCE 가 좋은 task",-0.0064496114,0.0,0.006449611391872168
Loss Function 관련 실무 경험 -> 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ (남은 답변: 상세 경험),Multi-Label 에서 CE + Softmax 적용 문제점,-0.0116221635,0.0,0.011622163467109203
Loss Function 관련 실무 경험 -> 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ (남은 답변: 상세 경험),PEFT 방법 5가지,0.0016827491,0.0,0.0016827491344884038
Loss Function 관련 실무 경험 -> 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ (남은 답변: 상세 경험),거대 언어 모델 정의,-0.0060482565,0.0,0.006048256531357765
Loss Function 관련 실무 경험 -> 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ (남은 답변: 상세 경험),마지막 할 말,-0.028438823,0.0,0.02843882329761982
Loss Function 관련 실무 경험 -> 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ (남은 답변: 상세 경험),면접 시작 인사,-0.0007912354,0.0,0.0007912354194559157
Loss Function 관련 실무 경험 -> 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ (남은 답변: 상세 경험),면접 종료,0.0052006575,0.0,0.005200657527893782
Loss Function 관련 실무 경험 -> 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ (남은 답변: 상세 경험),"인공지능, 머신러닝, 딥러닝 차이",-0.001765942,0.0,0.0017659419681876898
Loss Function 관련 실무 경험 -> 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ (남은 답변: 상세 경험),잠시 휴식,0.010981365,0.0,0.010981365106999874
Loss Function 관련 실무 경험 -> 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ (남은 답변: 상세 경험),확률 예측에서 MSE Loss 미 사용 이유,-0.012805509,0.0,0.012805509380996227
Loss Function 관련 실무 경험 -> 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어 (남은 답변: 모든 질문 해결 완료),BCE Loss 설명,-0.0042849802,0.0,0.004284980241209269
Loss Function 관련 실무 경험 -> 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어 (남은 답변: 모든 질문 해결 완료),LLM Fine-Tuning 의 PEFT,0.0003320508,0.0,0.0003320508112665266
Loss Function 관련 실무 경험 -> 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어 (남은 답변: 모든 질문 해결 완료),LoRA,-0.053802185,0.0,0.053802184760570526
Loss Function 관련 실무 경험 -> 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어 (남은 답변: 모든 질문 해결 완료),LoRA 와 QLoRA 의 차이,0.03199972,0.0,0.03199971839785576
Loss Function 관련 실무 경험 -> 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어 (남은 답변: 모든 질문 해결 완료),Loss Function 관련 실무 경험,-0.006576755,0.0,0.006576755084097385
Loss Function 관련 실무 경험 -> 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어 (남은 답변: 모든 질문 해결 완료),Loss Function 예시,-0.01370176,0.0,0.013701760210096836
Loss Function 관련 실무 경험 -> 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어 (남은 답변: 모든 질문 해결 완료),Loss Function 정의,-0.03100624,0.0,0.031006239354610443
Loss Function 관련 실무 경험 -> 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어 (남은 답변: 모든 질문 해결 완료),MBTI / 좋아하는 아이돌,0.9078446,1.0,0.09215539693832397
Loss Function 관련 실무 경험 -> 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어 (남은 답변: 모든 질문 해결 완료),MSE Loss 설명,0.016094744,0.0,0.016094744205474854
Loss Function 관련 실무 경험 -> 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어 (남은 답변: 모든 질문 해결 완료),MSE Loss 용도,0.025175191,0.0,0.025175191462039948
Loss Function 관련 실무 경험 -> 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어 (남은 답변: 모든 질문 해결 완료),"Multi-Class, Multi-Label 중 BCE 가 좋은 task",-0.0027484812,0.0,0.0027484812308102846
Loss Function 관련 실무 경험 -> 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어 (남은 답변: 모든 질문 해결 완료),Multi-Label 에서 CE + Softmax 적용 문제점,0.0020775043,0.0,0.0020775042939931154
Loss Function 관련 실무 경험 -> 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어 (남은 답변: 모든 질문 해결 완료),PEFT 방법 5가지,-0.038239356,0.0,0.03823935613036156
Loss Function 관련 실무 경험 -> 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어 (남은 답변: 모든 질문 해결 완료),거대 언어 모델 정의,-0.010241554,0.0,0.010241554118692875
Loss Function 관련 실무 경험 -> 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어 (남은 답변: 모든 질문 해결 완료),마지막 할 말,0.042621154,0.0,0.042621154338121414
Loss Function 관련 실무 경험 -> 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어 (남은 답변: 모든 질문 해결 완료),면접 시작 인사,-6.9379406e-05,0.0,6.937940634088591e-05
Loss Function 관련 실무 경험 -> 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어 (남은 답변: 모든 질문 해결 완료),면접 종료,-0.04248825,0.0,0.04248825088143349
Loss Function 관련 실무 경험 -> 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어 (남은 답변: 모든 질문 해결 완료),"인공지능, 머신러닝, 딥러닝 차이",-0.003831749,0.0,0.0038317490834742785
Loss Function 관련 실무 경험 -> 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어 (남은 답변: 모든 질문 해결 완료),잠시 휴식,-0.058025885,0.0,0.05802588537335396
Loss Function 관련 실무 경험 -> 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어 (남은 답변: 모든 질문 해결 완료),확률 예측에서 MSE Loss 미 사용 이유,-0.0008883229,0.0,0.0008883228874765337
Loss Function 관련 실무 경험 -> 기억 진짜 안 나는데 가물가물하다 (남은 답변: 상세 경험),BCE Loss 설명,0.016824832,0.0,0.016824832186102867
Loss Function 관련 실무 경험 -> 기억 진짜 안 나는데 가물가물하다 (남은 답변: 상세 경험),LLM Fine-Tuning 의 PEFT,0.00886271,0.0,0.008862709626555443
Loss Function 관련 실무 경험 -> 기억 진짜 안 나는데 가물가물하다 (남은 답변: 상세 경험),LoRA,-0.013934722,0.0,0.013934722170233727
Loss Function 관련 실무 경험 -> 기억 진짜 안 나는데 가물가물하다 (남은 답변: 상세 경험),LoRA 와 QLoRA 의 차이,-0.013538869,0.0,0.013538869097828865
Loss Function 관련 실무 경험 -> 기억 진짜 안 나는데 가물가물하다 (남은 답변: 상세 경험),Loss Function 관련 실무 경험,0.24801578,0.0,0.24801577627658844
Loss Function 관련 실무 경험 -> 기억 진짜 안 나는데 가물가물하다 (남은 답변: 상세 경험),Loss Function 예시,-0.01400367,0.0,0.01400366984307766
Loss Function 관련 실무 경험 -> 기억 진짜 안 나는데 가물가물하다 (남은 답변: 상세 경험),Loss Function 정의,0.0068843584,0.0,0.006884358357638121
Loss Function 관련 실무 경험 -> 기억 진짜 안 나는데 가물가물하다 (남은 답변: 상세 경험),MBTI / 좋아하는 아이돌,0.0462426,0.0,0.046242598444223404
Loss Function 관련 실무 경험 -> 기억 진짜 안 나는데 가물가물하다 (남은 답변: 상세 경험),MSE Loss 설명,0.010229153,0.0,0.010229152627289295
Loss Function 관련 실무 경험 -> 기억 진짜 안 나는데 가물가물하다 (남은 답변: 상세 경험),MSE Loss 용도,-0.019143287,0.0,0.01914328709244728
Loss Function 관련 실무 경험 -> 기억 진짜 안 나는데 가물가물하다 (남은 답변: 상세 경험),"Multi-Class, Multi-Label 중 BCE 가 좋은 task",-0.018037854,0.0,0.01803785376250744
Loss Function 관련 실무 경험 -> 기억 진짜 안 나는데 가물가물하다 (남은 답변: 상세 경험),Multi-Label 에서 CE + Softmax 적용 문제점,0.013287323,0.0,0.013287322595715523
Loss Function 관련 실무 경험 -> 기억 진짜 안 나는데 가물가물하다 (남은 답변: 상세 경험),PEFT 방법 5가지,0.0016533205,0.0,0.001653320505283773
Loss Function 관련 실무 경험 -> 기억 진짜 안 나는데 가물가물하다 (남은 답변: 상세 경험),거대 언어 모델 정의,-0.040564306,0.0,0.04056430608034134
Loss Function 관련 실무 경험 -> 기억 진짜 안 나는데 가물가물하다 (남은 답변: 상세 경험),마지막 할 말,-0.009102487,0.0,0.00910248700529337
Loss Function 관련 실무 경험 -> 기억 진짜 안 나는데 가물가물하다 (남은 답변: 상세 경험),면접 시작 인사,0.011291986,0.0,0.01129198633134365
Loss Function 관련 실무 경험 -> 기억 진짜 안 나는데 가물가물하다 (남은 답변: 상세 경험),면접 종료,-0.0025534083,0.0,0.0025534082669764757
Loss Function 관련 실무 경험 -> 기억 진짜 안 나는데 가물가물하다 (남은 답변: 상세 경험),"인공지능, 머신러닝, 딥러닝 차이",-0.01983352,0.0,0.0198335200548172
Loss Function 관련 실무 경험 -> 기억 진짜 안 나는데 가물가물하다 (남은 답변: 상세 경험),잠시 휴식,0.6345347,1.0,0.36546528339385986
Loss Function 관련 실무 경험 -> 기억 진짜 안 나는데 가물가물하다 (남은 답변: 상세 경험),확률 예측에서 MSE Loss 미 사용 이유,0.044167545,0.0,0.044167544692754745
Loss Function 관련 실무 경험 -> 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ (남은 답변: 모든 질문 해결 완료),BCE Loss 설명,-0.015813252,0.0,0.015813251957297325
Loss Function 관련 실무 경험 -> 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ (남은 답변: 모든 질문 해결 완료),LLM Fine-Tuning 의 PEFT,-0.013761141,0.0,0.013761141337454319
Loss Function 관련 실무 경험 -> 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ (남은 답변: 모든 질문 해결 완료),LoRA,-0.020452246,0.0,0.020452246069908142
Loss Function 관련 실무 경험 -> 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ (남은 답변: 모든 질문 해결 완료),LoRA 와 QLoRA 의 차이,-0.011083416,0.0,0.01108341570943594
Loss Function 관련 실무 경험 -> 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ (남은 답변: 모든 질문 해결 완료),Loss Function 관련 실무 경험,-0.046557862,0.0,0.04655786231160164
Loss Function 관련 실무 경험 -> 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ (남은 답변: 모든 질문 해결 완료),Loss Function 예시,-0.013518022,0.0,0.013518022373318672
Loss Function 관련 실무 경험 -> 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ (남은 답변: 모든 질문 해결 완료),Loss Function 정의,-0.04967156,0.0,0.049671560525894165
Loss Function 관련 실무 경험 -> 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ (남은 답변: 모든 질문 해결 완료),MBTI / 좋아하는 아이돌,0.9288526,1.0,0.0711473822593689
Loss Function 관련 실무 경험 -> 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ (남은 답변: 모든 질문 해결 완료),MSE Loss 설명,-0.0033227,0.0,0.0033227000385522842
Loss Function 관련 실무 경험 -> 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ (남은 답변: 모든 질문 해결 완료),MSE Loss 용도,-0.0016756045,0.0,0.0016756044933572412
Loss Function 관련 실무 경험 -> 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ (남은 답변: 모든 질문 해결 완료),"Multi-Class, Multi-Label 중 BCE 가 좋은 task",-0.008156701,0.0,0.008156700991094112
Loss Function 관련 실무 경험 -> 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ (남은 답변: 모든 질문 해결 완료),Multi-Label 에서 CE + Softmax 적용 문제점,-0.008911131,0.0,0.00891113094985485
Loss Function 관련 실무 경험 -> 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ (남은 답변: 모든 질문 해결 완료),PEFT 방법 5가지,-0.022533149,0.0,0.022533148527145386
Loss Function 관련 실무 경험 -> 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ (남은 답변: 모든 질문 해결 완료),거대 언어 모델 정의,-0.015793031,0.0,0.015793031081557274
Loss Function 관련 실무 경험 -> 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ (남은 답변: 모든 질문 해결 완료),마지막 할 말,-0.015555883,0.0,0.015555882826447487
Loss Function 관련 실무 경험 -> 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ (남은 답변: 모든 질문 해결 완료),면접 시작 인사,-0.0008182273,0.0,0.0008182273013517261
Loss Function 관련 실무 경험 -> 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ (남은 답변: 모든 질문 해결 완료),면접 종료,0.009458603,0.0,0.009458603337407112
Loss Function 관련 실무 경험 -> 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ (남은 답변: 모든 질문 해결 완료),"인공지능, 머신러닝, 딥러닝 차이",0.0048207333,0.0,0.004820733331143856
Loss Function 관련 실무 경험 -> 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ (남은 답변: 모든 질문 해결 완료),잠시 휴식,0.053890847,0.0,0.05389084666967392
Loss Function 관련 실무 경험 -> 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ (남은 답변: 모든 질문 해결 완료),확률 예측에서 MSE Loss 미 사용 이유,0.004497681,0.0,0.004497680813074112
"Loss Function 관련 실무 경험 -> 이걸 왜 알려줘야 하지 (남은 답변: 기본 경험, 상세 경험)",BCE Loss 설명,0.011381187,0.0,0.011381186544895172
"Loss Function 관련 실무 경험 -> 이걸 왜 알려줘야 하지 (남은 답변: 기본 경험, 상세 경험)",LLM Fine-Tuning 의 PEFT,0.04636557,0.0,0.04636557027697563
"Loss Function 관련 실무 경험 -> 이걸 왜 알려줘야 하지 (남은 답변: 기본 경험, 상세 경험)",LoRA,-0.001476908,0.0,0.001476907986216247
"Loss Function 관련 실무 경험 -> 이걸 왜 알려줘야 하지 (남은 답변: 기본 경험, 상세 경험)",LoRA 와 QLoRA 의 차이,0.012054066,0.0,0.01205406617373228
"Loss Function 관련 실무 경험 -> 이걸 왜 알려줘야 하지 (남은 답변: 기본 경험, 상세 경험)",Loss Function 관련 실무 경험,-0.002090151,0.0,0.0020901509560644627
"Loss Function 관련 실무 경험 -> 이걸 왜 알려줘야 하지 (남은 답변: 기본 경험, 상세 경험)",Loss Function 예시,0.008364892,0.0,0.00836489163339138
"Loss Function 관련 실무 경험 -> 이걸 왜 알려줘야 하지 (남은 답변: 기본 경험, 상세 경험)",Loss Function 정의,0.0114007965,0.0,0.011400796473026276
"Loss Function 관련 실무 경험 -> 이걸 왜 알려줘야 하지 (남은 답변: 기본 경험, 상세 경험)",MBTI / 좋아하는 아이돌,0.0624638,0.0,0.062463801354169846
"Loss Function 관련 실무 경험 -> 이걸 왜 알려줘야 하지 (남은 답변: 기본 경험, 상세 경험)",MSE Loss 설명,0.013973573,0.0,0.013973573222756386
"Loss Function 관련 실무 경험 -> 이걸 왜 알려줘야 하지 (남은 답변: 기본 경험, 상세 경험)",MSE Loss 용도,-0.01846537,0.0,0.018465369939804077
"Loss Function 관련 실무 경험 -> 이걸 왜 알려줘야 하지 (남은 답변: 기본 경험, 상세 경험)","Multi-Class, Multi-Label 중 BCE 가 좋은 task",0.004554913,0.0,0.00455491291359067
"Loss Function 관련 실무 경험 -> 이걸 왜 알려줘야 하지 (남은 답변: 기본 경험, 상세 경험)",Multi-Label 에서 CE + Softmax 적용 문제점,0.0051395944,0.0,0.005139594431966543
"Loss Function 관련 실무 경험 -> 이걸 왜 알려줘야 하지 (남은 답변: 기본 경험, 상세 경험)",PEFT 방법 5가지,0.022083381,0.0,0.022083381190896034
"Loss Function 관련 실무 경험 -> 이걸 왜 알려줘야 하지 (남은 답변: 기본 경험, 상세 경험)",거대 언어 모델 정의,-0.011945676,0.0,0.011945676058530807
"Loss Function 관련 실무 경험 -> 이걸 왜 알려줘야 하지 (남은 답변: 기본 경험, 상세 경험)",마지막 할 말,-0.054863375,0.0,0.054863374680280685
"Loss Function 관련 실무 경험 -> 이걸 왜 알려줘야 하지 (남은 답변: 기본 경험, 상세 경험)",면접 시작 인사,0.037766133,0.0,0.03776613250374794
"Loss Function 관련 실무 경험 -> 이걸 왜 알려줘야 하지 (남은 답변: 기본 경험, 상세 경험)",면접 종료,0.0170904,0.0,0.01709040068089962
"Loss Function 관련 실무 경험 -> 이걸 왜 알려줘야 하지 (남은 답변: 기본 경험, 상세 경험)","인공지능, 머신러닝, 딥러닝 차이",0.063638814,0.0,0.06363881379365921
"Loss Function 관련 실무 경험 -> 이걸 왜 알려줘야 하지 (남은 답변: 기본 경험, 상세 경험)",잠시 휴식,0.7271089,1.0,0.272891104221344
"Loss Function 관련 실무 경험 -> 이걸 왜 알려줘야 하지 (남은 답변: 기본 경험, 상세 경험)",확률 예측에서 MSE Loss 미 사용 이유,0.016379846,0.0,0.01637984625995159
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야! (남은 답변: 모든 질문 해결 완료),BCE Loss 설명,0.008722107,0.0,0.00872210692614317
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야! (남은 답변: 모든 질문 해결 완료),LLM Fine-Tuning 의 PEFT,0.023978574,0.0,0.023978574201464653
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야! (남은 답변: 모든 질문 해결 완료),LoRA,-0.025363153,0.0,0.025363152846693993
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야! (남은 답변: 모든 질문 해결 완료),LoRA 와 QLoRA 의 차이,-0.009277348,0.0,0.009277348406612873
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야! (남은 답변: 모든 질문 해결 완료),Loss Function 관련 실무 경험,-0.015701136,0.0,0.015701135620474815
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야! (남은 답변: 모든 질문 해결 완료),Loss Function 예시,0.00065713393,0.0,0.0006571339326910675
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야! (남은 답변: 모든 질문 해결 완료),Loss Function 정의,-0.006530961,0.0,0.006530961021780968
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야! (남은 답변: 모든 질문 해결 완료),MBTI / 좋아하는 아이돌,0.045689557,0.0,0.04568955674767494
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야! (남은 답변: 모든 질문 해결 완료),MSE Loss 설명,0.006414946,0.0,0.0064149461686611176
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야! (남은 답변: 모든 질문 해결 완료),MSE Loss 용도,-0.015035232,0.0,0.015035231597721577
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야! (남은 답변: 모든 질문 해결 완료),"Multi-Class, Multi-Label 중 BCE 가 좋은 task",-0.011623184,0.0,0.011623184196650982
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야! (남은 답변: 모든 질문 해결 완료),Multi-Label 에서 CE + Softmax 적용 문제점,0.01286231,0.0,0.01286230981349945
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야! (남은 답변: 모든 질문 해결 완료),PEFT 방법 5가지,0.015391758,0.0,0.01539175771176815
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야! (남은 답변: 모든 질문 해결 완료),거대 언어 모델 정의,-0.025217252,0.0,0.025217251852154732
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야! (남은 답변: 모든 질문 해결 완료),마지막 할 말,0.0009081686,0.0,0.0009081686148419976
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야! (남은 답변: 모든 질문 해결 완료),면접 시작 인사,0.015309113,0.0,0.015309113077819347
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야! (남은 답변: 모든 질문 해결 완료),면접 종료,0.033957034,0.0,0.03395703434944153
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야! (남은 답변: 모든 질문 해결 완료),"인공지능, 머신러닝, 딥러닝 차이",-0.0037356373,0.0,0.003735637292265892
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야! (남은 답변: 모든 질문 해결 완료),잠시 휴식,0.7841556,1.0,0.21584439277648926
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야! (남은 답변: 모든 질문 해결 완료),확률 예측에서 MSE Loss 미 사용 이유,-0.0054485337,0.0,0.005448533687740564
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해 (남은 답변: 모든 질문 해결 완료),BCE Loss 설명,-0.0076531284,0.0,0.007653128355741501
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해 (남은 답변: 모든 질문 해결 완료),LLM Fine-Tuning 의 PEFT,0.013231629,0.0,0.013231628574430943
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해 (남은 답변: 모든 질문 해결 완료),LoRA,0.023406567,0.0,0.02340656705200672
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해 (남은 답변: 모든 질문 해결 완료),LoRA 와 QLoRA 의 차이,0.043795638,0.0,0.0437956377863884
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해 (남은 답변: 모든 질문 해결 완료),Loss Function 관련 실무 경험,-0.016649768,0.0,0.016649767756462097
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해 (남은 답변: 모든 질문 해결 완료),Loss Function 예시,0.01528895,0.0,0.015288949944078922
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해 (남은 답변: 모든 질문 해결 완료),Loss Function 정의,-0.0012598437,0.0,0.0012598437024280429
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해 (남은 답변: 모든 질문 해결 완료),MBTI / 좋아하는 아이돌,0.06522993,0.0,0.06522992998361588
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해 (남은 답변: 모든 질문 해결 완료),MSE Loss 설명,-0.0010573993,0.0,0.0010573993204161525
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해 (남은 답변: 모든 질문 해결 완료),MSE Loss 용도,-0.027495619,0.0,0.027495618909597397
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해 (남은 답변: 모든 질문 해결 완료),"Multi-Class, Multi-Label 중 BCE 가 좋은 task",0.020503832,0.0,0.020503832027316093
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해 (남은 답변: 모든 질문 해결 완료),Multi-Label 에서 CE + Softmax 적용 문제점,0.012788893,0.0,0.012788892723619938
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해 (남은 답변: 모든 질문 해결 완료),PEFT 방법 5가지,0.004481761,0.0,0.004481760784983635
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해 (남은 답변: 모든 질문 해결 완료),거대 언어 모델 정의,-0.02113427,0.0,0.0211342703551054
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해 (남은 답변: 모든 질문 해결 완료),마지막 할 말,-0.025789298,0.0,0.025789298117160797
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해 (남은 답변: 모든 질문 해결 완료),면접 시작 인사,0.0092611145,0.0,0.00926111452281475
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해 (남은 답변: 모든 질문 해결 완료),면접 종료,0.0140147265,0.0,0.014014726504683495
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해 (남은 답변: 모든 질문 해결 완료),"인공지능, 머신러닝, 딥러닝 차이",-0.0010238219,0.0,0.0010238218819722533
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해 (남은 답변: 모든 질문 해결 완료),잠시 휴식,0.8101941,1.0,0.18980592489242554
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해 (남은 답변: 모든 질문 해결 완료),확률 예측에서 MSE Loss 미 사용 이유,0.013921291,0.0,0.013921290636062622
잠시 휴식 -> 재미있는 이야기 해줄래? (남은 답변: 잠시 휴식),BCE Loss 설명,-0.008950954,0.0,0.008950954303145409
잠시 휴식 -> 재미있는 이야기 해줄래? (남은 답변: 잠시 휴식),LLM Fine-Tuning 의 PEFT,0.98473614,1.0,0.015263855457305908
잠시 휴식 -> 재미있는 이야기 해줄래? (남은 답변: 잠시 휴식),LoRA,0.01022821,0.0,0.010228210128843784
잠시 휴식 -> 재미있는 이야기 해줄래? (남은 답변: 잠시 휴식),LoRA 와 QLoRA 의 차이,-0.0037055626,0.0,0.0037055625580251217
잠시 휴식 -> 재미있는 이야기 해줄래? (남은 답변: 잠시 휴식),Loss Function 관련 실무 경험,-0.01163125,0.0,0.011631250381469727
잠시 휴식 -> 재미있는 이야기 해줄래? (남은 답변: 잠시 휴식),Loss Function 예시,-0.010394007,0.0,0.010394006967544556
잠시 휴식 -> 재미있는 이야기 해줄래? (남은 답변: 잠시 휴식),Loss Function 정의,-0.0053901086,0.0,0.005390108563005924
잠시 휴식 -> 재미있는 이야기 해줄래? (남은 답변: 잠시 휴식),MBTI / 좋아하는 아이돌,0.0052633733,0.0,0.005263373255729675
잠시 휴식 -> 재미있는 이야기 해줄래? (남은 답변: 잠시 휴식),MSE Loss 설명,0.013310467,0.0,0.013310466893017292
잠시 휴식 -> 재미있는 이야기 해줄래? (남은 답변: 잠시 휴식),MSE Loss 용도,0.0041181236,0.0,0.004118123557418585
잠시 휴식 -> 재미있는 이야기 해줄래? (남은 답변: 잠시 휴식),"Multi-Class, Multi-Label 중 BCE 가 좋은 task",-0.0085364245,0.0,0.008536424487829208
잠시 휴식 -> 재미있는 이야기 해줄래? (남은 답변: 잠시 휴식),Multi-Label 에서 CE + Softmax 적용 문제점,0.0027875681,0.0,0.0027875681407749653
잠시 휴식 -> 재미있는 이야기 해줄래? (남은 답변: 잠시 휴식),PEFT 방법 5가지,-0.019330392,0.0,0.01933039166033268
잠시 휴식 -> 재미있는 이야기 해줄래? (남은 답변: 잠시 휴식),거대 언어 모델 정의,0.005148779,0.0,0.005148779135197401
잠시 휴식 -> 재미있는 이야기 해줄래? (남은 답변: 잠시 휴식),마지막 할 말,-0.004043969,0.0,0.004043968860059977
잠시 휴식 -> 재미있는 이야기 해줄래? (남은 답변: 잠시 휴식),면접 시작 인사,-0.005815117,0.0,0.0058151171542704105
잠시 휴식 -> 재미있는 이야기 해줄래? (남은 답변: 잠시 휴식),면접 종료,0.00466107,0.0,0.004661070182919502
잠시 휴식 -> 재미있는 이야기 해줄래? (남은 답변: 잠시 휴식),"인공지능, 머신러닝, 딥러닝 차이",0.014032431,0.0,0.014032430946826935
잠시 휴식 -> 재미있는 이야기 해줄래? (남은 답변: 잠시 휴식),잠시 휴식,0.007861233,0.0,0.007861233316361904
잠시 휴식 -> 재미있는 이야기 해줄래? (남은 답변: 잠시 휴식),확률 예측에서 MSE Loss 미 사용 이유,-0.0046811053,0.0,0.004681105259805918
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야 (남은 답변: 모든 질문 해결 완료)",BCE Loss 설명,0.006559151,0.0,0.006559151224792004
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야 (남은 답변: 모든 질문 해결 완료)",LLM Fine-Tuning 의 PEFT,0.027311314,0.0,0.027311313897371292
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야 (남은 답변: 모든 질문 해결 완료)",LoRA,-0.05216801,0.0,0.05216801166534424
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야 (남은 답변: 모든 질문 해결 완료)",LoRA 와 QLoRA 의 차이,0.02711422,0.0,0.027114219963550568
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야 (남은 답변: 모든 질문 해결 완료)",Loss Function 관련 실무 경험,0.0012711592,0.0,0.0012711591552942991
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야 (남은 답변: 모든 질문 해결 완료)",Loss Function 예시,0.0038812822,0.0,0.0038812821730971336
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야 (남은 답변: 모든 질문 해결 완료)",Loss Function 정의,0.014904196,0.0,0.014904196374118328
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야 (남은 답변: 모든 질문 해결 완료)",MBTI / 좋아하는 아이돌,-0.010454818,0.0,0.010454817675054073
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야 (남은 답변: 모든 질문 해결 완료)",MSE Loss 설명,-0.026641814,0.0,0.026641814038157463
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야 (남은 답변: 모든 질문 해결 완료)",MSE Loss 용도,0.00279702,0.0,0.002797019900754094
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야 (남은 답변: 모든 질문 해결 완료)","Multi-Class, Multi-Label 중 BCE 가 좋은 task",-0.00041055042,0.0,0.0004105504194740206
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야 (남은 답변: 모든 질문 해결 완료)",Multi-Label 에서 CE + Softmax 적용 문제점,-0.019511664,0.0,0.019511664286255836
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야 (남은 답변: 모든 질문 해결 완료)",PEFT 방법 5가지,0.9412787,1.0,0.058721303939819336
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야 (남은 답변: 모든 질문 해결 완료)",거대 언어 모델 정의,-0.009239127,0.0,0.009239126928150654
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야 (남은 답변: 모든 질문 해결 완료)",마지막 할 말,-0.021789996,0.0,0.021789995953440666
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야 (남은 답변: 모든 질문 해결 완료)",면접 시작 인사,-0.04116647,0.0,0.04116646945476532
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야 (남은 답변: 모든 질문 해결 완료)",면접 종료,-0.025683096,0.0,0.025683095678687096
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야 (남은 답변: 모든 질문 해결 완료)","인공지능, 머신러닝, 딥러닝 차이",-0.016337415,0.0,0.01633741520345211
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야 (남은 답변: 모든 질문 해결 완료)",잠시 휴식,-0.016161146,0.0,0.016161145642399788
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야 (남은 답변: 모든 질문 해결 완료)",확률 예측에서 MSE Loss 미 사용 이유,-0.013099237,0.0,0.013099237345159054
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지? (남은 답변: LLM Fine-Tuning 의 PEFT),BCE Loss 설명,0.0018805643,0.0,0.0018805642612278461
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지? (남은 답변: LLM Fine-Tuning 의 PEFT),LLM Fine-Tuning 의 PEFT,0.9817718,1.0,0.01822817325592041
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지? (남은 답변: LLM Fine-Tuning 의 PEFT),LoRA,0.011899853,0.0,0.011899853125214577
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지? (남은 답변: LLM Fine-Tuning 의 PEFT),LoRA 와 QLoRA 의 차이,0.007224716,0.0,0.007224715780466795
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지? (남은 답변: LLM Fine-Tuning 의 PEFT),Loss Function 관련 실무 경험,0.005711852,0.0,0.005711852107197046
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지? (남은 답변: LLM Fine-Tuning 의 PEFT),Loss Function 예시,-0.0010031041,0.0,0.0010031041456386447
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지? (남은 답변: LLM Fine-Tuning 의 PEFT),Loss Function 정의,-0.0030896666,0.0,0.003089666599407792
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지? (남은 답변: LLM Fine-Tuning 의 PEFT),MBTI / 좋아하는 아이돌,-0.007885806,0.0,0.007885806262493134
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지? (남은 답변: LLM Fine-Tuning 의 PEFT),MSE Loss 설명,0.009571925,0.0,0.009571924805641174
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지? (남은 답변: LLM Fine-Tuning 의 PEFT),MSE Loss 용도,2.5557287e-05,0.0,2.555728678999003e-05
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지? (남은 답변: LLM Fine-Tuning 의 PEFT),"Multi-Class, Multi-Label 중 BCE 가 좋은 task",0.007747809,0.0,0.007747808936983347
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지? (남은 답변: LLM Fine-Tuning 의 PEFT),Multi-Label 에서 CE + Softmax 적용 문제점,0.009840755,0.0,0.00984075479209423
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지? (남은 답변: LLM Fine-Tuning 의 PEFT),PEFT 방법 5가지,-0.016886573,0.0,0.016886573284864426
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지? (남은 답변: LLM Fine-Tuning 의 PEFT),거대 언어 모델 정의,0.013676002,0.0,0.013676001690328121
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지? (남은 답변: LLM Fine-Tuning 의 PEFT),마지막 할 말,-0.007400512,0.0,0.007400512229651213
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지? (남은 답변: LLM Fine-Tuning 의 PEFT),면접 시작 인사,-0.008677458,0.0,0.008677458390593529
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지? (남은 답변: LLM Fine-Tuning 의 PEFT),면접 종료,-0.00023035306,0.0,0.00023035306367091835
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지? (남은 답변: LLM Fine-Tuning 의 PEFT),"인공지능, 머신러닝, 딥러닝 차이",-0.009487303,0.0,0.009487302973866463
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지? (남은 답변: LLM Fine-Tuning 의 PEFT),잠시 휴식,-0.013458413,0.0,0.013458413071930408
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지? (남은 답변: LLM Fine-Tuning 의 PEFT),확률 예측에서 MSE Loss 미 사용 이유,0.0046460666,0.0,0.004646066576242447
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야! (남은 답변: 모든 질문 해결 완료),BCE Loss 설명,-0.020628992,0.0,0.020628992468118668
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야! (남은 답변: 모든 질문 해결 완료),LLM Fine-Tuning 의 PEFT,-0.011168635,0.0,0.011168635450303555
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야! (남은 답변: 모든 질문 해결 완료),LoRA,-0.033956736,0.0,0.03395673632621765
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야! (남은 답변: 모든 질문 해결 완료),LoRA 와 QLoRA 의 차이,0.024973273,0.0,0.024973273277282715
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야! (남은 답변: 모든 질문 해결 완료),Loss Function 관련 실무 경험,0.009909866,0.0,0.009909866377711296
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야! (남은 답변: 모든 질문 해결 완료),Loss Function 예시,-0.01768235,0.0,0.017682349309325218
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야! (남은 답변: 모든 질문 해결 완료),Loss Function 정의,-0.004157163,0.0,0.004157163202762604
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야! (남은 답변: 모든 질문 해결 완료),MBTI / 좋아하는 아이돌,-0.0019167294,0.0,0.0019167293794453144
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야! (남은 답변: 모든 질문 해결 완료),MSE Loss 설명,-0.017110577,0.0,0.01711057685315609
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야! (남은 답변: 모든 질문 해결 완료),MSE Loss 용도,0.00994365,0.0,0.009943650104105473
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야! (남은 답변: 모든 질문 해결 완료),"Multi-Class, Multi-Label 중 BCE 가 좋은 task",-0.00088242977,0.0,0.0008824297692626715
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야! (남은 답변: 모든 질문 해결 완료),Multi-Label 에서 CE + Softmax 적용 문제점,-0.005414322,0.0,0.005414322018623352
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야! (남은 답변: 모든 질문 해결 완료),PEFT 방법 5가지,0.9495228,1.0,0.05047720670700073
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야! (남은 답변: 모든 질문 해결 완료),거대 언어 모델 정의,0.011369333,0.0,0.011369332671165466
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야! (남은 답변: 모든 질문 해결 완료),마지막 할 말,-0.022444114,0.0,0.022444114089012146
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야! (남은 답변: 모든 질문 해결 완료),면접 시작 인사,-0.022211045,0.0,0.022211045026779175
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야! (남은 답변: 모든 질문 해결 완료),면접 종료,-0.02079158,0.0,0.02079158090054989
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야! (남은 답변: 모든 질문 해결 완료),"인공지능, 머신러닝, 딥러닝 차이",-0.0054741665,0.0,0.005474166478961706
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야! (남은 답변: 모든 질문 해결 완료),잠시 휴식,-0.012896703,0.0,0.012896702624857426
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야! (남은 답변: 모든 질문 해결 완료),확률 예측에서 MSE Loss 미 사용 이유,-0.022547489,0.0,0.022547489032149315
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다 (남은 답변: LLM Fine-Tuning 의 PEFT),BCE Loss 설명,-0.007600884,0.0,0.007600883953273296
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다 (남은 답변: LLM Fine-Tuning 의 PEFT),LLM Fine-Tuning 의 PEFT,0.98012316,1.0,0.019876837730407715
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다 (남은 답변: LLM Fine-Tuning 의 PEFT),LoRA,0.016460408,0.0,0.01646040752530098
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다 (남은 답변: LLM Fine-Tuning 의 PEFT),LoRA 와 QLoRA 의 차이,0.0050233086,0.0,0.00502330856397748
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다 (남은 답변: LLM Fine-Tuning 의 PEFT),Loss Function 관련 실무 경험,-0.0016328674,0.0,0.001632867380976677
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다 (남은 답변: LLM Fine-Tuning 의 PEFT),Loss Function 예시,0.0085061975,0.0,0.008506197482347488
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다 (남은 답변: LLM Fine-Tuning 의 PEFT),Loss Function 정의,0.007028805,0.0,0.0070288050919771194
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다 (남은 답변: LLM Fine-Tuning 의 PEFT),MBTI / 좋아하는 아이돌,-0.00027769312,0.0,0.00027769312146119773
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다 (남은 답변: LLM Fine-Tuning 의 PEFT),MSE Loss 설명,0.0061604073,0.0,0.006160407327115536
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다 (남은 답변: LLM Fine-Tuning 의 PEFT),MSE Loss 용도,-0.0023574,0.0,0.0023574000224471092
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다 (남은 답변: LLM Fine-Tuning 의 PEFT),"Multi-Class, Multi-Label 중 BCE 가 좋은 task",-0.001451102,0.0,0.0014511019689962268
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다 (남은 답변: LLM Fine-Tuning 의 PEFT),Multi-Label 에서 CE + Softmax 적용 문제점,3.948659e-05,0.0,3.948658923036419e-05
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다 (남은 답변: LLM Fine-Tuning 의 PEFT),PEFT 방법 5가지,-0.0013727865,0.0,0.001372786471620202
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다 (남은 답변: LLM Fine-Tuning 의 PEFT),거대 언어 모델 정의,0.009282608,0.0,0.009282607585191727
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다 (남은 답변: LLM Fine-Tuning 의 PEFT),마지막 할 말,-0.00523133,0.0,0.005231330171227455
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다 (남은 답변: LLM Fine-Tuning 의 PEFT),면접 시작 인사,-0.0053785336,0.0,0.005378533620387316
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다 (남은 답변: LLM Fine-Tuning 의 PEFT),면접 종료,0.00024222085,0.0,0.00024222084903158247
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다 (남은 답변: LLM Fine-Tuning 의 PEFT),"인공지능, 머신러닝, 딥러닝 차이",-0.017063713,0.0,0.01706371270120144
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다 (남은 답변: LLM Fine-Tuning 의 PEFT),잠시 휴식,-0.008132794,0.0,0.008132793940603733
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다 (남은 답변: LLM Fine-Tuning 의 PEFT),확률 예측에서 MSE Loss 미 사용 이유,0.0065100268,0.0,0.006510026752948761
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지? (남은 답변: 모든 질문 해결 완료)",BCE Loss 설명,0.047282677,0.0,0.04728267714381218
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지? (남은 답변: 모든 질문 해결 완료)",LLM Fine-Tuning 의 PEFT,-0.019587105,0.0,0.01958710514008999
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지? (남은 답변: 모든 질문 해결 완료)",LoRA,0.90365213,1.0,0.0963478684425354
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지? (남은 답변: 모든 질문 해결 완료)",LoRA 와 QLoRA 의 차이,-0.033017058,0.0,0.03301705792546272
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지? (남은 답변: 모든 질문 해결 완료)",Loss Function 관련 실무 경험,0.004222649,0.0,0.004222649149596691
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지? (남은 답변: 모든 질문 해결 완료)",Loss Function 예시,0.0025765633,0.0,0.002576563274487853
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지? (남은 답변: 모든 질문 해결 완료)",Loss Function 정의,0.02075411,0.0,0.02075411006808281
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지? (남은 답변: 모든 질문 해결 완료)",MBTI / 좋아하는 아이돌,-0.0024783558,0.0,0.002478355774655938
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지? (남은 답변: 모든 질문 해결 완료)",MSE Loss 설명,-0.025817817,0.0,0.025817817077040672
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지? (남은 답변: 모든 질문 해결 완료)",MSE Loss 용도,-0.016326757,0.0,0.01632675714790821
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지? (남은 답변: 모든 질문 해결 완료)","Multi-Class, Multi-Label 중 BCE 가 좋은 task",-0.027571332,0.0,0.027571331709623337
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지? (남은 답변: 모든 질문 해결 완료)",Multi-Label 에서 CE + Softmax 적용 문제점,0.012917733,0.0,0.012917732819914818
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지? (남은 답변: 모든 질문 해결 완료)",PEFT 방법 5가지,-0.053371273,0.0,0.05337127298116684
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지? (남은 답변: 모든 질문 해결 완료)",거대 언어 모델 정의,-0.02333506,0.0,0.023335060104727745
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지? (남은 답변: 모든 질문 해결 완료)",마지막 할 말,0.03857412,0.0,0.03857411816716194
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지? (남은 답변: 모든 질문 해결 완료)",면접 시작 인사,-0.043039363,0.0,0.043039362877607346
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지? (남은 답변: 모든 질문 해결 완료)",면접 종료,0.009264789,0.0,0.009264788590371609
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지? (남은 답변: 모든 질문 해결 완료)","인공지능, 머신러닝, 딥러닝 차이",-0.027496897,0.0,0.02749689668416977
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지? (남은 답변: 모든 질문 해결 완료)",잠시 휴식,-0.02750673,0.0,0.02750672958791256
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지? (남은 답변: 모든 질문 해결 완료)",확률 예측에서 MSE Loss 미 사용 이유,2.316593e-06,0.0,2.3165930542745627e-06
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야? (남은 답변: PEFT 방법 5가지),BCE Loss 설명,-0.005351971,0.0,0.00535197090357542
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야? (남은 답변: PEFT 방법 5가지),LLM Fine-Tuning 의 PEFT,0.01102085,0.0,0.011020850390195847
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야? (남은 답변: PEFT 방법 5가지),LoRA,0.026830409,0.0,0.026830408722162247
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야? (남은 답변: PEFT 방법 5가지),LoRA 와 QLoRA 의 차이,-0.026105506,0.0,0.026105506345629692
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야? (남은 답변: PEFT 방법 5가지),Loss Function 관련 실무 경험,-0.025982091,0.0,0.025982091203331947
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야? (남은 답변: PEFT 방법 5가지),Loss Function 예시,-0.020280052,0.0,0.020280051976442337
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야? (남은 답변: PEFT 방법 5가지),Loss Function 정의,-0.011210979,0.0,0.011210978962481022
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야? (남은 답변: PEFT 방법 5가지),MBTI / 좋아하는 아이돌,-0.024270516,0.0,0.024270515888929367
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야? (남은 답변: PEFT 방법 5가지),MSE Loss 설명,-0.04738104,0.0,0.04738103970885277
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야? (남은 답변: PEFT 방법 5가지),MSE Loss 용도,-0.022607679,0.0,0.022607678547501564
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야? (남은 답변: PEFT 방법 5가지),"Multi-Class, Multi-Label 중 BCE 가 좋은 task",-0.0006980821,0.0,0.0006980820908211172
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야? (남은 답변: PEFT 방법 5가지),Multi-Label 에서 CE + Softmax 적용 문제점,-0.005829645,0.0,0.005829644855111837
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야? (남은 답변: PEFT 방법 5가지),PEFT 방법 5가지,0.9534885,1.0,0.04651147127151489
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야? (남은 답변: PEFT 방법 5가지),거대 언어 모델 정의,-0.019396344,0.0,0.01939634419977665
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야? (남은 답변: PEFT 방법 5가지),마지막 할 말,-0.04329397,0.0,0.043293971568346024
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야? (남은 답변: PEFT 방법 5가지),면접 시작 인사,-0.039646294,0.0,0.039646293967962265
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야? (남은 답변: PEFT 방법 5가지),면접 종료,-0.0017658431,0.0,0.0017658431315794587
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야? (남은 답변: PEFT 방법 5가지),"인공지능, 머신러닝, 딥러닝 차이",-0.014341787,0.0,0.014341787435114384
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야? (남은 답변: PEFT 방법 5가지),잠시 휴식,-0.0047549475,0.0,0.0047549474984407425
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야? (남은 답변: PEFT 방법 5가지),확률 예측에서 MSE Loss 미 사용 이유,0.012267936,0.0,0.012267936021089554
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야! (남은 답변: 모든 질문 해결 완료),BCE Loss 설명,0.0067474074,0.0,0.0067474073730409145
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야! (남은 답변: 모든 질문 해결 완료),LLM Fine-Tuning 의 PEFT,0.004148772,0.0,0.004148771986365318
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야! (남은 답변: 모든 질문 해결 완료),LoRA,0.0029247114,0.0,0.0029247114434838295
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야! (남은 답변: 모든 질문 해결 완료),LoRA 와 QLoRA 의 차이,0.9507056,1.0,0.04929441213607788
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야! (남은 답변: 모든 질문 해결 완료),Loss Function 관련 실무 경험,0.007521367,0.0,0.0075213671661913395
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야! (남은 답변: 모든 질문 해결 완료),Loss Function 예시,0.003663729,0.0,0.0036637289449572563
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야! (남은 답변: 모든 질문 해결 완료),Loss Function 정의,-0.009532384,0.0,0.009532383643090725
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야! (남은 답변: 모든 질문 해결 완료),MBTI / 좋아하는 아이돌,-0.0016257281,0.0,0.0016257280949503183
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야! (남은 답변: 모든 질문 해결 완료),MSE Loss 설명,-0.015498437,0.0,0.015498436987400055
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야! (남은 답변: 모든 질문 해결 완료),MSE Loss 용도,-0.0018215117,0.0,0.0018215116579085588
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야! (남은 답변: 모든 질문 해결 완료),"Multi-Class, Multi-Label 중 BCE 가 좋은 task",0.018721957,0.0,0.01872195675969124
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야! (남은 답변: 모든 질문 해결 완료),Multi-Label 에서 CE + Softmax 적용 문제점,0.00039583116,0.0,0.00039583115722052753
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야! (남은 답변: 모든 질문 해결 완료),PEFT 방법 5가지,-0.01643139,0.0,0.01643138937652111
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야! (남은 답변: 모든 질문 해결 완료),거대 언어 모델 정의,-0.020208858,0.0,0.02020885795354843
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야! (남은 답변: 모든 질문 해결 완료),마지막 할 말,0.0015698369,0.0,0.0015698368661105633
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야! (남은 답변: 모든 질문 해결 완료),면접 시작 인사,-0.0070568463,0.0,0.007056846283376217
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야! (남은 답변: 모든 질문 해결 완료),면접 종료,0.006681577,0.0,0.0066815768368542194
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야! (남은 답변: 모든 질문 해결 완료),"인공지능, 머신러닝, 딥러닝 차이",0.0069871554,0.0,0.006987155415117741
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야! (남은 답변: 모든 질문 해결 완료),잠시 휴식,0.0083442535,0.0,0.008344253525137901
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야! (남은 답변: 모든 질문 해결 완료),확률 예측에서 MSE Loss 미 사용 이유,-0.0019971838,0.0,0.0019971837755292654
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아? (남은 답변: LoRA),BCE Loss 설명,2.3751163e-05,0.0,2.3751163098495454e-05
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아? (남은 답변: LoRA),LLM Fine-Tuning 의 PEFT,-0.0018591585,0.0,0.0018591585103422403
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아? (남은 답변: LoRA),LoRA,0.92957044,1.0,0.07042956352233887
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아? (남은 답변: LoRA),LoRA 와 QLoRA 의 차이,-0.057293802,0.0,0.05729380249977112
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아? (남은 답변: LoRA),Loss Function 관련 실무 경험,-0.012191031,0.0,0.012191031128168106
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아? (남은 답변: LoRA),Loss Function 예시,-0.031619143,0.0,0.03161914274096489
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아? (남은 답변: LoRA),Loss Function 정의,-0.004252298,0.0,0.004252297803759575
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아? (남은 답변: LoRA),MBTI / 좋아하는 아이돌,-0.022776803,0.0,0.022776803001761436
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아? (남은 답변: LoRA),MSE Loss 설명,-0.025687752,0.0,0.025687752291560173
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아? (남은 답변: LoRA),MSE Loss 용도,-0.016222758,0.0,0.01622275821864605
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아? (남은 답변: LoRA),"Multi-Class, Multi-Label 중 BCE 가 좋은 task",0.004129575,0.0,0.004129575099796057
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아? (남은 답변: LoRA),Multi-Label 에서 CE + Softmax 적용 문제점,-0.0015928956,0.0,0.0015928955981507897
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아? (남은 답변: LoRA),PEFT 방법 5가지,-0.021347204,0.0,0.021347204223275185
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아? (남은 답변: LoRA),거대 언어 모델 정의,0.01728848,0.0,0.017288479954004288
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아? (남은 답변: LoRA),마지막 할 말,-0.028203418,0.0,0.028203418478369713
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아? (남은 답변: LoRA),면접 시작 인사,-0.02134829,0.0,0.021348290145397186
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아? (남은 답변: LoRA),면접 종료,-0.02813377,0.0,0.02813377045094967
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아? (남은 답변: LoRA),"인공지능, 머신러닝, 딥러닝 차이",-0.0092839375,0.0,0.009283937513828278
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아? (남은 답변: LoRA),잠시 휴식,0.02103131,0.0,0.02103131078183651
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아? (남은 답변: LoRA),확률 예측에서 MSE Loss 미 사용 이유,-0.001093744,0.0,0.0010937439510598779
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야! (남은 답변: 모든 질문 해결 완료),BCE Loss 설명,0.0049037845,0.0,0.004903784487396479
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야! (남은 답변: 모든 질문 해결 완료),LLM Fine-Tuning 의 PEFT,0.0035306802,0.0,0.003530680201947689
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야! (남은 답변: 모든 질문 해결 완료),LoRA,-0.04734041,0.0,0.04734041169285774
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야! (남은 답변: 모든 질문 해결 완료),LoRA 와 QLoRA 의 차이,0.95898086,1.0,0.04101914167404175
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야! (남은 답변: 모든 질문 해결 완료),Loss Function 관련 실무 경험,0.03910472,0.0,0.03910471871495247
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야! (남은 답변: 모든 질문 해결 완료),Loss Function 예시,-0.0036227144,0.0,0.0036227144300937653
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야! (남은 답변: 모든 질문 해결 완료),Loss Function 정의,-0.007146954,0.0,0.007146954070776701
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야! (남은 답변: 모든 질문 해결 완료),MBTI / 좋아하는 아이돌,0.0105046565,0.0,0.010504656471312046
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야! (남은 답변: 모든 질문 해결 완료),MSE Loss 설명,-0.01870529,0.0,0.01870528981089592
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야! (남은 답변: 모든 질문 해결 완료),MSE Loss 용도,-0.007530721,0.0,0.00753072090446949
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야! (남은 답변: 모든 질문 해결 완료),"Multi-Class, Multi-Label 중 BCE 가 좋은 task",0.0070024906,0.0,0.007002490572631359
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야! (남은 답변: 모든 질문 해결 완료),Multi-Label 에서 CE + Softmax 적용 문제점,-0.013193612,0.0,0.013193611986935139
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야! (남은 답변: 모든 질문 해결 완료),PEFT 방법 5가지,0.010417556,0.0,0.010417556390166283
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야! (남은 답변: 모든 질문 해결 완료),거대 언어 모델 정의,0.009958879,0.0,0.009958879090845585
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야! (남은 답변: 모든 질문 해결 완료),마지막 할 말,0.0048187682,0.0,0.004818768240511417
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야! (남은 답변: 모든 질문 해결 완료),면접 시작 인사,-0.012422538,0.0,0.012422538362443447
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야! (남은 답변: 모든 질문 해결 완료),면접 종료,-0.00016587805,0.0,0.00016587805293966085
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야! (남은 답변: 모든 질문 해결 완료),"인공지능, 머신러닝, 딥러닝 차이",0.00030775715,0.0,0.00030775714549236
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야! (남은 답변: 모든 질문 해결 완료),잠시 휴식,0.0003488509,0.0,0.00034885091008618474
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야! (남은 답변: 모든 질문 해결 완료),확률 예측에서 MSE Loss 미 사용 이유,0.004409265,0.0,0.004409264773130417
LoRA -> 무슨 OOM 없앤다는 것 같은데 (남은 답변: LoRA),BCE Loss 설명,0.015562807,0.0,0.015562807209789753
LoRA -> 무슨 OOM 없앤다는 것 같은데 (남은 답변: LoRA),LLM Fine-Tuning 의 PEFT,-0.00092495553,0.0,0.0009249555296264589
LoRA -> 무슨 OOM 없앤다는 것 같은데 (남은 답변: LoRA),LoRA,0.9298554,1.0,0.07014459371566772
LoRA -> 무슨 OOM 없앤다는 것 같은데 (남은 답변: LoRA),LoRA 와 QLoRA 의 차이,-0.09553461,0.0,0.09553460776805878
LoRA -> 무슨 OOM 없앤다는 것 같은데 (남은 답변: LoRA),Loss Function 관련 실무 경험,-0.003921962,0.0,0.00392196187749505
LoRA -> 무슨 OOM 없앤다는 것 같은데 (남은 답변: LoRA),Loss Function 예시,-0.029252794,0.0,0.0292527936398983
LoRA -> 무슨 OOM 없앤다는 것 같은데 (남은 답변: LoRA),Loss Function 정의,-0.015411193,0.0,0.015411192551255226
LoRA -> 무슨 OOM 없앤다는 것 같은데 (남은 답변: LoRA),MBTI / 좋아하는 아이돌,-0.02777851,0.0,0.027778510004281998
LoRA -> 무슨 OOM 없앤다는 것 같은데 (남은 답변: LoRA),MSE Loss 설명,-0.0218917,0.0,0.021891700103878975
LoRA -> 무슨 OOM 없앤다는 것 같은데 (남은 답변: LoRA),MSE Loss 용도,-0.015189959,0.0,0.015189958736300468
LoRA -> 무슨 OOM 없앤다는 것 같은데 (남은 답변: LoRA),"Multi-Class, Multi-Label 중 BCE 가 좋은 task",0.010003001,0.0,0.010003001429140568
LoRA -> 무슨 OOM 없앤다는 것 같은데 (남은 답변: LoRA),Multi-Label 에서 CE + Softmax 적용 문제점,0.0021186832,0.0,0.002118683187291026
LoRA -> 무슨 OOM 없앤다는 것 같은데 (남은 답변: LoRA),PEFT 방법 5가지,-0.013541509,0.0,0.0135415093973279
LoRA -> 무슨 OOM 없앤다는 것 같은데 (남은 답변: LoRA),거대 언어 모델 정의,0.02442348,0.0,0.02442348003387451
LoRA -> 무슨 OOM 없앤다는 것 같은데 (남은 답변: LoRA),마지막 할 말,-0.012802729,0.0,0.012802729383111
LoRA -> 무슨 OOM 없앤다는 것 같은데 (남은 답변: LoRA),면접 시작 인사,-0.0012203462,0.0,0.0012203461956232786
LoRA -> 무슨 OOM 없앤다는 것 같은데 (남은 답변: LoRA),면접 종료,-0.012713667,0.0,0.012713667005300522
LoRA -> 무슨 OOM 없앤다는 것 같은데 (남은 답변: LoRA),"인공지능, 머신러닝, 딥러닝 차이",-0.015322628,0.0,0.015322628431022167
LoRA -> 무슨 OOM 없앤다는 것 같은데 (남은 답변: LoRA),잠시 휴식,0.01464876,0.0,0.014648759737610817
LoRA -> 무슨 OOM 없앤다는 것 같은데 (남은 답변: LoRA),확률 예측에서 MSE Loss 미 사용 이유,-0.031166272,0.0,0.03116627223789692
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야! (남은 답변: 모든 질문 해결 완료),BCE Loss 설명,0.017428381,0.0,0.017428381368517876
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야! (남은 답변: 모든 질문 해결 완료),LLM Fine-Tuning 의 PEFT,0.0015261709,0.0,0.001526170875877142
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야! (남은 답변: 모든 질문 해결 완료),LoRA,-0.08646827,0.0,0.08646827191114426
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야! (남은 답변: 모든 질문 해결 완료),LoRA 와 QLoRA 의 차이,0.048461426,0.0,0.0484614260494709
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야! (남은 답변: 모든 질문 해결 완료),Loss Function 관련 실무 경험,-0.01835345,0.0,0.018353449180722237
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야! (남은 답변: 모든 질문 해결 완료),Loss Function 예시,0.02445153,0.0,0.02445152960717678
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야! (남은 답변: 모든 질문 해결 완료),Loss Function 정의,0.076082155,0.0,0.07608215510845184
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야! (남은 답변: 모든 질문 해결 완료),MBTI / 좋아하는 아이돌,0.009939185,0.0,0.009939185343682766
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야! (남은 답변: 모든 질문 해결 완료),MSE Loss 설명,-0.06074304,0.0,0.06074304133653641
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야! (남은 답변: 모든 질문 해결 완료),MSE Loss 용도,-0.026337601,0.0,0.026337601244449615
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야! (남은 답변: 모든 질문 해결 완료),"Multi-Class, Multi-Label 중 BCE 가 좋은 task",-0.038090806,0.0,0.03809080645442009
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야! (남은 답변: 모든 질문 해결 완료),Multi-Label 에서 CE + Softmax 적용 문제점,0.04637168,0.0,0.04637167975306511
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야! (남은 답변: 모든 질문 해결 완료),PEFT 방법 5가지,0.02714288,0.0,0.027142880484461784
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야! (남은 답변: 모든 질문 해결 완료),거대 언어 모델 정의,0.023579802,0.0,0.023579802364110947
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야! (남은 답변: 모든 질문 해결 완료),마지막 할 말,0.60973567,1.0,0.3902643322944641
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야! (남은 답변: 모든 질문 해결 완료),면접 시작 인사,0.06534702,0.0,0.06534702330827713
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야! (남은 답변: 모든 질문 해결 완료),면접 종료,0.010796863,0.0,0.010796862654387951
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야! (남은 답변: 모든 질문 해결 완료),"인공지능, 머신러닝, 딥러닝 차이",0.010563311,0.0,0.010563311167061329
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야! (남은 답변: 모든 질문 해결 완료),잠시 휴식,0.04600272,0.0,0.046002719551324844
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야! (남은 답변: 모든 질문 해결 완료),확률 예측에서 MSE Loss 미 사용 이유,-0.015802264,0.0,0.01580226421356201
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데 (남은 답변: LoRA 와 QLoRA 의 차이),BCE Loss 설명,-0.013377174,0.0,0.0133771738037467
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데 (남은 답변: LoRA 와 QLoRA 의 차이),LLM Fine-Tuning 의 PEFT,-0.0025352524,0.0,0.0025352523662149906
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데 (남은 답변: LoRA 와 QLoRA 의 차이),LoRA,-0.052020427,0.0,0.05202042683959007
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데 (남은 답변: LoRA 와 QLoRA 의 차이),LoRA 와 QLoRA 의 차이,0.95083857,1.0,0.049161434173583984
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데 (남은 답변: LoRA 와 QLoRA 의 차이),Loss Function 관련 실무 경험,-0.0009843676,0.0,0.0009843675652518868
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데 (남은 답변: LoRA 와 QLoRA 의 차이),Loss Function 예시,0.0016747188,0.0,0.0016747188055887818
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데 (남은 답변: LoRA 와 QLoRA 의 차이),Loss Function 정의,-0.00649052,0.0,0.00649052020162344
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데 (남은 답변: LoRA 와 QLoRA 의 차이),MBTI / 좋아하는 아이돌,0.016341994,0.0,0.01634199358522892
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데 (남은 답변: LoRA 와 QLoRA 의 차이),MSE Loss 설명,-0.022689346,0.0,0.022689346224069595
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데 (남은 답변: LoRA 와 QLoRA 의 차이),MSE Loss 용도,-0.010972559,0.0,0.010972559452056885
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데 (남은 답변: LoRA 와 QLoRA 의 차이),"Multi-Class, Multi-Label 중 BCE 가 좋은 task",-0.006983397,0.0,0.00698339706286788
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데 (남은 답변: LoRA 와 QLoRA 의 차이),Multi-Label 에서 CE + Softmax 적용 문제점,-0.00616188,0.0,0.00616188021376729
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데 (남은 답변: LoRA 와 QLoRA 의 차이),PEFT 방법 5가지,-0.0013353905,0.0,0.0013353904942050576
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데 (남은 답변: LoRA 와 QLoRA 의 차이),거대 언어 모델 정의,0.007137441,0.0,0.007137441076338291
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데 (남은 답변: LoRA 와 QLoRA 의 차이),마지막 할 말,0.009385104,0.0,0.009385104291141033
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데 (남은 답변: LoRA 와 QLoRA 의 차이),면접 시작 인사,6.242636e-05,0.0,6.242636300157756e-05
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데 (남은 답변: LoRA 와 QLoRA 의 차이),면접 종료,0.019063668,0.0,0.019063668325543404
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데 (남은 답변: LoRA 와 QLoRA 의 차이),"인공지능, 머신러닝, 딥러닝 차이",-0.0062503535,0.0,0.006250353530049324
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데 (남은 답변: LoRA 와 QLoRA 의 차이),잠시 휴식,-0.01001259,0.0,0.010012590326368809
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데 (남은 답변: LoRA 와 QLoRA 의 차이),확률 예측에서 MSE Loss 미 사용 이유,0.012102413,0.0,0.01210241299122572
마지막 할 말 -> 로라야 정말 고마워! (남은 답변: 마지막 할 말),BCE Loss 설명,-9.887316e-05,0.0,9.887316264212132e-05
마지막 할 말 -> 로라야 정말 고마워! (남은 답변: 마지막 할 말),LLM Fine-Tuning 의 PEFT,-0.00025642104,0.0,0.0002564210444688797
마지막 할 말 -> 로라야 정말 고마워! (남은 답변: 마지막 할 말),LoRA,-0.0120948395,0.0,0.012094839476048946
마지막 할 말 -> 로라야 정말 고마워! (남은 답변: 마지막 할 말),LoRA 와 QLoRA 의 차이,0.0013358563,0.0,0.0013358562719076872
마지막 할 말 -> 로라야 정말 고마워! (남은 답변: 마지막 할 말),Loss Function 관련 실무 경험,0.008051006,0.0,0.008051006123423576
마지막 할 말 -> 로라야 정말 고마워! (남은 답변: 마지막 할 말),Loss Function 예시,-0.008828163,0.0,0.008828163146972656
마지막 할 말 -> 로라야 정말 고마워! (남은 답변: 마지막 할 말),Loss Function 정의,-0.027188627,0.0,0.027188627049326897
마지막 할 말 -> 로라야 정말 고마워! (남은 답변: 마지막 할 말),MBTI / 좋아하는 아이돌,-0.009343734,0.0,0.009343734011054039
마지막 할 말 -> 로라야 정말 고마워! (남은 답변: 마지막 할 말),MSE Loss 설명,-0.0015686045,0.0,0.0015686044935137033
마지막 할 말 -> 로라야 정말 고마워! (남은 답변: 마지막 할 말),MSE Loss 용도,-0.0065224445,0.0,0.0065224445424973965
마지막 할 말 -> 로라야 정말 고마워! (남은 답변: 마지막 할 말),"Multi-Class, Multi-Label 중 BCE 가 좋은 task",0.0043092435,0.0,0.004309243522584438
마지막 할 말 -> 로라야 정말 고마워! (남은 답변: 마지막 할 말),Multi-Label 에서 CE + Softmax 적용 문제점,0.0116845425,0.0,0.011684542521834373
마지막 할 말 -> 로라야 정말 고마워! (남은 답변: 마지막 할 말),PEFT 방법 5가지,-0.027301522,0.0,0.027301521971821785
마지막 할 말 -> 로라야 정말 고마워! (남은 답변: 마지막 할 말),거대 언어 모델 정의,-0.0070113596,0.0,0.007011359557509422
마지막 할 말 -> 로라야 정말 고마워! (남은 답변: 마지막 할 말),마지막 할 말,-0.054731477,0.0,0.05473147705197334
마지막 할 말 -> 로라야 정말 고마워! (남은 답변: 마지막 할 말),면접 시작 인사,0.009160297,0.0,0.009160296991467476
마지막 할 말 -> 로라야 정말 고마워! (남은 답변: 마지막 할 말),면접 종료,0.9555779,1.0,0.04442209005355835
마지막 할 말 -> 로라야 정말 고마워! (남은 답변: 마지막 할 말),"인공지능, 머신러닝, 딥러닝 차이",-0.019017037,0.0,0.019017037004232407
마지막 할 말 -> 로라야 정말 고마워! (남은 답변: 마지막 할 말),잠시 휴식,0.020423468,0.0,0.020423468202352524
마지막 할 말 -> 로라야 정말 고마워! (남은 답변: 마지막 할 말),확률 예측에서 MSE Loss 미 사용 이유,-0.0005820017,0.0,0.0005820016958750784
마지막 할 말 -> 로라야 사랑해 (남은 답변: 마지막 할 말),BCE Loss 설명,-0.008161241,0.0,0.008161241188645363
마지막 할 말 -> 로라야 사랑해 (남은 답변: 마지막 할 말),LLM Fine-Tuning 의 PEFT,0.00924726,0.0,0.00924726016819477
마지막 할 말 -> 로라야 사랑해 (남은 답변: 마지막 할 말),LoRA,-0.0065963455,0.0,0.006596345454454422
마지막 할 말 -> 로라야 사랑해 (남은 답변: 마지막 할 말),LoRA 와 QLoRA 의 차이,-0.0050292052,0.0,0.005029205232858658
마지막 할 말 -> 로라야 사랑해 (남은 답변: 마지막 할 말),Loss Function 관련 실무 경험,0.005063421,0.0,0.005063421092927456
마지막 할 말 -> 로라야 사랑해 (남은 답변: 마지막 할 말),Loss Function 예시,-0.01149587,0.0,0.011495869606733322
마지막 할 말 -> 로라야 사랑해 (남은 답변: 마지막 할 말),Loss Function 정의,-0.02394271,0.0,0.02394271083176136
마지막 할 말 -> 로라야 사랑해 (남은 답변: 마지막 할 말),MBTI / 좋아하는 아이돌,-0.010070324,0.0,0.010070323944091797
마지막 할 말 -> 로라야 사랑해 (남은 답변: 마지막 할 말),MSE Loss 설명,-0.00876717,0.0,0.008767169900238514
마지막 할 말 -> 로라야 사랑해 (남은 답변: 마지막 할 말),MSE Loss 용도,-0.012406005,0.0,0.012406004592776299
마지막 할 말 -> 로라야 사랑해 (남은 답변: 마지막 할 말),"Multi-Class, Multi-Label 중 BCE 가 좋은 task",-0.00026792637,0.0,0.00026792637072503567
마지막 할 말 -> 로라야 사랑해 (남은 답변: 마지막 할 말),Multi-Label 에서 CE + Softmax 적용 문제점,0.012985523,0.0,0.012985522858798504
마지막 할 말 -> 로라야 사랑해 (남은 답변: 마지막 할 말),PEFT 방법 5가지,-0.028982824,0.0,0.028982823714613914
마지막 할 말 -> 로라야 사랑해 (남은 답변: 마지막 할 말),거대 언어 모델 정의,-0.0016861612,0.0,0.0016861611511558294
마지막 할 말 -> 로라야 사랑해 (남은 답변: 마지막 할 말),마지막 할 말,-0.053503346,0.0,0.05350334569811821
마지막 할 말 -> 로라야 사랑해 (남은 답변: 마지막 할 말),면접 시작 인사,0.0055207424,0.0,0.0055207423865795135
마지막 할 말 -> 로라야 사랑해 (남은 답변: 마지막 할 말),면접 종료,0.9544874,1.0,0.045512616634368896
마지막 할 말 -> 로라야 사랑해 (남은 답변: 마지막 할 말),"인공지능, 머신러닝, 딥러닝 차이",-0.017360093,0.0,0.01736009307205677
마지막 할 말 -> 로라야 사랑해 (남은 답변: 마지막 할 말),잠시 휴식,0.010439522,0.0,0.010439521633088589
마지막 할 말 -> 로라야 사랑해 (남은 답변: 마지막 할 말),확률 예측에서 MSE Loss 미 사용 이유,0.013274086,0.0,0.013274085707962513
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어 (남은 답변: 마지막 할 말),BCE Loss 설명,-0.0063942815,0.0,0.006394281517714262
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어 (남은 답변: 마지막 할 말),LLM Fine-Tuning 의 PEFT,-0.0015853222,0.0,0.0015853221993893385
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어 (남은 답변: 마지막 할 말),LoRA,-0.0038013759,0.0,0.003801375860348344
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어 (남은 답변: 마지막 할 말),LoRA 와 QLoRA 의 차이,0.0041645397,0.0,0.004164539743214846
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어 (남은 답변: 마지막 할 말),Loss Function 관련 실무 경험,0.005720145,0.0,0.00572014506906271
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어 (남은 답변: 마지막 할 말),Loss Function 예시,-0.009593763,0.0,0.009593763388693333
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어 (남은 답변: 마지막 할 말),Loss Function 정의,-0.021545727,0.0,0.02154572680592537
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어 (남은 답변: 마지막 할 말),MBTI / 좋아하는 아이돌,-0.0036348265,0.0,0.003634826513007283
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어 (남은 답변: 마지막 할 말),MSE Loss 설명,-0.011100947,0.0,0.011100946925580502
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어 (남은 답변: 마지막 할 말),MSE Loss 용도,-0.016442357,0.0,0.016442356631159782
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어 (남은 답변: 마지막 할 말),"Multi-Class, Multi-Label 중 BCE 가 좋은 task",-0.002395146,0.0,0.0023951460607349873
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어 (남은 답변: 마지막 할 말),Multi-Label 에서 CE + Softmax 적용 문제점,0.014859166,0.0,0.014859165996313095
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어 (남은 답변: 마지막 할 말),PEFT 방법 5가지,-0.030281844,0.0,0.03028184361755848
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어 (남은 답변: 마지막 할 말),거대 언어 모델 정의,0.0021832313,0.0,0.0021832312922924757
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어 (남은 답변: 마지막 할 말),마지막 할 말,-0.058629442,0.0,0.058629442006349564
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어 (남은 답변: 마지막 할 말),면접 시작 인사,0.0138805015,0.0,0.013880501501262188
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어 (남은 답변: 마지막 할 말),면접 종료,0.95142615,1.0,0.048573851585388184
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어 (남은 답변: 마지막 할 말),"인공지능, 머신러닝, 딥러닝 차이",-0.022115925,0.0,0.022115925326943398
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어 (남은 답변: 마지막 할 말),잠시 휴식,0.009644866,0.0,0.009644865989685059
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어 (남은 답변: 마지막 할 말),확률 예측에서 MSE Loss 미 사용 이유,0.013971536,0.0,0.013971536420285702
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워 (남은 답변: 마지막 할 말),BCE Loss 설명,0.007040059,0.0,0.007040059193968773
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워 (남은 답변: 마지막 할 말),LLM Fine-Tuning 의 PEFT,-0.0014679414,0.0,0.001467941445298493
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워 (남은 답변: 마지막 할 말),LoRA,-0.012760064,0.0,0.012760063633322716
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워 (남은 답변: 마지막 할 말),LoRA 와 QLoRA 의 차이,-0.0017436047,0.0,0.0017436046618968248
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워 (남은 답변: 마지막 할 말),Loss Function 관련 실무 경험,0.020343263,0.0,0.02034326270222664
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워 (남은 답변: 마지막 할 말),Loss Function 예시,-0.0082615325,0.0,0.00826153252273798
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워 (남은 답변: 마지막 할 말),Loss Function 정의,-0.025952818,0.0,0.025952817872166634
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워 (남은 답변: 마지막 할 말),MBTI / 좋아하는 아이돌,0.011682633,0.0,0.011682633310556412
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워 (남은 답변: 마지막 할 말),MSE Loss 설명,-0.0062418547,0.0,0.0062418547458946705
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워 (남은 답변: 마지막 할 말),MSE Loss 용도,-0.010715698,0.0,0.010715697892010212
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워 (남은 답변: 마지막 할 말),"Multi-Class, Multi-Label 중 BCE 가 좋은 task",0.0021434654,0.0,0.0021434654481709003
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워 (남은 답변: 마지막 할 말),Multi-Label 에서 CE + Softmax 적용 문제점,0.016788753,0.0,0.016788752749562263
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워 (남은 답변: 마지막 할 말),PEFT 방법 5가지,-0.02533271,0.0,0.025332709774374962
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워 (남은 답변: 마지막 할 말),거대 언어 모델 정의,-0.0036535475,0.0,0.0036535474937409163
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워 (남은 답변: 마지막 할 말),마지막 할 말,-0.054450855,0.0,0.054450854659080505
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워 (남은 답변: 마지막 할 말),면접 시작 인사,0.004762096,0.0,0.004762095864862204
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워 (남은 답변: 마지막 할 말),면접 종료,0.9503309,1.0,0.049669086933135986
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워 (남은 답변: 마지막 할 말),"인공지능, 머신러닝, 딥러닝 차이",-0.026209246,0.0,0.02620924636721611
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워 (남은 답변: 마지막 할 말),잠시 휴식,0.02020908,0.0,0.02020907960832119
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워 (남은 답변: 마지막 할 말),확률 예측에서 MSE Loss 미 사용 이유,-0.0040355064,0.0,0.004035506397485733
