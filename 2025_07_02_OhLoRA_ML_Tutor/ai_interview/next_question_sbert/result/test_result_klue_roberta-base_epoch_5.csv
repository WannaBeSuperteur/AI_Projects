input_part,next_question,predicted_similarity,ground_truth_similarity,absolute_error
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서? (남은 답변: 면접 시작 인사),BCE Loss 설명,-0.024632392,0.0,0.024632392451167107
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서? (남은 답변: 면접 시작 인사),LLM Fine-Tuning 의 PEFT,-0.015514073,0.0,0.015514072962105274
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서? (남은 답변: 면접 시작 인사),LoRA,-0.04252452,0.0,0.04252452030777931
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서? (남은 답변: 면접 시작 인사),LoRA 와 QLoRA 의 차이,0.033714995,0.0,0.03371499478816986
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서? (남은 답변: 면접 시작 인사),Loss Function 관련 실무 경험,0.0810754,0.0,0.08107540011405945
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서? (남은 답변: 면접 시작 인사),Loss Function 예시,0.044942573,0.0,0.044942572712898254
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서? (남은 답변: 면접 시작 인사),Loss Function 정의,0.043074872,0.0,0.043074872344732285
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서? (남은 답변: 면접 시작 인사),MBTI,0.00805842,0.0,0.00805842038244009
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서? (남은 답변: 면접 시작 인사),MSE Loss 설명,0.011272916,0.0,0.01127291563898325
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서? (남은 답변: 면접 시작 인사),MSE Loss 용도,0.025354633,0.0,0.02535463310778141
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서? (남은 답변: 면접 시작 인사),"Multi-Class, Multi-Label 중 BCE 가 좋은 task",0.016588468,0.0,0.016588468104600906
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서? (남은 답변: 면접 시작 인사),Multi-Label 에서 CE + Softmax 적용 문제점,-0.03272828,0.0,0.03272828087210655
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서? (남은 답변: 면접 시작 인사),PEFT 방법 5가지,-0.034350794,0.0,0.034350793808698654
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서? (남은 답변: 면접 시작 인사),거대 언어 모델 정의,0.03577067,0.0,0.03577066957950592
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서? (남은 답변: 면접 시작 인사),마지막 할 말,0.020319661,0.0,0.020319661125540733
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서? (남은 답변: 면접 시작 인사),면접 시작 인사,0.26335037,1.0,0.7366496324539185
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서? (남은 답변: 면접 시작 인사),면접 종료,0.18004142,0.0,0.18004141747951508
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서? (남은 답변: 면접 시작 인사),"인공지능, 머신러닝, 딥러닝 차이",0.4067946,0.0,0.40679460763931274
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서? (남은 답변: 면접 시작 인사),잠시 휴식,0.15348655,0.0,0.15348654985427856
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서? (남은 답변: 면접 시작 인사),좋아하는 아이돌,0.015960628,0.0,0.015960628166794777
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서? (남은 답변: 면접 시작 인사),확률 예측에서 MSE Loss 미 사용 이유,-0.02514445,0.0,0.02514445036649704
면접 시작 인사 -> 로라야 안녕 정말 반가워 (남은 답변: 면접 시작 인사),BCE Loss 설명,-0.06750776,0.0,0.06750775873661041
면접 시작 인사 -> 로라야 안녕 정말 반가워 (남은 답변: 면접 시작 인사),LLM Fine-Tuning 의 PEFT,-0.053054053,0.0,0.05305405333638191
면접 시작 인사 -> 로라야 안녕 정말 반가워 (남은 답변: 면접 시작 인사),LoRA,0.07218596,0.0,0.07218596339225769
면접 시작 인사 -> 로라야 안녕 정말 반가워 (남은 답변: 면접 시작 인사),LoRA 와 QLoRA 의 차이,0.08037754,0.0,0.08037754148244858
면접 시작 인사 -> 로라야 안녕 정말 반가워 (남은 답변: 면접 시작 인사),Loss Function 관련 실무 경험,0.040782195,0.0,0.04078219458460808
면접 시작 인사 -> 로라야 안녕 정말 반가워 (남은 답변: 면접 시작 인사),Loss Function 예시,0.06483959,0.0,0.06483958661556244
면접 시작 인사 -> 로라야 안녕 정말 반가워 (남은 답변: 면접 시작 인사),Loss Function 정의,0.059961542,0.0,0.05996154248714447
면접 시작 인사 -> 로라야 안녕 정말 반가워 (남은 답변: 면접 시작 인사),MBTI,0.0088384645,0.0,0.008838464505970478
면접 시작 인사 -> 로라야 안녕 정말 반가워 (남은 답변: 면접 시작 인사),MSE Loss 설명,0.010202541,0.0,0.010202541016042233
면접 시작 인사 -> 로라야 안녕 정말 반가워 (남은 답변: 면접 시작 인사),MSE Loss 용도,0.03247887,0.0,0.03247886896133423
면접 시작 인사 -> 로라야 안녕 정말 반가워 (남은 답변: 면접 시작 인사),"Multi-Class, Multi-Label 중 BCE 가 좋은 task",0.0070387204,0.0,0.007038720417767763
면접 시작 인사 -> 로라야 안녕 정말 반가워 (남은 답변: 면접 시작 인사),Multi-Label 에서 CE + Softmax 적용 문제점,-0.054697428,0.0,0.0546974278986454
면접 시작 인사 -> 로라야 안녕 정말 반가워 (남은 답변: 면접 시작 인사),PEFT 방법 5가지,-0.08098872,0.0,0.08098872005939484
면접 시작 인사 -> 로라야 안녕 정말 반가워 (남은 답변: 면접 시작 인사),거대 언어 모델 정의,0.049627736,0.0,0.04962773621082306
면접 시작 인사 -> 로라야 안녕 정말 반가워 (남은 답변: 면접 시작 인사),마지막 할 말,-0.0059760674,0.0,0.005976067390292883
면접 시작 인사 -> 로라야 안녕 정말 반가워 (남은 답변: 면접 시작 인사),면접 시작 인사,0.25842848,1.0,0.7415715157985687
면접 시작 인사 -> 로라야 안녕 정말 반가워 (남은 답변: 면접 시작 인사),면접 종료,0.29591104,0.0,0.29591104388237
면접 시작 인사 -> 로라야 안녕 정말 반가워 (남은 답변: 면접 시작 인사),"인공지능, 머신러닝, 딥러닝 차이",0.1998709,0.0,0.19987089931964874
면접 시작 인사 -> 로라야 안녕 정말 반가워 (남은 답변: 면접 시작 인사),잠시 휴식,0.19063868,0.0,0.1906386762857437
면접 시작 인사 -> 로라야 안녕 정말 반가워 (남은 답변: 면접 시작 인사),좋아하는 아이돌,0.0030422239,0.0,0.0030422238633036613
면접 시작 인사 -> 로라야 안녕 정말 반가워 (남은 답변: 면접 시작 인사),확률 예측에서 MSE Loss 미 사용 이유,-0.032117482,0.0,0.03211748227477074
면접 시작 인사 -> 로라야 그럼 네가 면접관이야? (남은 답변: 면접 시작 인사),BCE Loss 설명,-0.030254759,0.0,0.030254758894443512
면접 시작 인사 -> 로라야 그럼 네가 면접관이야? (남은 답변: 면접 시작 인사),LLM Fine-Tuning 의 PEFT,-0.0015165113,0.0,0.001516511314548552
면접 시작 인사 -> 로라야 그럼 네가 면접관이야? (남은 답변: 면접 시작 인사),LoRA,0.027526332,0.0,0.027526332065463066
면접 시작 인사 -> 로라야 그럼 네가 면접관이야? (남은 답변: 면접 시작 인사),LoRA 와 QLoRA 의 차이,0.025792228,0.0,0.025792228057980537
면접 시작 인사 -> 로라야 그럼 네가 면접관이야? (남은 답변: 면접 시작 인사),Loss Function 관련 실무 경험,0.048816282,0.0,0.04881628230214119
면접 시작 인사 -> 로라야 그럼 네가 면접관이야? (남은 답변: 면접 시작 인사),Loss Function 예시,0.017444463,0.0,0.017444463446736336
면접 시작 인사 -> 로라야 그럼 네가 면접관이야? (남은 답변: 면접 시작 인사),Loss Function 정의,0.0062368503,0.0,0.006236850284039974
면접 시작 인사 -> 로라야 그럼 네가 면접관이야? (남은 답변: 면접 시작 인사),MBTI,-0.041012995,0.0,0.041012994945049286
면접 시작 인사 -> 로라야 그럼 네가 면접관이야? (남은 답변: 면접 시작 인사),MSE Loss 설명,-0.0074120914,0.0,0.007412091363221407
면접 시작 인사 -> 로라야 그럼 네가 면접관이야? (남은 답변: 면접 시작 인사),MSE Loss 용도,0.016197732,0.0,0.016197731718420982
면접 시작 인사 -> 로라야 그럼 네가 면접관이야? (남은 답변: 면접 시작 인사),"Multi-Class, Multi-Label 중 BCE 가 좋은 task",-0.009406455,0.0,0.009406454861164093
면접 시작 인사 -> 로라야 그럼 네가 면접관이야? (남은 답변: 면접 시작 인사),Multi-Label 에서 CE + Softmax 적용 문제점,-0.007843238,0.0,0.007843238301575184
면접 시작 인사 -> 로라야 그럼 네가 면접관이야? (남은 답변: 면접 시작 인사),PEFT 방법 5가지,-0.0546589,0.0,0.05465890094637871
면접 시작 인사 -> 로라야 그럼 네가 면접관이야? (남은 답변: 면접 시작 인사),거대 언어 모델 정의,0.026638363,0.0,0.026638362556695938
면접 시작 인사 -> 로라야 그럼 네가 면접관이야? (남은 답변: 면접 시작 인사),마지막 할 말,-0.026999302,0.0,0.026999302208423615
면접 시작 인사 -> 로라야 그럼 네가 면접관이야? (남은 답변: 면접 시작 인사),면접 시작 인사,0.18151918,1.0,0.8184808194637299
면접 시작 인사 -> 로라야 그럼 네가 면접관이야? (남은 답변: 면접 시작 인사),면접 종료,0.078932144,0.0,0.07893214374780655
면접 시작 인사 -> 로라야 그럼 네가 면접관이야? (남은 답변: 면접 시작 인사),"인공지능, 머신러닝, 딥러닝 차이",0.48013014,0.0,0.480130136013031
면접 시작 인사 -> 로라야 그럼 네가 면접관이야? (남은 답변: 면접 시작 인사),잠시 휴식,0.088124216,0.0,0.08812421560287476
면접 시작 인사 -> 로라야 그럼 네가 면접관이야? (남은 답변: 면접 시작 인사),좋아하는 아이돌,-0.017572751,0.0,0.01757275126874447
면접 시작 인사 -> 로라야 그럼 네가 면접관이야? (남은 답변: 면접 시작 인사),확률 예측에서 MSE Loss 미 사용 이유,-0.03864616,0.0,0.038646161556243896
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까 (남은 답변: 면접 시작 인사),BCE Loss 설명,-0.052962985,0.0,0.05296298488974571
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까 (남은 답변: 면접 시작 인사),LLM Fine-Tuning 의 PEFT,0.078957796,0.0,0.07895779609680176
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까 (남은 답변: 면접 시작 인사),LoRA,0.16400012,0.0,0.16400012373924255
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까 (남은 답변: 면접 시작 인사),LoRA 와 QLoRA 의 차이,0.0951595,0.0,0.09515950083732605
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까 (남은 답변: 면접 시작 인사),Loss Function 관련 실무 경험,0.010414549,0.0,0.01041454914957285
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까 (남은 답변: 면접 시작 인사),Loss Function 예시,0.00045104022,0.0,0.0004510402213782072
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까 (남은 답변: 면접 시작 인사),Loss Function 정의,-0.0018441152,0.0,0.001844115206040442
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까 (남은 답변: 면접 시작 인사),MBTI,-0.05431546,0.0,0.05431545898318291
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까 (남은 답변: 면접 시작 인사),MSE Loss 설명,-0.023087626,0.0,0.023087626323103905
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까 (남은 답변: 면접 시작 인사),MSE Loss 용도,0.008774559,0.0,0.008774559013545513
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까 (남은 답변: 면접 시작 인사),"Multi-Class, Multi-Label 중 BCE 가 좋은 task",-0.04621436,0.0,0.04621436074376106
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까 (남은 답변: 면접 시작 인사),Multi-Label 에서 CE + Softmax 적용 문제점,-0.019716917,0.0,0.019716916605830193
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까 (남은 답변: 면접 시작 인사),PEFT 방법 5가지,0.031787917,0.0,0.03178791701793671
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까 (남은 답변: 면접 시작 인사),거대 언어 모델 정의,-0.026490474,0.0,0.026490474119782448
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까 (남은 답변: 면접 시작 인사),마지막 할 말,-0.009542286,0.0,0.009542286396026611
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까 (남은 답변: 면접 시작 인사),면접 시작 인사,0.16313994,1.0,0.8368600606918335
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까 (남은 답변: 면접 시작 인사),면접 종료,0.14781739,0.0,0.14781738817691803
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까 (남은 답변: 면접 시작 인사),"인공지능, 머신러닝, 딥러닝 차이",0.41634285,0.0,0.4163428544998169
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까 (남은 답변: 면접 시작 인사),잠시 휴식,0.09187288,0.0,0.09187287837266922
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까 (남은 답변: 면접 시작 인사),좋아하는 아이돌,-0.011889947,0.0,0.011889946646988392
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까 (남은 답변: 면접 시작 인사),확률 예측에서 MSE Loss 미 사용 이유,0.010693146,0.0,0.010693145915865898
면접 시작 인사 -> 파이팅! 시작하자 (남은 답변: 면접 시작 인사),BCE Loss 설명,0.05592462,0.0,0.05592462047934532
면접 시작 인사 -> 파이팅! 시작하자 (남은 답변: 면접 시작 인사),LLM Fine-Tuning 의 PEFT,0.025496233,0.0,0.025496233254671097
면접 시작 인사 -> 파이팅! 시작하자 (남은 답변: 면접 시작 인사),LoRA,-0.003340733,0.0,0.00334073300473392
면접 시작 인사 -> 파이팅! 시작하자 (남은 답변: 면접 시작 인사),LoRA 와 QLoRA 의 차이,0.016287036,0.0,0.01628703624010086
면접 시작 인사 -> 파이팅! 시작하자 (남은 답변: 면접 시작 인사),Loss Function 관련 실무 경험,0.034101926,0.0,0.034101925790309906
면접 시작 인사 -> 파이팅! 시작하자 (남은 답변: 면접 시작 인사),Loss Function 예시,0.04102049,0.0,0.04102049022912979
면접 시작 인사 -> 파이팅! 시작하자 (남은 답변: 면접 시작 인사),Loss Function 정의,0.027860865,0.0,0.027860864996910095
면접 시작 인사 -> 파이팅! 시작하자 (남은 답변: 면접 시작 인사),MBTI,-0.040198136,0.0,0.04019813612103462
면접 시작 인사 -> 파이팅! 시작하자 (남은 답변: 면접 시작 인사),MSE Loss 설명,-0.0035242953,0.0,0.003524295287206769
면접 시작 인사 -> 파이팅! 시작하자 (남은 답변: 면접 시작 인사),MSE Loss 용도,0.0055286735,0.0,0.005528673529624939
면접 시작 인사 -> 파이팅! 시작하자 (남은 답변: 면접 시작 인사),"Multi-Class, Multi-Label 중 BCE 가 좋은 task",-0.018531404,0.0,0.018531404435634613
면접 시작 인사 -> 파이팅! 시작하자 (남은 답변: 면접 시작 인사),Multi-Label 에서 CE + Softmax 적용 문제점,-0.010388471,0.0,0.010388471186161041
면접 시작 인사 -> 파이팅! 시작하자 (남은 답변: 면접 시작 인사),PEFT 방법 5가지,0.0030957654,0.0,0.0030957653652876616
면접 시작 인사 -> 파이팅! 시작하자 (남은 답변: 면접 시작 인사),거대 언어 모델 정의,-0.02561849,0.0,0.02561848983168602
면접 시작 인사 -> 파이팅! 시작하자 (남은 답변: 면접 시작 인사),마지막 할 말,-0.02115524,0.0,0.021155240014195442
면접 시작 인사 -> 파이팅! 시작하자 (남은 답변: 면접 시작 인사),면접 시작 인사,0.20355657,0.0,0.20355656743049622
면접 시작 인사 -> 파이팅! 시작하자 (남은 답변: 면접 시작 인사),면접 종료,0.15827054,0.0,0.15827053785324097
면접 시작 인사 -> 파이팅! 시작하자 (남은 답변: 면접 시작 인사),"인공지능, 머신러닝, 딥러닝 차이",0.51579976,1.0,0.48420023918151855
면접 시작 인사 -> 파이팅! 시작하자 (남은 답변: 면접 시작 인사),잠시 휴식,0.07010955,0.0,0.07010955363512039
면접 시작 인사 -> 파이팅! 시작하자 (남은 답변: 면접 시작 인사),좋아하는 아이돌,-0.044181537,0.0,0.04418153688311577
면접 시작 인사 -> 파이팅! 시작하자 (남은 답변: 면접 시작 인사),확률 예측에서 MSE Loss 미 사용 이유,-0.009870379,0.0,0.009870379231870174
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데 (남은 답변: 인공지능, 머신러닝, 딥러닝)",BCE Loss 설명,0.008777223,0.0,0.008777222596108913
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데 (남은 답변: 인공지능, 머신러닝, 딥러닝)",LLM Fine-Tuning 의 PEFT,0.0029074852,0.0,0.0029074852354824543
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데 (남은 답변: 인공지능, 머신러닝, 딥러닝)",LoRA,0.012423184,0.0,0.012423183768987656
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데 (남은 답변: 인공지능, 머신러닝, 딥러닝)",LoRA 와 QLoRA 의 차이,-0.0016255094,0.0,0.0016255093505606055
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데 (남은 답변: 인공지능, 머신러닝, 딥러닝)",Loss Function 관련 실무 경험,0.044702623,0.0,0.044702623039484024
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데 (남은 답변: 인공지능, 머신러닝, 딥러닝)",Loss Function 예시,0.0026958976,0.0,0.0026958975940942764
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데 (남은 답변: 인공지능, 머신러닝, 딥러닝)",Loss Function 정의,-0.0018616887,0.0,0.001861688680946827
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데 (남은 답변: 인공지능, 머신러닝, 딥러닝)",MBTI,-0.03297909,0.0,0.0329790897667408
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데 (남은 답변: 인공지능, 머신러닝, 딥러닝)",MSE Loss 설명,-0.0063966196,0.0,0.006396619603037834
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데 (남은 답변: 인공지능, 머신러닝, 딥러닝)",MSE Loss 용도,0.0036720308,0.0,0.0036720307543873787
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데 (남은 답변: 인공지능, 머신러닝, 딥러닝)","Multi-Class, Multi-Label 중 BCE 가 좋은 task",0.008461062,0.0,0.008461061865091324
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데 (남은 답변: 인공지능, 머신러닝, 딥러닝)",Multi-Label 에서 CE + Softmax 적용 문제점,0.005516841,0.0,0.005516841076314449
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데 (남은 답변: 인공지능, 머신러닝, 딥러닝)",PEFT 방법 5가지,0.010956665,0.0,0.010956664569675922
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데 (남은 답변: 인공지능, 머신러닝, 딥러닝)",거대 언어 모델 정의,0.031781193,0.0,0.03178119286894798
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데 (남은 답변: 인공지능, 머신러닝, 딥러닝)",마지막 할 말,-0.033688698,0.0,0.03368869796395302
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데 (남은 답변: 인공지능, 머신러닝, 딥러닝)",면접 시작 인사,-0.027342716,0.0,0.027342716231942177
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데 (남은 답변: 인공지능, 머신러닝, 딥러닝)",면접 종료,-0.0022880943,0.0,0.0022880942560732365
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데 (남은 답변: 인공지능, 머신러닝, 딥러닝)","인공지능, 머신러닝, 딥러닝 차이",0.9456627,1.0,0.05433732271194458
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데 (남은 답변: 인공지능, 머신러닝, 딥러닝)",잠시 휴식,-0.014910331,0.0,0.01491033099591732
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데 (남은 답변: 인공지능, 머신러닝, 딥러닝)",좋아하는 아이돌,-0.019276982,0.0,0.01927698217332363
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데 (남은 답변: 인공지능, 머신러닝, 딥러닝)",확률 예측에서 MSE Loss 미 사용 이유,-0.009769013,0.0,0.009769013151526451
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거! (남은 답변: 머신러닝, 딥러닝)",BCE Loss 설명,0.019193618,0.0,0.01919361762702465
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거! (남은 답변: 머신러닝, 딥러닝)",LLM Fine-Tuning 의 PEFT,0.0024208133,0.0,0.00242081331089139
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거! (남은 답변: 머신러닝, 딥러닝)",LoRA,0.0039109057,0.0,0.003910905681550503
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거! (남은 답변: 머신러닝, 딥러닝)",LoRA 와 QLoRA 의 차이,-0.0046977345,0.0,0.004697734490036964
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거! (남은 답변: 머신러닝, 딥러닝)",Loss Function 관련 실무 경험,0.021978207,0.0,0.021978206932544708
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거! (남은 답변: 머신러닝, 딥러닝)",Loss Function 예시,-0.015268035,0.0,0.015268035233020782
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거! (남은 답변: 머신러닝, 딥러닝)",Loss Function 정의,-0.01678237,0.0,0.01678236946463585
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거! (남은 답변: 머신러닝, 딥러닝)",MBTI,-0.0388605,0.0,0.0388604998588562
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거! (남은 답변: 머신러닝, 딥러닝)",MSE Loss 설명,-0.009398528,0.0,0.00939852837473154
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거! (남은 답변: 머신러닝, 딥러닝)",MSE Loss 용도,-0.00052642945,0.0,0.0005264294450171292
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거! (남은 답변: 머신러닝, 딥러닝)","Multi-Class, Multi-Label 중 BCE 가 좋은 task",0.013132782,0.0,0.013132781721651554
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거! (남은 답변: 머신러닝, 딥러닝)",Multi-Label 에서 CE + Softmax 적용 문제점,-0.011817937,0.0,0.011817936785519123
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거! (남은 답변: 머신러닝, 딥러닝)",PEFT 방법 5가지,0.0021097355,0.0,0.002109735505655408
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거! (남은 답변: 머신러닝, 딥러닝)",거대 언어 모델 정의,0.09824991,0.0,0.09824991226196289
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거! (남은 답변: 머신러닝, 딥러닝)",마지막 할 말,-0.045024812,0.0,0.0450248122215271
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거! (남은 답변: 머신러닝, 딥러닝)",면접 시작 인사,-0.03350846,0.0,0.033508460968732834
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거! (남은 답변: 머신러닝, 딥러닝)",면접 종료,-0.012480187,0.0,0.012480187229812145
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거! (남은 답변: 머신러닝, 딥러닝)","인공지능, 머신러닝, 딥러닝 차이",0.9290792,1.0,0.07092082500457764
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거! (남은 답변: 머신러닝, 딥러닝)",잠시 휴식,-0.036442515,0.0,0.03644251450896263
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거! (남은 답변: 머신러닝, 딥러닝)",좋아하는 아이돌,-0.027512139,0.0,0.027512138709425926
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거! (남은 답변: 머신러닝, 딥러닝)",확률 예측에서 MSE Loss 미 사용 이유,-0.021780068,0.0,0.021780068054795265
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지 (남은 답변: 딥러닝)",BCE Loss 설명,0.0105869975,0.0,0.010586997494101524
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지 (남은 답변: 딥러닝)",LLM Fine-Tuning 의 PEFT,-0.0052402215,0.0,0.005240221507847309
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지 (남은 답변: 딥러닝)",LoRA,0.024573522,0.0,0.024573521688580513
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지 (남은 답변: 딥러닝)",LoRA 와 QLoRA 의 차이,0.002661862,0.0,0.0026618619449436665
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지 (남은 답변: 딥러닝)",Loss Function 관련 실무 경험,0.011005725,0.0,0.011005724780261517
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지 (남은 답변: 딥러닝)",Loss Function 예시,-0.0062738038,0.0,0.0062738037668168545
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지 (남은 답변: 딥러닝)",Loss Function 정의,-0.011345776,0.0,0.011345775797963142
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지 (남은 답변: 딥러닝)",MBTI,-0.05619637,0.0,0.05619636923074722
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지 (남은 답변: 딥러닝)",MSE Loss 설명,-0.042092483,0.0,0.04209248349070549
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지 (남은 답변: 딥러닝)",MSE Loss 용도,-0.03259735,0.0,0.03259735181927681
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지 (남은 답변: 딥러닝)","Multi-Class, Multi-Label 중 BCE 가 좋은 task",-0.011218122,0.0,0.011218122206628323
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지 (남은 답변: 딥러닝)",Multi-Label 에서 CE + Softmax 적용 문제점,-0.015989156,0.0,0.015989156439900398
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지 (남은 답변: 딥러닝)",PEFT 방법 5가지,-0.0017467554,0.0,0.0017467554425820708
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지 (남은 답변: 딥러닝)",거대 언어 모델 정의,0.11353348,0.0,0.11353348195552826
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지 (남은 답변: 딥러닝)",마지막 할 말,-0.049610008,0.0,0.04961000755429268
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지 (남은 답변: 딥러닝)",면접 시작 인사,-0.021525236,0.0,0.02152523584663868
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지 (남은 답변: 딥러닝)",면접 종료,-0.01649162,0.0,0.016491619870066643
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지 (남은 답변: 딥러닝)","인공지능, 머신러닝, 딥러닝 차이",0.88888115,1.0,0.1111188530921936
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지 (남은 답변: 딥러닝)",잠시 휴식,-0.06658086,0.0,0.0665808618068695
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지 (남은 답변: 딥러닝)",좋아하는 아이돌,-0.039674893,0.0,0.03967489302158356
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지 (남은 답변: 딥러닝)",확률 예측에서 MSE Loss 미 사용 이유,0.13689145,0.0,0.1368914544582367
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지? (남은 답변: 인공지능, 머신러닝)",BCE Loss 설명,-0.0074146846,0.0,0.007414684630930424
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지? (남은 답변: 인공지능, 머신러닝)",LLM Fine-Tuning 의 PEFT,0.020811561,0.0,0.02081156149506569
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지? (남은 답변: 인공지능, 머신러닝)",LoRA,0.00030836606,0.0,0.0003083660558331758
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지? (남은 답변: 인공지능, 머신러닝)",LoRA 와 QLoRA 의 차이,0.012445513,0.0,0.012445513159036636
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지? (남은 답변: 인공지능, 머신러닝)",Loss Function 관련 실무 경험,0.019149192,0.0,0.019149191677570343
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지? (남은 답변: 인공지능, 머신러닝)",Loss Function 예시,-0.009060661,0.0,0.009060661308467388
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지? (남은 답변: 인공지능, 머신러닝)",Loss Function 정의,-0.008636461,0.0,0.00863646063953638
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지? (남은 답변: 인공지능, 머신러닝)",MBTI,-0.049771383,0.0,0.04977138340473175
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지? (남은 답변: 인공지능, 머신러닝)",MSE Loss 설명,-0.01493341,0.0,0.014933410100638866
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지? (남은 답변: 인공지능, 머신러닝)",MSE Loss 용도,-0.00045208752,0.0,0.00045208752271719277
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지? (남은 답변: 인공지능, 머신러닝)","Multi-Class, Multi-Label 중 BCE 가 좋은 task",0.0012723935,0.0,0.0012723935069516301
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지? (남은 답변: 인공지능, 머신러닝)",Multi-Label 에서 CE + Softmax 적용 문제점,0.00062686414,0.0,0.0006268641445785761
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지? (남은 답변: 인공지능, 머신러닝)",PEFT 방법 5가지,0.0010350434,0.0,0.0010350433876737952
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지? (남은 답변: 인공지능, 머신러닝)",거대 언어 모델 정의,0.13446374,0.0,0.13446374237537384
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지? (남은 답변: 인공지능, 머신러닝)",마지막 할 말,-0.03888146,0.0,0.03888145834207535
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지? (남은 답변: 인공지능, 머신러닝)",면접 시작 인사,-0.025332464,0.0,0.025332463905215263
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지? (남은 답변: 인공지능, 머신러닝)",면접 종료,-0.029689554,0.0,0.029689554125070572
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지? (남은 답변: 인공지능, 머신러닝)","인공지능, 머신러닝, 딥러닝 차이",0.91112053,1.0,0.08887946605682373
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지? (남은 답변: 인공지능, 머신러닝)",잠시 휴식,-0.071995184,0.0,0.07199518382549286
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지? (남은 답변: 인공지능, 머신러닝)",좋아하는 아이돌,-0.033951063,0.0,0.033951062709093094
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지? (남은 답변: 인공지능, 머신러닝)",확률 예측에서 MSE Loss 미 사용 이유,0.011548308,0.0,0.011548307724297047
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야 (남은 답변: 모든 질문 해결 완료)",BCE Loss 설명,-0.09610228,0.0,0.0961022824048996
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야 (남은 답변: 모든 질문 해결 완료)",LLM Fine-Tuning 의 PEFT,0.024596317,0.0,0.0245963167399168
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야 (남은 답변: 모든 질문 해결 완료)",LoRA,-0.025664786,0.0,0.025664785876870155
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야 (남은 답변: 모든 질문 해결 완료)",LoRA 와 QLoRA 의 차이,-0.03340982,0.0,0.03340981900691986
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야 (남은 답변: 모든 질문 해결 완료)",Loss Function 관련 실무 경험,-0.047679912,0.0,0.04767991229891777
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야 (남은 답변: 모든 질문 해결 완료)",Loss Function 예시,-0.076301605,0.0,0.07630160450935364
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야 (남은 답변: 모든 질문 해결 완료)",Loss Function 정의,-0.04635455,0.0,0.04635455086827278
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야 (남은 답변: 모든 질문 해결 완료)",MBTI,0.07066717,0.0,0.07066716998815536
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야 (남은 답변: 모든 질문 해결 완료)",MSE Loss 설명,-0.0068993615,0.0,0.006899361498653889
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야 (남은 답변: 모든 질문 해결 완료)",MSE Loss 용도,0.016067432,0.0,0.01606743223965168
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야 (남은 답변: 모든 질문 해결 완료)","Multi-Class, Multi-Label 중 BCE 가 좋은 task",0.0029105237,0.0,0.0029105236753821373
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야 (남은 답변: 모든 질문 해결 완료)",Multi-Label 에서 CE + Softmax 적용 문제점,-0.0023968397,0.0,0.0023968396708369255
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야 (남은 답변: 모든 질문 해결 완료)",PEFT 방법 5가지,-0.055902343,0.0,0.05590234324336052
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야 (남은 답변: 모든 질문 해결 완료)",거대 언어 모델 정의,0.6494792,1.0,0.3505207896232605
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야 (남은 답변: 모든 질문 해결 완료)",마지막 할 말,0.05959136,0.0,0.05959136039018631
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야 (남은 답변: 모든 질문 해결 완료)",면접 시작 인사,0.004427334,0.0,0.004427333828061819
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야 (남은 답변: 모든 질문 해결 완료)",면접 종료,-0.028395178,0.0,0.02839517779648304
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야 (남은 답변: 모든 질문 해결 완료)","인공지능, 머신러닝, 딥러닝 차이",0.20195234,0.0,0.20195233821868896
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야 (남은 답변: 모든 질문 해결 완료)",잠시 휴식,-0.043463167,0.0,0.04346316680312157
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야 (남은 답변: 모든 질문 해결 완료)",좋아하는 아이돌,0.052643206,0.0,0.05264320597052574
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야 (남은 답변: 모든 질문 해결 완료)",확률 예측에서 MSE Loss 미 사용 이유,0.03574001,0.0,0.03574001044034958
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야? (남은 답변: 머신러닝)",BCE Loss 설명,0.021170463,0.0,0.021170463413000107
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야? (남은 답변: 머신러닝)",LLM Fine-Tuning 의 PEFT,0.0091923,0.0,0.009192300029098988
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야? (남은 답변: 머신러닝)",LoRA,0.014549424,0.0,0.014549423940479755
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야? (남은 답변: 머신러닝)",LoRA 와 QLoRA 의 차이,-0.0059648105,0.0,0.005964810494333506
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야? (남은 답변: 머신러닝)",Loss Function 관련 실무 경험,0.022322336,0.0,0.022322336211800575
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야? (남은 답변: 머신러닝)",Loss Function 예시,-0.009353641,0.0,0.009353641420602798
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야? (남은 답변: 머신러닝)",Loss Function 정의,-0.012118485,0.0,0.012118484824895859
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야? (남은 답변: 머신러닝)",MBTI,-0.040009554,0.0,0.04000955447554588
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야? (남은 답변: 머신러닝)",MSE Loss 설명,0.006070084,0.0,0.006070083938539028
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야? (남은 답변: 머신러닝)",MSE Loss 용도,0.015133617,0.0,0.01513361744582653
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야? (남은 답변: 머신러닝)","Multi-Class, Multi-Label 중 BCE 가 좋은 task",0.0059010666,0.0,0.0059010665863752365
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야? (남은 답변: 머신러닝)",Multi-Label 에서 CE + Softmax 적용 문제점,-0.016049052,0.0,0.01604905165731907
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야? (남은 답변: 머신러닝)",PEFT 방법 5가지,0.0139121255,0.0,0.013912125490605831
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야? (남은 답변: 머신러닝)",거대 언어 모델 정의,0.06411017,0.0,0.064110167324543
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야? (남은 답변: 머신러닝)",마지막 할 말,-0.037279908,0.0,0.03727990761399269
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야? (남은 답변: 머신러닝)",면접 시작 인사,-0.027192589,0.0,0.02719258889555931
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야? (남은 답변: 머신러닝)",면접 종료,-0.0025709798,0.0,0.0025709797628223896
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야? (남은 답변: 머신러닝)","인공지능, 머신러닝, 딥러닝 차이",0.9471866,1.0,0.05281341075897217
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야? (남은 답변: 머신러닝)",잠시 휴식,-0.024994953,0.0,0.024994952604174614
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야? (남은 답변: 머신러닝)",좋아하는 아이돌,-0.024016747,0.0,0.024016747251152992
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야? (남은 답변: 머신러닝)",확률 예측에서 MSE Loss 미 사용 이유,-0.012984488,0.0,0.012984488159418106
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야 (남은 답변: 모든 질문 해결 완료),BCE Loss 설명,0.012732953,0.0,0.01273295283317566
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야 (남은 답변: 모든 질문 해결 완료),LLM Fine-Tuning 의 PEFT,-0.06737095,0.0,0.0673709511756897
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야 (남은 답변: 모든 질문 해결 완료),LoRA,-0.053859834,0.0,0.053859833627939224
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야 (남은 답변: 모든 질문 해결 완료),LoRA 와 QLoRA 의 차이,0.061391335,0.0,0.06139133498072624
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야 (남은 답변: 모든 질문 해결 완료),Loss Function 관련 실무 경험,0.050197918,0.0,0.050197917968034744
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야 (남은 답변: 모든 질문 해결 완료),Loss Function 예시,0.24057266,0.0,0.24057266116142273
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야 (남은 답변: 모든 질문 해결 완료),Loss Function 정의,0.2792647,1.0,0.7207353115081787
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야 (남은 답변: 모든 질문 해결 완료),MBTI,0.020125056,0.0,0.02012505568563938
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야 (남은 답변: 모든 질문 해결 완료),MSE Loss 설명,-0.03202077,0.0,0.03202077001333237
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야 (남은 답변: 모든 질문 해결 완료),MSE Loss 용도,-0.011012165,0.0,0.011012164875864983
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야 (남은 답변: 모든 질문 해결 완료),"Multi-Class, Multi-Label 중 BCE 가 좋은 task",-0.013523805,0.0,0.01352380495518446
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야 (남은 답변: 모든 질문 해결 완료),Multi-Label 에서 CE + Softmax 적용 문제점,0.009499064,0.0,0.009499063715338707
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야 (남은 답변: 모든 질문 해결 완료),PEFT 방법 5가지,0.025524374,0.0,0.025524374097585678
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야 (남은 답변: 모든 질문 해결 완료),거대 언어 모델 정의,0.27259392,0.0,0.2725939154624939
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야 (남은 답변: 모든 질문 해결 완료),마지막 할 말,0.021584366,0.0,0.021584365516901016
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야 (남은 답변: 모든 질문 해결 완료),면접 시작 인사,0.003725583,0.0,0.003725582966580987
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야 (남은 답변: 모든 질문 해결 완료),면접 종료,-0.07007388,0.0,0.07007388025522232
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야 (남은 답변: 모든 질문 해결 완료),"인공지능, 머신러닝, 딥러닝 차이",-0.09988997,0.0,0.09988997131586075
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야 (남은 답변: 모든 질문 해결 완료),잠시 휴식,-0.054506056,0.0,0.054506056010723114
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야 (남은 답변: 모든 질문 해결 완료),좋아하는 아이돌,0.04273313,0.0,0.04273312911391258
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야 (남은 답변: 모든 질문 해결 완료),확률 예측에서 MSE Loss 미 사용 이유,0.018970454,0.0,0.01897045411169529
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데 (남은 답변: 거대 언어 모델 정의),BCE Loss 설명,-0.025622722,0.0,0.025622721761465073
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데 (남은 답변: 거대 언어 모델 정의),LLM Fine-Tuning 의 PEFT,-0.057217523,0.0,0.05721752345561981
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데 (남은 답변: 거대 언어 모델 정의),LoRA,0.011221586,0.0,0.011221585795283318
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데 (남은 답변: 거대 언어 모델 정의),LoRA 와 QLoRA 의 차이,-0.0003476447,0.0,0.0003476447018329054
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데 (남은 답변: 거대 언어 모델 정의),Loss Function 관련 실무 경험,-0.053368162,0.0,0.053368162363767624
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데 (남은 답변: 거대 언어 모델 정의),Loss Function 예시,0.17442398,0.0,0.1744239777326584
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데 (남은 답변: 거대 언어 모델 정의),Loss Function 정의,0.22555074,0.0,0.22555074095726013
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데 (남은 답변: 거대 언어 모델 정의),MBTI,-0.0019022541,0.0,0.0019022540654987097
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데 (남은 답변: 거대 언어 모델 정의),MSE Loss 설명,-0.040000156,0.0,0.040000155568122864
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데 (남은 답변: 거대 언어 모델 정의),MSE Loss 용도,0.0025705975,0.0,0.00257059745490551
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데 (남은 답변: 거대 언어 모델 정의),"Multi-Class, Multi-Label 중 BCE 가 좋은 task",-0.0201448,0.0,0.02014479972422123
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데 (남은 답변: 거대 언어 모델 정의),Multi-Label 에서 CE + Softmax 적용 문제점,0.017076243,0.0,0.017076242715120316
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데 (남은 답변: 거대 언어 모델 정의),PEFT 방법 5가지,0.122117475,0.0,0.12211747467517853
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데 (남은 답변: 거대 언어 모델 정의),거대 언어 모델 정의,0.44682047,1.0,0.553179532289505
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데 (남은 답변: 거대 언어 모델 정의),마지막 할 말,0.00034127777,0.0,0.0003412777732592076
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데 (남은 답변: 거대 언어 모델 정의),면접 시작 인사,-0.007834579,0.0,0.00783457886427641
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데 (남은 답변: 거대 언어 모델 정의),면접 종료,-0.04165741,0.0,0.04165741056203842
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데 (남은 답변: 거대 언어 모델 정의),"인공지능, 머신러닝, 딥러닝 차이",-0.08900238,0.0,0.08900237828493118
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데 (남은 답변: 거대 언어 모델 정의),잠시 휴식,-0.041673757,0.0,0.04167375713586807
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데 (남은 답변: 거대 언어 모델 정의),좋아하는 아이돌,-0.0022415163,0.0,0.002241516252979636
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데 (남은 답변: 거대 언어 모델 정의),확률 예측에서 MSE Loss 미 사용 이유,-0.0067657307,0.0,0.006765730679035187
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야 (남은 답변: Loss Function 정의),BCE Loss 설명,-0.04208757,0.0,0.04208756983280182
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야 (남은 답변: Loss Function 정의),LLM Fine-Tuning 의 PEFT,-0.00014180753,0.0,0.00014180752623360604
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야 (남은 답변: Loss Function 정의),LoRA,-0.020822756,0.0,0.020822755992412567
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야 (남은 답변: Loss Function 정의),LoRA 와 QLoRA 의 차이,0.06697269,0.0,0.06697268784046173
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야 (남은 답변: Loss Function 정의),Loss Function 관련 실무 경험,0.12487909,0.0,0.12487909197807312
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야 (남은 답변: Loss Function 정의),Loss Function 예시,0.49920306,0.0,0.49920305609703064
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야 (남은 답변: Loss Function 정의),Loss Function 정의,0.4754242,1.0,0.524575799703598
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야 (남은 답변: Loss Function 정의),MBTI,-0.03016346,0.0,0.030163459479808807
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야 (남은 답변: Loss Function 정의),MSE Loss 설명,-0.03655524,0.0,0.03655524179339409
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야 (남은 답변: Loss Function 정의),MSE Loss 용도,0.0024142433,0.0,0.002414243295788765
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야 (남은 답변: Loss Function 정의),"Multi-Class, Multi-Label 중 BCE 가 좋은 task",0.003730445,0.0,0.003730444936081767
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야 (남은 답변: Loss Function 정의),Multi-Label 에서 CE + Softmax 적용 문제점,-0.07091785,0.0,0.07091785222291946
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야 (남은 답변: Loss Function 정의),PEFT 방법 5가지,0.034399506,0.0,0.03439950570464134
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야 (남은 답변: Loss Function 정의),거대 언어 모델 정의,-0.027041279,0.0,0.027041278779506683
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야 (남은 답변: Loss Function 정의),마지막 할 말,-0.021228341,0.0,0.02122834138572216
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야 (남은 답변: Loss Function 정의),면접 시작 인사,-0.008771358,0.0,0.00877135805785656
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야 (남은 답변: Loss Function 정의),면접 종료,-0.10630382,0.0,0.10630381852388382
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야 (남은 답변: Loss Function 정의),"인공지능, 머신러닝, 딥러닝 차이",-0.0016841834,0.0,0.0016841833712533116
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야 (남은 답변: Loss Function 정의),잠시 휴식,-0.06259012,0.0,0.06259012222290039
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야 (남은 답변: Loss Function 정의),좋아하는 아이돌,-0.028339097,0.0,0.028339097276329994
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야 (남은 답변: Loss Function 정의),확률 예측에서 MSE Loss 미 사용 이유,-0.008157201,0.0,0.008157201111316681
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지! (남은 답변: 모든 질문 해결 완료)",BCE Loss 설명,0.10654557,0.0,0.10654556751251221
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지! (남은 답변: 모든 질문 해결 완료)",LLM Fine-Tuning 의 PEFT,-0.022123035,0.0,0.022123035043478012
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지! (남은 답변: 모든 질문 해결 완료)",LoRA,-0.017180882,0.0,0.017180882394313812
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지! (남은 답변: 모든 질문 해결 완료)",LoRA 와 QLoRA 의 차이,0.057309497,0.0,0.05730949714779854
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지! (남은 답변: 모든 질문 해결 완료)",Loss Function 관련 실무 경험,0.15409385,0.0,0.15409384667873383
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지! (남은 답변: 모든 질문 해결 완료)",Loss Function 예시,0.3779272,1.0,0.6220727860927582
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지! (남은 답변: 모든 질문 해결 완료)",Loss Function 정의,0.3663896,0.0,0.36638960242271423
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지! (남은 답변: 모든 질문 해결 완료)",MBTI,0.02866113,0.0,0.028661129996180534
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지! (남은 답변: 모든 질문 해결 완료)",MSE Loss 설명,-0.011465988,0.0,0.011465988121926785
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지! (남은 답변: 모든 질문 해결 완료)",MSE Loss 용도,-0.0047266628,0.0,0.004726662766188383
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지! (남은 답변: 모든 질문 해결 완료)","Multi-Class, Multi-Label 중 BCE 가 좋은 task",0.049174435,0.0,0.049174435436725616
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지! (남은 답변: 모든 질문 해결 완료)",Multi-Label 에서 CE + Softmax 적용 문제점,-0.09456377,0.0,0.09456376731395721
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지! (남은 답변: 모든 질문 해결 완료)",PEFT 방법 5가지,0.046795793,0.0,0.0467957928776741
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지! (남은 답변: 모든 질문 해결 완료)",거대 언어 모델 정의,-0.030952772,0.0,0.03095277212560177
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지! (남은 답변: 모든 질문 해결 완료)",마지막 할 말,0.029959746,0.0,0.029959745705127716
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지! (남은 답변: 모든 질문 해결 완료)",면접 시작 인사,0.010749913,0.0,0.010749912820756435
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지! (남은 답변: 모든 질문 해결 완료)",면접 종료,-0.07969784,0.0,0.07969783991575241
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지! (남은 답변: 모든 질문 해결 완료)","인공지능, 머신러닝, 딥러닝 차이",-0.03884123,0.0,0.03884122893214226
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지! (남은 답변: 모든 질문 해결 완료)",잠시 휴식,-0.02752261,0.0,0.027522610500454903
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지! (남은 답변: 모든 질문 해결 완료)",좋아하는 아이돌,0.06317614,0.0,0.06317614018917084
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지! (남은 답변: 모든 질문 해결 완료)",확률 예측에서 MSE Loss 미 사용 이유,-0.0042518806,0.0,0.004251880571246147
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데 (남은 답변: Loss Function 예시),BCE Loss 설명,-0.0445277,0.0,0.044527698308229446
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데 (남은 답변: Loss Function 예시),LLM Fine-Tuning 의 PEFT,-0.012240818,0.0,0.0122408177703619
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데 (남은 답변: Loss Function 예시),LoRA,0.024322335,0.0,0.024322334676980972
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데 (남은 답변: Loss Function 예시),LoRA 와 QLoRA 의 차이,0.05090402,0.0,0.05090402066707611
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데 (남은 답변: Loss Function 예시),Loss Function 관련 실무 경험,0.20171523,0.0,0.20171523094177246
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데 (남은 답변: Loss Function 예시),Loss Function 예시,0.51635325,1.0,0.4836467504501343
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데 (남은 답변: Loss Function 예시),Loss Function 정의,0.4447655,0.0,0.44476550817489624
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데 (남은 답변: Loss Function 예시),MBTI,-0.0036186655,0.0,0.0036186655052006245
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데 (남은 답변: Loss Function 예시),MSE Loss 설명,0.015561421,0.0,0.015561421401798725
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데 (남은 답변: Loss Function 예시),MSE Loss 용도,0.03324671,0.0,0.033246710896492004
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데 (남은 답변: Loss Function 예시),"Multi-Class, Multi-Label 중 BCE 가 좋은 task",0.020515697,0.0,0.020515697076916695
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데 (남은 답변: Loss Function 예시),Multi-Label 에서 CE + Softmax 적용 문제점,0.0288121,0.0,0.028812099248170853
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데 (남은 답변: Loss Function 예시),PEFT 방법 5가지,0.008638838,0.0,0.008638838306069374
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데 (남은 답변: Loss Function 예시),거대 언어 모델 정의,-0.09085556,0.0,0.09085556119680405
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데 (남은 답변: Loss Function 예시),마지막 할 말,-0.0140711125,0.0,0.014071112498641014
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데 (남은 답변: Loss Function 예시),면접 시작 인사,0.010212313,0.0,0.010212313383817673
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데 (남은 답변: Loss Function 예시),면접 종료,-0.04490324,0.0,0.044903241097927094
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데 (남은 답변: Loss Function 예시),"인공지능, 머신러닝, 딥러닝 차이",0.017942911,0.0,0.01794291101396084
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데 (남은 답변: Loss Function 예시),잠시 휴식,-0.0036946002,0.0,0.0036946001928299665
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데 (남은 답변: Loss Function 예시),좋아하는 아이돌,-0.021592712,0.0,0.02159271202981472
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데 (남은 답변: Loss Function 예시),확률 예측에서 MSE Loss 미 사용 이유,-0.020548455,0.0,0.02054845541715622
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지 (남은 답변: 모든 질문 해결 완료)",BCE Loss 설명,0.18031988,0.0,0.1803198754787445
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지 (남은 답변: 모든 질문 해결 완료)",LLM Fine-Tuning 의 PEFT,-0.033566758,0.0,0.033566758036613464
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지 (남은 답변: 모든 질문 해결 완료)",LoRA,0.08281384,0.0,0.08281383663415909
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지 (남은 답변: 모든 질문 해결 완료)",LoRA 와 QLoRA 의 차이,-0.0063199867,0.0,0.006319986656308174
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지 (남은 답변: 모든 질문 해결 완료)",Loss Function 관련 실무 경험,0.0025397534,0.0,0.0025397534482181072
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지 (남은 답변: 모든 질문 해결 완료)",Loss Function 예시,0.008792135,0.0,0.008792134933173656
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지 (남은 답변: 모든 질문 해결 완료)",Loss Function 정의,-0.021851234,0.0,0.021851234138011932
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지 (남은 답변: 모든 질문 해결 완료)",MBTI,0.059248548,0.0,0.05924854800105095
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지 (남은 답변: 모든 질문 해결 완료)",MSE Loss 설명,0.52065474,1.0,0.47934526205062866
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지 (남은 답변: 모든 질문 해결 완료)",MSE Loss 용도,0.4239582,0.0,0.4239582121372223
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지 (남은 답변: 모든 질문 해결 완료)","Multi-Class, Multi-Label 중 BCE 가 좋은 task",0.027282497,0.0,0.02728249691426754
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지 (남은 답변: 모든 질문 해결 완료)",Multi-Label 에서 CE + Softmax 적용 문제점,-0.011432765,0.0,0.011432765051722527
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지 (남은 답변: 모든 질문 해결 완료)",PEFT 방법 5가지,-0.09483433,0.0,0.0948343276977539
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지 (남은 답변: 모든 질문 해결 완료)",거대 언어 모델 정의,-0.08857814,0.0,0.08857814222574234
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지 (남은 답변: 모든 질문 해결 완료)",마지막 할 말,0.008398006,0.0,0.008398005738854408
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지 (남은 답변: 모든 질문 해결 완료)",면접 시작 인사,-0.040009152,0.0,0.04000915214419365
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지 (남은 답변: 모든 질문 해결 완료)",면접 종료,0.0029737651,0.0,0.0029737651348114014
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지 (남은 답변: 모든 질문 해결 완료)","인공지능, 머신러닝, 딥러닝 차이",-0.018634863,0.0,0.018634863197803497
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지 (남은 답변: 모든 질문 해결 완료)",잠시 휴식,0.07549187,0.0,0.07549186795949936
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지 (남은 답변: 모든 질문 해결 완료)",좋아하는 아이돌,-0.0022728532,0.0,0.002272853162139654
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지 (남은 답변: 모든 질문 해결 완료)",확률 예측에서 MSE Loss 미 사용 이유,-0.014660612,0.0,0.014660611748695374
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야 (남은 답변: 모든 질문 해결 완료),BCE Loss 설명,0.10937438,0.0,0.10937438160181046
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야 (남은 답변: 모든 질문 해결 완료),LLM Fine-Tuning 의 PEFT,-0.00063809415,0.0,0.0006380941485986114
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야 (남은 답변: 모든 질문 해결 완료),LoRA,-0.090838805,0.0,0.09083880484104156
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야 (남은 답변: 모든 질문 해결 완료),LoRA 와 QLoRA 의 차이,0.027227703,0.0,0.027227703481912613
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야 (남은 답변: 모든 질문 해결 완료),Loss Function 관련 실무 경험,-0.033699933,0.0,0.03369993343949318
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야 (남은 답변: 모든 질문 해결 완료),Loss Function 예시,-0.019338015,0.0,0.019338015466928482
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야 (남은 답변: 모든 질문 해결 완료),Loss Function 정의,-0.035105642,0.0,0.035105641931295395
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야 (남은 답변: 모든 질문 해결 완료),MBTI,-0.027954752,0.0,0.02795475162565708
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야 (남은 답변: 모든 질문 해결 완료),MSE Loss 설명,0.5582184,0.0,0.5582184195518494
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야 (남은 답변: 모든 질문 해결 완료),MSE Loss 용도,0.55522186,1.0,0.4447781443595886
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야 (남은 답변: 모든 질문 해결 완료),"Multi-Class, Multi-Label 중 BCE 가 좋은 task",0.0045664287,0.0,0.00456642871722579
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야 (남은 답변: 모든 질문 해결 완료),Multi-Label 에서 CE + Softmax 적용 문제점,0.036976326,0.0,0.03697632625699043
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야 (남은 답변: 모든 질문 해결 완료),PEFT 방법 5가지,-0.02619318,0.0,0.026193179190158844
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야 (남은 답변: 모든 질문 해결 완료),거대 언어 모델 정의,-0.04141494,0.0,0.04141493886709213
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야 (남은 답변: 모든 질문 해결 완료),마지막 할 말,0.016650802,0.0,0.01665080152451992
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야 (남은 답변: 모든 질문 해결 완료),면접 시작 인사,-0.013435342,0.0,0.013435342349112034
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야 (남은 답변: 모든 질문 해결 완료),면접 종료,-0.027852373,0.0,0.02785237319767475
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야 (남은 답변: 모든 질문 해결 완료),"인공지능, 머신러닝, 딥러닝 차이",0.010990642,0.0,0.010990642011165619
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야 (남은 답변: 모든 질문 해결 완료),잠시 휴식,-0.054198653,0.0,0.054198652505874634
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야 (남은 답변: 모든 질문 해결 완료),좋아하는 아이돌,-0.032478034,0.0,0.03247803449630737
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야 (남은 답변: 모든 질문 해결 완료),확률 예측에서 MSE Loss 미 사용 이유,-0.016409619,0.0,0.0164096187800169
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지? (남은 답변: MSE Loss 설명),BCE Loss 설명,0.1541192,0.0,0.15411919355392456
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지? (남은 답변: MSE Loss 설명),LLM Fine-Tuning 의 PEFT,0.0011605978,0.0,0.0011605977779254317
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지? (남은 답변: MSE Loss 설명),LoRA,-0.059659734,0.0,0.05965973436832428
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지? (남은 답변: MSE Loss 설명),LoRA 와 QLoRA 의 차이,0.007686209,0.0,0.007686208933591843
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지? (남은 답변: MSE Loss 설명),Loss Function 관련 실무 경험,-0.050616827,0.0,0.050616826862096786
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지? (남은 답변: MSE Loss 설명),Loss Function 예시,-0.036237586,0.0,0.03623758628964424
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지? (남은 답변: MSE Loss 설명),Loss Function 정의,-0.05571676,0.0,0.05571676045656204
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지? (남은 답변: MSE Loss 설명),MBTI,-0.06397503,0.0,0.063975028693676
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지? (남은 답변: MSE Loss 설명),MSE Loss 설명,0.4908902,1.0,0.5091097950935364
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지? (남은 답변: MSE Loss 설명),MSE Loss 용도,0.4775705,0.0,0.477570503950119
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지? (남은 답변: MSE Loss 설명),"Multi-Class, Multi-Label 중 BCE 가 좋은 task",0.026182635,0.0,0.026182634755969048
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지? (남은 답변: MSE Loss 설명),Multi-Label 에서 CE + Softmax 적용 문제점,0.017216055,0.0,0.01721605472266674
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지? (남은 답변: MSE Loss 설명),PEFT 방법 5가지,-0.032853585,0.0,0.03285358473658562
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지? (남은 답변: MSE Loss 설명),거대 언어 모델 정의,0.005664218,0.0,0.005664218217134476
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지? (남은 답변: MSE Loss 설명),마지막 할 말,0.0038328643,0.0,0.0038328643422573805
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지? (남은 답변: MSE Loss 설명),면접 시작 인사,0.0019395137,0.0,0.0019395137205719948
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지? (남은 답변: MSE Loss 설명),면접 종료,0.0031948895,0.0,0.0031948895193636417
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지? (남은 답변: MSE Loss 설명),"인공지능, 머신러닝, 딥러닝 차이",-0.026058184,0.0,0.02605818398296833
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지? (남은 답변: MSE Loss 설명),잠시 휴식,-0.033169556,0.0,0.03316955640912056
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지? (남은 답변: MSE Loss 설명),좋아하는 아이돌,-0.04775762,0.0,0.047757621854543686
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지? (남은 답변: MSE Loss 설명),확률 예측에서 MSE Loss 미 사용 이유,-0.0150175765,0.0,0.015017576515674591
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지 (남은 답변: 모든 질문 해결 완료)",BCE Loss 설명,0.0058434233,0.0,0.005843423306941986
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지 (남은 답변: 모든 질문 해결 완료)",LLM Fine-Tuning 의 PEFT,0.022929372,0.0,0.022929372265934944
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지 (남은 답변: 모든 질문 해결 완료)",LoRA,-0.00797068,0.0,0.007970679551362991
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지 (남은 답변: 모든 질문 해결 완료)",LoRA 와 QLoRA 의 차이,0.010266115,0.0,0.010266114957630634
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지 (남은 답변: 모든 질문 해결 완료)",Loss Function 관련 실무 경험,-0.024813231,0.0,0.024813231080770493
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지 (남은 답변: 모든 질문 해결 완료)",Loss Function 예시,-0.02530689,0.0,0.025306889787316322
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지 (남은 답변: 모든 질문 해결 완료)",Loss Function 정의,-0.05814109,0.0,0.05814108997583389
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지 (남은 답변: 모든 질문 해결 완료)",MBTI,-0.017768657,0.0,0.017768656834959984
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지 (남은 답변: 모든 질문 해결 완료)",MSE Loss 설명,-0.03505018,0.0,0.035050179809331894
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지 (남은 답변: 모든 질문 해결 완료)",MSE Loss 용도,-0.00970104,0.0,0.009701039642095566
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지 (남은 답변: 모든 질문 해결 완료)","Multi-Class, Multi-Label 중 BCE 가 좋은 task",-0.021029439,0.0,0.021029438823461533
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지 (남은 답변: 모든 질문 해결 완료)",Multi-Label 에서 CE + Softmax 적용 문제점,0.011889274,0.0,0.01188927423208952
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지 (남은 답변: 모든 질문 해결 완료)",PEFT 방법 5가지,-0.005649044,0.0,0.005649044178426266
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지 (남은 답변: 모든 질문 해결 완료)",거대 언어 모델 정의,0.07126344,0.0,0.07126343995332718
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지 (남은 답변: 모든 질문 해결 완료)",마지막 할 말,-0.009679258,0.0,0.009679257869720459
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지 (남은 답변: 모든 질문 해결 완료)",면접 시작 인사,-0.0069888225,0.0,0.006988822482526302
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지 (남은 답변: 모든 질문 해결 완료)",면접 종료,-0.05596162,0.0,0.055961620062589645
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지 (남은 답변: 모든 질문 해결 완료)","인공지능, 머신러닝, 딥러닝 차이",-0.022295484,0.0,0.022295484319329262
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지 (남은 답변: 모든 질문 해결 완료)",잠시 휴식,-0.10681522,0.0,0.10681521892547607
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지 (남은 답변: 모든 질문 해결 완료)",좋아하는 아이돌,-0.020041224,0.0,0.020041223615407944
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지 (남은 답변: 모든 질문 해결 완료)",확률 예측에서 MSE Loss 미 사용 이유,0.8066174,1.0,0.1933826208114624
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지 (남은 답변: MSE Loss 용도),BCE Loss 설명,-0.052405562,0.0,0.05240556225180626
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지 (남은 답변: MSE Loss 용도),LLM Fine-Tuning 의 PEFT,0.015137564,0.0,0.01513756439089775
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지 (남은 답변: MSE Loss 용도),LoRA,-0.025353502,0.0,0.025353502482175827
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지 (남은 답변: MSE Loss 용도),LoRA 와 QLoRA 의 차이,0.019242624,0.0,0.019242623820900917
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지 (남은 답변: MSE Loss 용도),Loss Function 관련 실무 경험,-0.080590256,0.0,0.08059025555849075
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지 (남은 답변: MSE Loss 용도),Loss Function 예시,0.010934251,0.0,0.010934251360595226
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지 (남은 답변: MSE Loss 용도),Loss Function 정의,-0.00016806378,0.0,0.000168063779710792
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지 (남은 답변: MSE Loss 용도),MBTI,-0.0038649787,0.0,0.0038649786729365587
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지 (남은 답변: MSE Loss 용도),MSE Loss 설명,0.5775066,0.0,0.5775066018104553
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지 (남은 답변: MSE Loss 용도),MSE Loss 용도,0.674403,1.0,0.32559698820114136
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지 (남은 답변: MSE Loss 용도),"Multi-Class, Multi-Label 중 BCE 가 좋은 task",-0.01648074,0.0,0.016480740159749985
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지 (남은 답변: MSE Loss 용도),Multi-Label 에서 CE + Softmax 적용 문제점,-0.03855318,0.0,0.03855317831039429
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지 (남은 답변: MSE Loss 용도),PEFT 방법 5가지,0.021293841,0.0,0.021293841302394867
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지 (남은 답변: MSE Loss 용도),거대 언어 모델 정의,0.03504984,0.0,0.035049840807914734
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지 (남은 답변: MSE Loss 용도),마지막 할 말,0.021505335,0.0,0.021505335345864296
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지 (남은 답변: MSE Loss 용도),면접 시작 인사,0.011635835,0.0,0.011635835282504559
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지 (남은 답변: MSE Loss 용도),면접 종료,0.015207637,0.0,0.01520763710141182
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지 (남은 답변: MSE Loss 용도),"인공지능, 머신러닝, 딥러닝 차이",-0.0035335154,0.0,0.0035335153806954622
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지 (남은 답변: MSE Loss 용도),잠시 휴식,-0.039410394,0.0,0.03941039368510246
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지 (남은 답변: MSE Loss 용도),좋아하는 아이돌,-0.043651026,0.0,0.043651025742292404
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지 (남은 답변: MSE Loss 용도),확률 예측에서 MSE Loss 미 사용 이유,-0.027306039,0.0,0.02730603888630867
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야? (남은 답변: 확률 예측에서 MSE Loss 미 사용 이유),BCE Loss 설명,0.0999337,0.0,0.0999336987733841
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야? (남은 답변: 확률 예측에서 MSE Loss 미 사용 이유),LLM Fine-Tuning 의 PEFT,0.021608511,0.0,0.021608510985970497
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야? (남은 답변: 확률 예측에서 MSE Loss 미 사용 이유),LoRA,0.013437179,0.0,0.013437178917229176
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야? (남은 답변: 확률 예측에서 MSE Loss 미 사용 이유),LoRA 와 QLoRA 의 차이,-0.03851971,0.0,0.038519710302352905
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야? (남은 답변: 확률 예측에서 MSE Loss 미 사용 이유),Loss Function 관련 실무 경험,0.037629716,0.0,0.03762971609830856
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야? (남은 답변: 확률 예측에서 MSE Loss 미 사용 이유),Loss Function 예시,-0.011632732,0.0,0.01163273211568594
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야? (남은 답변: 확률 예측에서 MSE Loss 미 사용 이유),Loss Function 정의,-0.0502446,0.0,0.05024459958076477
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야? (남은 답변: 확률 예측에서 MSE Loss 미 사용 이유),MBTI,-0.016408328,0.0,0.016408327966928482
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야? (남은 답변: 확률 예측에서 MSE Loss 미 사용 이유),MSE Loss 설명,-0.07884659,0.0,0.07884658873081207
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야? (남은 답변: 확률 예측에서 MSE Loss 미 사용 이유),MSE Loss 용도,-0.07070733,0.0,0.07070732861757278
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야? (남은 답변: 확률 예측에서 MSE Loss 미 사용 이유),"Multi-Class, Multi-Label 중 BCE 가 좋은 task",-0.0035932187,0.0,0.003593218745663762
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야? (남은 답변: 확률 예측에서 MSE Loss 미 사용 이유),Multi-Label 에서 CE + Softmax 적용 문제점,0.041125067,0.0,0.041125066578388214
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야? (남은 답변: 확률 예측에서 MSE Loss 미 사용 이유),PEFT 방법 5가지,0.0036404477,0.0,0.0036404477432370186
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야? (남은 답변: 확률 예측에서 MSE Loss 미 사용 이유),거대 언어 모델 정의,-0.012653593,0.0,0.012653592973947525
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야? (남은 답변: 확률 예측에서 MSE Loss 미 사용 이유),마지막 할 말,-0.01015108,0.0,0.01015107985585928
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야? (남은 답변: 확률 예측에서 MSE Loss 미 사용 이유),면접 시작 인사,0.02798641,0.0,0.02798640914261341
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야? (남은 답변: 확률 예측에서 MSE Loss 미 사용 이유),면접 종료,0.018920327,0.0,0.018920326605439186
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야? (남은 답변: 확률 예측에서 MSE Loss 미 사용 이유),"인공지능, 머신러닝, 딥러닝 차이",-0.014042493,0.0,0.01404249295592308
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야? (남은 답변: 확률 예측에서 MSE Loss 미 사용 이유),잠시 휴식,-0.053704873,0.0,0.053704872727394104
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야? (남은 답변: 확률 예측에서 MSE Loss 미 사용 이유),좋아하는 아이돌,0.0069461484,0.0,0.0069461483508348465
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야? (남은 답변: 확률 예측에서 MSE Loss 미 사용 이유),확률 예측에서 MSE Loss 미 사용 이유,0.92276216,1.0,0.07723784446716309
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고? (남은 답변: 모든 질문 해결 완료),BCE Loss 설명,0.4586209,1.0,0.5413790941238403
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고? (남은 답변: 모든 질문 해결 완료),LLM Fine-Tuning 의 PEFT,-0.0383188,0.0,0.038318801671266556
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고? (남은 답변: 모든 질문 해결 완료),LoRA,0.0017230443,0.0,0.0017230443190783262
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고? (남은 답변: 모든 질문 해결 완료),LoRA 와 QLoRA 의 차이,-0.021444023,0.0,0.02144402265548706
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고? (남은 답변: 모든 질문 해결 완료),Loss Function 관련 실무 경험,0.06671355,0.0,0.06671354919672012
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고? (남은 답변: 모든 질문 해결 완료),Loss Function 예시,-0.022054601,0.0,0.022054601460695267
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고? (남은 답변: 모든 질문 해결 완료),Loss Function 정의,-0.05458345,0.0,0.05458344891667366
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고? (남은 답변: 모든 질문 해결 완료),MBTI,-0.022993146,0.0,0.022993145510554314
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고? (남은 답변: 모든 질문 해결 완료),MSE Loss 설명,-0.04492663,0.0,0.04492662847042084
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고? (남은 답변: 모든 질문 해결 완료),MSE Loss 용도,-0.08575484,0.0,0.08575484156608582
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고? (남은 답변: 모든 질문 해결 완료),"Multi-Class, Multi-Label 중 BCE 가 좋은 task",0.06663742,0.0,0.06663741916418076
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고? (남은 답변: 모든 질문 해결 완료),Multi-Label 에서 CE + Softmax 적용 문제점,0.089405514,0.0,0.08940551429986954
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고? (남은 답변: 모든 질문 해결 완료),PEFT 방법 5가지,-0.010118507,0.0,0.010118506848812103
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고? (남은 답변: 모든 질문 해결 완료),거대 언어 모델 정의,-0.0065891035,0.0,0.006589103490114212
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고? (남은 답변: 모든 질문 해결 완료),마지막 할 말,0.04681159,0.0,0.04681159183382988
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고? (남은 답변: 모든 질문 해결 완료),면접 시작 인사,-0.008503811,0.0,0.008503811433911324
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고? (남은 답변: 모든 질문 해결 완료),면접 종료,-0.0020849074,0.0,0.002084907377138734
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고? (남은 답변: 모든 질문 해결 완료),"인공지능, 머신러닝, 딥러닝 차이",-0.034225024,0.0,0.03422502428293228
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고? (남은 답변: 모든 질문 해결 완료),잠시 휴식,0.030388096,0.0,0.03038809634745121
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고? (남은 답변: 모든 질문 해결 완료),좋아하는 아이돌,0.015959594,0.0,0.015959594398736954
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고? (남은 답변: 모든 질문 해결 완료),확률 예측에서 MSE Loss 미 사용 이유,0.38078842,0.0,0.3807884156703949
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고 (남은 답변: 핵심 아이디어),BCE Loss 설명,0.83019716,1.0,0.16980284452438354
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고 (남은 답변: 핵심 아이디어),LLM Fine-Tuning 의 PEFT,0.011821765,0.0,0.011821765452623367
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고 (남은 답변: 핵심 아이디어),LoRA,-0.0012423714,0.0,0.0012423713924363256
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고 (남은 답변: 핵심 아이디어),LoRA 와 QLoRA 의 차이,-0.04717672,0.0,0.04717671871185303
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고 (남은 답변: 핵심 아이디어),Loss Function 관련 실무 경험,0.004559662,0.0,0.004559662193059921
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고 (남은 답변: 핵심 아이디어),Loss Function 예시,-0.009854382,0.0,0.009854381904006004
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고 (남은 답변: 핵심 아이디어),Loss Function 정의,-0.026849154,0.0,0.026849154382944107
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고 (남은 답변: 핵심 아이디어),MBTI,-0.0480919,0.0,0.04809189960360527
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고 (남은 답변: 핵심 아이디어),MSE Loss 설명,-0.049482554,0.0,0.0494825541973114
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고 (남은 답변: 핵심 아이디어),MSE Loss 용도,-0.11301908,0.0,0.11301907896995544
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고 (남은 답변: 핵심 아이디어),"Multi-Class, Multi-Label 중 BCE 가 좋은 task",0.15370671,0.0,0.15370671451091766
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고 (남은 답변: 핵심 아이디어),Multi-Label 에서 CE + Softmax 적용 문제점,0.021862106,0.0,0.021862106397747993
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고 (남은 답변: 핵심 아이디어),PEFT 방법 5가지,0.019683693,0.0,0.01968369260430336
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고 (남은 답변: 핵심 아이디어),거대 언어 모델 정의,0.024795603,0.0,0.02479560300707817
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고 (남은 답변: 핵심 아이디어),마지막 할 말,0.01695328,0.0,0.016953280195593834
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고 (남은 답변: 핵심 아이디어),면접 시작 인사,0.02763236,0.0,0.02763235941529274
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고 (남은 답변: 핵심 아이디어),면접 종료,0.023414757,0.0,0.02341475710272789
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고 (남은 답변: 핵심 아이디어),"인공지능, 머신러닝, 딥러닝 차이",0.025181828,0.0,0.025181828066706657
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고 (남은 답변: 핵심 아이디어),잠시 휴식,0.04535159,0.0,0.04535159096121788
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고 (남은 답변: 핵심 아이디어),좋아하는 아이돌,-0.015996763,0.0,0.015996763482689857
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고 (남은 답변: 핵심 아이디어),확률 예측에서 MSE Loss 미 사용 이유,-0.059651997,0.0,0.059651996940374374
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거 (남은 답변: 모든 질문 해결 완료)",BCE Loss 설명,0.47472188,0.0,0.47472187876701355
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거 (남은 답변: 모든 질문 해결 완료)",LLM Fine-Tuning 의 PEFT,-0.016712552,0.0,0.016712551936507225
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거 (남은 답변: 모든 질문 해결 완료)",LoRA,0.033033106,0.0,0.0330331064760685
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거 (남은 답변: 모든 질문 해결 완료)",LoRA 와 QLoRA 의 차이,-0.0065139146,0.0,0.006513914559036493
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거 (남은 답변: 모든 질문 해결 완료)",Loss Function 관련 실무 경험,0.04105406,0.0,0.04105405882000923
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거 (남은 답변: 모든 질문 해결 완료)",Loss Function 예시,-0.012719495,0.0,0.012719495221972466
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거 (남은 답변: 모든 질문 해결 완료)",Loss Function 정의,-0.01757763,0.0,0.017577629536390305
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거 (남은 답변: 모든 질문 해결 완료)",MBTI,-0.03815994,0.0,0.038159940391778946
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거 (남은 답변: 모든 질문 해결 완료)",MSE Loss 설명,-0.06933877,0.0,0.06933876872062683
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거 (남은 답변: 모든 질문 해결 완료)",MSE Loss 용도,-0.108264744,0.0,0.1082647442817688
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거 (남은 답변: 모든 질문 해결 완료)","Multi-Class, Multi-Label 중 BCE 가 좋은 task",0.54899263,1.0,0.4510073661804199
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거 (남은 답변: 모든 질문 해결 완료)",Multi-Label 에서 CE + Softmax 적용 문제점,0.08397212,0.0,0.08397211879491806
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거 (남은 답변: 모든 질문 해결 완료)",PEFT 방법 5가지,0.018317435,0.0,0.018317434936761856
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거 (남은 답변: 모든 질문 해결 완료)",거대 언어 모델 정의,-0.0248389,0.0,0.024838900193572044
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거 (남은 답변: 모든 질문 해결 완료)",마지막 할 말,0.008556057,0.0,0.008556056767702103
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거 (남은 답변: 모든 질문 해결 완료)",면접 시작 인사,-0.0036072447,0.0,0.003607244696468115
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거 (남은 답변: 모든 질문 해결 완료)",면접 종료,-0.019019062,0.0,0.01901906169950962
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거 (남은 답변: 모든 질문 해결 완료)","인공지능, 머신러닝, 딥러닝 차이",-0.023493659,0.0,0.023493658751249313
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거 (남은 답변: 모든 질문 해결 완료)",잠시 휴식,0.006592669,0.0,0.006592669058591127
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거 (남은 답변: 모든 질문 해결 완료)",좋아하는 아이돌,0.00070832204,0.0,0.0007083220407366753
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거 (남은 답변: 모든 질문 해결 완료)",확률 예측에서 MSE Loss 미 사용 이유,-0.023475517,0.0,0.023475516587495804
"BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야? (남은 답변: 핵심 아이디어, 수식)",BCE Loss 설명,0.7392236,1.0,0.2607764005661011
"BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야? (남은 답변: 핵심 아이디어, 수식)",LLM Fine-Tuning 의 PEFT,-0.020924335,0.0,0.020924335345625877
"BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야? (남은 답변: 핵심 아이디어, 수식)",LoRA,-0.07553571,0.0,0.07553570717573166
"BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야? (남은 답변: 핵심 아이디어, 수식)",LoRA 와 QLoRA 의 차이,-0.00052411284,0.0,0.0005241128383204341
"BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야? (남은 답변: 핵심 아이디어, 수식)",Loss Function 관련 실무 경험,0.07667187,0.0,0.07667186856269836
"BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야? (남은 답변: 핵심 아이디어, 수식)",Loss Function 예시,0.015727239,0.0,0.015727238729596138
"BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야? (남은 답변: 핵심 아이디어, 수식)",Loss Function 정의,0.0019833539,0.0,0.001983353868126869
"BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야? (남은 답변: 핵심 아이디어, 수식)",MBTI,-0.01764234,0.0,0.017642339691519737
"BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야? (남은 답변: 핵심 아이디어, 수식)",MSE Loss 설명,-0.036418073,0.0,0.03641807287931442
"BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야? (남은 답변: 핵심 아이디어, 수식)",MSE Loss 용도,-0.08515974,0.0,0.08515974134206772
"BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야? (남은 답변: 핵심 아이디어, 수식)","Multi-Class, Multi-Label 중 BCE 가 좋은 task",0.07571576,0.0,0.07571575790643692
"BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야? (남은 답변: 핵심 아이디어, 수식)",Multi-Label 에서 CE + Softmax 적용 문제점,0.0789772,0.0,0.07897719740867615
"BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야? (남은 답변: 핵심 아이디어, 수식)",PEFT 방법 5가지,0.025674384,0.0,0.025674384087324142
"BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야? (남은 답변: 핵심 아이디어, 수식)",거대 언어 모델 정의,-0.045246292,0.0,0.045246291905641556
"BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야? (남은 답변: 핵심 아이디어, 수식)",마지막 할 말,0.041792803,0.0,0.04179280251264572
"BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야? (남은 답변: 핵심 아이디어, 수식)",면접 시작 인사,0.014375325,0.0,0.014375325292348862
"BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야? (남은 답변: 핵심 아이디어, 수식)",면접 종료,-0.019913275,0.0,0.01991327479481697
"BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야? (남은 답변: 핵심 아이디어, 수식)","인공지능, 머신러닝, 딥러닝 차이",-0.004429364,0.0,0.004429364111274481
"BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야? (남은 답변: 핵심 아이디어, 수식)",잠시 휴식,0.0114861205,0.0,0.011486120522022247
"BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야? (남은 답변: 핵심 아이디어, 수식)",좋아하는 아이돌,0.018802317,0.0,0.01880231685936451
"BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야? (남은 답변: 핵심 아이디어, 수식)",확률 예측에서 MSE Loss 미 사용 이유,-0.057331238,0.0,0.05733123794198036
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고 (남은 답변: 수식)",BCE Loss 설명,0.7779169,1.0,0.22208309173583984
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고 (남은 답변: 수식)",LLM Fine-Tuning 의 PEFT,0.01678211,0.0,0.016782110556960106
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고 (남은 답변: 수식)",LoRA,-0.03043361,0.0,0.03043361008167267
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고 (남은 답변: 수식)",LoRA 와 QLoRA 의 차이,0.014289676,0.0,0.01428967621177435
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고 (남은 답변: 수식)",Loss Function 관련 실무 경험,0.052550565,0.0,0.05255056545138359
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고 (남은 답변: 수식)",Loss Function 예시,-0.005183021,0.0,0.005183021072298288
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고 (남은 답변: 수식)",Loss Function 정의,-0.023355741,0.0,0.023355741053819656
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고 (남은 답변: 수식)",MBTI,-0.055420138,0.0,0.05542013794183731
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고 (남은 답변: 수식)",MSE Loss 설명,-0.07026883,0.0,0.07026883214712143
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고 (남은 답변: 수식)",MSE Loss 용도,-0.11922828,0.0,0.11922828108072281
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고 (남은 답변: 수식)","Multi-Class, Multi-Label 중 BCE 가 좋은 task",0.114107564,0.0,0.11410756409168243
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고 (남은 답변: 수식)",Multi-Label 에서 CE + Softmax 적용 문제점,0.04653516,0.0,0.04653516039252281
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고 (남은 답변: 수식)",PEFT 방법 5가지,-0.009841298,0.0,0.009841297753155231
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고 (남은 답변: 수식)",거대 언어 모델 정의,-0.012264247,0.0,0.012264247052371502
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고 (남은 답변: 수식)",마지막 할 말,0.016091228,0.0,0.016091227531433105
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고 (남은 답변: 수식)",면접 시작 인사,0.017311858,0.0,0.017311858013272285
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고 (남은 답변: 수식)",면접 종료,-0.0027799427,0.0,0.0027799427043646574
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고 (남은 답변: 수식)","인공지능, 머신러닝, 딥러닝 차이",0.036328465,0.0,0.03632846474647522
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고 (남은 답변: 수식)",잠시 휴식,-0.0058354368,0.0,0.005835436750203371
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고 (남은 답변: 수식)",좋아하는 아이돌,-0.011675734,0.0,0.01167573407292366
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고 (남은 답변: 수식)",확률 예측에서 MSE Loss 미 사용 이유,-0.02033715,0.0,0.020337149500846863
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!! (남은 답변: BCE 가 좋은 task, 이유)",BCE Loss 설명,0.051260173,0.0,0.051260173320770264
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!! (남은 답변: BCE 가 좋은 task, 이유)",LLM Fine-Tuning 의 PEFT,-0.004654948,0.0,0.004654948133975267
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!! (남은 답변: BCE 가 좋은 task, 이유)",LoRA,-0.00471252,0.0,0.00471252016723156
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!! (남은 답변: BCE 가 좋은 task, 이유)",LoRA 와 QLoRA 의 차이,-0.006866272,0.0,0.006866272073239088
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!! (남은 답변: BCE 가 좋은 task, 이유)",Loss Function 관련 실무 경험,-0.016643818,0.0,0.016643818467855453
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!! (남은 답변: BCE 가 좋은 task, 이유)",Loss Function 예시,0.014986043,0.0,0.014986042864620686
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!! (남은 답변: BCE 가 좋은 task, 이유)",Loss Function 정의,0.018445944,0.0,0.018445944413542747
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!! (남은 답변: BCE 가 좋은 task, 이유)",MBTI,-0.013806713,0.0,0.013806712813675404
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!! (남은 답변: BCE 가 좋은 task, 이유)",MSE Loss 설명,-0.0094471555,0.0,0.009447155520319939
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!! (남은 답변: BCE 가 좋은 task, 이유)",MSE Loss 용도,-0.004596444,0.0,0.004596443846821785
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!! (남은 답변: BCE 가 좋은 task, 이유)","Multi-Class, Multi-Label 중 BCE 가 좋은 task",0.985943,1.0,0.014056980609893799
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!! (남은 답변: BCE 가 좋은 task, 이유)",Multi-Label 에서 CE + Softmax 적용 문제점,-0.020376721,0.0,0.020376721397042274
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!! (남은 답변: BCE 가 좋은 task, 이유)",PEFT 방법 5가지,0.0015804862,0.0,0.0015804861905053258
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!! (남은 답변: BCE 가 좋은 task, 이유)",거대 언어 모델 정의,0.008309542,0.0,0.008309542201459408
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!! (남은 답변: BCE 가 좋은 task, 이유)",마지막 할 말,0.0034878494,0.0,0.003487849375233054
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!! (남은 답변: BCE 가 좋은 task, 이유)",면접 시작 인사,0.0025277743,0.0,0.0025277743116021156
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!! (남은 답변: BCE 가 좋은 task, 이유)",면접 종료,-0.012344386,0.0,0.01234438642859459
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!! (남은 답변: BCE 가 좋은 task, 이유)","인공지능, 머신러닝, 딥러닝 차이",0.013483026,0.0,0.013483026064932346
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!! (남은 답변: BCE 가 좋은 task, 이유)",잠시 휴식,-0.02759173,0.0,0.027591729536652565
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!! (남은 답변: BCE 가 좋은 task, 이유)",좋아하는 아이돌,-0.0051260586,0.0,0.005126058589667082
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!! (남은 답변: BCE 가 좋은 task, 이유)",확률 예측에서 MSE Loss 미 사용 이유,0.012539867,0.0,0.01253986731171608
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어 (남은 답변: 이유)",BCE Loss 설명,0.04557534,0.0,0.0455753393471241
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어 (남은 답변: 이유)",LLM Fine-Tuning 의 PEFT,-0.002596084,0.0,0.002596084028482437
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어 (남은 답변: 이유)",LoRA,-0.01145949,0.0,0.011459490284323692
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어 (남은 답변: 이유)",LoRA 와 QLoRA 의 차이,-0.01702625,0.0,0.017026249319314957
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어 (남은 답변: 이유)",Loss Function 관련 실무 경험,-0.018032981,0.0,0.01803298108279705
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어 (남은 답변: 이유)",Loss Function 예시,0.005043829,0.0,0.00504382885992527
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어 (남은 답변: 이유)",Loss Function 정의,0.009416401,0.0,0.009416401386260986
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어 (남은 답변: 이유)",MBTI,-0.012977984,0.0,0.012977983802556992
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어 (남은 답변: 이유)",MSE Loss 설명,-0.008370666,0.0,0.008370665833353996
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어 (남은 답변: 이유)",MSE Loss 용도,-0.0049040895,0.0,0.004904089495539665
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어 (남은 답변: 이유)","Multi-Class, Multi-Label 중 BCE 가 좋은 task",0.97894573,1.0,0.02105426788330078
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어 (남은 답변: 이유)",Multi-Label 에서 CE + Softmax 적용 문제점,-0.014493903,0.0,0.014493903145194054
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어 (남은 답변: 이유)",PEFT 방법 5가지,0.004787091,0.0,0.004787091165781021
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어 (남은 답변: 이유)",거대 언어 모델 정의,0.008900211,0.0,0.008900211192667484
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어 (남은 답변: 이유)",마지막 할 말,-0.0014618316,0.0,0.0014618316199630499
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어 (남은 답변: 이유)",면접 시작 인사,-0.0082088085,0.0,0.008208808489143848
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어 (남은 답변: 이유)",면접 종료,-0.012726241,0.0,0.012726240791380405
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어 (남은 답변: 이유)","인공지능, 머신러닝, 딥러닝 차이",0.0094383545,0.0,0.009438354521989822
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어 (남은 답변: 이유)",잠시 휴식,-0.022424232,0.0,0.022424232214689255
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어 (남은 답변: 이유)",좋아하는 아이돌,-0.007537252,0.0,0.007537251804023981
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어 (남은 답변: 이유)",확률 예측에서 MSE Loss 미 사용 이유,0.0062527247,0.0,0.006252724677324295
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아 (남은 답변: 모든 질문 해결 완료)",BCE Loss 설명,-0.031173455,0.0,0.031173454597592354
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아 (남은 답변: 모든 질문 해결 완료)",LLM Fine-Tuning 의 PEFT,-0.05070975,0.0,0.05070975050330162
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아 (남은 답변: 모든 질문 해결 완료)",LoRA,-0.008424167,0.0,0.008424166589975357
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아 (남은 답변: 모든 질문 해결 완료)",LoRA 와 QLoRA 의 차이,0.01750327,0.0,0.017503270879387856
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아 (남은 답변: 모든 질문 해결 완료)",Loss Function 관련 실무 경험,0.06835133,0.0,0.06835132837295532
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아 (남은 답변: 모든 질문 해결 완료)",Loss Function 예시,0.003533121,0.0,0.0035331209655851126
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아 (남은 답변: 모든 질문 해결 완료)",Loss Function 정의,0.013600125,0.0,0.013600124977529049
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아 (남은 답변: 모든 질문 해결 완료)",MBTI,-0.030529188,0.0,0.030529187992215157
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아 (남은 답변: 모든 질문 해결 완료)",MSE Loss 설명,0.06979547,0.0,0.06979546695947647
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아 (남은 답변: 모든 질문 해결 완료)",MSE Loss 용도,0.07708326,0.0,0.07708325982093811
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아 (남은 답변: 모든 질문 해결 완료)","Multi-Class, Multi-Label 중 BCE 가 좋은 task",0.1615633,0.0,0.16156330704689026
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아 (남은 답변: 모든 질문 해결 완료)",Multi-Label 에서 CE + Softmax 적용 문제점,0.7929354,1.0,0.20706462860107422
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아 (남은 답변: 모든 질문 해결 완료)",PEFT 방법 5가지,0.0041462285,0.0,0.004146228544414043
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아 (남은 답변: 모든 질문 해결 완료)",거대 언어 모델 정의,0.007956064,0.0,0.00795606430619955
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아 (남은 답변: 모든 질문 해결 완료)",마지막 할 말,-0.0059621544,0.0,0.005962154362350702
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아 (남은 답변: 모든 질문 해결 완료)",면접 시작 인사,-0.0075574424,0.0,0.007557442411780357
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아 (남은 답변: 모든 질문 해결 완료)",면접 종료,-0.009394999,0.0,0.009394998662173748
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아 (남은 답변: 모든 질문 해결 완료)","인공지능, 머신러닝, 딥러닝 차이",-0.007736788,0.0,0.007736788131296635
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아 (남은 답변: 모든 질문 해결 완료)",잠시 휴식,-0.029309357,0.0,0.029309356585144997
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아 (남은 답변: 모든 질문 해결 완료)",좋아하는 아이돌,-0.051429257,0.0,0.05142925679683685
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아 (남은 답변: 모든 질문 해결 완료)",확률 예측에서 MSE Loss 미 사용 이유,-0.058891,0.0,0.05889099836349487
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지 (남은 답변: 모든 질문 해결 완료),BCE Loss 설명,0.15656269,0.0,0.1565626859664917
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지 (남은 답변: 모든 질문 해결 완료),LLM Fine-Tuning 의 PEFT,-0.011886274,0.0,0.011886274442076683
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지 (남은 답변: 모든 질문 해결 완료),LoRA,-0.06636744,0.0,0.06636743992567062
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지 (남은 답변: 모든 질문 해결 완료),LoRA 와 QLoRA 의 차이,0.020271525,0.0,0.020271524786949158
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지 (남은 답변: 모든 질문 해결 완료),Loss Function 관련 실무 경험,0.3338993,1.0,0.6661007106304169
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지 (남은 답변: 모든 질문 해결 완료),Loss Function 예시,0.14203846,0.0,0.1420384645462036
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지 (남은 답변: 모든 질문 해결 완료),Loss Function 정의,0.101609536,0.0,0.10160953551530838
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지 (남은 답변: 모든 질문 해결 완료),MBTI,0.06974483,0.0,0.06974483281373978
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지 (남은 답변: 모든 질문 해결 완료),MSE Loss 설명,-0.014565736,0.0,0.014565736055374146
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지 (남은 답변: 모든 질문 해결 완료),MSE Loss 용도,-0.046571776,0.0,0.046571776270866394
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지 (남은 답변: 모든 질문 해결 완료),"Multi-Class, Multi-Label 중 BCE 가 좋은 task",0.001237555,0.0,0.0012375550577417016
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지 (남은 답변: 모든 질문 해결 완료),Multi-Label 에서 CE + Softmax 적용 문제점,0.14253661,0.0,0.14253661036491394
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지 (남은 답변: 모든 질문 해결 완료),PEFT 방법 5가지,-0.019385621,0.0,0.019385620951652527
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지 (남은 답변: 모든 질문 해결 완료),거대 언어 모델 정의,-0.0444575,0.0,0.04445749893784523
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지 (남은 답변: 모든 질문 해결 완료),마지막 할 말,0.12691523,0.0,0.12691523134708405
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지 (남은 답변: 모든 질문 해결 완료),면접 시작 인사,0.04224896,0.0,0.042248960584402084
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지 (남은 답변: 모든 질문 해결 완료),면접 종료,-0.023738367,0.0,0.02373836748301983
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지 (남은 답변: 모든 질문 해결 완료),"인공지능, 머신러닝, 딥러닝 차이",0.046851683,0.0,0.04685168340802193
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지 (남은 답변: 모든 질문 해결 완료),잠시 휴식,0.046825163,0.0,0.046825163066387177
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지 (남은 답변: 모든 질문 해결 완료),좋아하는 아이돌,0.11959226,0.0,0.11959225684404373
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지 (남은 답변: 모든 질문 해결 완료),확률 예측에서 MSE Loss 미 사용 이유,-0.04458309,0.0,0.04458308964967728
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나 (남은 답변: Multi-Label 에서 CE + Softmax 적용 문제점),BCE Loss 설명,-0.0013911189,0.0,0.0013911188580095768
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나 (남은 답변: Multi-Label 에서 CE + Softmax 적용 문제점),LLM Fine-Tuning 의 PEFT,0.019754231,0.0,0.019754230976104736
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나 (남은 답변: Multi-Label 에서 CE + Softmax 적용 문제점),LoRA,-0.0030195497,0.0,0.0030195496510714293
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나 (남은 답변: Multi-Label 에서 CE + Softmax 적용 문제점),LoRA 와 QLoRA 의 차이,0.0039300025,0.0,0.003930002450942993
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나 (남은 답변: Multi-Label 에서 CE + Softmax 적용 문제점),Loss Function 관련 실무 경험,0.08295931,0.0,0.08295930922031403
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나 (남은 답변: Multi-Label 에서 CE + Softmax 적용 문제점),Loss Function 예시,-0.0057448726,0.0,0.005744872614741325
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나 (남은 답변: Multi-Label 에서 CE + Softmax 적용 문제점),Loss Function 정의,-0.0013104888,0.0,0.001310488791204989
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나 (남은 답변: Multi-Label 에서 CE + Softmax 적용 문제점),MBTI,-0.014633592,0.0,0.01463359221816063
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나 (남은 답변: Multi-Label 에서 CE + Softmax 적용 문제점),MSE Loss 설명,0.024749294,0.0,0.02474929392337799
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나 (남은 답변: Multi-Label 에서 CE + Softmax 적용 문제점),MSE Loss 용도,0.039931852,0.0,0.039931852370500565
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나 (남은 답변: Multi-Label 에서 CE + Softmax 적용 문제점),"Multi-Class, Multi-Label 중 BCE 가 좋은 task",0.0016259699,0.0,0.0016259698895737529
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나 (남은 답변: Multi-Label 에서 CE + Softmax 적용 문제점),Multi-Label 에서 CE + Softmax 적용 문제점,0.94849426,1.0,0.05150574445724487
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나 (남은 답변: Multi-Label 에서 CE + Softmax 적용 문제점),PEFT 방법 5가지,0.011791334,0.0,0.011791333556175232
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나 (남은 답변: Multi-Label 에서 CE + Softmax 적용 문제점),거대 언어 모델 정의,-0.014458148,0.0,0.014458147808909416
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나 (남은 답변: Multi-Label 에서 CE + Softmax 적용 문제점),마지막 할 말,0.0050400174,0.0,0.005040017422288656
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나 (남은 답변: Multi-Label 에서 CE + Softmax 적용 문제점),면접 시작 인사,-0.018351853,0.0,0.018351852893829346
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나 (남은 답변: Multi-Label 에서 CE + Softmax 적용 문제점),면접 종료,-0.0006742993,0.0,0.0006742993136867881
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나 (남은 답변: Multi-Label 에서 CE + Softmax 적용 문제점),"인공지능, 머신러닝, 딥러닝 차이",0.004607418,0.0,0.0046074180863797665
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나 (남은 답변: Multi-Label 에서 CE + Softmax 적용 문제점),잠시 휴식,-0.018667897,0.0,0.01866789720952511
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나 (남은 답변: Multi-Label 에서 CE + Softmax 적용 문제점),좋아하는 아이돌,-0.018701624,0.0,0.018701624125242233
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나 (남은 답변: Multi-Label 에서 CE + Softmax 적용 문제점),확률 예측에서 MSE Loss 미 사용 이유,-0.0045148903,0.0,0.004514890257269144
Loss Function 관련 실무 경험 -> 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어 (남은 답변: 상세 경험),BCE Loss 설명,0.080950074,0.0,0.08095007389783859
Loss Function 관련 실무 경험 -> 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어 (남은 답변: 상세 경험),LLM Fine-Tuning 의 PEFT,-0.03877941,0.0,0.03877941146492958
Loss Function 관련 실무 경험 -> 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어 (남은 답변: 상세 경험),LoRA,-0.07466085,0.0,0.07466085255146027
Loss Function 관련 실무 경험 -> 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어 (남은 답변: 상세 경험),LoRA 와 QLoRA 의 차이,0.20103177,0.0,0.20103177428245544
Loss Function 관련 실무 경험 -> 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어 (남은 답변: 상세 경험),Loss Function 관련 실무 경험,0.35065332,1.0,0.6493466794490814
Loss Function 관련 실무 경험 -> 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어 (남은 답변: 상세 경험),Loss Function 예시,0.12953511,0.0,0.12953510880470276
Loss Function 관련 실무 경험 -> 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어 (남은 답변: 상세 경험),Loss Function 정의,0.09319634,0.0,0.093196339905262
Loss Function 관련 실무 경험 -> 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어 (남은 답변: 상세 경험),MBTI,0.09655213,0.0,0.09655213356018066
Loss Function 관련 실무 경험 -> 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어 (남은 답변: 상세 경험),MSE Loss 설명,0.08445635,0.0,0.08445634692907333
Loss Function 관련 실무 경험 -> 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어 (남은 답변: 상세 경험),MSE Loss 용도,0.08465426,0.0,0.08465425670146942
Loss Function 관련 실무 경험 -> 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어 (남은 답변: 상세 경험),"Multi-Class, Multi-Label 중 BCE 가 좋은 task",-0.004586978,0.0,0.004586977884173393
Loss Function 관련 실무 경험 -> 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어 (남은 답변: 상세 경험),Multi-Label 에서 CE + Softmax 적용 문제점,0.124121115,0.0,0.12412111461162567
Loss Function 관련 실무 경험 -> 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어 (남은 답변: 상세 경험),PEFT 방법 5가지,-0.02658621,0.0,0.02658621035516262
Loss Function 관련 실무 경험 -> 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어 (남은 답변: 상세 경험),거대 언어 모델 정의,-0.103167586,0.0,0.1031675860285759
Loss Function 관련 실무 경험 -> 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어 (남은 답변: 상세 경험),마지막 할 말,0.08882112,0.0,0.08882112056016922
Loss Function 관련 실무 경험 -> 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어 (남은 답변: 상세 경험),면접 시작 인사,0.030291973,0.0,0.030291972681879997
Loss Function 관련 실무 경험 -> 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어 (남은 답변: 상세 경험),면접 종료,-0.07084473,0.0,0.07084473222494125
Loss Function 관련 실무 경험 -> 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어 (남은 답변: 상세 경험),"인공지능, 머신러닝, 딥러닝 차이",0.031698104,0.0,0.03169810399413109
Loss Function 관련 실무 경험 -> 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어 (남은 답변: 상세 경험),잠시 휴식,0.056102794,0.0,0.05610279366374016
Loss Function 관련 실무 경험 -> 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어 (남은 답변: 상세 경험),좋아하는 아이돌,0.13542464,0.0,0.1354246437549591
Loss Function 관련 실무 경험 -> 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어 (남은 답변: 상세 경험),확률 예측에서 MSE Loss 미 사용 이유,0.005463526,0.0,0.005463526118546724
Loss Function 관련 실무 경험 -> 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지? (남은 답변: 모든 질문 해결 완료),BCE Loss 설명,-0.11274532,0.0,0.11274532228708267
Loss Function 관련 실무 경험 -> 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지? (남은 답변: 모든 질문 해결 완료),LLM Fine-Tuning 의 PEFT,0.037479952,0.0,0.0374799519777298
Loss Function 관련 실무 경험 -> 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지? (남은 답변: 모든 질문 해결 완료),LoRA,0.05188744,0.0,0.05188744142651558
Loss Function 관련 실무 경험 -> 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지? (남은 답변: 모든 질문 해결 완료),LoRA 와 QLoRA 의 차이,0.034136504,0.0,0.03413650393486023
Loss Function 관련 실무 경험 -> 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지? (남은 답변: 모든 질문 해결 완료),Loss Function 관련 실무 경험,0.23658182,0.0,0.23658181726932526
Loss Function 관련 실무 경험 -> 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지? (남은 답변: 모든 질문 해결 완료),Loss Function 예시,0.044367466,0.0,0.044367466121912
Loss Function 관련 실무 경험 -> 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지? (남은 답변: 모든 질문 해결 완료),Loss Function 정의,0.037454844,0.0,0.037454843521118164
Loss Function 관련 실무 경험 -> 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지? (남은 답변: 모든 질문 해결 완료),MBTI,0.25699273,1.0,0.7430072724819183
Loss Function 관련 실무 경험 -> 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지? (남은 답변: 모든 질문 해결 완료),MSE Loss 설명,0.012169881,0.0,0.012169880792498589
Loss Function 관련 실무 경험 -> 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지? (남은 답변: 모든 질문 해결 완료),MSE Loss 용도,-0.0036873946,0.0,0.0036873945500701666
Loss Function 관련 실무 경험 -> 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지? (남은 답변: 모든 질문 해결 완료),"Multi-Class, Multi-Label 중 BCE 가 좋은 task",0.008058922,0.0,0.008058922365307808
Loss Function 관련 실무 경험 -> 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지? (남은 답변: 모든 질문 해결 완료),Multi-Label 에서 CE + Softmax 적용 문제점,-0.014647439,0.0,0.014647439122200012
Loss Function 관련 실무 경험 -> 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지? (남은 답변: 모든 질문 해결 완료),PEFT 방법 5가지,0.021450084,0.0,0.021450083702802658
Loss Function 관련 실무 경험 -> 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지? (남은 답변: 모든 질문 해결 완료),거대 언어 모델 정의,0.10567177,0.0,0.10567177087068558
Loss Function 관련 실무 경험 -> 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지? (남은 답변: 모든 질문 해결 완료),마지막 할 말,0.10102636,0.0,0.10102636367082596
Loss Function 관련 실무 경험 -> 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지? (남은 답변: 모든 질문 해결 완료),면접 시작 인사,0.007086629,0.0,0.007086629047989845
Loss Function 관련 실무 경험 -> 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지? (남은 답변: 모든 질문 해결 완료),면접 종료,0.07856021,0.0,0.07856021076440811
Loss Function 관련 실무 경험 -> 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지? (남은 답변: 모든 질문 해결 완료),"인공지능, 머신러닝, 딥러닝 차이",-0.08023611,0.0,0.08023610711097717
Loss Function 관련 실무 경험 -> 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지? (남은 답변: 모든 질문 해결 완료),잠시 휴식,0.25337964,0.0,0.2533796429634094
Loss Function 관련 실무 경험 -> 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지? (남은 답변: 모든 질문 해결 완료),좋아하는 아이돌,0.229515,0.0,0.22951500117778778
Loss Function 관련 실무 경험 -> 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지? (남은 답변: 모든 질문 해결 완료),확률 예측에서 MSE Loss 미 사용 이유,-0.014448626,0.0,0.014448625966906548
Loss Function 관련 실무 경험 -> 근데 기억 진짜 안나는데 어떡하지? (남은 답변: 상세 경험),BCE Loss 설명,-0.0059768143,0.0,0.0059768143109977245
Loss Function 관련 실무 경험 -> 근데 기억 진짜 안나는데 어떡하지? (남은 답변: 상세 경험),LLM Fine-Tuning 의 PEFT,0.01634403,0.0,0.01634402945637703
Loss Function 관련 실무 경험 -> 근데 기억 진짜 안나는데 어떡하지? (남은 답변: 상세 경험),LoRA,0.034648895,0.0,0.034648895263671875
Loss Function 관련 실무 경험 -> 근데 기억 진짜 안나는데 어떡하지? (남은 답변: 상세 경험),LoRA 와 QLoRA 의 차이,-0.012895421,0.0,0.012895421124994755
Loss Function 관련 실무 경험 -> 근데 기억 진짜 안나는데 어떡하지? (남은 답변: 상세 경험),Loss Function 관련 실무 경험,0.29220363,1.0,0.7077963650226593
Loss Function 관련 실무 경험 -> 근데 기억 진짜 안나는데 어떡하지? (남은 답변: 상세 경험),Loss Function 예시,0.055266187,0.0,0.055266186594963074
Loss Function 관련 실무 경험 -> 근데 기억 진짜 안나는데 어떡하지? (남은 답변: 상세 경험),Loss Function 정의,0.036635377,0.0,0.0366353765130043
Loss Function 관련 실무 경험 -> 근데 기억 진짜 안나는데 어떡하지? (남은 답변: 상세 경험),MBTI,0.18782279,0.0,0.18782278895378113
Loss Function 관련 실무 경험 -> 근데 기억 진짜 안나는데 어떡하지? (남은 답변: 상세 경험),MSE Loss 설명,-0.011262423,0.0,0.011262423358857632
Loss Function 관련 실무 경험 -> 근데 기억 진짜 안나는데 어떡하지? (남은 답변: 상세 경험),MSE Loss 용도,-0.0478147,0.0,0.04781470075249672
Loss Function 관련 실무 경험 -> 근데 기억 진짜 안나는데 어떡하지? (남은 답변: 상세 경험),"Multi-Class, Multi-Label 중 BCE 가 좋은 task",0.018832045,0.0,0.018832044675946236
Loss Function 관련 실무 경험 -> 근데 기억 진짜 안나는데 어떡하지? (남은 답변: 상세 경험),Multi-Label 에서 CE + Softmax 적용 문제점,-0.01714006,0.0,0.017140060663223267
Loss Function 관련 실무 경험 -> 근데 기억 진짜 안나는데 어떡하지? (남은 답변: 상세 경험),PEFT 방법 5가지,0.018426746,0.0,0.018426746129989624
Loss Function 관련 실무 경험 -> 근데 기억 진짜 안나는데 어떡하지? (남은 답변: 상세 경험),거대 언어 모델 정의,-0.09085807,0.0,0.09085807204246521
Loss Function 관련 실무 경험 -> 근데 기억 진짜 안나는데 어떡하지? (남은 답변: 상세 경험),마지막 할 말,0.018026186,0.0,0.018026186153292656
Loss Function 관련 실무 경험 -> 근데 기억 진짜 안나는데 어떡하지? (남은 답변: 상세 경험),면접 시작 인사,0.039258726,0.0,0.03925872594118118
Loss Function 관련 실무 경험 -> 근데 기억 진짜 안나는데 어떡하지? (남은 답변: 상세 경험),면접 종료,0.13238147,0.0,0.13238146901130676
Loss Function 관련 실무 경험 -> 근데 기억 진짜 안나는데 어떡하지? (남은 답변: 상세 경험),"인공지능, 머신러닝, 딥러닝 차이",0.034680028,0.0,0.03468002751469612
Loss Function 관련 실무 경험 -> 근데 기억 진짜 안나는데 어떡하지? (남은 답변: 상세 경험),잠시 휴식,0.40859488,0.0,0.40859487652778625
Loss Function 관련 실무 경험 -> 근데 기억 진짜 안나는데 어떡하지? (남은 답변: 상세 경험),좋아하는 아이돌,0.14282441,0.0,0.14282441139221191
Loss Function 관련 실무 경험 -> 근데 기억 진짜 안나는데 어떡하지? (남은 답변: 상세 경험),확률 예측에서 MSE Loss 미 사용 이유,-0.050220642,0.0,0.05022064223885536
Loss Function 관련 실무 경험 -> 나 아직 실무 경험 없는데 ㅠㅠ (남은 답변: 모든 질문 해결 완료),BCE Loss 설명,-0.08371628,0.0,0.08371628075838089
Loss Function 관련 실무 경험 -> 나 아직 실무 경험 없는데 ㅠㅠ (남은 답변: 모든 질문 해결 완료),LLM Fine-Tuning 의 PEFT,0.017747546,0.0,0.0177475456148386
Loss Function 관련 실무 경험 -> 나 아직 실무 경험 없는데 ㅠㅠ (남은 답변: 모든 질문 해결 완료),LoRA,0.005775613,0.0,0.0057756127789616585
Loss Function 관련 실무 경험 -> 나 아직 실무 경험 없는데 ㅠㅠ (남은 답변: 모든 질문 해결 완료),LoRA 와 QLoRA 의 차이,-0.009084156,0.0,0.009084155783057213
Loss Function 관련 실무 경험 -> 나 아직 실무 경험 없는데 ㅠㅠ (남은 답변: 모든 질문 해결 완료),Loss Function 관련 실무 경험,0.24832791,0.0,0.24832791090011597
Loss Function 관련 실무 경험 -> 나 아직 실무 경험 없는데 ㅠㅠ (남은 답변: 모든 질문 해결 완료),Loss Function 예시,0.04717305,0.0,0.04717304930090904
Loss Function 관련 실무 경험 -> 나 아직 실무 경험 없는데 ㅠㅠ (남은 답변: 모든 질문 해결 완료),Loss Function 정의,0.038735345,0.0,0.038735345005989075
Loss Function 관련 실무 경험 -> 나 아직 실무 경험 없는데 ㅠㅠ (남은 답변: 모든 질문 해결 완료),MBTI,0.27768087,0.0,0.2776808738708496
Loss Function 관련 실무 경험 -> 나 아직 실무 경험 없는데 ㅠㅠ (남은 답변: 모든 질문 해결 완료),MSE Loss 설명,0.023235174,0.0,0.023235173895955086
Loss Function 관련 실무 경험 -> 나 아직 실무 경험 없는데 ㅠㅠ (남은 답변: 모든 질문 해결 완료),MSE Loss 용도,-0.008053236,0.0,0.008053235709667206
Loss Function 관련 실무 경험 -> 나 아직 실무 경험 없는데 ㅠㅠ (남은 답변: 모든 질문 해결 완료),"Multi-Class, Multi-Label 중 BCE 가 좋은 task",0.0121171065,0.0,0.012117106467485428
Loss Function 관련 실무 경험 -> 나 아직 실무 경험 없는데 ㅠㅠ (남은 답변: 모든 질문 해결 완료),Multi-Label 에서 CE + Softmax 적용 문제점,-0.03892062,0.0,0.03892061859369278
Loss Function 관련 실무 경험 -> 나 아직 실무 경험 없는데 ㅠㅠ (남은 답변: 모든 질문 해결 완료),PEFT 방법 5가지,-0.047579963,0.0,0.04757996276021004
Loss Function 관련 실무 경험 -> 나 아직 실무 경험 없는데 ㅠㅠ (남은 답변: 모든 질문 해결 완료),거대 언어 모델 정의,0.0545028,0.0,0.05450280010700226
Loss Function 관련 실무 경험 -> 나 아직 실무 경험 없는데 ㅠㅠ (남은 답변: 모든 질문 해결 완료),마지막 할 말,0.097519524,0.0,0.09751952439546585
Loss Function 관련 실무 경험 -> 나 아직 실무 경험 없는데 ㅠㅠ (남은 답변: 모든 질문 해결 완료),면접 시작 인사,0.01310211,0.0,0.013102109543979168
Loss Function 관련 실무 경험 -> 나 아직 실무 경험 없는데 ㅠㅠ (남은 답변: 모든 질문 해결 완료),면접 종료,0.082377285,0.0,0.08237728476524353
Loss Function 관련 실무 경험 -> 나 아직 실무 경험 없는데 ㅠㅠ (남은 답변: 모든 질문 해결 완료),"인공지능, 머신러닝, 딥러닝 차이",-0.04052423,0.0,0.040524229407310486
Loss Function 관련 실무 경험 -> 나 아직 실무 경험 없는데 ㅠㅠ (남은 답변: 모든 질문 해결 완료),잠시 휴식,0.28727365,0.0,0.287273645401001
Loss Function 관련 실무 경험 -> 나 아직 실무 경험 없는데 ㅠㅠ (남은 답변: 모든 질문 해결 완료),좋아하는 아이돌,0.23499009,1.0,0.7650099098682404
Loss Function 관련 실무 경험 -> 나 아직 실무 경험 없는데 ㅠㅠ (남은 답변: 모든 질문 해결 완료),확률 예측에서 MSE Loss 미 사용 이유,-0.008266186,0.0,0.008266186341643333
"Loss Function 관련 실무 경험 -> 대답하기 싫은데 힝 (남은 답변: 기본 경험, 상세 경험)",BCE Loss 설명,-0.019275125,0.0,0.019275125116109848
"Loss Function 관련 실무 경험 -> 대답하기 싫은데 힝 (남은 답변: 기본 경험, 상세 경험)",LLM Fine-Tuning 의 PEFT,0.013664963,0.0,0.013664962723851204
"Loss Function 관련 실무 경험 -> 대답하기 싫은데 힝 (남은 답변: 기본 경험, 상세 경험)",LoRA,0.038031906,0.0,0.03803190588951111
"Loss Function 관련 실무 경험 -> 대답하기 싫은데 힝 (남은 답변: 기본 경험, 상세 경험)",LoRA 와 QLoRA 의 차이,0.013185285,0.0,0.013185285031795502
"Loss Function 관련 실무 경험 -> 대답하기 싫은데 힝 (남은 답변: 기본 경험, 상세 경험)",Loss Function 관련 실무 경험,0.2407547,1.0,0.7592453062534332
"Loss Function 관련 실무 경험 -> 대답하기 싫은데 힝 (남은 답변: 기본 경험, 상세 경험)",Loss Function 예시,0.019224994,0.0,0.019224993884563446
"Loss Function 관련 실무 경험 -> 대답하기 싫은데 힝 (남은 답변: 기본 경험, 상세 경험)",Loss Function 정의,0.0016146529,0.0,0.001614652923308313
"Loss Function 관련 실무 경험 -> 대답하기 싫은데 힝 (남은 답변: 기본 경험, 상세 경험)",MBTI,0.1714366,0.0,0.1714365929365158
"Loss Function 관련 실무 경험 -> 대답하기 싫은데 힝 (남은 답변: 기본 경험, 상세 경험)",MSE Loss 설명,-0.036523465,0.0,0.03652346506714821
"Loss Function 관련 실무 경험 -> 대답하기 싫은데 힝 (남은 답변: 기본 경험, 상세 경험)",MSE Loss 용도,-0.073648274,0.0,0.07364827394485474
"Loss Function 관련 실무 경험 -> 대답하기 싫은데 힝 (남은 답변: 기본 경험, 상세 경험)","Multi-Class, Multi-Label 중 BCE 가 좋은 task",0.0027071089,0.0,0.0027071088552474976
"Loss Function 관련 실무 경험 -> 대답하기 싫은데 힝 (남은 답변: 기본 경험, 상세 경험)",Multi-Label 에서 CE + Softmax 적용 문제점,0.008635091,0.0,0.008635090664029121
"Loss Function 관련 실무 경험 -> 대답하기 싫은데 힝 (남은 답변: 기본 경험, 상세 경험)",PEFT 방법 5가지,-0.018739088,0.0,0.018739087507128716
"Loss Function 관련 실무 경험 -> 대답하기 싫은데 힝 (남은 답변: 기본 경험, 상세 경험)",거대 언어 모델 정의,-0.032105487,0.0,0.03210548684000969
"Loss Function 관련 실무 경험 -> 대답하기 싫은데 힝 (남은 답변: 기본 경험, 상세 경험)",마지막 할 말,0.0071570766,0.0,0.007157076615840197
"Loss Function 관련 실무 경험 -> 대답하기 싫은데 힝 (남은 답변: 기본 경험, 상세 경험)",면접 시작 인사,0.025117623,0.0,0.025117622688412666
"Loss Function 관련 실무 경험 -> 대답하기 싫은데 힝 (남은 답변: 기본 경험, 상세 경험)",면접 종료,0.14662355,0.0,0.14662355184555054
"Loss Function 관련 실무 경험 -> 대답하기 싫은데 힝 (남은 답변: 기본 경험, 상세 경험)","인공지능, 머신러닝, 딥러닝 차이",0.042194154,0.0,0.04219415411353111
"Loss Function 관련 실무 경험 -> 대답하기 싫은데 힝 (남은 답변: 기본 경험, 상세 경험)",잠시 휴식,0.41834235,0.0,0.41834235191345215
"Loss Function 관련 실무 경험 -> 대답하기 싫은데 힝 (남은 답변: 기본 경험, 상세 경험)",좋아하는 아이돌,0.13695252,0.0,0.13695251941680908
"Loss Function 관련 실무 경험 -> 대답하기 싫은데 힝 (남은 답변: 기본 경험, 상세 경험)",확률 예측에서 MSE Loss 미 사용 이유,-0.048462648,0.0,0.0484626479446888
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야! (남은 답변: 모든 질문 해결 완료),BCE Loss 설명,-0.025566835,0.0,0.025566834956407547
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야! (남은 답변: 모든 질문 해결 완료),LLM Fine-Tuning 의 PEFT,0.02460023,0.0,0.024600230157375336
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야! (남은 답변: 모든 질문 해결 완료),LoRA,0.04424545,0.0,0.04424545168876648
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야! (남은 답변: 모든 질문 해결 완료),LoRA 와 QLoRA 의 차이,-0.14540702,0.0,0.14540702104568481
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야! (남은 답변: 모든 질문 해결 완료),Loss Function 관련 실무 경험,0.047697663,0.0,0.04769766330718994
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야! (남은 답변: 모든 질문 해결 완료),Loss Function 예시,-0.04708607,0.0,0.047086071223020554
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야! (남은 답변: 모든 질문 해결 완료),Loss Function 정의,-0.051694766,0.0,0.05169476568698883
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야! (남은 답변: 모든 질문 해결 완료),MBTI,0.14581826,0.0,0.14581826329231262
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야! (남은 답변: 모든 질문 해결 완료),MSE Loss 설명,0.112750806,0.0,0.11275080591440201
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야! (남은 답변: 모든 질문 해결 완료),MSE Loss 용도,0.098431714,0.0,0.09843171387910843
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야! (남은 답변: 모든 질문 해결 완료),"Multi-Class, Multi-Label 중 BCE 가 좋은 task",0.025360612,0.0,0.02536061219871044
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야! (남은 답변: 모든 질문 해결 완료),Multi-Label 에서 CE + Softmax 적용 문제점,0.08108806,0.0,0.08108805865049362
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야! (남은 답변: 모든 질문 해결 완료),PEFT 방법 5가지,0.100564964,0.0,0.10056496411561966
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야! (남은 답변: 모든 질문 해결 완료),거대 언어 모델 정의,0.019090239,0.0,0.019090238958597183
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야! (남은 답변: 모든 질문 해결 완료),마지막 할 말,-0.052547667,0.0,0.05254766717553139
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야! (남은 답변: 모든 질문 해결 완료),면접 시작 인사,-0.019115984,0.0,0.019115984439849854
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야! (남은 답변: 모든 질문 해결 완료),면접 종료,0.1980946,0.0,0.19809460639953613
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야! (남은 답변: 모든 질문 해결 완료),"인공지능, 머신러닝, 딥러닝 차이",0.031859268,0.0,0.03185926750302315
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야! (남은 답변: 모든 질문 해결 완료),잠시 휴식,0.39995882,1.0,0.6000411808490753
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야! (남은 답변: 모든 질문 해결 완료),좋아하는 아이돌,0.039144196,0.0,0.03914419561624527
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야! (남은 답변: 모든 질문 해결 완료),확률 예측에서 MSE Loss 미 사용 이유,0.0049804403,0.0,0.004980440251529217
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해 (남은 답변: 모든 질문 해결 완료),BCE Loss 설명,-0.05898281,0.0,0.05898280814290047
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해 (남은 답변: 모든 질문 해결 완료),LLM Fine-Tuning 의 PEFT,0.04943869,0.0,0.04943868890404701
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해 (남은 답변: 모든 질문 해결 완료),LoRA,0.17933351,0.0,0.17933350801467896
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해 (남은 답변: 모든 질문 해결 완료),LoRA 와 QLoRA 의 차이,0.020613315,0.0,0.02061331458389759
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해 (남은 답변: 모든 질문 해결 완료),Loss Function 관련 실무 경험,0.049473412,0.0,0.049473412334918976
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해 (남은 답변: 모든 질문 해결 완료),Loss Function 예시,0.012200573,0.0,0.012200573459267616
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해 (남은 답변: 모든 질문 해결 완료),Loss Function 정의,-0.003970774,0.0,0.00397077389061451
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해 (남은 답변: 모든 질문 해결 완료),MBTI,0.059117515,0.0,0.05911751464009285
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해 (남은 답변: 모든 질문 해결 완료),MSE Loss 설명,-0.038969494,0.0,0.0389694944024086
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해 (남은 답변: 모든 질문 해결 완료),MSE Loss 용도,-0.036007904,0.0,0.03600790351629257
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해 (남은 답변: 모든 질문 해결 완료),"Multi-Class, Multi-Label 중 BCE 가 좋은 task",0.03696253,0.0,0.03696253150701523
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해 (남은 답변: 모든 질문 해결 완료),Multi-Label 에서 CE + Softmax 적용 문제점,0.07564264,0.0,0.07564263790845871
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해 (남은 답변: 모든 질문 해결 완료),PEFT 방법 5가지,0.016792709,0.0,0.01679270900785923
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해 (남은 답변: 모든 질문 해결 완료),거대 언어 모델 정의,-0.007837317,0.0,0.007837316952645779
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해 (남은 답변: 모든 질문 해결 완료),마지막 할 말,-0.03300568,0.0,0.03300568088889122
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해 (남은 답변: 모든 질문 해결 완료),면접 시작 인사,-0.020260936,0.0,0.02026093564927578
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해 (남은 답변: 모든 질문 해결 완료),면접 종료,0.23048396,0.0,0.23048396408557892
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해 (남은 답변: 모든 질문 해결 완료),"인공지능, 머신러닝, 딥러닝 차이",0.007651733,0.0,0.0076517327688634396
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해 (남은 답변: 모든 질문 해결 완료),잠시 휴식,0.42379043,1.0,0.5762095749378204
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해 (남은 답변: 모든 질문 해결 완료),좋아하는 아이돌,0.08426948,0.0,0.08426947891712189
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해 (남은 답변: 모든 질문 해결 완료),확률 예측에서 MSE Loss 미 사용 이유,-0.030810745,0.0,0.030810745432972908
잠시 휴식 -> 재미있는 이야기 해줄래? (남은 답변: 잠시 휴식),BCE Loss 설명,-0.007087719,0.0,0.007087719161063433
잠시 휴식 -> 재미있는 이야기 해줄래? (남은 답변: 잠시 휴식),LLM Fine-Tuning 의 PEFT,0.9644085,1.0,0.0355914831161499
잠시 휴식 -> 재미있는 이야기 해줄래? (남은 답변: 잠시 휴식),LoRA,-0.042443164,0.0,0.0424431636929512
잠시 휴식 -> 재미있는 이야기 해줄래? (남은 답변: 잠시 휴식),LoRA 와 QLoRA 의 차이,-0.020261038,0.0,0.020261038094758987
잠시 휴식 -> 재미있는 이야기 해줄래? (남은 답변: 잠시 휴식),Loss Function 관련 실무 경험,0.0077775973,0.0,0.007777597289532423
잠시 휴식 -> 재미있는 이야기 해줄래? (남은 답변: 잠시 휴식),Loss Function 예시,-0.008895477,0.0,0.008895477280020714
잠시 휴식 -> 재미있는 이야기 해줄래? (남은 답변: 잠시 휴식),Loss Function 정의,-0.008535501,0.0,0.00853550061583519
잠시 휴식 -> 재미있는 이야기 해줄래? (남은 답변: 잠시 휴식),MBTI,-0.012532603,0.0,0.012532602995634079
잠시 휴식 -> 재미있는 이야기 해줄래? (남은 답변: 잠시 휴식),MSE Loss 설명,-0.01851905,0.0,0.018519049510359764
잠시 휴식 -> 재미있는 이야기 해줄래? (남은 답변: 잠시 휴식),MSE Loss 용도,0.0015782249,0.0,0.0015782249392941594
잠시 휴식 -> 재미있는 이야기 해줄래? (남은 답변: 잠시 휴식),"Multi-Class, Multi-Label 중 BCE 가 좋은 task",0.0030280282,0.0,0.003028028178960085
잠시 휴식 -> 재미있는 이야기 해줄래? (남은 답변: 잠시 휴식),Multi-Label 에서 CE + Softmax 적용 문제점,-0.009830605,0.0,0.009830605238676071
잠시 휴식 -> 재미있는 이야기 해줄래? (남은 답변: 잠시 휴식),PEFT 방법 5가지,-0.016992534,0.0,0.016992533579468727
잠시 휴식 -> 재미있는 이야기 해줄래? (남은 답변: 잠시 휴식),거대 언어 모델 정의,-0.0018296713,0.0,0.0018296713242307305
잠시 휴식 -> 재미있는 이야기 해줄래? (남은 답변: 잠시 휴식),마지막 할 말,0.025135277,0.0,0.025135276839137077
잠시 휴식 -> 재미있는 이야기 해줄래? (남은 답변: 잠시 휴식),면접 시작 인사,0.011146528,0.0,0.011146527715027332
잠시 휴식 -> 재미있는 이야기 해줄래? (남은 답변: 잠시 휴식),면접 종료,0.034803107,0.0,0.034803107380867004
잠시 휴식 -> 재미있는 이야기 해줄래? (남은 답변: 잠시 휴식),"인공지능, 머신러닝, 딥러닝 차이",-0.0112109035,0.0,0.011210903525352478
잠시 휴식 -> 재미있는 이야기 해줄래? (남은 답변: 잠시 휴식),잠시 휴식,-0.01730546,0.0,0.017305459827184677
잠시 휴식 -> 재미있는 이야기 해줄래? (남은 답변: 잠시 휴식),좋아하는 아이돌,0.009558277,0.0,0.009558277204632759
잠시 휴식 -> 재미있는 이야기 해줄래? (남은 답변: 잠시 휴식),확률 예측에서 MSE Loss 미 사용 이유,-0.005392043,0.0,0.005392042919993401
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야 (남은 답변: 모든 질문 해결 완료)",BCE Loss 설명,0.06962926,0.0,0.0696292594075203
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야 (남은 답변: 모든 질문 해결 완료)",LLM Fine-Tuning 의 PEFT,0.19936958,0.0,0.19936957955360413
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야 (남은 답변: 모든 질문 해결 완료)",LoRA,0.025819292,0.0,0.025819292291998863
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야 (남은 답변: 모든 질문 해결 완료)",LoRA 와 QLoRA 의 차이,0.016710365,0.0,0.016710365191102028
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야 (남은 답변: 모든 질문 해결 완료)",Loss Function 관련 실무 경험,-0.052479304,0.0,0.05247930437326431
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야 (남은 답변: 모든 질문 해결 완료)",Loss Function 예시,0.014511347,0.0,0.014511346817016602
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야 (남은 답변: 모든 질문 해결 완료)",Loss Function 정의,0.031177212,0.0,0.031177211552858353
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야 (남은 답변: 모든 질문 해결 완료)",MBTI,-0.04440482,0.0,0.04440481960773468
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야 (남은 답변: 모든 질문 해결 완료)",MSE Loss 설명,-0.028218431,0.0,0.028218431398272514
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야 (남은 답변: 모든 질문 해결 완료)",MSE Loss 용도,0.059703324,0.0,0.05970332399010658
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야 (남은 답변: 모든 질문 해결 완료)","Multi-Class, Multi-Label 중 BCE 가 좋은 task",0.013558613,0.0,0.013558613136410713
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야 (남은 답변: 모든 질문 해결 완료)",Multi-Label 에서 CE + Softmax 적용 문제점,0.087130025,0.0,0.08713002502918243
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야 (남은 답변: 모든 질문 해결 완료)",PEFT 방법 5가지,0.80102444,1.0,0.1989755630493164
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야 (남은 답변: 모든 질문 해결 완료)",거대 언어 모델 정의,-0.0035795558,0.0,0.0035795557778328657
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야 (남은 답변: 모든 질문 해결 완료)",마지막 할 말,0.0013580994,0.0,0.0013580993982031941
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야 (남은 답변: 모든 질문 해결 완료)",면접 시작 인사,-0.0039363108,0.0,0.003936310764402151
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야 (남은 답변: 모든 질문 해결 완료)",면접 종료,-0.06723804,0.0,0.06723804026842117
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야 (남은 답변: 모든 질문 해결 완료)","인공지능, 머신러닝, 딥러닝 차이",-0.03672251,0.0,0.03672251105308533
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야 (남은 답변: 모든 질문 해결 완료)",잠시 휴식,-0.040906217,0.0,0.04090621694922447
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야 (남은 답변: 모든 질문 해결 완료)",좋아하는 아이돌,-0.054237172,0.0,0.05423717200756073
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야 (남은 답변: 모든 질문 해결 완료)",확률 예측에서 MSE Loss 미 사용 이유,0.0146312015,0.0,0.014631201513111591
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지? (남은 답변: LLM Fine-Tuning 의 PEFT),BCE Loss 설명,0.0011189851,0.0,0.0011189851211383939
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지? (남은 답변: LLM Fine-Tuning 의 PEFT),LLM Fine-Tuning 의 PEFT,0.9498378,1.0,0.05016219615936279
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지? (남은 답변: LLM Fine-Tuning 의 PEFT),LoRA,-0.0386329,0.0,0.03863289952278137
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지? (남은 답변: LLM Fine-Tuning 의 PEFT),LoRA 와 QLoRA 의 차이,0.012152973,0.0,0.012152972631156445
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지? (남은 답변: LLM Fine-Tuning 의 PEFT),Loss Function 관련 실무 경험,0.006724236,0.0,0.006724236067384481
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지? (남은 답변: LLM Fine-Tuning 의 PEFT),Loss Function 예시,-0.0143031115,0.0,0.014303111471235752
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지? (남은 답변: LLM Fine-Tuning 의 PEFT),Loss Function 정의,-0.0133403,0.0,0.01334029994904995
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지? (남은 답변: LLM Fine-Tuning 의 PEFT),MBTI,-0.022488704,0.0,0.022488703951239586
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지? (남은 답변: LLM Fine-Tuning 의 PEFT),MSE Loss 설명,-0.0045374376,0.0,0.004537437576800585
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지? (남은 답변: LLM Fine-Tuning 의 PEFT),MSE Loss 용도,0.01798701,0.0,0.01798700913786888
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지? (남은 답변: LLM Fine-Tuning 의 PEFT),"Multi-Class, Multi-Label 중 BCE 가 좋은 task",0.011078857,0.0,0.011078856885433197
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지? (남은 답변: LLM Fine-Tuning 의 PEFT),Multi-Label 에서 CE + Softmax 적용 문제점,0.005033305,0.0,0.005033304914832115
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지? (남은 답변: LLM Fine-Tuning 의 PEFT),PEFT 방법 5가지,0.018646503,0.0,0.01864650286734104
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지? (남은 답변: LLM Fine-Tuning 의 PEFT),거대 언어 모델 정의,-0.006201393,0.0,0.006201392970979214
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지? (남은 답변: LLM Fine-Tuning 의 PEFT),마지막 할 말,0.04340606,0.0,0.043406061828136444
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지? (남은 답변: LLM Fine-Tuning 의 PEFT),면접 시작 인사,0.00017763826,0.0,0.00017763825599104166
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지? (남은 답변: LLM Fine-Tuning 의 PEFT),면접 종료,-8.7054774e-05,0.0,8.705477375769988e-05
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지? (남은 답변: LLM Fine-Tuning 의 PEFT),"인공지능, 머신러닝, 딥러닝 차이",-0.009607578,0.0,0.009607577696442604
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지? (남은 답변: LLM Fine-Tuning 의 PEFT),잠시 휴식,-0.056869134,0.0,0.056869134306907654
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지? (남은 답변: LLM Fine-Tuning 의 PEFT),좋아하는 아이돌,0.015813049,0.0,0.01581304892897606
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지? (남은 답변: LLM Fine-Tuning 의 PEFT),확률 예측에서 MSE Loss 미 사용 이유,-0.00051253545,0.0,0.0005125354509800673
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지? (남은 답변: 모든 질문 해결 완료)",BCE Loss 설명,0.052630283,0.0,0.05263028293848038
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지? (남은 답변: 모든 질문 해결 완료)",LLM Fine-Tuning 의 PEFT,-0.08401912,0.0,0.08401911705732346
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지? (남은 답변: 모든 질문 해결 완료)",LoRA,0.70678306,1.0,0.2932169437408447
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지? (남은 답변: 모든 질문 해결 완료)",LoRA 와 QLoRA 의 차이,0.35353383,0.0,0.3535338342189789
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지? (남은 답변: 모든 질문 해결 완료)",Loss Function 관련 실무 경험,-0.061494917,0.0,0.061494916677474976
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지? (남은 답변: 모든 질문 해결 완료)",Loss Function 예시,-0.048078638,0.0,0.048078637570142746
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지? (남은 답변: 모든 질문 해결 완료)",Loss Function 정의,-0.049767885,0.0,0.049767885357141495
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지? (남은 답변: 모든 질문 해결 완료)",MBTI,-0.045713834,0.0,0.04571383446455002
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지? (남은 답변: 모든 질문 해결 완료)",MSE Loss 설명,-0.06080599,0.0,0.06080599129199982
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지? (남은 답변: 모든 질문 해결 완료)",MSE Loss 용도,-0.061780415,0.0,0.0617804154753685
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지? (남은 답변: 모든 질문 해결 완료)","Multi-Class, Multi-Label 중 BCE 가 좋은 task",-0.0062657264,0.0,0.006265726406127214
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지? (남은 답변: 모든 질문 해결 완료)",Multi-Label 에서 CE + Softmax 적용 문제점,0.047779944,0.0,0.04777994379401207
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지? (남은 답변: 모든 질문 해결 완료)",PEFT 방법 5가지,0.05167032,0.0,0.051670320332050323
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지? (남은 답변: 모든 질문 해결 완료)",거대 언어 모델 정의,-0.024391314,0.0,0.024391314014792442
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지? (남은 답변: 모든 질문 해결 완료)",마지막 할 말,0.028308747,0.0,0.028308747336268425
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지? (남은 답변: 모든 질문 해결 완료)",면접 시작 인사,-0.018632928,0.0,0.018632927909493446
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지? (남은 답변: 모든 질문 해결 완료)",면접 종료,-0.024698103,0.0,0.024698102846741676
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지? (남은 답변: 모든 질문 해결 완료)","인공지능, 머신러닝, 딥러닝 차이",-0.01078804,0.0,0.010788040235638618
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지? (남은 답변: 모든 질문 해결 완료)",잠시 휴식,-0.019081432,0.0,0.01908143237233162
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지? (남은 답변: 모든 질문 해결 완료)",좋아하는 아이돌,0.0256313,0.0,0.02563129924237728
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지? (남은 답변: 모든 질문 해결 완료)",확률 예측에서 MSE Loss 미 사용 이유,0.024897201,0.0,0.024897200986742973
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야? (남은 답변: PEFT 방법 5가지),BCE Loss 설명,0.038585514,0.0,0.038585513830184937
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야? (남은 답변: PEFT 방법 5가지),LLM Fine-Tuning 의 PEFT,0.043984234,0.0,0.04398423433303833
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야? (남은 답변: PEFT 방법 5가지),LoRA,0.22883616,0.0,0.22883616387844086
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야? (남은 답변: PEFT 방법 5가지),LoRA 와 QLoRA 의 차이,0.010721619,0.0,0.010721619240939617
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야? (남은 답변: PEFT 방법 5가지),Loss Function 관련 실무 경험,-0.102380104,0.0,0.10238010436296463
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야? (남은 답변: PEFT 방법 5가지),Loss Function 예시,-0.030526428,0.0,0.030526427552103996
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야? (남은 답변: PEFT 방법 5가지),Loss Function 정의,-0.012489746,0.0,0.012489746324717999
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야? (남은 답변: PEFT 방법 5가지),MBTI,-0.038797792,0.0,0.03879779204726219
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야? (남은 답변: PEFT 방법 5가지),MSE Loss 설명,-0.042676087,0.0,0.042676087468862534
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야? (남은 답변: PEFT 방법 5가지),MSE Loss 용도,0.037308145,0.0,0.03730814531445503
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야? (남은 답변: PEFT 방법 5가지),"Multi-Class, Multi-Label 중 BCE 가 좋은 task",0.0067685824,0.0,0.006768582388758659
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야? (남은 답변: PEFT 방법 5가지),Multi-Label 에서 CE + Softmax 적용 문제점,0.009219262,0.0,0.009219261817634106
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야? (남은 답변: PEFT 방법 5가지),PEFT 방법 5가지,0.86141366,1.0,0.13858634233474731
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야? (남은 답변: PEFT 방법 5가지),거대 언어 모델 정의,-0.03769033,0.0,0.03769033029675484
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야? (남은 답변: PEFT 방법 5가지),마지막 할 말,0.0048171226,0.0,0.004817122593522072
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야? (남은 답변: PEFT 방법 5가지),면접 시작 인사,0.030068943,0.0,0.03006894327700138
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야? (남은 답변: PEFT 방법 5가지),면접 종료,0.01988351,0.0,0.01988350972533226
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야? (남은 답변: PEFT 방법 5가지),"인공지능, 머신러닝, 딥러닝 차이",0.0011385096,0.0,0.0011385096004232764
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야? (남은 답변: PEFT 방법 5가지),잠시 휴식,0.031355,0.0,0.03135500103235245
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야? (남은 답변: PEFT 방법 5가지),좋아하는 아이돌,-0.025610082,0.0,0.02561008185148239
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야? (남은 답변: PEFT 방법 5가지),확률 예측에서 MSE Loss 미 사용 이유,0.013504676,0.0,0.013504675589501858
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야! (남은 답변: 모든 질문 해결 완료),BCE Loss 설명,0.07017335,0.0,0.07017335295677185
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야! (남은 답변: 모든 질문 해결 완료),LLM Fine-Tuning 의 PEFT,-0.011928652,0.0,0.011928652413189411
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야! (남은 답변: 모든 질문 해결 완료),LoRA,0.3432037,0.0,0.34320369362831116
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야! (남은 답변: 모든 질문 해결 완료),LoRA 와 QLoRA 의 차이,0.66038686,1.0,0.339613139629364
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야! (남은 답변: 모든 질문 해결 완료),Loss Function 관련 실무 경험,0.03127366,0.0,0.03127365931868553
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야! (남은 답변: 모든 질문 해결 완료),Loss Function 예시,0.023445077,0.0,0.02344507724046707
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야! (남은 답변: 모든 질문 해결 완료),Loss Function 정의,0.028946988,0.0,0.02894698828458786
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야! (남은 답변: 모든 질문 해결 완료),MBTI,-0.060119912,0.0,0.06011991202831268
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야! (남은 답변: 모든 질문 해결 완료),MSE Loss 설명,-0.057594664,0.0,0.0575946643948555
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야! (남은 답변: 모든 질문 해결 완료),MSE Loss 용도,-0.047494866,0.0,0.04749486595392227
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야! (남은 답변: 모든 질문 해결 완료),"Multi-Class, Multi-Label 중 BCE 가 좋은 task",-0.008938333,0.0,0.00893833301961422
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야! (남은 답변: 모든 질문 해결 완료),Multi-Label 에서 CE + Softmax 적용 문제점,0.08452926,0.0,0.08452925831079483
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야! (남은 답변: 모든 질문 해결 완료),PEFT 방법 5가지,-0.09693648,0.0,0.09693647921085358
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야! (남은 답변: 모든 질문 해결 완료),거대 언어 모델 정의,-0.033179056,0.0,0.03317905589938164
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야! (남은 답변: 모든 질문 해결 완료),마지막 할 말,0.08833832,0.0,0.08833832293748856
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야! (남은 답변: 모든 질문 해결 완료),면접 시작 인사,-0.004776889,0.0,0.004776888992637396
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야! (남은 답변: 모든 질문 해결 완료),면접 종료,-0.09019631,0.0,0.09019631147384644
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야! (남은 답변: 모든 질문 해결 완료),"인공지능, 머신러닝, 딥러닝 차이",-0.047316622,0.0,0.047316621989011765
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야! (남은 답변: 모든 질문 해결 완료),잠시 휴식,-0.15801537,0.0,0.15801537036895752
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야! (남은 답변: 모든 질문 해결 완료),좋아하는 아이돌,0.03621733,0.0,0.036217328161001205
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야! (남은 답변: 모든 질문 해결 완료),확률 예측에서 MSE Loss 미 사용 이유,-0.052498877,0.0,0.05249887704849243
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아? (남은 답변: LoRA),BCE Loss 설명,-0.02448579,0.0,0.024485789239406586
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아? (남은 답변: LoRA),LLM Fine-Tuning 의 PEFT,0.0021210941,0.0,0.002121094148606062
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아? (남은 답변: LoRA),LoRA,0.7333231,1.0,0.2666769027709961
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아? (남은 답변: LoRA),LoRA 와 QLoRA 의 차이,0.2338456,0.0,0.23384560644626617
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아? (남은 답변: LoRA),Loss Function 관련 실무 경험,-0.1078665,0.0,0.10786650329828262
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아? (남은 답변: LoRA),Loss Function 예시,-0.011577194,0.0,0.01157719362527132
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아? (남은 답변: LoRA),Loss Function 정의,-0.010566023,0.0,0.01056602317839861
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아? (남은 답변: LoRA),MBTI,-0.042627543,0.0,0.042627543210983276
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아? (남은 답변: LoRA),MSE Loss 설명,-0.050832387,0.0,0.05083238705992699
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아? (남은 답변: LoRA),MSE Loss 용도,-0.031153265,0.0,0.03115326538681984
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아? (남은 답변: LoRA),"Multi-Class, Multi-Label 중 BCE 가 좋은 task",0.041333064,0.0,0.04133306443691254
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아? (남은 답변: LoRA),Multi-Label 에서 CE + Softmax 적용 문제점,0.034009855,0.0,0.03400985524058342
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아? (남은 답변: LoRA),PEFT 방법 5가지,0.048209585,0.0,0.04820958524942398
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아? (남은 답변: LoRA),거대 언어 모델 정의,0.017381897,0.0,0.017381897196173668
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아? (남은 답변: LoRA),마지막 할 말,0.024924433,0.0,0.02492443285882473
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아? (남은 답변: LoRA),면접 시작 인사,-0.030452568,0.0,0.03045256808400154
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아? (남은 답변: LoRA),면접 종료,0.09201609,0.0,0.09201609343290329
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아? (남은 답변: LoRA),"인공지능, 머신러닝, 딥러닝 차이",-0.00082955405,0.0,0.0008295540465041995
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아? (남은 답변: LoRA),잠시 휴식,0.037322074,0.0,0.03732207417488098
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아? (남은 답변: LoRA),좋아하는 아이돌,0.009004506,0.0,0.009004506282508373
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아? (남은 답변: LoRA),확률 예측에서 MSE Loss 미 사용 이유,0.034243096,0.0,0.03424309566617012
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야! (남은 답변: 모든 질문 해결 완료),BCE Loss 설명,0.1442296,0.0,0.14422960579395294
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야! (남은 답변: 모든 질문 해결 완료),LLM Fine-Tuning 의 PEFT,-0.00642398,0.0,0.006423979997634888
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야! (남은 답변: 모든 질문 해결 완료),LoRA,-0.11253693,0.0,0.11253692954778671
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야! (남은 답변: 모든 질문 해결 완료),LoRA 와 QLoRA 의 차이,0.06294729,0.0,0.06294728815555573
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야! (남은 답변: 모든 질문 해결 완료),Loss Function 관련 실무 경험,0.18520182,0.0,0.18520182371139526
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야! (남은 답변: 모든 질문 해결 완료),Loss Function 예시,0.04606905,0.0,0.04606904834508896
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야! (남은 답변: 모든 질문 해결 완료),Loss Function 정의,0.030470312,0.0,0.030470311641693115
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야! (남은 답변: 모든 질문 해결 완료),MBTI,0.10945865,0.0,0.10945864766836166
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야! (남은 답변: 모든 질문 해결 완료),MSE Loss 설명,-0.045179237,0.0,0.04517923668026924
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야! (남은 답변: 모든 질문 해결 완료),MSE Loss 용도,-0.040570408,0.0,0.04057040810585022
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야! (남은 답변: 모든 질문 해결 완료),"Multi-Class, Multi-Label 중 BCE 가 좋은 task",0.03007684,0.0,0.03007684089243412
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야! (남은 답변: 모든 질문 해결 완료),Multi-Label 에서 CE + Softmax 적용 문제점,0.01243143,0.0,0.012431429699063301
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야! (남은 답변: 모든 질문 해결 완료),PEFT 방법 5가지,-0.0002406217,0.0,0.00024062169541139156
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야! (남은 답변: 모든 질문 해결 완료),거대 언어 모델 정의,0.08356319,0.0,0.0835631936788559
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야! (남은 답변: 모든 질문 해결 완료),마지막 할 말,0.22659457,1.0,0.7734054327011108
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야! (남은 답변: 모든 질문 해결 완료),면접 시작 인사,0.048579607,0.0,0.04857960715889931
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야! (남은 답변: 모든 질문 해결 완료),면접 종료,0.02670412,0.0,0.02670411951839924
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야! (남은 답변: 모든 질문 해결 완료),"인공지능, 머신러닝, 딥러닝 차이",-0.03913921,0.0,0.03913921117782593
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야! (남은 답변: 모든 질문 해결 완료),잠시 휴식,0.060512204,0.0,0.060512203723192215
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야! (남은 답변: 모든 질문 해결 완료),좋아하는 아이돌,0.16397554,0.0,0.1639755368232727
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야! (남은 답변: 모든 질문 해결 완료),확률 예측에서 MSE Loss 미 사용 이유,-0.07888247,0.0,0.07888247072696686
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데 (남은 답변: LoRA 와 QLoRA 의 차이),BCE Loss 설명,-0.10988489,0.0,0.10988488793373108
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데 (남은 답변: LoRA 와 QLoRA 의 차이),LLM Fine-Tuning 의 PEFT,-0.031309884,0.0,0.031309884041547775
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데 (남은 답변: LoRA 와 QLoRA 의 차이),LoRA,0.18934038,0.0,0.18934038281440735
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데 (남은 답변: LoRA 와 QLoRA 의 차이),LoRA 와 QLoRA 의 차이,0.75802255,1.0,0.24197745323181152
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데 (남은 답변: LoRA 와 QLoRA 의 차이),Loss Function 관련 실무 경험,0.06284564,0.0,0.0628456398844719
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데 (남은 답변: LoRA 와 QLoRA 의 차이),Loss Function 예시,-0.016633037,0.0,0.016633037477731705
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데 (남은 답변: LoRA 와 QLoRA 의 차이),Loss Function 정의,-0.012243426,0.0,0.012243426404893398
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데 (남은 답변: LoRA 와 QLoRA 의 차이),MBTI,0.019722577,0.0,0.019722577184438705
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데 (남은 답변: LoRA 와 QLoRA 의 차이),MSE Loss 설명,-0.00730645,0.0,0.007306450046598911
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데 (남은 답변: LoRA 와 QLoRA 의 차이),MSE Loss 용도,0.022349052,0.0,0.022349052131175995
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데 (남은 답변: LoRA 와 QLoRA 의 차이),"Multi-Class, Multi-Label 중 BCE 가 좋은 task",0.028887577,0.0,0.02888757735490799
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데 (남은 답변: LoRA 와 QLoRA 의 차이),Multi-Label 에서 CE + Softmax 적용 문제점,0.08224083,0.0,0.08224082738161087
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데 (남은 답변: LoRA 와 QLoRA 의 차이),PEFT 방법 5가지,-0.040254917,0.0,0.04025491699576378
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데 (남은 답변: LoRA 와 QLoRA 의 차이),거대 언어 모델 정의,-0.025254954,0.0,0.025254953652620316
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데 (남은 답변: LoRA 와 QLoRA 의 차이),마지막 할 말,0.070978515,0.0,0.07097851485013962
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데 (남은 답변: LoRA 와 QLoRA 의 차이),면접 시작 인사,0.009832686,0.0,0.009832685813307762
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데 (남은 답변: LoRA 와 QLoRA 의 차이),면접 종료,-0.038345575,0.0,0.0383455753326416
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데 (남은 답변: LoRA 와 QLoRA 의 차이),"인공지능, 머신러닝, 딥러닝 차이",-0.0010023508,0.0,0.0010023508220911026
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데 (남은 답변: LoRA 와 QLoRA 의 차이),잠시 휴식,-0.030469188,0.0,0.03046918846666813
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데 (남은 답변: LoRA 와 QLoRA 의 차이),좋아하는 아이돌,0.022019638,0.0,0.022019637748599052
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데 (남은 답변: LoRA 와 QLoRA 의 차이),확률 예측에서 MSE Loss 미 사용 이유,-0.023654949,0.0,0.02365494892001152
마지막 할 말 -> 로라야 정말 고마워! (남은 답변: 마지막 할 말),BCE Loss 설명,-0.06394425,0.0,0.0639442503452301
마지막 할 말 -> 로라야 정말 고마워! (남은 답변: 마지막 할 말),LLM Fine-Tuning 의 PEFT,0.009062077,0.0,0.009062076918780804
마지막 할 말 -> 로라야 정말 고마워! (남은 답변: 마지막 할 말),LoRA,0.12438836,0.0,0.12438835948705673
마지막 할 말 -> 로라야 정말 고마워! (남은 답변: 마지막 할 말),LoRA 와 QLoRA 의 차이,-0.025466423,0.0,0.025466423481702805
마지막 할 말 -> 로라야 정말 고마워! (남은 답변: 마지막 할 말),Loss Function 관련 실무 경험,-0.0077970796,0.0,0.007797079626470804
마지막 할 말 -> 로라야 정말 고마워! (남은 답변: 마지막 할 말),Loss Function 예시,0.04810547,0.0,0.04810547083616257
마지막 할 말 -> 로라야 정말 고마워! (남은 답변: 마지막 할 말),Loss Function 정의,0.041636966,0.0,0.04163696616888046
마지막 할 말 -> 로라야 정말 고마워! (남은 답변: 마지막 할 말),MBTI,0.028679792,0.0,0.02867979183793068
마지막 할 말 -> 로라야 정말 고마워! (남은 답변: 마지막 할 말),MSE Loss 설명,0.035166133,0.0,0.03516613319516182
마지막 할 말 -> 로라야 정말 고마워! (남은 답변: 마지막 할 말),MSE Loss 용도,0.048547547,0.0,0.048547547310590744
마지막 할 말 -> 로라야 정말 고마워! (남은 답변: 마지막 할 말),"Multi-Class, Multi-Label 중 BCE 가 좋은 task",0.012450402,0.0,0.012450401671230793
마지막 할 말 -> 로라야 정말 고마워! (남은 답변: 마지막 할 말),Multi-Label 에서 CE + Softmax 적용 문제점,0.01731134,0.0,0.0173113401979208
마지막 할 말 -> 로라야 정말 고마워! (남은 답변: 마지막 할 말),PEFT 방법 5가지,-0.030531352,0.0,0.030531352385878563
마지막 할 말 -> 로라야 정말 고마워! (남은 답변: 마지막 할 말),거대 언어 모델 정의,-0.018729988,0.0,0.018729988485574722
마지막 할 말 -> 로라야 정말 고마워! (남은 답변: 마지막 할 말),마지막 할 말,-0.04890272,0.0,0.0489027202129364
마지막 할 말 -> 로라야 정말 고마워! (남은 답변: 마지막 할 말),면접 시작 인사,0.09122193,0.0,0.09122192859649658
마지막 할 말 -> 로라야 정말 고마워! (남은 답변: 마지막 할 말),면접 종료,0.47213033,1.0,0.5278696715831757
마지막 할 말 -> 로라야 정말 고마워! (남은 답변: 마지막 할 말),"인공지능, 머신러닝, 딥러닝 차이",-0.025994018,0.0,0.025994017720222473
마지막 할 말 -> 로라야 정말 고마워! (남은 답변: 마지막 할 말),잠시 휴식,0.2922539,0.0,0.29225391149520874
마지막 할 말 -> 로라야 정말 고마워! (남은 답변: 마지막 할 말),좋아하는 아이돌,-0.022219708,0.0,0.022219708189368248
마지막 할 말 -> 로라야 정말 고마워! (남은 답변: 마지막 할 말),확률 예측에서 MSE Loss 미 사용 이유,0.025966842,0.0,0.025966841727495193
마지막 할 말 -> 로라야 사랑해 (남은 답변: 마지막 할 말),BCE Loss 설명,-0.032459844,0.0,0.032459843903779984
마지막 할 말 -> 로라야 사랑해 (남은 답변: 마지막 할 말),LLM Fine-Tuning 의 PEFT,0.027192732,0.0,0.0271927323192358
마지막 할 말 -> 로라야 사랑해 (남은 답변: 마지막 할 말),LoRA,0.12788853,0.0,0.1278885304927826
마지막 할 말 -> 로라야 사랑해 (남은 답변: 마지막 할 말),LoRA 와 QLoRA 의 차이,-0.035093453,0.0,0.03509345278143883
마지막 할 말 -> 로라야 사랑해 (남은 답변: 마지막 할 말),Loss Function 관련 실무 경험,-0.021579098,0.0,0.02157909795641899
마지막 할 말 -> 로라야 사랑해 (남은 답변: 마지막 할 말),Loss Function 예시,0.04624202,0.0,0.04624202102422714
마지막 할 말 -> 로라야 사랑해 (남은 답변: 마지막 할 말),Loss Function 정의,0.037990168,0.0,0.03799016773700714
마지막 할 말 -> 로라야 사랑해 (남은 답변: 마지막 할 말),MBTI,0.027873004,0.0,0.027873003855347633
마지막 할 말 -> 로라야 사랑해 (남은 답변: 마지막 할 말),MSE Loss 설명,0.03798897,0.0,0.037988968193531036
마지막 할 말 -> 로라야 사랑해 (남은 답변: 마지막 할 말),MSE Loss 용도,0.051465634,0.0,0.051465634256601334
마지막 할 말 -> 로라야 사랑해 (남은 답변: 마지막 할 말),"Multi-Class, Multi-Label 중 BCE 가 좋은 task",0.020320754,0.0,0.020320754498243332
마지막 할 말 -> 로라야 사랑해 (남은 답변: 마지막 할 말),Multi-Label 에서 CE + Softmax 적용 문제점,0.027941497,0.0,0.027941497042775154
마지막 할 말 -> 로라야 사랑해 (남은 답변: 마지막 할 말),PEFT 방법 5가지,-0.010370339,0.0,0.010370339266955853
마지막 할 말 -> 로라야 사랑해 (남은 답변: 마지막 할 말),거대 언어 모델 정의,-0.010598163,0.0,0.01059816312044859
마지막 할 말 -> 로라야 사랑해 (남은 답변: 마지막 할 말),마지막 할 말,-0.038259495,0.0,0.03825949504971504
마지막 할 말 -> 로라야 사랑해 (남은 답변: 마지막 할 말),면접 시작 인사,0.08792164,0.0,0.087921641767025
마지막 할 말 -> 로라야 사랑해 (남은 답변: 마지막 할 말),면접 종료,0.491805,1.0,0.5081950128078461
마지막 할 말 -> 로라야 사랑해 (남은 답변: 마지막 할 말),"인공지능, 머신러닝, 딥러닝 차이",-0.027988324,0.0,0.02798832394182682
마지막 할 말 -> 로라야 사랑해 (남은 답변: 마지막 할 말),잠시 휴식,0.28517827,0.0,0.2851782739162445
마지막 할 말 -> 로라야 사랑해 (남은 답변: 마지막 할 말),좋아하는 아이돌,-0.021792756,0.0,0.021792756393551826
마지막 할 말 -> 로라야 사랑해 (남은 답변: 마지막 할 말),확률 예측에서 MSE Loss 미 사용 이유,0.05271438,0.0,0.052714381366968155
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어 (남은 답변: 마지막 할 말),BCE Loss 설명,-0.032500654,0.0,0.032500654458999634
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어 (남은 답변: 마지막 할 말),LLM Fine-Tuning 의 PEFT,0.040680252,0.0,0.04068025201559067
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어 (남은 답변: 마지막 할 말),LoRA,0.13424088,0.0,0.13424088060855865
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어 (남은 답변: 마지막 할 말),LoRA 와 QLoRA 의 차이,-0.0015170628,0.0,0.0015170627739280462
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어 (남은 답변: 마지막 할 말),Loss Function 관련 실무 경험,-0.02085473,0.0,0.020854730159044266
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어 (남은 답변: 마지막 할 말),Loss Function 예시,0.018201007,0.0,0.018201006576418877
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어 (남은 답변: 마지막 할 말),Loss Function 정의,0.012001128,0.0,0.012001127935945988
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어 (남은 답변: 마지막 할 말),MBTI,0.010610763,0.0,0.010610762983560562
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어 (남은 답변: 마지막 할 말),MSE Loss 설명,0.024564745,0.0,0.024564744904637337
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어 (남은 답변: 마지막 할 말),MSE Loss 용도,0.042839438,0.0,0.04283943772315979
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어 (남은 답변: 마지막 할 말),"Multi-Class, Multi-Label 중 BCE 가 좋은 task",0.0019630922,0.0,0.0019630922470241785
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어 (남은 답변: 마지막 할 말),Multi-Label 에서 CE + Softmax 적용 문제점,0.0016842706,0.0,0.00168427056632936
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어 (남은 답변: 마지막 할 말),PEFT 방법 5가지,-0.032658145,0.0,0.03265814483165741
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어 (남은 답변: 마지막 할 말),거대 언어 모델 정의,-0.05253377,0.0,0.052533771842718124
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어 (남은 답변: 마지막 할 말),마지막 할 말,-0.060708955,0.0,0.06070895493030548
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어 (남은 답변: 마지막 할 말),면접 시작 인사,0.07069218,0.0,0.07069218158721924
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어 (남은 답변: 마지막 할 말),면접 종료,0.4611166,1.0,0.53888338804245
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어 (남은 답변: 마지막 할 말),"인공지능, 머신러닝, 딥러닝 차이",-0.0020255966,0.0,0.0020255965646356344
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어 (남은 답변: 마지막 할 말),잠시 휴식,0.26635444,0.0,0.26635444164276123
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어 (남은 답변: 마지막 할 말),좋아하는 아이돌,-0.039218713,0.0,0.0392187125980854
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어 (남은 답변: 마지막 할 말),확률 예측에서 MSE Loss 미 사용 이유,0.03522004,0.0,0.03522003814578056
