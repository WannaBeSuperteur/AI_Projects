input_part,next_question,predicted_similarity,ground_truth_similarity,absolute_error
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서? (남은 답변: 면접 시작 인사),BCE Loss 설명,0.013599948,0.0,0.013599948026239872
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서? (남은 답변: 면접 시작 인사),LLM Fine-Tuning 의 PEFT,-0.02381515,0.0,0.023815149441361427
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서? (남은 답변: 면접 시작 인사),LoRA,0.010295794,0.0,0.01029579434543848
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서? (남은 답변: 면접 시작 인사),LoRA 와 QLoRA 의 차이,-0.012755539,0.0,0.012755539268255234
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서? (남은 답변: 면접 시작 인사),Loss Function 관련 실무 경험,-0.010892223,0.0,0.010892222635447979
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서? (남은 답변: 면접 시작 인사),Loss Function 예시,0.011330753,0.0,0.01133075263351202
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서? (남은 답변: 면접 시작 인사),Loss Function 정의,-0.022745758,0.0,0.022745758295059204
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서? (남은 답변: 면접 시작 인사),MBTI / 좋아하는 아이돌,0.003625312,0.0,0.003625311888754368
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서? (남은 답변: 면접 시작 인사),MSE Loss 설명,0.00038807132,0.0,0.00038807131932117045
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서? (남은 답변: 면접 시작 인사),MSE Loss 용도,0.034469552,0.0,0.03446955233812332
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서? (남은 답변: 면접 시작 인사),"Multi-Class, Multi-Label 중 BCE 가 좋은 task",-0.0069994326,0.0,0.006999432574957609
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서? (남은 답변: 면접 시작 인사),Multi-Label 에서 CE + Softmax 적용 문제점,-0.022389675,0.0,0.022389674559235573
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서? (남은 답변: 면접 시작 인사),PEFT 방법 5가지,-0.009615721,0.0,0.009615721181035042
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서? (남은 답변: 면접 시작 인사),거대 언어 모델 정의,-0.019940022,0.0,0.019940022379159927
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서? (남은 답변: 면접 시작 인사),마지막 할 말,0.0075349505,0.0,0.007534950505942106
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서? (남은 답변: 면접 시작 인사),면접 시작 인사,0.912318,1.0,0.08768200874328613
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서? (남은 답변: 면접 시작 인사),면접 종료,-0.06465888,0.0,0.06465888023376465
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서? (남은 답변: 면접 시작 인사),"인공지능, 머신러닝, 딥러닝 차이",0.031876124,0.0,0.03187612444162369
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서? (남은 답변: 면접 시작 인사),잠시 휴식,-0.014209698,0.0,0.01420969795435667
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서? (남은 답변: 면접 시작 인사),확률 예측에서 MSE Loss 미 사용 이유,-0.008523142,0.0,0.008523141965270042
면접 시작 인사 -> 로라야 안녕 정말 반가워 (남은 답변: 면접 시작 인사),BCE Loss 설명,-0.0091229165,0.0,0.009122916497290134
면접 시작 인사 -> 로라야 안녕 정말 반가워 (남은 답변: 면접 시작 인사),LLM Fine-Tuning 의 PEFT,-0.017324856,0.0,0.01732485555112362
면접 시작 인사 -> 로라야 안녕 정말 반가워 (남은 답변: 면접 시작 인사),LoRA,0.04478075,0.0,0.04478074982762337
면접 시작 인사 -> 로라야 안녕 정말 반가워 (남은 답변: 면접 시작 인사),LoRA 와 QLoRA 의 차이,0.0058503617,0.0,0.005850361660122871
면접 시작 인사 -> 로라야 안녕 정말 반가워 (남은 답변: 면접 시작 인사),Loss Function 관련 실무 경험,-0.0048638713,0.0,0.00486387126147747
면접 시작 인사 -> 로라야 안녕 정말 반가워 (남은 답변: 면접 시작 인사),Loss Function 예시,-0.017448248,0.0,0.017448248341679573
면접 시작 인사 -> 로라야 안녕 정말 반가워 (남은 답변: 면접 시작 인사),Loss Function 정의,-0.00415125,0.0,0.004151249770075083
면접 시작 인사 -> 로라야 안녕 정말 반가워 (남은 답변: 면접 시작 인사),MBTI / 좋아하는 아이돌,-0.014417385,0.0,0.014417384751141071
면접 시작 인사 -> 로라야 안녕 정말 반가워 (남은 답변: 면접 시작 인사),MSE Loss 설명,-0.0066305716,0.0,0.006630571559071541
면접 시작 인사 -> 로라야 안녕 정말 반가워 (남은 답변: 면접 시작 인사),MSE Loss 용도,0.019761821,0.0,0.019761821255087852
면접 시작 인사 -> 로라야 안녕 정말 반가워 (남은 답변: 면접 시작 인사),"Multi-Class, Multi-Label 중 BCE 가 좋은 task",0.010120752,0.0,0.010120752267539501
면접 시작 인사 -> 로라야 안녕 정말 반가워 (남은 답변: 면접 시작 인사),Multi-Label 에서 CE + Softmax 적용 문제점,-0.022984445,0.0,0.022984445095062256
면접 시작 인사 -> 로라야 안녕 정말 반가워 (남은 답변: 면접 시작 인사),PEFT 방법 5가지,-0.024598759,0.0,0.024598758667707443
면접 시작 인사 -> 로라야 안녕 정말 반가워 (남은 답변: 면접 시작 인사),거대 언어 모델 정의,0.014573203,0.0,0.014573203399777412
면접 시작 인사 -> 로라야 안녕 정말 반가워 (남은 답변: 면접 시작 인사),마지막 할 말,-0.00078918564,0.0,0.0007891856366768479
면접 시작 인사 -> 로라야 안녕 정말 반가워 (남은 답변: 면접 시작 인사),면접 시작 인사,0.88715875,1.0,0.11284124851226807
면접 시작 인사 -> 로라야 안녕 정말 반가워 (남은 답변: 면접 시작 인사),면접 종료,-0.014287149,0.0,0.014287148602306843
면접 시작 인사 -> 로라야 안녕 정말 반가워 (남은 답변: 면접 시작 인사),"인공지능, 머신러닝, 딥러닝 차이",-0.032900177,0.0,0.03290017694234848
면접 시작 인사 -> 로라야 안녕 정말 반가워 (남은 답변: 면접 시작 인사),잠시 휴식,0.012188215,0.0,0.012188214808702469
면접 시작 인사 -> 로라야 안녕 정말 반가워 (남은 답변: 면접 시작 인사),확률 예측에서 MSE Loss 미 사용 이유,-0.002385874,0.0,0.002385874046012759
면접 시작 인사 -> 로라야 그럼 네가 면접관이야? (남은 답변: 면접 시작 인사),BCE Loss 설명,-0.00036201297,0.0,0.0003620129718910903
면접 시작 인사 -> 로라야 그럼 네가 면접관이야? (남은 답변: 면접 시작 인사),LLM Fine-Tuning 의 PEFT,-0.015038448,0.0,0.015038448385894299
면접 시작 인사 -> 로라야 그럼 네가 면접관이야? (남은 답변: 면접 시작 인사),LoRA,0.009420916,0.0,0.009420916438102722
면접 시작 인사 -> 로라야 그럼 네가 면접관이야? (남은 답변: 면접 시작 인사),LoRA 와 QLoRA 의 차이,-0.0014037607,0.0,0.0014037607470527291
면접 시작 인사 -> 로라야 그럼 네가 면접관이야? (남은 답변: 면접 시작 인사),Loss Function 관련 실무 경험,-0.018660985,0.0,0.018660984933376312
면접 시작 인사 -> 로라야 그럼 네가 면접관이야? (남은 답변: 면접 시작 인사),Loss Function 예시,0.002069987,0.0,0.002069986891001463
면접 시작 인사 -> 로라야 그럼 네가 면접관이야? (남은 답변: 면접 시작 인사),Loss Function 정의,-0.0270448,0.0,0.02704479917883873
면접 시작 인사 -> 로라야 그럼 네가 면접관이야? (남은 답변: 면접 시작 인사),MBTI / 좋아하는 아이돌,-0.021039672,0.0,0.021039672195911407
면접 시작 인사 -> 로라야 그럼 네가 면접관이야? (남은 답변: 면접 시작 인사),MSE Loss 설명,-0.015579791,0.0,0.01557979080826044
면접 시작 인사 -> 로라야 그럼 네가 면접관이야? (남은 답변: 면접 시작 인사),MSE Loss 용도,0.015446707,0.0,0.015446706674993038
면접 시작 인사 -> 로라야 그럼 네가 면접관이야? (남은 답변: 면접 시작 인사),"Multi-Class, Multi-Label 중 BCE 가 좋은 task",-0.021161968,0.0,0.021161967888474464
면접 시작 인사 -> 로라야 그럼 네가 면접관이야? (남은 답변: 면접 시작 인사),Multi-Label 에서 CE + Softmax 적용 문제점,-0.01997729,0.0,0.01997729018330574
면접 시작 인사 -> 로라야 그럼 네가 면접관이야? (남은 답변: 면접 시작 인사),PEFT 방법 5가지,-0.030011516,0.0,0.03001151606440544
면접 시작 인사 -> 로라야 그럼 네가 면접관이야? (남은 답변: 면접 시작 인사),거대 언어 모델 정의,-0.01641616,0.0,0.016416160389780998
면접 시작 인사 -> 로라야 그럼 네가 면접관이야? (남은 답변: 면접 시작 인사),마지막 할 말,0.013388264,0.0,0.013388263992965221
면접 시작 인사 -> 로라야 그럼 네가 면접관이야? (남은 답변: 면접 시작 인사),면접 시작 인사,0.8701027,1.0,0.12989729642868042
면접 시작 인사 -> 로라야 그럼 네가 면접관이야? (남은 답변: 면접 시작 인사),면접 종료,-0.1087776,0.0,0.10877759754657745
면접 시작 인사 -> 로라야 그럼 네가 면접관이야? (남은 답변: 면접 시작 인사),"인공지능, 머신러닝, 딥러닝 차이",0.15869728,0.0,0.15869727730751038
면접 시작 인사 -> 로라야 그럼 네가 면접관이야? (남은 답변: 면접 시작 인사),잠시 휴식,0.018118335,0.0,0.01811833493411541
면접 시작 인사 -> 로라야 그럼 네가 면접관이야? (남은 답변: 면접 시작 인사),확률 예측에서 MSE Loss 미 사용 이유,-0.028913604,0.0,0.028913604095578194
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까 (남은 답변: 면접 시작 인사),BCE Loss 설명,-0.029802863,0.0,0.02980286255478859
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까 (남은 답변: 면접 시작 인사),LLM Fine-Tuning 의 PEFT,0.02899173,0.0,0.028991730883717537
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까 (남은 답변: 면접 시작 인사),LoRA,0.13500008,0.0,0.135000079870224
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까 (남은 답변: 면접 시작 인사),LoRA 와 QLoRA 의 차이,0.043274477,0.0,0.04327447712421417
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까 (남은 답변: 면접 시작 인사),Loss Function 관련 실무 경험,-0.05265249,0.0,0.05265248939394951
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까 (남은 답변: 면접 시작 인사),Loss Function 예시,-0.08135227,0.0,0.08135227113962173
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까 (남은 답변: 면접 시작 인사),Loss Function 정의,-0.012425219,0.0,0.01242521870881319
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까 (남은 답변: 면접 시작 인사),MBTI / 좋아하는 아이돌,-0.026155056,0.0,0.026155056431889534
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까 (남은 답변: 면접 시작 인사),MSE Loss 설명,-0.016980315,0.0,0.016980314627289772
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까 (남은 답변: 면접 시작 인사),MSE Loss 용도,0.0048865266,0.0,0.0048865266144275665
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까 (남은 답변: 면접 시작 인사),"Multi-Class, Multi-Label 중 BCE 가 좋은 task",-0.018670848,0.0,0.01867084763944149
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까 (남은 답변: 면접 시작 인사),Multi-Label 에서 CE + Softmax 적용 문제점,-0.016469374,0.0,0.016469374299049377
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까 (남은 답변: 면접 시작 인사),PEFT 방법 5가지,-0.025034195,0.0,0.025034194812178612
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까 (남은 답변: 면접 시작 인사),거대 언어 모델 정의,-0.038264576,0.0,0.038264576345682144
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까 (남은 답변: 면접 시작 인사),마지막 할 말,0.078778505,0.0,0.07877850532531738
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까 (남은 답변: 면접 시작 인사),면접 시작 인사,0.75354916,1.0,0.24645084142684937
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까 (남은 답변: 면접 시작 인사),면접 종료,0.06833124,0.0,0.06833124160766602
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까 (남은 답변: 면접 시작 인사),"인공지능, 머신러닝, 딥러닝 차이",0.1847721,0.0,0.18477210402488708
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까 (남은 답변: 면접 시작 인사),잠시 휴식,-0.047623858,0.0,0.047623857855796814
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까 (남은 답변: 면접 시작 인사),확률 예측에서 MSE Loss 미 사용 이유,-0.021502133,0.0,0.021502133458852768
면접 시작 인사 -> 파이팅! 시작하자 (남은 답변: 면접 시작 인사),BCE Loss 설명,0.0055271382,0.0,0.005527138244360685
면접 시작 인사 -> 파이팅! 시작하자 (남은 답변: 면접 시작 인사),LLM Fine-Tuning 의 PEFT,-0.013226364,0.0,0.013226363807916641
면접 시작 인사 -> 파이팅! 시작하자 (남은 답변: 면접 시작 인사),LoRA,-0.02456813,0.0,0.02456812933087349
면접 시작 인사 -> 파이팅! 시작하자 (남은 답변: 면접 시작 인사),LoRA 와 QLoRA 의 차이,0.020859176,0.0,0.02085917629301548
면접 시작 인사 -> 파이팅! 시작하자 (남은 답변: 면접 시작 인사),Loss Function 관련 실무 경험,-0.014439686,0.0,0.014439686201512814
면접 시작 인사 -> 파이팅! 시작하자 (남은 답변: 면접 시작 인사),Loss Function 예시,0.0038997629,0.0,0.003899762872606516
면접 시작 인사 -> 파이팅! 시작하자 (남은 답변: 면접 시작 인사),Loss Function 정의,-0.009105111,0.0,0.009105110540986061
면접 시작 인사 -> 파이팅! 시작하자 (남은 답변: 면접 시작 인사),MBTI / 좋아하는 아이돌,-0.0054544806,0.0,0.0054544806480407715
면접 시작 인사 -> 파이팅! 시작하자 (남은 답변: 면접 시작 인사),MSE Loss 설명,-0.01328851,0.0,0.013288510031998158
면접 시작 인사 -> 파이팅! 시작하자 (남은 답변: 면접 시작 인사),MSE Loss 용도,-0.016601369,0.0,0.01660136878490448
면접 시작 인사 -> 파이팅! 시작하자 (남은 답변: 면접 시작 인사),"Multi-Class, Multi-Label 중 BCE 가 좋은 task",-0.011832664,0.0,0.011832663789391518
면접 시작 인사 -> 파이팅! 시작하자 (남은 답변: 면접 시작 인사),Multi-Label 에서 CE + Softmax 적용 문제점,-0.013094097,0.0,0.013094097375869751
면접 시작 인사 -> 파이팅! 시작하자 (남은 답변: 면접 시작 인사),PEFT 방법 5가지,0.006442627,0.0,0.006442626938223839
면접 시작 인사 -> 파이팅! 시작하자 (남은 답변: 면접 시작 인사),거대 언어 모델 정의,-0.03265981,0.0,0.03265981003642082
면접 시작 인사 -> 파이팅! 시작하자 (남은 답변: 면접 시작 인사),마지막 할 말,-0.0329592,0.0,0.03295920044183731
면접 시작 인사 -> 파이팅! 시작하자 (남은 답변: 면접 시작 인사),면접 시작 인사,0.056916606,0.0,0.056916605681180954
면접 시작 인사 -> 파이팅! 시작하자 (남은 답변: 면접 시작 인사),면접 종료,-0.010642123,0.0,0.01064212340861559
면접 시작 인사 -> 파이팅! 시작하자 (남은 답변: 면접 시작 인사),"인공지능, 머신러닝, 딥러닝 차이",0.91502666,1.0,0.08497333526611328
면접 시작 인사 -> 파이팅! 시작하자 (남은 답변: 면접 시작 인사),잠시 휴식,-0.024389258,0.0,0.02438925765454769
면접 시작 인사 -> 파이팅! 시작하자 (남은 답변: 면접 시작 인사),확률 예측에서 MSE Loss 미 사용 이유,-0.025637344,0.0,0.025637343525886536
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데 (남은 답변: 인공지능, 머신러닝, 딥러닝)",BCE Loss 설명,-0.004036339,0.0,0.004036338999867439
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데 (남은 답변: 인공지능, 머신러닝, 딥러닝)",LLM Fine-Tuning 의 PEFT,-0.0030315868,0.0,0.0030315867625176907
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데 (남은 답변: 인공지능, 머신러닝, 딥러닝)",LoRA,-0.0038259244,0.0,0.0038259243592619896
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데 (남은 답변: 인공지능, 머신러닝, 딥러닝)",LoRA 와 QLoRA 의 차이,0.009400498,0.0,0.009400498121976852
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데 (남은 답변: 인공지능, 머신러닝, 딥러닝)",Loss Function 관련 실무 경험,0.006827329,0.0,0.006827328819781542
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데 (남은 답변: 인공지능, 머신러닝, 딥러닝)",Loss Function 예시,-0.007279407,0.0,0.007279406767338514
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데 (남은 답변: 인공지능, 머신러닝, 딥러닝)",Loss Function 정의,-0.011369766,0.0,0.011369765736162663
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데 (남은 답변: 인공지능, 머신러닝, 딥러닝)",MBTI / 좋아하는 아이돌,-0.0022716774,0.0,0.002271677367389202
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데 (남은 답변: 인공지능, 머신러닝, 딥러닝)",MSE Loss 설명,-0.0059034456,0.0,0.005903445649892092
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데 (남은 답변: 인공지능, 머신러닝, 딥러닝)",MSE Loss 용도,-0.01603663,0.0,0.016036629676818848
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데 (남은 답변: 인공지능, 머신러닝, 딥러닝)","Multi-Class, Multi-Label 중 BCE 가 좋은 task",0.0029074014,0.0,0.002907401416450739
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데 (남은 답변: 인공지능, 머신러닝, 딥러닝)",Multi-Label 에서 CE + Softmax 적용 문제점,0.007065056,0.0,0.007065055891871452
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데 (남은 답변: 인공지능, 머신러닝, 딥러닝)",PEFT 방법 5가지,-0.015856212,0.0,0.015856212005019188
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데 (남은 답변: 인공지능, 머신러닝, 딥러닝)",거대 언어 모델 정의,-0.006284283,0.0,0.006284283008426428
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데 (남은 답변: 인공지능, 머신러닝, 딥러닝)",마지막 할 말,-0.011386226,0.0,0.011386225931346416
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데 (남은 답변: 인공지능, 머신러닝, 딥러닝)",면접 시작 인사,-0.017341187,0.0,0.017341187223792076
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데 (남은 답변: 인공지능, 머신러닝, 딥러닝)",면접 종료,-0.027795585,0.0,0.027795584872364998
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데 (남은 답변: 인공지능, 머신러닝, 딥러닝)","인공지능, 머신러닝, 딥러닝 차이",0.9891119,1.0,0.010888099670410156
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데 (남은 답변: 인공지능, 머신러닝, 딥러닝)",잠시 휴식,0.001491508,0.0,0.0014915079809725285
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데 (남은 답변: 인공지능, 머신러닝, 딥러닝)",확률 예측에서 MSE Loss 미 사용 이유,-0.0052848225,0.0,0.005284822545945644
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거! (남은 답변: 머신러닝, 딥러닝)",BCE Loss 설명,0.001461857,0.0,0.0014618569985032082
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거! (남은 답변: 머신러닝, 딥러닝)",LLM Fine-Tuning 의 PEFT,0.0019591374,0.0,0.001959137385711074
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거! (남은 답변: 머신러닝, 딥러닝)",LoRA,-0.004256656,0.0,0.004256655927747488
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거! (남은 답변: 머신러닝, 딥러닝)",LoRA 와 QLoRA 의 차이,0.011584143,0.0,0.011584143154323101
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거! (남은 답변: 머신러닝, 딥러닝)",Loss Function 관련 실무 경험,-0.0037833282,0.0,0.003783328225836158
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거! (남은 답변: 머신러닝, 딥러닝)",Loss Function 예시,4.2209216e-05,0.0,4.2209216189803556e-05
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거! (남은 답변: 머신러닝, 딥러닝)",Loss Function 정의,-0.006677629,0.0,0.006677628960460424
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거! (남은 답변: 머신러닝, 딥러닝)",MBTI / 좋아하는 아이돌,-0.0027533611,0.0,0.002753361128270626
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거! (남은 답변: 머신러닝, 딥러닝)",MSE Loss 설명,-0.0011117149,0.0,0.00111171486787498
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거! (남은 답변: 머신러닝, 딥러닝)",MSE Loss 용도,-0.017089607,0.0,0.017089607194066048
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거! (남은 답변: 머신러닝, 딥러닝)","Multi-Class, Multi-Label 중 BCE 가 좋은 task",-0.0017955095,0.0,0.0017955094808712602
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거! (남은 답변: 머신러닝, 딥러닝)",Multi-Label 에서 CE + Softmax 적용 문제점,0.0028895356,0.0,0.0028895356226712465
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거! (남은 답변: 머신러닝, 딥러닝)",PEFT 방법 5가지,-0.0102170305,0.0,0.0102170305326581
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거! (남은 답변: 머신러닝, 딥러닝)",거대 언어 모델 정의,-0.0035197583,0.0,0.0035197583492845297
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거! (남은 답변: 머신러닝, 딥러닝)",마지막 할 말,-0.008952802,0.0,0.008952802047133446
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거! (남은 답변: 머신러닝, 딥러닝)",면접 시작 인사,-0.023205658,0.0,0.023205658420920372
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거! (남은 답변: 머신러닝, 딥러닝)",면접 종료,-0.018361984,0.0,0.018361983820796013
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거! (남은 답변: 머신러닝, 딥러닝)","인공지능, 머신러닝, 딥러닝 차이",0.9899963,1.0,0.01000368595123291
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거! (남은 답변: 머신러닝, 딥러닝)",잠시 휴식,-0.0004372013,0.0,0.0004372012917883694
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거! (남은 답변: 머신러닝, 딥러닝)",확률 예측에서 MSE Loss 미 사용 이유,-0.012912345,0.0,0.012912345118820667
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지 (남은 답변: 딥러닝)",BCE Loss 설명,0.0018436664,0.0,0.0018436664249747992
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지 (남은 답변: 딥러닝)",LLM Fine-Tuning 의 PEFT,-0.005974907,0.0,0.005974906962364912
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지 (남은 답변: 딥러닝)",LoRA,0.0076897168,0.0,0.007689716760069132
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지 (남은 답변: 딥러닝)",LoRA 와 QLoRA 의 차이,0.011108352,0.0,0.01110835187137127
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지 (남은 답변: 딥러닝)",Loss Function 관련 실무 경험,-0.008086293,0.0,0.008086293004453182
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지 (남은 답변: 딥러닝)",Loss Function 예시,0.0044660657,0.0,0.004466065671294928
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지 (남은 답변: 딥러닝)",Loss Function 정의,-0.005740165,0.0,0.005740164779126644
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지 (남은 답변: 딥러닝)",MBTI / 좋아하는 아이돌,-0.009089848,0.0,0.009089848026633263
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지 (남은 답변: 딥러닝)",MSE Loss 설명,-0.010912917,0.0,0.010912916623055935
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지 (남은 답변: 딥러닝)",MSE Loss 용도,-0.011299812,0.0,0.011299812234938145
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지 (남은 답변: 딥러닝)","Multi-Class, Multi-Label 중 BCE 가 좋은 task",-0.003126588,0.0,0.003126587951555848
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지 (남은 답변: 딥러닝)",Multi-Label 에서 CE + Softmax 적용 문제점,0.0004776927,0.0,0.00047769269440323114
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지 (남은 답변: 딥러닝)",PEFT 방법 5가지,-0.004125883,0.0,0.004125882871448994
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지 (남은 답변: 딥러닝)",거대 언어 모델 정의,-0.015134107,0.0,0.015134107321500778
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지 (남은 답변: 딥러닝)",마지막 할 말,-0.01117752,0.0,0.011177520267665386
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지 (남은 답변: 딥러닝)",면접 시작 인사,-0.028573774,0.0,0.02857377380132675
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지 (남은 답변: 딥러닝)",면접 종료,-0.018025985,0.0,0.01802598498761654
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지 (남은 답변: 딥러닝)","인공지능, 머신러닝, 딥러닝 차이",0.9894278,1.0,0.010572195053100586
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지 (남은 답변: 딥러닝)",잠시 휴식,-0.00765338,0.0,0.007653379812836647
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지 (남은 답변: 딥러닝)",확률 예측에서 MSE Loss 미 사용 이유,-9.992664e-05,0.0,9.992664126912132e-05
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지? (남은 답변: 인공지능, 머신러닝)",BCE Loss 설명,0.0017672706,0.0,0.0017672706162557006
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지? (남은 답변: 인공지능, 머신러닝)",LLM Fine-Tuning 의 PEFT,-0.0024815588,0.0,0.0024815588258206844
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지? (남은 답변: 인공지능, 머신러닝)",LoRA,0.00051434,0.0,0.0005143400048837066
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지? (남은 답변: 인공지능, 머신러닝)",LoRA 와 QLoRA 의 차이,0.004800166,0.0,0.004800166003406048
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지? (남은 답변: 인공지능, 머신러닝)",Loss Function 관련 실무 경험,0.0025059795,0.0,0.002505979500710964
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지? (남은 답변: 인공지능, 머신러닝)",Loss Function 예시,-0.0034348064,0.0,0.003434806363657117
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지? (남은 답변: 인공지능, 머신러닝)",Loss Function 정의,-0.007837648,0.0,0.007837647572159767
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지? (남은 답변: 인공지능, 머신러닝)",MBTI / 좋아하는 아이돌,-0.011476175,0.0,0.011476174928247929
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지? (남은 답변: 인공지능, 머신러닝)",MSE Loss 설명,-0.002071891,0.0,0.0020718909800052643
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지? (남은 답변: 인공지능, 머신러닝)",MSE Loss 용도,-0.016229294,0.0,0.0162292942404747
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지? (남은 답변: 인공지능, 머신러닝)","Multi-Class, Multi-Label 중 BCE 가 좋은 task",-0.0036529703,0.0,0.0036529703065752983
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지? (남은 답변: 인공지능, 머신러닝)",Multi-Label 에서 CE + Softmax 적용 문제점,-0.00023332151,0.0,0.0002333215088583529
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지? (남은 답변: 인공지능, 머신러닝)",PEFT 방법 5가지,-0.0120733045,0.0,0.012073304504156113
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지? (남은 답변: 인공지능, 머신러닝)",거대 언어 모델 정의,0.006801496,0.0,0.006801495794206858
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지? (남은 답변: 인공지능, 머신러닝)",마지막 할 말,-0.016677361,0.0,0.016677360981702805
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지? (남은 답변: 인공지능, 머신러닝)",면접 시작 인사,-0.020820938,0.0,0.020820938050746918
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지? (남은 답변: 인공지능, 머신러닝)",면접 종료,-0.020983307,0.0,0.02098330669105053
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지? (남은 답변: 인공지능, 머신러닝)","인공지능, 머신러닝, 딥러닝 차이",0.99105316,1.0,0.008946835994720459
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지? (남은 답변: 인공지능, 머신러닝)",잠시 휴식,-0.0077686324,0.0,0.007768632378429174
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지? (남은 답변: 인공지능, 머신러닝)",확률 예측에서 MSE Loss 미 사용 이유,-0.00095462665,0.0,0.0009546266519464552
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야 (남은 답변: 모든 질문 해결 완료)",BCE Loss 설명,0.009600367,0.0,0.009600367397069931
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야 (남은 답변: 모든 질문 해결 완료)",LLM Fine-Tuning 의 PEFT,0.0038118488,0.0,0.0038118488155305386
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야 (남은 답변: 모든 질문 해결 완료)",LoRA,0.0122533785,0.0,0.01225337851792574
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야 (남은 답변: 모든 질문 해결 완료)",LoRA 와 QLoRA 의 차이,-0.006556473,0.0,0.006556473206728697
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야 (남은 답변: 모든 질문 해결 완료)",Loss Function 관련 실무 경험,0.01992808,0.0,0.01992807909846306
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야 (남은 답변: 모든 질문 해결 완료)",Loss Function 예시,0.005890259,0.0,0.005890259053558111
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야 (남은 답변: 모든 질문 해결 완료)",Loss Function 정의,-0.0032460743,0.0,0.0032460743095725775
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야 (남은 답변: 모든 질문 해결 완료)",MBTI / 좋아하는 아이돌,-0.024482884,0.0,0.024482883512973785
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야 (남은 답변: 모든 질문 해결 완료)",MSE Loss 설명,0.009438002,0.0,0.009438001550734043
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야 (남은 답변: 모든 질문 해결 완료)",MSE Loss 용도,0.008569623,0.0,0.008569623343646526
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야 (남은 답변: 모든 질문 해결 완료)","Multi-Class, Multi-Label 중 BCE 가 좋은 task",0.00325679,0.0,0.003256790107116103
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야 (남은 답변: 모든 질문 해결 완료)",Multi-Label 에서 CE + Softmax 적용 문제점,0.001981129,0.0,0.001981128938496113
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야 (남은 답변: 모든 질문 해결 완료)",PEFT 방법 5가지,0.003988158,0.0,0.003988157957792282
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야 (남은 답변: 모든 질문 해결 완료)",거대 언어 모델 정의,0.9750839,1.0,0.024916112422943115
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야 (남은 답변: 모든 질문 해결 완료)",마지막 할 말,-0.014330568,0.0,0.014330567792057991
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야 (남은 답변: 모든 질문 해결 완료)",면접 시작 인사,-0.010134591,0.0,0.010134590789675713
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야 (남은 답변: 모든 질문 해결 완료)",면접 종료,-0.012079578,0.0,0.012079577893018723
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야 (남은 답변: 모든 질문 해결 완료)","인공지능, 머신러닝, 딥러닝 차이",0.0014114606,0.0,0.0014114605728536844
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야 (남은 답변: 모든 질문 해결 완료)",잠시 휴식,-0.02922771,0.0,0.029227709397673607
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야 (남은 답변: 모든 질문 해결 완료)",확률 예측에서 MSE Loss 미 사용 이유,-0.008133386,0.0,0.008133386261761189
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야? (남은 답변: 머신러닝)",BCE Loss 설명,-0.004648861,0.0,0.004648861009627581
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야? (남은 답변: 머신러닝)",LLM Fine-Tuning 의 PEFT,-0.0016158079,0.0,0.001615807879716158
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야? (남은 답변: 머신러닝)",LoRA,-5.3788623e-05,0.0,5.378862260840833e-05
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야? (남은 답변: 머신러닝)",LoRA 와 QLoRA 의 차이,0.0018620817,0.0,0.0018620816990733147
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야? (남은 답변: 머신러닝)",Loss Function 관련 실무 경험,-0.0043149493,0.0,0.00431494927033782
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야? (남은 답변: 머신러닝)",Loss Function 예시,-0.0016406187,0.0,0.0016406186623498797
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야? (남은 답변: 머신러닝)",Loss Function 정의,-0.0066030263,0.0,0.006603026296943426
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야? (남은 답변: 머신러닝)",MBTI / 좋아하는 아이돌,-0.0060030823,0.0,0.006003082264214754
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야? (남은 답변: 머신러닝)",MSE Loss 설명,0.0008619485,0.0,0.0008619484724476933
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야? (남은 답변: 머신러닝)",MSE Loss 용도,-0.016367353,0.0,0.0163673534989357
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야? (남은 답변: 머신러닝)","Multi-Class, Multi-Label 중 BCE 가 좋은 task",-0.0038623181,0.0,0.003862318117171526
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야? (남은 답변: 머신러닝)",Multi-Label 에서 CE + Softmax 적용 문제점,-0.0034065149,0.0,0.0034065148793160915
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야? (남은 답변: 머신러닝)",PEFT 방법 5가지,-0.007313773,0.0,0.007313773036003113
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야? (남은 답변: 머신러닝)",거대 언어 모델 정의,-0.0047821957,0.0,0.004782195668667555
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야? (남은 답변: 머신러닝)",마지막 할 말,-0.014270055,0.0,0.01427005510777235
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야? (남은 답변: 머신러닝)",면접 시작 인사,-0.019943006,0.0,0.019943006336688995
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야? (남은 답변: 머신러닝)",면접 종료,-0.019546498,0.0,0.019546497613191605
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야? (남은 답변: 머신러닝)","인공지능, 머신러닝, 딥러닝 차이",0.9904743,1.0,0.009525716304779053
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야? (남은 답변: 머신러닝)",잠시 휴식,0.0053506135,0.0,0.005350613500922918
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야? (남은 답변: 머신러닝)",확률 예측에서 MSE Loss 미 사용 이유,-0.0023861637,0.0,0.0023861636873334646
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아 (남은 답변: 인공지능, 머신러닝)",BCE Loss 설명,0.001576958,0.0,0.0015769579913467169
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아 (남은 답변: 인공지능, 머신러닝)",LLM Fine-Tuning 의 PEFT,-0.007983123,0.0,0.007983122952282429
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아 (남은 답변: 인공지능, 머신러닝)",LoRA,-0.0045607453,0.0,0.004560745321214199
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아 (남은 답변: 인공지능, 머신러닝)",LoRA 와 QLoRA 의 차이,0.00906733,0.0,0.009067329578101635
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아 (남은 답변: 인공지능, 머신러닝)",Loss Function 관련 실무 경험,-0.0012894998,0.0,0.0012894998071715236
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아 (남은 답변: 인공지능, 머신러닝)",Loss Function 예시,-0.0034219052,0.0,0.003421905217692256
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아 (남은 답변: 인공지능, 머신러닝)",Loss Function 정의,-0.013367424,0.0,0.01336742378771305
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아 (남은 답변: 인공지능, 머신러닝)",MBTI / 좋아하는 아이돌,0.00440274,0.0,0.004402739927172661
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아 (남은 답변: 인공지능, 머신러닝)",MSE Loss 설명,-0.007926527,0.0,0.00792652741074562
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아 (남은 답변: 인공지능, 머신러닝)",MSE Loss 용도,-0.013638727,0.0,0.013638727366924286
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아 (남은 답변: 인공지능, 머신러닝)","Multi-Class, Multi-Label 중 BCE 가 좋은 task",-0.007452914,0.0,0.007452914025634527
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아 (남은 답변: 인공지능, 머신러닝)",Multi-Label 에서 CE + Softmax 적용 문제점,-0.008002043,0.0,0.008002042770385742
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아 (남은 답변: 인공지능, 머신러닝)",PEFT 방법 5가지,-0.0101901945,0.0,0.010190194472670555
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아 (남은 답변: 인공지능, 머신러닝)",거대 언어 모델 정의,-0.014378192,0.0,0.014378191903233528
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아 (남은 답변: 인공지능, 머신러닝)",마지막 할 말,-0.007851854,0.0,0.007851853966712952
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아 (남은 답변: 인공지능, 머신러닝)",면접 시작 인사,-0.018868143,0.0,0.01886814273893833
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아 (남은 답변: 인공지능, 머신러닝)",면접 종료,-0.018687446,0.0,0.018687445670366287
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아 (남은 답변: 인공지능, 머신러닝)","인공지능, 머신러닝, 딥러닝 차이",0.99045664,1.0,0.009543359279632568
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아 (남은 답변: 인공지능, 머신러닝)",잠시 휴식,0.0066363853,0.0,0.006636385340243578
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아 (남은 답변: 인공지능, 머신러닝)",확률 예측에서 MSE Loss 미 사용 이유,2.3242786e-05,0.0,2.3242786483024247e-05
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지? (남은 답변: 머신러닝)",BCE Loss 설명,-0.0013887894,0.0,0.0013887893874198198
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지? (남은 답변: 머신러닝)",LLM Fine-Tuning 의 PEFT,-0.0024163416,0.0,0.0024163415655493736
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지? (남은 답변: 머신러닝)",LoRA,0.0025230162,0.0,0.002523016184568405
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지? (남은 답변: 머신러닝)",LoRA 와 QLoRA 의 차이,0.007317867,0.0,0.0073178671300411224
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지? (남은 답변: 머신러닝)",Loss Function 관련 실무 경험,-0.003544532,0.0,0.0035445319954305887
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지? (남은 답변: 머신러닝)",Loss Function 예시,0.0032802701,0.0,0.0032802701462060213
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지? (남은 답변: 머신러닝)",Loss Function 정의,-0.0033363588,0.0,0.003336358815431595
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지? (남은 답변: 머신러닝)",MBTI / 좋아하는 아이돌,-0.002406412,0.0,0.002406412037089467
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지? (남은 답변: 머신러닝)",MSE Loss 설명,0.009267434,0.0,0.009267433546483517
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지? (남은 답변: 머신러닝)",MSE Loss 용도,-0.019748913,0.0,0.019748913124203682
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지? (남은 답변: 머신러닝)","Multi-Class, Multi-Label 중 BCE 가 좋은 task",-0.0022574395,0.0,0.0022574395406991243
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지? (남은 답변: 머신러닝)",Multi-Label 에서 CE + Softmax 적용 문제점,-0.0012848326,0.0,0.00128483260050416
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지? (남은 답변: 머신러닝)",PEFT 방법 5가지,-0.010075683,0.0,0.010075682774186134
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지? (남은 답변: 머신러닝)",거대 언어 모델 정의,-0.004548507,0.0,0.004548506811261177
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지? (남은 답변: 머신러닝)",마지막 할 말,-0.004005295,0.0,0.0040052952244877815
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지? (남은 답변: 머신러닝)",면접 시작 인사,-0.026035633,0.0,0.02603563293814659
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지? (남은 답변: 머신러닝)",면접 종료,-0.029832462,0.0,0.02983246184885502
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지? (남은 답변: 머신러닝)","인공지능, 머신러닝, 딥러닝 차이",0.98466516,1.0,0.015334844589233398
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지? (남은 답변: 머신러닝)",잠시 휴식,0.0049334476,0.0,0.004933447577059269
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지? (남은 답변: 머신러닝)",확률 예측에서 MSE Loss 미 사용 이유,-0.0027002736,0.0,0.002700273646041751
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야! (남은 답변: 모든 질문 해결 완료)",BCE Loss 설명,0.00926554,0.0,0.009265540167689323
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야! (남은 답변: 모든 질문 해결 완료)",LLM Fine-Tuning 의 PEFT,-0.0030440947,0.0,0.00304409465752542
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야! (남은 답변: 모든 질문 해결 완료)",LoRA,0.012276509,0.0,0.01227650884538889
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야! (남은 답변: 모든 질문 해결 완료)",LoRA 와 QLoRA 의 차이,-0.0048292438,0.0,0.004829243756830692
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야! (남은 답변: 모든 질문 해결 완료)",Loss Function 관련 실무 경험,0.027917001,0.0,0.027917001396417618
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야! (남은 답변: 모든 질문 해결 완료)",Loss Function 예시,0.0027116481,0.0,0.0027116481214761734
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야! (남은 답변: 모든 질문 해결 완료)",Loss Function 정의,-0.0011692601,0.0,0.0011692601256072521
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야! (남은 답변: 모든 질문 해결 완료)",MBTI / 좋아하는 아이돌,-0.025985133,0.0,0.02598513290286064
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야! (남은 답변: 모든 질문 해결 완료)",MSE Loss 설명,-0.00018262744,0.0,0.00018262743833474815
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야! (남은 답변: 모든 질문 해결 완료)",MSE Loss 용도,0.011020731,0.0,0.011020731180906296
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야! (남은 답변: 모든 질문 해결 완료)","Multi-Class, Multi-Label 중 BCE 가 좋은 task",-0.0017233999,0.0,0.0017233998514711857
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야! (남은 답변: 모든 질문 해결 완료)",Multi-Label 에서 CE + Softmax 적용 문제점,0.003064149,0.0,0.003064149059355259
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야! (남은 답변: 모든 질문 해결 완료)",PEFT 방법 5가지,0.012513397,0.0,0.012513397261500359
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야! (남은 답변: 모든 질문 해결 완료)",거대 언어 모델 정의,0.9722698,1.0,0.027730226516723633
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야! (남은 답변: 모든 질문 해결 완료)",마지막 할 말,-0.013458592,0.0,0.013458591885864735
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야! (남은 답변: 모든 질문 해결 완료)",면접 시작 인사,-0.004657668,0.0,0.004657668061554432
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야! (남은 답변: 모든 질문 해결 완료)",면접 종료,-0.01426754,0.0,0.014267539605498314
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야! (남은 답변: 모든 질문 해결 완료)","인공지능, 머신러닝, 딥러닝 차이",0.00837424,0.0,0.00837424024939537
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야! (남은 답변: 모든 질문 해결 완료)",잠시 휴식,-0.030712081,0.0,0.030712081119418144
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야! (남은 답변: 모든 질문 해결 완료)",확률 예측에서 MSE Loss 미 사용 이유,-0.0144601455,0.0,0.014460145495831966
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지 (남은 답변: 모든 질문 해결 완료)",BCE Loss 설명,0.010760956,0.0,0.010760956443846226
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지 (남은 답변: 모든 질문 해결 완료)",LLM Fine-Tuning 의 PEFT,-0.0034125429,0.0,0.0034125428646802902
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지 (남은 답변: 모든 질문 해결 완료)",LoRA,0.0073268963,0.0,0.0073268963024020195
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지 (남은 답변: 모든 질문 해결 완료)",LoRA 와 QLoRA 의 차이,0.0040087174,0.0,0.004008717369288206
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지 (남은 답변: 모든 질문 해결 완료)",Loss Function 관련 실무 경험,0.019313801,0.0,0.01931380107998848
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지 (남은 답변: 모든 질문 해결 완료)",Loss Function 예시,0.006113097,0.0,0.006113097071647644
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지 (남은 답변: 모든 질문 해결 완료)",Loss Function 정의,-0.006498505,0.0,0.006498504895716906
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지 (남은 답변: 모든 질문 해결 완료)",MBTI / 좋아하는 아이돌,-0.027374605,0.0,0.02737460471689701
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지 (남은 답변: 모든 질문 해결 완료)",MSE Loss 설명,0.008890101,0.0,0.008890100754797459
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지 (남은 답변: 모든 질문 해결 완료)",MSE Loss 용도,0.012660662,0.0,0.012660661712288857
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지 (남은 답변: 모든 질문 해결 완료)","Multi-Class, Multi-Label 중 BCE 가 좋은 task",0.0023816335,0.0,0.0023816335014998913
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지 (남은 답변: 모든 질문 해결 완료)",Multi-Label 에서 CE + Softmax 적용 문제점,-0.0022324715,0.0,0.0022324714809656143
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지 (남은 답변: 모든 질문 해결 완료)",PEFT 방법 5가지,0.012057961,0.0,0.012057960964739323
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지 (남은 답변: 모든 질문 해결 완료)",거대 언어 모델 정의,0.97445256,1.0,0.025547444820404053
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지 (남은 답변: 모든 질문 해결 완료)",마지막 할 말,-0.0073246667,0.0,0.00732466671615839
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지 (남은 답변: 모든 질문 해결 완료)",면접 시작 인사,-0.009428558,0.0,0.009428557939827442
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지 (남은 답변: 모든 질문 해결 완료)",면접 종료,-0.00808301,0.0,0.008083010092377663
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지 (남은 답변: 모든 질문 해결 완료)","인공지능, 머신러닝, 딥러닝 차이",0.0037215694,0.0,0.0037215694319456816
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지 (남은 답변: 모든 질문 해결 완료)",잠시 휴식,-0.031041373,0.0,0.031041372567415237
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지 (남은 답변: 모든 질문 해결 완료)",확률 예측에서 MSE Loss 미 사용 이유,-0.012401671,0.0,0.012401671148836613
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야. (남은 답변: 모든 질문 해결 완료)",BCE Loss 설명,0.0072474126,0.0,0.0072474125772714615
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야. (남은 답변: 모든 질문 해결 완료)",LLM Fine-Tuning 의 PEFT,-0.004269924,0.0,0.0042699240148067474
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야. (남은 답변: 모든 질문 해결 완료)",LoRA,0.006195174,0.0,0.006195174064487219
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야. (남은 답변: 모든 질문 해결 완료)",LoRA 와 QLoRA 의 차이,0.004293177,0.0,0.004293176811188459
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야. (남은 답변: 모든 질문 해결 완료)",Loss Function 관련 실무 경험,0.019785415,0.0,0.01978541538119316
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야. (남은 답변: 모든 질문 해결 완료)",Loss Function 예시,0.009200272,0.0,0.009200272150337696
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야. (남은 답변: 모든 질문 해결 완료)",Loss Function 정의,-0.0041902643,0.0,0.004190264269709587
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야. (남은 답변: 모든 질문 해결 완료)",MBTI / 좋아하는 아이돌,-0.026767755,0.0,0.026767754927277565
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야. (남은 답변: 모든 질문 해결 완료)",MSE Loss 설명,0.008026049,0.0,0.00802604854106903
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야. (남은 답변: 모든 질문 해결 완료)",MSE Loss 용도,0.0063597104,0.0,0.006359710358083248
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야. (남은 답변: 모든 질문 해결 완료)","Multi-Class, Multi-Label 중 BCE 가 좋은 task",-0.000707544,0.0,0.0007075439789332449
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야. (남은 답변: 모든 질문 해결 완료)",Multi-Label 에서 CE + Softmax 적용 문제점,0.004507779,0.0,0.004507779143750668
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야. (남은 답변: 모든 질문 해결 완료)",PEFT 방법 5가지,0.014892751,0.0,0.014892751350998878
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야. (남은 답변: 모든 질문 해결 완료)",거대 언어 모델 정의,0.9745418,1.0,0.025458216667175293
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야. (남은 답변: 모든 질문 해결 완료)",마지막 할 말,-0.015114611,0.0,0.015114611014723778
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야. (남은 답변: 모든 질문 해결 완료)",면접 시작 인사,-0.004602279,0.0,0.004602279048413038
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야. (남은 답변: 모든 질문 해결 완료)",면접 종료,-0.011919263,0.0,0.011919262818992138
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야. (남은 답변: 모든 질문 해결 완료)","인공지능, 머신러닝, 딥러닝 차이",0.001344346,0.0,0.0013443459756672382
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야. (남은 답변: 모든 질문 해결 완료)",잠시 휴식,-0.022664253,0.0,0.022664252668619156
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야. (남은 답변: 모든 질문 해결 완료)",확률 예측에서 MSE Loss 미 사용 이유,-0.0130713275,0.0,0.013071327470242977
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로 (남은 답변: 머신러닝, 딥러닝)",BCE Loss 설명,0.00025111085,0.0,0.00025111084687523544
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로 (남은 답변: 머신러닝, 딥러닝)",LLM Fine-Tuning 의 PEFT,0.0012795749,0.0,0.00127957493532449
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로 (남은 답변: 머신러닝, 딥러닝)",LoRA,-0.0024954146,0.0,0.002495414577424526
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로 (남은 답변: 머신러닝, 딥러닝)",LoRA 와 QLoRA 의 차이,0.0073190895,0.0,0.007319089490920305
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로 (남은 답변: 머신러닝, 딥러닝)",Loss Function 관련 실무 경험,-0.0022791163,0.0,0.0022791163064539433
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로 (남은 답변: 머신러닝, 딥러닝)",Loss Function 예시,-0.0018456253,0.0,0.001845625345595181
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로 (남은 답변: 머신러닝, 딥러닝)",Loss Function 정의,-0.013567812,0.0,0.01356781180948019
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로 (남은 답변: 머신러닝, 딥러닝)",MBTI / 좋아하는 아이돌,-0.00991939,0.0,0.009919390082359314
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로 (남은 답변: 머신러닝, 딥러닝)",MSE Loss 설명,-0.004715786,0.0,0.004715785849839449
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로 (남은 답변: 머신러닝, 딥러닝)",MSE Loss 용도,-0.015361418,0.0,0.015361418016254902
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로 (남은 답변: 머신러닝, 딥러닝)","Multi-Class, Multi-Label 중 BCE 가 좋은 task",-0.00505102,0.0,0.0050510200671851635
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로 (남은 답변: 머신러닝, 딥러닝)",Multi-Label 에서 CE + Softmax 적용 문제점,0.002987297,0.0,0.0029872970189899206
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로 (남은 답변: 머신러닝, 딥러닝)",PEFT 방법 5가지,-0.006877627,0.0,0.006877627223730087
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로 (남은 답변: 머신러닝, 딥러닝)",거대 언어 모델 정의,0.0007044868,0.0,0.0007044867961667478
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로 (남은 답변: 머신러닝, 딥러닝)",마지막 할 말,-0.009874102,0.0,0.009874101728200912
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로 (남은 답변: 머신러닝, 딥러닝)",면접 시작 인사,-0.021620069,0.0,0.021620068699121475
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로 (남은 답변: 머신러닝, 딥러닝)",면접 종료,-0.011125495,0.0,0.011125494726002216
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로 (남은 답변: 머신러닝, 딥러닝)","인공지능, 머신러닝, 딥러닝 차이",0.9914319,1.0,0.008568108081817627
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로 (남은 답변: 머신러닝, 딥러닝)",잠시 휴식,-0.00048566947,0.0,0.0004856694722548127
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로 (남은 답변: 머신러닝, 딥러닝)",확률 예측에서 MSE Loss 미 사용 이유,-0.0076226904,0.0,0.0076226904056966305
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야 (남은 답변: 모든 질문 해결 완료),BCE Loss 설명,-0.016673278,0.0,0.01667327806353569
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야 (남은 답변: 모든 질문 해결 완료),LLM Fine-Tuning 의 PEFT,0.001150337,0.0,0.0011503370478749275
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야 (남은 답변: 모든 질문 해결 완료),LoRA,0.02199743,0.0,0.02199742943048477
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야 (남은 답변: 모든 질문 해결 완료),LoRA 와 QLoRA 의 차이,-0.002435578,0.0,0.0024355780333280563
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야 (남은 답변: 모든 질문 해결 완료),Loss Function 관련 실무 경험,-0.0023058567,0.0,0.0023058566730469465
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야 (남은 답변: 모든 질문 해결 완료),Loss Function 예시,-0.034036618,0.0,0.03403661772608757
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야 (남은 답변: 모든 질문 해결 완료),Loss Function 정의,0.9814529,1.0,0.018547117710113525
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야 (남은 답변: 모든 질문 해결 완료),MBTI / 좋아하는 아이돌,-0.014179155,0.0,0.014179155230522156
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야 (남은 답변: 모든 질문 해결 완료),MSE Loss 설명,-0.012587478,0.0,0.012587478384375572
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야 (남은 답변: 모든 질문 해결 완료),MSE Loss 용도,-0.009141274,0.0,0.00914127379655838
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야 (남은 답변: 모든 질문 해결 완료),"Multi-Class, Multi-Label 중 BCE 가 좋은 task",-0.0047068694,0.0,0.00470686936751008
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야 (남은 답변: 모든 질문 해결 완료),Multi-Label 에서 CE + Softmax 적용 문제점,-0.0038944033,0.0,0.0038944033440202475
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야 (남은 답변: 모든 질문 해결 완료),PEFT 방법 5가지,-0.0056319544,0.0,0.005631954409182072
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야 (남은 답변: 모든 질문 해결 완료),거대 언어 모델 정의,0.025196763,0.0,0.02519676275551319
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야 (남은 답변: 모든 질문 해결 완료),마지막 할 말,-0.0025155891,0.0,0.0025155891198664904
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야 (남은 답변: 모든 질문 해결 완료),면접 시작 인사,-0.00930847,0.0,0.009308470413088799
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야 (남은 답변: 모든 질문 해결 완료),면접 종료,-0.031620063,0.0,0.03162006288766861
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야 (남은 답변: 모든 질문 해결 완료),"인공지능, 머신러닝, 딥러닝 차이",-0.012015184,0.0,0.012015184387564659
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야 (남은 답변: 모든 질문 해결 완료),잠시 휴식,-0.015870204,0.0,0.01587020419538021
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야 (남은 답변: 모든 질문 해결 완료),확률 예측에서 MSE Loss 미 사용 이유,-0.008772636,0.0,0.008772635832428932
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데 (남은 답변: 거대 언어 모델 정의),BCE Loss 설명,-0.02642255,0.0,0.026422550901770592
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데 (남은 답변: 거대 언어 모델 정의),LLM Fine-Tuning 의 PEFT,-0.007090668,0.0,0.007090668193995953
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데 (남은 답변: 거대 언어 모델 정의),LoRA,-0.0023165739,0.0,0.002316573867574334
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데 (남은 답변: 거대 언어 모델 정의),LoRA 와 QLoRA 의 차이,-0.030637244,0.0,0.030637243762612343
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데 (남은 답변: 거대 언어 모델 정의),Loss Function 관련 실무 경험,-0.004730378,0.0,0.004730377811938524
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데 (남은 답변: 거대 언어 모델 정의),Loss Function 예시,-0.03081016,0.0,0.03081016056239605
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데 (남은 답변: 거대 언어 모델 정의),Loss Function 정의,0.4058453,0.0,0.4058453142642975
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데 (남은 답변: 거대 언어 모델 정의),MBTI / 좋아하는 아이돌,-0.0077017476,0.0,0.007701747585088015
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데 (남은 답변: 거대 언어 모델 정의),MSE Loss 설명,-0.021499492,0.0,0.02149949222803116
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데 (남은 답변: 거대 언어 모델 정의),MSE Loss 용도,-0.019413965,0.0,0.019413964822888374
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데 (남은 답변: 거대 언어 모델 정의),"Multi-Class, Multi-Label 중 BCE 가 좋은 task",-0.004555821,0.0,0.00455582095310092
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데 (남은 답변: 거대 언어 모델 정의),Multi-Label 에서 CE + Softmax 적용 문제점,0.00030638362,0.0,0.0003063836193177849
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데 (남은 답변: 거대 언어 모델 정의),PEFT 방법 5가지,0.01741408,0.0,0.01741407997906208
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데 (남은 답변: 거대 언어 모델 정의),거대 언어 모델 정의,0.8697996,1.0,0.13020038604736328
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데 (남은 답변: 거대 언어 모델 정의),마지막 할 말,-0.011388028,0.0,0.011388028040528297
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데 (남은 답변: 거대 언어 모델 정의),면접 시작 인사,-0.019306242,0.0,0.0193062424659729
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데 (남은 답변: 거대 언어 모델 정의),면접 종료,-0.024197305,0.0,0.024197304621338844
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데 (남은 답변: 거대 언어 모델 정의),"인공지능, 머신러닝, 딥러닝 차이",-0.008386375,0.0,0.00838637538254261
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데 (남은 답변: 거대 언어 모델 정의),잠시 휴식,-0.0634776,0.0,0.06347759813070297
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데 (남은 답변: 거대 언어 모델 정의),확률 예측에서 MSE Loss 미 사용 이유,-0.038322866,0.0,0.03832286596298218
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델 (남은 답변: 모든 질문 해결 완료),BCE Loss 설명,-0.005467272,0.0,0.005467271897941828
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델 (남은 답변: 모든 질문 해결 완료),LLM Fine-Tuning 의 PEFT,-0.007327879,0.0,0.007327878847718239
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델 (남은 답변: 모든 질문 해결 완료),LoRA,0.015458781,0.0,0.015458781272172928
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델 (남은 답변: 모든 질문 해결 완료),LoRA 와 QLoRA 의 차이,-0.0061779935,0.0,0.006177993491292
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델 (남은 답변: 모든 질문 해결 완료),Loss Function 관련 실무 경험,0.0038889968,0.0,0.003888996783643961
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델 (남은 답변: 모든 질문 해결 완료),Loss Function 예시,-0.020513467,0.0,0.020513467490673065
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델 (남은 답변: 모든 질문 해결 완료),Loss Function 정의,0.9784332,1.0,0.021566808223724365
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델 (남은 답변: 모든 질문 해결 완료),MBTI / 좋아하는 아이돌,-0.01468906,0.0,0.014689059928059578
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델 (남은 답변: 모든 질문 해결 완료),MSE Loss 설명,-0.008007539,0.0,0.008007539436221123
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델 (남은 답변: 모든 질문 해결 완료),MSE Loss 용도,-0.014237266,0.0,0.014237266033887863
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델 (남은 답변: 모든 질문 해결 완료),"Multi-Class, Multi-Label 중 BCE 가 좋은 task",0.0003994537,0.0,0.0003994537109974772
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델 (남은 답변: 모든 질문 해결 완료),Multi-Label 에서 CE + Softmax 적용 문제점,0.00040436437,0.0,0.0004043643712066114
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델 (남은 답변: 모든 질문 해결 완료),PEFT 방법 5가지,0.0097447205,0.0,0.009744720533490181
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델 (남은 답변: 모든 질문 해결 완료),거대 언어 모델 정의,-0.005893587,0.0,0.0058935871347785
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델 (남은 답변: 모든 질문 해결 완료),마지막 할 말,0.0013987935,0.0,0.0013987935381010175
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델 (남은 답변: 모든 질문 해결 완료),면접 시작 인사,-0.01520364,0.0,0.01520363986492157
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델 (남은 답변: 모든 질문 해결 완료),면접 종료,-0.033345807,0.0,0.03334580734372139
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델 (남은 답변: 모든 질문 해결 완료),"인공지능, 머신러닝, 딥러닝 차이",-0.0138442265,0.0,0.013844226486980915
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델 (남은 답변: 모든 질문 해결 완료),잠시 휴식,-0.015874738,0.0,0.01587473787367344
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델 (남은 답변: 모든 질문 해결 완료),확률 예측에서 MSE Loss 미 사용 이유,-0.016622601,0.0,0.016622601076960564
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지. (남은 답변: 거대 언어 모델 정의),BCE Loss 설명,-0.005156203,0.0,0.005156203173100948
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지. (남은 답변: 거대 언어 모델 정의),LLM Fine-Tuning 의 PEFT,-0.0057713655,0.0,0.005771365482360125
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지. (남은 답변: 거대 언어 모델 정의),LoRA,0.0076084435,0.0,0.00760844349861145
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지. (남은 답변: 거대 언어 모델 정의),LoRA 와 QLoRA 의 차이,-0.01628475,0.0,0.016284750774502754
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지. (남은 답변: 거대 언어 모델 정의),Loss Function 관련 실무 경험,0.015214695,0.0,0.015214694663882256
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지. (남은 답변: 거대 언어 모델 정의),Loss Function 예시,0.0062377285,0.0,0.006237728521227837
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지. (남은 답변: 거대 언어 모델 정의),Loss Function 정의,-0.007224211,0.0,0.007224211003631353
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지. (남은 답변: 거대 언어 모델 정의),MBTI / 좋아하는 아이돌,-0.0036844518,0.0,0.0036844518035650253
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지. (남은 답변: 거대 언어 모델 정의),MSE Loss 설명,0.0056594852,0.0,0.00565948523581028
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지. (남은 답변: 거대 언어 모델 정의),MSE Loss 용도,0.0060085873,0.0,0.006008587311953306
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지. (남은 답변: 거대 언어 모델 정의),"Multi-Class, Multi-Label 중 BCE 가 좋은 task",0.013226701,0.0,0.013226700946688652
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지. (남은 답변: 거대 언어 모델 정의),Multi-Label 에서 CE + Softmax 적용 문제점,0.01424687,0.0,0.014246869832277298
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지. (남은 답변: 거대 언어 모델 정의),PEFT 방법 5가지,0.0040160636,0.0,0.004016063641756773
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지. (남은 답변: 거대 언어 모델 정의),거대 언어 모델 정의,0.98401445,1.0,0.015985548496246338
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지. (남은 답변: 거대 언어 모델 정의),마지막 할 말,-0.014829874,0.0,0.014829874038696289
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지. (남은 답변: 거대 언어 모델 정의),면접 시작 인사,-0.0122246295,0.0,0.012224629521369934
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지. (남은 답변: 거대 언어 모델 정의),면접 종료,-0.006718927,0.0,0.006718927063047886
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지. (남은 답변: 거대 언어 모델 정의),"인공지능, 머신러닝, 딥러닝 차이",0.007936456,0.0,0.007936456240713596
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지. (남은 답변: 거대 언어 모델 정의),잠시 휴식,-0.020548465,0.0,0.020548464730381966
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지. (남은 답변: 거대 언어 모델 정의),확률 예측에서 MSE Loss 미 사용 이유,-0.01668293,0.0,0.016682930290699005
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야 (남은 답변: Loss Function 정의),BCE Loss 설명,-0.018333381,0.0,0.018333381041884422
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야 (남은 답변: Loss Function 정의),LLM Fine-Tuning 의 PEFT,-0.00047370844,0.0,0.0004737084382213652
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야 (남은 답변: Loss Function 정의),LoRA,0.025945658,0.0,0.02594565786421299
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야 (남은 답변: Loss Function 정의),LoRA 와 QLoRA 의 차이,-0.014585864,0.0,0.014585863798856735
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야 (남은 답변: Loss Function 정의),Loss Function 관련 실무 경험,-0.008258617,0.0,0.008258616551756859
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야 (남은 답변: Loss Function 정의),Loss Function 예시,-0.023244565,0.0,0.023244565352797508
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야 (남은 답변: Loss Function 정의),Loss Function 정의,0.98974526,1.0,0.010254740715026855
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야 (남은 답변: Loss Function 정의),MBTI / 좋아하는 아이돌,-0.001146232,0.0,0.0011462320107966661
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야 (남은 답변: Loss Function 정의),MSE Loss 설명,-0.0117504,0.0,0.011750400066375732
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야 (남은 답변: Loss Function 정의),MSE Loss 용도,-0.021727728,0.0,0.021727727726101875
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야 (남은 답변: Loss Function 정의),"Multi-Class, Multi-Label 중 BCE 가 좋은 task",-0.009331451,0.0,0.009331450797617435
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야 (남은 답변: Loss Function 정의),Multi-Label 에서 CE + Softmax 적용 문제점,0.0014051502,0.0,0.0014051501639187336
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야 (남은 답변: Loss Function 정의),PEFT 방법 5가지,-0.019714177,0.0,0.019714176654815674
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야 (남은 답변: Loss Function 정의),거대 언어 모델 정의,-0.0074867574,0.0,0.007486757356673479
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야 (남은 답변: Loss Function 정의),마지막 할 말,-0.0025523792,0.0,0.0025523791555315256
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야 (남은 답변: Loss Function 정의),면접 시작 인사,-0.020430923,0.0,0.020430922508239746
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야 (남은 답변: Loss Function 정의),면접 종료,-0.019160245,0.0,0.01916024461388588
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야 (남은 답변: Loss Function 정의),"인공지능, 머신러닝, 딥러닝 차이",-0.0049771857,0.0,0.004977185744792223
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야 (남은 답변: Loss Function 정의),잠시 휴식,-0.010452166,0.0,0.010452166199684143
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야 (남은 답변: Loss Function 정의),확률 예측에서 MSE Loss 미 사용 이유,-0.001362069,0.0,0.0013620690442621708
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지! (남은 답변: 모든 질문 해결 완료)",BCE Loss 설명,-0.013035721,0.0,0.013035721145570278
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지! (남은 답변: 모든 질문 해결 완료)",LLM Fine-Tuning 의 PEFT,-0.011303622,0.0,0.011303622275590897
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지! (남은 답변: 모든 질문 해결 완료)",LoRA,0.029490178,0.0,0.02949017845094204
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지! (남은 답변: 모든 질문 해결 완료)",LoRA 와 QLoRA 의 차이,-0.010954761,0.0,0.010954760946333408
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지! (남은 답변: 모든 질문 해결 완료)",Loss Function 관련 실무 경험,-0.04397191,0.0,0.04397191107273102
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지! (남은 답변: 모든 질문 해결 완료)",Loss Function 예시,0.96272904,1.0,0.037270963191986084
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지! (남은 답변: 모든 질문 해결 완료)",Loss Function 정의,0.0043307524,0.0,0.004330752417445183
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지! (남은 답변: 모든 질문 해결 완료)",MBTI / 좋아하는 아이돌,-0.007988991,0.0,0.007988991215825081
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지! (남은 답변: 모든 질문 해결 완료)",MSE Loss 설명,-0.0162245,0.0,0.01622449979186058
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지! (남은 답변: 모든 질문 해결 완료)",MSE Loss 용도,-0.021095317,0.0,0.021095316857099533
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지! (남은 답변: 모든 질문 해결 완료)","Multi-Class, Multi-Label 중 BCE 가 좋은 task",-0.010469083,0.0,0.010469082742929459
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지! (남은 답변: 모든 질문 해결 완료)",Multi-Label 에서 CE + Softmax 적용 문제점,-0.02824234,0.0,0.028242340311408043
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지! (남은 답변: 모든 질문 해결 완료)",PEFT 방법 5가지,-0.013700913,0.0,0.013700912706553936
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지! (남은 답변: 모든 질문 해결 완료)",거대 언어 모델 정의,0.00084437575,0.0,0.0008443757542409003
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지! (남은 답변: 모든 질문 해결 완료)",마지막 할 말,0.0009075788,0.0,0.0009075787966139615
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지! (남은 답변: 모든 질문 해결 완료)",면접 시작 인사,-0.011419968,0.0,0.011419967748224735
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지! (남은 답변: 모든 질문 해결 완료)",면접 종료,-0.013445945,0.0,0.013445945456624031
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지! (남은 답변: 모든 질문 해결 완료)","인공지능, 머신러닝, 딥러닝 차이",-0.009388152,0.0,0.009388151578605175
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지! (남은 답변: 모든 질문 해결 완료)",잠시 휴식,-0.042084023,0.0,0.04208402335643768
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지! (남은 답변: 모든 질문 해결 완료)",확률 예측에서 MSE Loss 미 사용 이유,-0.028880423,0.0,0.028880422934889793
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야 (남은 답변: Loss Function 정의),BCE Loss 설명,-0.020794032,0.0,0.020794032141566277
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야 (남은 답변: Loss Function 정의),LLM Fine-Tuning 의 PEFT,0.002589141,0.0,0.0025891410186886787
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야 (남은 답변: Loss Function 정의),LoRA,0.027493771,0.0,0.02749377116560936
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야 (남은 답변: Loss Function 정의),LoRA 와 QLoRA 의 차이,-0.012579424,0.0,0.012579424306750298
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야 (남은 답변: Loss Function 정의),Loss Function 관련 실무 경험,-0.0065216627,0.0,0.006521662697196007
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야 (남은 답변: Loss Function 정의),Loss Function 예시,-0.034874313,0.0,0.034874312579631805
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야 (남은 답변: Loss Function 정의),Loss Function 정의,0.9909387,1.0,0.009061276912689209
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야 (남은 답변: Loss Function 정의),MBTI / 좋아하는 아이돌,-0.0065597035,0.0,0.006559703499078751
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야 (남은 답변: Loss Function 정의),MSE Loss 설명,-0.010766896,0.0,0.010766896419227123
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야 (남은 답변: Loss Function 정의),MSE Loss 용도,-0.016795833,0.0,0.01679583266377449
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야 (남은 답변: Loss Function 정의),"Multi-Class, Multi-Label 중 BCE 가 좋은 task",-0.008853599,0.0,0.008853599429130554
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야 (남은 답변: Loss Function 정의),Multi-Label 에서 CE + Softmax 적용 문제점,-0.0006203792,0.0,0.0006203792290762067
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야 (남은 답변: Loss Function 정의),PEFT 방법 5가지,-0.014567555,0.0,0.01456755492836237
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야 (남은 답변: Loss Function 정의),거대 언어 모델 정의,-0.006356575,0.0,0.006356575060635805
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야 (남은 답변: Loss Function 정의),마지막 할 말,-0.00039351845,0.0,0.00039351845043711364
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야 (남은 답변: Loss Function 정의),면접 시작 인사,-0.020271925,0.0,0.020271925255656242
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야 (남은 답변: Loss Function 정의),면접 종료,-0.017199602,0.0,0.017199601978063583
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야 (남은 답변: Loss Function 정의),"인공지능, 머신러닝, 딥러닝 차이",-0.0025839508,0.0,0.0025839507579803467
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야 (남은 답변: Loss Function 정의),잠시 휴식,-0.014610413,0.0,0.0146104134619236
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야 (남은 답변: Loss Function 정의),확률 예측에서 MSE Loss 미 사용 이유,-5.677955e-05,0.0,5.677955050487071e-05
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지 (남은 답변: 모든 질문 해결 완료),BCE Loss 설명,-0.011008015,0.0,0.011008014902472496
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지 (남은 답변: 모든 질문 해결 완료),LLM Fine-Tuning 의 PEFT,-0.008445264,0.0,0.008445263840258121
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지 (남은 답변: 모든 질문 해결 완료),LoRA,0.037743326,0.0,0.037743326276540756
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지 (남은 답변: 모든 질문 해결 완료),LoRA 와 QLoRA 의 차이,-0.0040751444,0.0,0.004075144417583942
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지 (남은 답변: 모든 질문 해결 완료),Loss Function 관련 실무 경험,-0.057046816,0.0,0.05704681575298309
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지 (남은 답변: 모든 질문 해결 완료),Loss Function 예시,0.9584269,1.0,0.04157310724258423
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지 (남은 답변: 모든 질문 해결 완료),Loss Function 정의,0.0030419833,0.0,0.003041983349248767
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지 (남은 답변: 모든 질문 해결 완료),MBTI / 좋아하는 아이돌,0.0037088837,0.0,0.0037088836543262005
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지 (남은 답변: 모든 질문 해결 완료),MSE Loss 설명,-0.009527138,0.0,0.00952713843435049
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지 (남은 답변: 모든 질문 해결 완료),MSE Loss 용도,0.0119539825,0.0,0.011953982524573803
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지 (남은 답변: 모든 질문 해결 완료),"Multi-Class, Multi-Label 중 BCE 가 좋은 task",-0.012178419,0.0,0.012178419157862663
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지 (남은 답변: 모든 질문 해결 완료),Multi-Label 에서 CE + Softmax 적용 문제점,-0.018527238,0.0,0.018527237698435783
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지 (남은 답변: 모든 질문 해결 완료),PEFT 방법 5가지,-0.028466392,0.0,0.028466392308473587
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지 (남은 답변: 모든 질문 해결 완료),거대 언어 모델 정의,-0.00039548386,0.0,0.0003954838612116873
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지 (남은 답변: 모든 질문 해결 완료),마지막 할 말,-0.008943781,0.0,0.00894378125667572
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지 (남은 답변: 모든 질문 해결 완료),면접 시작 인사,-0.0059210337,0.0,0.005921033676713705
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지 (남은 답변: 모든 질문 해결 완료),면접 종료,-0.010038052,0.0,0.010038051754236221
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지 (남은 답변: 모든 질문 해결 완료),"인공지능, 머신러닝, 딥러닝 차이",-0.021130903,0.0,0.02113090269267559
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지 (남은 답변: 모든 질문 해결 완료),잠시 휴식,-0.05703369,0.0,0.05703369155526161
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지 (남은 답변: 모든 질문 해결 완료),확률 예측에서 MSE Loss 미 사용 이유,-0.013273613,0.0,0.013273612596094608
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데 (남은 답변: Loss Function 예시),BCE Loss 설명,-0.011568611,0.0,0.011568610556423664
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데 (남은 답변: Loss Function 예시),LLM Fine-Tuning 의 PEFT,-0.01162692,0.0,0.011626919731497765
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데 (남은 답변: Loss Function 예시),LoRA,0.01714761,0.0,0.0171476099640131
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데 (남은 답변: Loss Function 예시),LoRA 와 QLoRA 의 차이,0.0043868674,0.0,0.004386867396533489
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데 (남은 답변: Loss Function 예시),Loss Function 관련 실무 경험,-0.026132079,0.0,0.02613207884132862
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데 (남은 답변: Loss Function 예시),Loss Function 예시,0.98090667,1.0,0.019093334674835205
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데 (남은 답변: Loss Function 예시),Loss Function 정의,-0.031816136,0.0,0.031816136091947556
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데 (남은 답변: Loss Function 예시),MBTI / 좋아하는 아이돌,0.011758221,0.0,0.011758221313357353
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데 (남은 답변: Loss Function 예시),MSE Loss 설명,-0.013028125,0.0,0.013028125278651714
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데 (남은 답변: Loss Function 예시),MSE Loss 용도,-0.037274107,0.0,0.037274107336997986
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데 (남은 답변: Loss Function 예시),"Multi-Class, Multi-Label 중 BCE 가 좋은 task",-0.013068694,0.0,0.013068693690001965
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데 (남은 답변: Loss Function 예시),Multi-Label 에서 CE + Softmax 적용 문제점,-0.018938841,0.0,0.018938841298222542
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데 (남은 답변: Loss Function 예시),PEFT 방법 5가지,-0.026673194,0.0,0.026673194020986557
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데 (남은 답변: Loss Function 예시),거대 언어 모델 정의,-0.009419039,0.0,0.009419038891792297
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데 (남은 답변: Loss Function 예시),마지막 할 말,0.01574879,0.0,0.01574878953397274
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데 (남은 답변: Loss Function 예시),면접 시작 인사,-0.017357027,0.0,0.017357027158141136
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데 (남은 답변: Loss Function 예시),면접 종료,0.00049541553,0.0,0.00049541553016752
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데 (남은 답변: Loss Function 예시),"인공지능, 머신러닝, 딥러닝 차이",-0.00059119327,0.0,0.0005911932676099241
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데 (남은 답변: Loss Function 예시),잠시 휴식,-0.02283494,0.0,0.022834939882159233
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데 (남은 답변: Loss Function 예시),확률 예측에서 MSE Loss 미 사용 이유,0.00499627,0.0,0.004996269941329956
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지 (남은 답변: 모든 질문 해결 완료)",BCE Loss 설명,-0.004354695,0.0,0.0043546948581933975
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지 (남은 답변: 모든 질문 해결 완료)",LLM Fine-Tuning 의 PEFT,0.010644602,0.0,0.010644601657986641
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지 (남은 답변: 모든 질문 해결 완료)",LoRA,0.022532133,0.0,0.022532133385539055
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지 (남은 답변: 모든 질문 해결 완료)",LoRA 와 QLoRA 의 차이,0.0037312193,0.0,0.00373121933080256
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지 (남은 답변: 모든 질문 해결 완료)",Loss Function 관련 실무 경험,0.0047897818,0.0,0.004789781756699085
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지 (남은 답변: 모든 질문 해결 완료)",Loss Function 예시,-0.016584666,0.0,0.016584666445851326
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지 (남은 답변: 모든 질문 해결 완료)",Loss Function 정의,-0.0048473584,0.0,0.004847358446568251
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지 (남은 답변: 모든 질문 해결 완료)",MBTI / 좋아하는 아이돌,-0.02252745,0.0,0.022527450695633888
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지 (남은 답변: 모든 질문 해결 완료)",MSE Loss 설명,0.98074704,1.0,0.0192529559135437
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지 (남은 답변: 모든 질문 해결 완료)",MSE Loss 용도,-0.029667994,0.0,0.029667994007468224
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지 (남은 답변: 모든 질문 해결 완료)","Multi-Class, Multi-Label 중 BCE 가 좋은 task",-0.01099332,0.0,0.010993319563567638
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지 (남은 답변: 모든 질문 해결 완료)",Multi-Label 에서 CE + Softmax 적용 문제점,-0.0060783746,0.0,0.006078374572098255
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지 (남은 답변: 모든 질문 해결 완료)",PEFT 방법 5가지,-0.0065964027,0.0,0.006596402730792761
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지 (남은 답변: 모든 질문 해결 완료)",거대 언어 모델 정의,0.006936658,0.0,0.006936658173799515
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지 (남은 답변: 모든 질문 해결 완료)",마지막 할 말,-0.019931272,0.0,0.01993127167224884
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지 (남은 답변: 모든 질문 해결 완료)",면접 시작 인사,-0.0160052,0.0,0.016005199402570724
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지 (남은 답변: 모든 질문 해결 완료)",면접 종료,-0.019774394,0.0,0.01977439410984516
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지 (남은 답변: 모든 질문 해결 완료)","인공지능, 머신러닝, 딥러닝 차이",-0.00080504816,0.0,0.0008050481555983424
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지 (남은 답변: 모든 질문 해결 완료)",잠시 휴식,-0.02483635,0.0,0.024836350232362747
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지 (남은 답변: 모든 질문 해결 완료)",확률 예측에서 MSE Loss 미 사용 이유,-0.010512252,0.0,0.01051225233823061
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어 (남은 답변: 모든 질문 해결 완료)",BCE Loss 설명,0.95414966,1.0,0.04585033655166626
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어 (남은 답변: 모든 질문 해결 완료)",LLM Fine-Tuning 의 PEFT,0.009039131,0.0,0.009039130993187428
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어 (남은 답변: 모든 질문 해결 완료)",LoRA,0.023489984,0.0,0.02348998375236988
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어 (남은 답변: 모든 질문 해결 완료)",LoRA 와 QLoRA 의 차이,-0.02150716,0.0,0.021507160738110542
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어 (남은 답변: 모든 질문 해결 완료)",Loss Function 관련 실무 경험,-0.004630921,0.0,0.004630920942872763
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어 (남은 답변: 모든 질문 해결 완료)",Loss Function 예시,-0.039731566,0.0,0.03973156586289406
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어 (남은 답변: 모든 질문 해결 완료)",Loss Function 정의,0.0037257995,0.0,0.003725799499079585
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어 (남은 답변: 모든 질문 해결 완료)",MBTI / 좋아하는 아이돌,-0.030344315,0.0,0.030344314873218536
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어 (남은 답변: 모든 질문 해결 완료)",MSE Loss 설명,0.011797697,0.0,0.01179769728332758
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어 (남은 답변: 모든 질문 해결 완료)",MSE Loss 용도,-0.034702167,0.0,0.03470216691493988
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어 (남은 답변: 모든 질문 해결 완료)","Multi-Class, Multi-Label 중 BCE 가 좋은 task",-0.0071231034,0.0,0.007123103365302086
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어 (남은 답변: 모든 질문 해결 완료)",Multi-Label 에서 CE + Softmax 적용 문제점,-0.0028659438,0.0,0.0028659438248723745
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어 (남은 답변: 모든 질문 해결 완료)",PEFT 방법 5가지,-0.009620724,0.0,0.009620724245905876
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어 (남은 답변: 모든 질문 해결 완료)",거대 언어 모델 정의,0.034874674,0.0,0.034874673932790756
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어 (남은 답변: 모든 질문 해결 완료)",마지막 할 말,0.005701199,0.0,0.005701199173927307
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어 (남은 답변: 모든 질문 해결 완료)",면접 시작 인사,-0.0066383644,0.0,0.006638364400714636
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어 (남은 답변: 모든 질문 해결 완료)",면접 종료,-0.01868647,0.0,0.01868646964430809
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어 (남은 답변: 모든 질문 해결 완료)","인공지능, 머신러닝, 딥러닝 차이",0.01415393,0.0,0.014153930358588696
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어 (남은 답변: 모든 질문 해결 완료)",잠시 휴식,-0.023262143,0.0,0.0232621431350708
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어 (남은 답변: 모든 질문 해결 완료)",확률 예측에서 MSE Loss 미 사용 이유,-0.016030354,0.0,0.01603035442531109
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야? (남은 답변: Loss Function 예시)",BCE Loss 설명,-0.024741398,0.0,0.0247413981705904
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야? (남은 답변: Loss Function 예시)",LLM Fine-Tuning 의 PEFT,-0.019736523,0.0,0.019736522808670998
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야? (남은 답변: Loss Function 예시)",LoRA,0.046466514,0.0,0.046466514468193054
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야? (남은 답변: Loss Function 예시)",LoRA 와 QLoRA 의 차이,-0.0068254946,0.0,0.006825494579970837
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야? (남은 답변: Loss Function 예시)",Loss Function 관련 실무 경험,-0.034182306,0.0,0.03418230637907982
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야? (남은 답변: Loss Function 예시)",Loss Function 예시,0.97352964,1.0,0.0264703631401062
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야? (남은 답변: Loss Function 예시)",Loss Function 정의,-0.023680178,0.0,0.023680178448557854
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야? (남은 답변: Loss Function 예시)",MBTI / 좋아하는 아이돌,-0.010779295,0.0,0.010779295116662979
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야? (남은 답변: Loss Function 예시)",MSE Loss 설명,0.02040686,0.0,0.02040686085820198
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야? (남은 답변: Loss Function 예시)",MSE Loss 용도,-0.043505907,0.0,0.04350590705871582
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야? (남은 답변: Loss Function 예시)","Multi-Class, Multi-Label 중 BCE 가 좋은 task",-0.004389069,0.0,0.00438906904309988
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야? (남은 답변: Loss Function 예시)",Multi-Label 에서 CE + Softmax 적용 문제점,-0.023636723,0.0,0.023636722937226295
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야? (남은 답변: Loss Function 예시)",PEFT 방법 5가지,-0.043518078,0.0,0.043518077582120895
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야? (남은 답변: Loss Function 예시)",거대 언어 모델 정의,-0.0016917579,0.0,0.0016917579341679811
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야? (남은 답변: Loss Function 예시)",마지막 할 말,-0.007117992,0.0,0.007117991801351309
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야? (남은 답변: Loss Function 예시)",면접 시작 인사,-0.013604489,0.0,0.013604489155113697
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야? (남은 답변: Loss Function 예시)",면접 종료,-0.007972019,0.0,0.007972018793225288
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야? (남은 답변: Loss Function 예시)","인공지능, 머신러닝, 딥러닝 차이",-0.0044559157,0.0,0.004455915652215481
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야? (남은 답변: Loss Function 예시)",잠시 휴식,-0.027336909,0.0,0.027336908504366875
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야? (남은 답변: Loss Function 예시)",확률 예측에서 MSE Loss 미 사용 이유,0.005469816,0.0,0.00546981580555439
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어 (남은 답변: 모든 질문 해결 완료)",BCE Loss 설명,-0.013895619,0.0,0.013895618729293346
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어 (남은 답변: 모든 질문 해결 완료)",LLM Fine-Tuning 의 PEFT,0.0020776214,0.0,0.0020776214078068733
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어 (남은 답변: 모든 질문 해결 완료)",LoRA,0.013990805,0.0,0.013990804553031921
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어 (남은 답변: 모든 질문 해결 완료)",LoRA 와 QLoRA 의 차이,-0.00089763076,0.0,0.0008976307581178844
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어 (남은 답변: 모든 질문 해결 완료)",Loss Function 관련 실무 경험,0.0058847074,0.0,0.005884707439690828
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어 (남은 답변: 모든 질문 해결 완료)",Loss Function 예시,0.0046977988,0.0,0.004697798751294613
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어 (남은 답변: 모든 질문 해결 완료)",Loss Function 정의,-0.00027627806,0.0,0.0002762780641205609
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어 (남은 답변: 모든 질문 해결 완료)",MBTI / 좋아하는 아이돌,-0.019296616,0.0,0.019296616315841675
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어 (남은 답변: 모든 질문 해결 완료)",MSE Loss 설명,0.9736258,1.0,0.026374220848083496
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어 (남은 답변: 모든 질문 해결 완료)",MSE Loss 용도,-0.05321449,0.0,0.05321449041366577
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어 (남은 답변: 모든 질문 해결 완료)","Multi-Class, Multi-Label 중 BCE 가 좋은 task",-0.011707107,0.0,0.011707106605172157
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어 (남은 답변: 모든 질문 해결 완료)",Multi-Label 에서 CE + Softmax 적용 문제점,-0.00535985,0.0,0.005359849892556667
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어 (남은 답변: 모든 질문 해결 완료)",PEFT 방법 5가지,0.010791282,0.0,0.010791282169520855
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어 (남은 답변: 모든 질문 해결 완료)",거대 언어 모델 정의,0.013299684,0.0,0.013299684040248394
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어 (남은 답변: 모든 질문 해결 완료)",마지막 할 말,-0.013640809,0.0,0.013640808872878551
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어 (남은 답변: 모든 질문 해결 완료)",면접 시작 인사,-0.011920678,0.0,0.011920678429305553
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어 (남은 답변: 모든 질문 해결 완료)",면접 종료,-0.023554174,0.0,0.023554174229502678
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어 (남은 답변: 모든 질문 해결 완료)","인공지능, 머신러닝, 딥러닝 차이",-0.0058826255,0.0,0.005882625468075275
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어 (남은 답변: 모든 질문 해결 완료)",잠시 휴식,-0.021942861,0.0,0.0219428613781929
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어 (남은 답변: 모든 질문 해결 완료)",확률 예측에서 MSE Loss 미 사용 이유,-0.017675193,0.0,0.017675193026661873
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아 (남은 답변: 모든 질문 해결 완료)",BCE Loss 설명,0.9818179,1.0,0.018182098865509033
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아 (남은 답변: 모든 질문 해결 완료)",LLM Fine-Tuning 의 PEFT,0.0064683985,0.0,0.006468398496508598
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아 (남은 답변: 모든 질문 해결 완료)",LoRA,0.002780027,0.0,0.00278002698905766
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아 (남은 답변: 모든 질문 해결 완료)",LoRA 와 QLoRA 의 차이,-0.012099788,0.0,0.012099787592887878
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아 (남은 답변: 모든 질문 해결 완료)",Loss Function 관련 실무 경험,0.00056436734,0.0,0.000564367335755378
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아 (남은 답변: 모든 질문 해결 완료)",Loss Function 예시,-0.03117987,0.0,0.031179869547486305
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아 (남은 답변: 모든 질문 해결 완료)",Loss Function 정의,-0.0057124235,0.0,0.005712423473596573
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아 (남은 답변: 모든 질문 해결 완료)",MBTI / 좋아하는 아이돌,-0.016734764,0.0,0.016734763979911804
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아 (남은 답변: 모든 질문 해결 완료)",MSE Loss 설명,-0.0024056784,0.0,0.0024056783877313137
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아 (남은 답변: 모든 질문 해결 완료)",MSE Loss 용도,-0.021633193,0.0,0.021633192896842957
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아 (남은 답변: 모든 질문 해결 완료)","Multi-Class, Multi-Label 중 BCE 가 좋은 task",0.003935693,0.0,0.003935692831873894
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아 (남은 답변: 모든 질문 해결 완료)",Multi-Label 에서 CE + Softmax 적용 문제점,0.008026638,0.0,0.008026638068258762
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아 (남은 답변: 모든 질문 해결 완료)",PEFT 방법 5가지,-0.019570978,0.0,0.019570978358387947
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아 (남은 답변: 모든 질문 해결 완료)",거대 언어 모델 정의,0.009368801,0.0,0.009368800558149815
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아 (남은 답변: 모든 질문 해결 완료)",마지막 할 말,-0.0036509966,0.0,0.0036509966012090445
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아 (남은 답변: 모든 질문 해결 완료)",면접 시작 인사,-0.009926987,0.0,0.009926986880600452
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아 (남은 답변: 모든 질문 해결 완료)",면접 종료,-0.0011470035,0.0,0.0011470034951344132
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아 (남은 답변: 모든 질문 해결 완료)","인공지능, 머신러닝, 딥러닝 차이",0.00936087,0.0,0.009360870346426964
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아 (남은 답변: 모든 질문 해결 완료)",잠시 휴식,-0.024960121,0.0,0.024960121139883995
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아 (남은 답변: 모든 질문 해결 완료)",확률 예측에서 MSE Loss 미 사용 이유,-0.015794802,0.0,0.015794802457094193
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야 (남은 답변: 모든 질문 해결 완료),BCE Loss 설명,0.0041083302,0.0,0.004108330234885216
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야 (남은 답변: 모든 질문 해결 완료),LLM Fine-Tuning 의 PEFT,0.0049953074,0.0,0.004995307419449091
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야 (남은 답변: 모든 질문 해결 완료),LoRA,0.0021065278,0.0,0.0021065277978777885
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야 (남은 답변: 모든 질문 해결 완료),LoRA 와 QLoRA 의 차이,-0.015348351,0.0,0.015348350629210472
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야 (남은 답변: 모든 질문 해결 완료),Loss Function 관련 실무 경험,-0.0076735006,0.0,0.007673500571399927
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야 (남은 답변: 모든 질문 해결 완료),Loss Function 예시,-0.034993306,0.0,0.034993305802345276
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야 (남은 답변: 모든 질문 해결 완료),Loss Function 정의,-0.011684816,0.0,0.01168481633067131
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야 (남은 답변: 모든 질문 해결 완료),MBTI / 좋아하는 아이돌,0.014078107,0.0,0.014078106731176376
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야 (남은 답변: 모든 질문 해결 완료),MSE Loss 설명,-0.01800892,0.0,0.018008919432759285
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야 (남은 답변: 모든 질문 해결 완료),MSE Loss 용도,0.9667788,1.0,0.03322118520736694
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야 (남은 답변: 모든 질문 해결 완료),"Multi-Class, Multi-Label 중 BCE 가 좋은 task",-0.0062859203,0.0,0.006285920273512602
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야 (남은 답변: 모든 질문 해결 완료),Multi-Label 에서 CE + Softmax 적용 문제점,-0.0024882567,0.0,0.0024882566649466753
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야 (남은 답변: 모든 질문 해결 완료),PEFT 방법 5가지,-0.007748884,0.0,0.0077488841488957405
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야 (남은 답변: 모든 질문 해결 완료),거대 언어 모델 정의,0.0049597467,0.0,0.004959746729582548
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야 (남은 답변: 모든 질문 해결 완료),마지막 할 말,-0.019713096,0.0,0.01971309632062912
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야 (남은 답변: 모든 질문 해결 완료),면접 시작 인사,0.020629194,0.0,0.020629193633794785
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야 (남은 답변: 모든 질문 해결 완료),면접 종료,-0.0031970136,0.0,0.003197013633325696
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야 (남은 답변: 모든 질문 해결 완료),"인공지능, 머신러닝, 딥러닝 차이",-0.007386869,0.0,0.007386868819594383
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야 (남은 답변: 모든 질문 해결 완료),잠시 휴식,-0.010018061,0.0,0.0100180609151721
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야 (남은 답변: 모든 질문 해결 완료),확률 예측에서 MSE Loss 미 사용 이유,-0.0061093513,0.0,0.006109351292252541
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지? (남은 답변: MSE Loss 설명),BCE Loss 설명,-0.001771297,0.0,0.001771296956576407
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지? (남은 답변: MSE Loss 설명),LLM Fine-Tuning 의 PEFT,0.00096989574,0.0,0.0009698957437649369
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지? (남은 답변: MSE Loss 설명),LoRA,0.014553818,0.0,0.014553817920386791
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지? (남은 답변: MSE Loss 설명),LoRA 와 QLoRA 의 차이,0.005545311,0.0,0.005545311141759157
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지? (남은 답변: MSE Loss 설명),Loss Function 관련 실무 경험,-0.00427701,0.0,0.004277009982615709
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지? (남은 답변: MSE Loss 설명),Loss Function 예시,-0.020851078,0.0,0.020851077511906624
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지? (남은 답변: MSE Loss 설명),Loss Function 정의,-0.00781277,0.0,0.007812770083546638
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지? (남은 답변: MSE Loss 설명),MBTI / 좋아하는 아이돌,-0.020267077,0.0,0.020267076790332794
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지? (남은 답변: MSE Loss 설명),MSE Loss 설명,0.9874611,1.0,0.012538909912109375
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지? (남은 답변: MSE Loss 설명),MSE Loss 용도,-0.026501972,0.0,0.02650197222828865
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지? (남은 답변: MSE Loss 설명),"Multi-Class, Multi-Label 중 BCE 가 좋은 task",-0.009852726,0.0,0.009852726012468338
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지? (남은 답변: MSE Loss 설명),Multi-Label 에서 CE + Softmax 적용 문제점,-0.0034532228,0.0,0.0034532228019088507
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지? (남은 답변: MSE Loss 설명),PEFT 방법 5가지,0.004800634,0.0,0.004800633992999792
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지? (남은 답변: MSE Loss 설명),거대 언어 모델 정의,0.012099897,0.0,0.012099896557629108
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지? (남은 답변: MSE Loss 설명),마지막 할 말,-0.015835447,0.0,0.01583544723689556
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지? (남은 답변: MSE Loss 설명),면접 시작 인사,-0.014974354,0.0,0.014974353834986687
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지? (남은 답변: MSE Loss 설명),면접 종료,-0.026811881,0.0,0.026811880990862846
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지? (남은 답변: MSE Loss 설명),"인공지능, 머신러닝, 딥러닝 차이",0.00041150436,0.0,0.00041150435572490096
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지? (남은 답변: MSE Loss 설명),잠시 휴식,-0.016188124,0.0,0.01618812419474125
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지? (남은 답변: MSE Loss 설명),확률 예측에서 MSE Loss 미 사용 이유,-0.016053235,0.0,0.01605323515832424
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지 (남은 답변: 모든 질문 해결 완료)",BCE Loss 설명,-0.0132719455,0.0,0.013271945528686047
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지 (남은 답변: 모든 질문 해결 완료)",LLM Fine-Tuning 의 PEFT,0.009346516,0.0,0.009346515871584415
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지 (남은 답변: 모든 질문 해결 완료)",LoRA,-0.011296558,0.0,0.011296558193862438
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지 (남은 답변: 모든 질문 해결 완료)",LoRA 와 QLoRA 의 차이,0.0044328324,0.0,0.004432832356542349
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지 (남은 답변: 모든 질문 해결 완료)",Loss Function 관련 실무 경험,-0.008126519,0.0,0.008126518689095974
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지 (남은 답변: 모든 질문 해결 완료)",Loss Function 예시,0.002251134,0.0,0.0022511340212076902
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지 (남은 답변: 모든 질문 해결 완료)",Loss Function 정의,0.00652769,0.0,0.006527690216898918
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지 (남은 답변: 모든 질문 해결 완료)",MBTI / 좋아하는 아이돌,-0.025763819,0.0,0.025763818994164467
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지 (남은 답변: 모든 질문 해결 완료)",MSE Loss 설명,0.008249982,0.0,0.008249982260167599
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지 (남은 답변: 모든 질문 해결 완료)",MSE Loss 용도,-0.002482044,0.0,0.002482044044882059
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지 (남은 답변: 모든 질문 해결 완료)","Multi-Class, Multi-Label 중 BCE 가 좋은 task",0.007490524,0.0,0.007490524090826511
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지 (남은 답변: 모든 질문 해결 완료)",Multi-Label 에서 CE + Softmax 적용 문제점,-0.0039100065,0.0,0.0039100064896047115
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지 (남은 답변: 모든 질문 해결 완료)",PEFT 방법 5가지,-0.0011970644,0.0,0.0011970644118264318
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지 (남은 답변: 모든 질문 해결 완료)",거대 언어 모델 정의,0.0014746148,0.0,0.0014746148372069001
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지 (남은 답변: 모든 질문 해결 완료)",마지막 할 말,-0.008326845,0.0,0.008326845243573189
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지 (남은 답변: 모든 질문 해결 완료)",면접 시작 인사,0.018304596,0.0,0.018304595723748207
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지 (남은 답변: 모든 질문 해결 완료)",면접 종료,-0.01893119,0.0,0.0189311895519495
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지 (남은 답변: 모든 질문 해결 완료)","인공지능, 머신러닝, 딥러닝 차이",0.007208165,0.0,0.007208164781332016
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지 (남은 답변: 모든 질문 해결 완료)",잠시 휴식,-0.018846586,0.0,0.018846586346626282
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지 (남은 답변: 모든 질문 해결 완료)",확률 예측에서 MSE Loss 미 사용 이유,0.9697824,1.0,0.03021758794784546
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지 (남은 답변: MSE Loss 용도),BCE Loss 설명,-0.012898965,0.0,0.012898964807391167
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지 (남은 답변: MSE Loss 용도),LLM Fine-Tuning 의 PEFT,0.0009690715,0.0,0.0009690715232864022
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지 (남은 답변: MSE Loss 용도),LoRA,-0.0015039659,0.0,0.0015039659338071942
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지 (남은 답변: MSE Loss 용도),LoRA 와 QLoRA 의 차이,-0.008516751,0.0,0.008516751229763031
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지 (남은 답변: MSE Loss 용도),Loss Function 관련 실무 경험,0.0073291035,0.0,0.007329103536903858
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지 (남은 답변: MSE Loss 용도),Loss Function 예시,-0.024284804,0.0,0.024284804239869118
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지 (남은 답변: MSE Loss 용도),Loss Function 정의,-0.021579022,0.0,0.021579021587967873
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지 (남은 답변: MSE Loss 용도),MBTI / 좋아하는 아이돌,0.0004613613,0.0,0.0004613613127730787
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지 (남은 답변: MSE Loss 용도),MSE Loss 설명,-0.02551164,0.0,0.025511639192700386
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지 (남은 답변: MSE Loss 용도),MSE Loss 용도,0.9853603,1.0,0.014639675617218018
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지 (남은 답변: MSE Loss 용도),"Multi-Class, Multi-Label 중 BCE 가 좋은 task",-0.012400261,0.0,0.012400261126458645
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지 (남은 답변: MSE Loss 용도),Multi-Label 에서 CE + Softmax 적용 문제점,-0.0035673422,0.0,0.0035673421807587147
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지 (남은 답변: MSE Loss 용도),PEFT 방법 5가지,-0.007363872,0.0,0.0073638721369206905
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지 (남은 답변: MSE Loss 용도),거대 언어 모델 정의,0.013096775,0.0,0.01309677492827177
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지 (남은 답변: MSE Loss 용도),마지막 할 말,-0.00019934475,0.0,0.00019934475130867213
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지 (남은 답변: MSE Loss 용도),면접 시작 인사,0.0058202925,0.0,0.005820292513817549
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지 (남은 답변: MSE Loss 용도),면접 종료,-0.009301575,0.0,0.009301574900746346
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지 (남은 답변: MSE Loss 용도),"인공지능, 머신러닝, 딥러닝 차이",-0.0059202258,0.0,0.005920225754380226
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지 (남은 답변: MSE Loss 용도),잠시 휴식,-0.018447474,0.0,0.018447473645210266
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지 (남은 답변: MSE Loss 용도),확률 예측에서 MSE Loss 미 사용 이유,0.0069231004,0.0,0.00692310044541955
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야? (남은 답변: 확률 예측에서 MSE Loss 미 사용 이유),BCE Loss 설명,-0.022743817,0.0,0.022743817418813705
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야? (남은 답변: 확률 예측에서 MSE Loss 미 사용 이유),LLM Fine-Tuning 의 PEFT,0.014340839,0.0,0.014340839348733425
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야? (남은 답변: 확률 예측에서 MSE Loss 미 사용 이유),LoRA,0.003959925,0.0,0.003959924913942814
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야? (남은 답변: 확률 예측에서 MSE Loss 미 사용 이유),LoRA 와 QLoRA 의 차이,-0.0008006973,0.0,0.0008006973075680435
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야? (남은 답변: 확률 예측에서 MSE Loss 미 사용 이유),Loss Function 관련 실무 경험,-0.004868674,0.0,0.004868674091994762
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야? (남은 답변: 확률 예측에서 MSE Loss 미 사용 이유),Loss Function 예시,0.0043036076,0.0,0.004303607624024153
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야? (남은 답변: 확률 예측에서 MSE Loss 미 사용 이유),Loss Function 정의,0.007240442,0.0,0.007240442093461752
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야? (남은 답변: 확률 예측에서 MSE Loss 미 사용 이유),MBTI / 좋아하는 아이돌,-0.01739954,0.0,0.017399540171027184
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야? (남은 답변: 확률 예측에서 MSE Loss 미 사용 이유),MSE Loss 설명,-0.0075623165,0.0,0.0075623164884746075
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야? (남은 답변: 확률 예측에서 MSE Loss 미 사용 이유),MSE Loss 용도,0.0023370322,0.0,0.0023370322305709124
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야? (남은 답변: 확률 예측에서 MSE Loss 미 사용 이유),"Multi-Class, Multi-Label 중 BCE 가 좋은 task",-0.012187642,0.0,0.01218764204531908
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야? (남은 답변: 확률 예측에서 MSE Loss 미 사용 이유),Multi-Label 에서 CE + Softmax 적용 문제점,0.0041091903,0.0,0.004109190311282873
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야? (남은 답변: 확률 예측에서 MSE Loss 미 사용 이유),PEFT 방법 5가지,-0.0032559969,0.0,0.0032559968531131744
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야? (남은 답변: 확률 예측에서 MSE Loss 미 사용 이유),거대 언어 모델 정의,-0.021314949,0.0,0.021314948797225952
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야? (남은 답변: 확률 예측에서 MSE Loss 미 사용 이유),마지막 할 말,-0.020617591,0.0,0.020617591217160225
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야? (남은 답변: 확률 예측에서 MSE Loss 미 사용 이유),면접 시작 인사,-0.014993505,0.0,0.014993504621088505
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야? (남은 답변: 확률 예측에서 MSE Loss 미 사용 이유),면접 종료,0.0038220454,0.0,0.003822045400738716
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야? (남은 답변: 확률 예측에서 MSE Loss 미 사용 이유),"인공지능, 머신러닝, 딥러닝 차이",0.0023371107,0.0,0.0023371106944978237
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야? (남은 답변: 확률 예측에서 MSE Loss 미 사용 이유),잠시 휴식,0.0059869024,0.0,0.005986902397125959
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야? (남은 답변: 확률 예측에서 MSE Loss 미 사용 이유),확률 예측에서 MSE Loss 미 사용 이유,0.991382,1.0,0.008617997169494629
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고? (남은 답변: 모든 질문 해결 완료),BCE Loss 설명,0.709307,1.0,0.2906929850578308
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고? (남은 답변: 모든 질문 해결 완료),LLM Fine-Tuning 의 PEFT,-0.032777514,0.0,0.032777514308691025
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고? (남은 답변: 모든 질문 해결 완료),LoRA,0.002493706,0.0,0.002493706066161394
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고? (남은 답변: 모든 질문 해결 완료),LoRA 와 QLoRA 의 차이,-0.028934916,0.0,0.028934916481375694
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고? (남은 답변: 모든 질문 해결 완료),Loss Function 관련 실무 경험,-0.020765554,0.0,0.020765554159879684
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고? (남은 답변: 모든 질문 해결 완료),Loss Function 예시,-0.010496357,0.0,0.010496357455849648
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고? (남은 답변: 모든 질문 해결 완료),Loss Function 정의,-0.035240334,0.0,0.035240333527326584
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고? (남은 답변: 모든 질문 해결 완료),MBTI / 좋아하는 아이돌,-0.023982663,0.0,0.023982662707567215
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고? (남은 답변: 모든 질문 해결 완료),MSE Loss 설명,-0.02211187,0.0,0.02211187034845352
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고? (남은 답변: 모든 질문 해결 완료),MSE Loss 용도,-0.05408141,0.0,0.05408141016960144
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고? (남은 답변: 모든 질문 해결 완료),"Multi-Class, Multi-Label 중 BCE 가 좋은 task",0.2576618,0.0,0.2576617896556854
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고? (남은 답변: 모든 질문 해결 완료),Multi-Label 에서 CE + Softmax 적용 문제점,0.0035064002,0.0,0.0035064001567661762
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고? (남은 답변: 모든 질문 해결 완료),PEFT 방법 5가지,0.00544801,0.0,0.005448009818792343
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고? (남은 답변: 모든 질문 해결 완료),거대 언어 모델 정의,0.013593042,0.0,0.013593042269349098
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고? (남은 답변: 모든 질문 해결 완료),마지막 할 말,-0.01412479,0.0,0.014124790206551552
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고? (남은 답변: 모든 질문 해결 완료),면접 시작 인사,0.0013422722,0.0,0.0013422721531242132
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고? (남은 답변: 모든 질문 해결 완료),면접 종료,0.01690926,0.0,0.01690926030278206
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고? (남은 답변: 모든 질문 해결 완료),"인공지능, 머신러닝, 딥러닝 차이",-0.011008277,0.0,0.011008276604115963
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고? (남은 답변: 모든 질문 해결 완료),잠시 휴식,-0.0036787386,0.0,0.0036787386052310467
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고? (남은 답변: 모든 질문 해결 완료),확률 예측에서 MSE Loss 미 사용 이유,0.15079017,0.0,0.15079016983509064
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고 (남은 답변: 핵심 아이디어),BCE Loss 설명,0.9841473,1.0,0.015852689743041992
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고 (남은 답변: 핵심 아이디어),LLM Fine-Tuning 의 PEFT,0.005123827,0.0,0.005123827140778303
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고 (남은 답변: 핵심 아이디어),LoRA,-0.0042798985,0.0,0.004279898479580879
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고 (남은 답변: 핵심 아이디어),LoRA 와 QLoRA 의 차이,-0.013398342,0.0,0.013398341834545135
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고 (남은 답변: 핵심 아이디어),Loss Function 관련 실무 경험,-0.007937441,0.0,0.007937440648674965
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고 (남은 답변: 핵심 아이디어),Loss Function 예시,-0.01000977,0.0,0.010009770281612873
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고 (남은 답변: 핵심 아이디어),Loss Function 정의,-0.02004338,0.0,0.020043380558490753
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고 (남은 답변: 핵심 아이디어),MBTI / 좋아하는 아이돌,-0.003460963,0.0,0.00346096302382648
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고 (남은 답변: 핵심 아이디어),MSE Loss 설명,-0.020808894,0.0,0.02080889418721199
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고 (남은 답변: 핵심 아이디어),MSE Loss 용도,-0.0030298575,0.0,0.0030298575293272734
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고 (남은 답변: 핵심 아이디어),"Multi-Class, Multi-Label 중 BCE 가 좋은 task",0.005319326,0.0,0.005319326184689999
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고 (남은 답변: 핵심 아이디어),Multi-Label 에서 CE + Softmax 적용 문제점,-0.013227928,0.0,0.013227928429841995
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고 (남은 답변: 핵심 아이디어),PEFT 방법 5가지,-0.004358365,0.0,0.004358365200459957
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고 (남은 답변: 핵심 아이디어),거대 언어 모델 정의,-0.0020725413,0.0,0.0020725412759929895
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고 (남은 답변: 핵심 아이디어),마지막 할 말,0.0030525546,0.0,0.0030525545589625835
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고 (남은 답변: 핵심 아이디어),면접 시작 인사,0.0037588945,0.0,0.0037588945124298334
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고 (남은 답변: 핵심 아이디어),면접 종료,0.0017586995,0.0,0.0017586995381861925
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고 (남은 답변: 핵심 아이디어),"인공지능, 머신러닝, 딥러닝 차이",-0.002560648,0.0,0.002560647903010249
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고 (남은 답변: 핵심 아이디어),잠시 휴식,-0.020392936,0.0,0.02039293572306633
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고 (남은 답변: 핵심 아이디어),확률 예측에서 MSE Loss 미 사용 이유,-0.02507147,0.0,0.02507147006690502
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거 (남은 답변: 모든 질문 해결 완료)",BCE Loss 설명,-0.025200583,0.0,0.025200583040714264
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거 (남은 답변: 모든 질문 해결 완료)",LLM Fine-Tuning 의 PEFT,-0.009389542,0.0,0.009389542043209076
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거 (남은 답변: 모든 질문 해결 완료)",LoRA,-0.034404762,0.0,0.03440476208925247
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거 (남은 답변: 모든 질문 해결 완료)",LoRA 와 QLoRA 의 차이,0.01518223,0.0,0.015182229690253735
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거 (남은 답변: 모든 질문 해결 완료)",Loss Function 관련 실무 경험,0.01158065,0.0,0.011580649763345718
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거 (남은 답변: 모든 질문 해결 완료)",Loss Function 예시,-0.011554053,0.0,0.01155405305325985
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거 (남은 답변: 모든 질문 해결 완료)",Loss Function 정의,-0.013871682,0.0,0.01387168187648058
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거 (남은 답변: 모든 질문 해결 완료)",MBTI / 좋아하는 아이돌,-0.00456346,0.0,0.004563460126519203
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거 (남은 답변: 모든 질문 해결 완료)",MSE Loss 설명,-0.016733931,0.0,0.016733931377530098
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거 (남은 답변: 모든 질문 해결 완료)",MSE Loss 용도,-0.00061309314,0.0,0.0006130931433290243
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거 (남은 답변: 모든 질문 해결 완료)","Multi-Class, Multi-Label 중 BCE 가 좋은 task",0.9405524,1.0,0.05944758653640747
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거 (남은 답변: 모든 질문 해결 완료)",Multi-Label 에서 CE + Softmax 적용 문제점,-0.0125928745,0.0,0.012592874467372894
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거 (남은 답변: 모든 질문 해결 완료)",PEFT 방법 5가지,0.013635461,0.0,0.01363546121865511
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거 (남은 답변: 모든 질문 해결 완료)",거대 언어 모델 정의,-0.009475511,0.0,0.009475510567426682
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거 (남은 답변: 모든 질문 해결 완료)",마지막 할 말,-0.023072535,0.0,0.023072535172104836
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거 (남은 답변: 모든 질문 해결 완료)",면접 시작 인사,0.0012231776,0.0,0.0012231776490807533
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거 (남은 답변: 모든 질문 해결 완료)",면접 종료,0.00803517,0.0,0.008035169914364815
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거 (남은 답변: 모든 질문 해결 완료)","인공지능, 머신러닝, 딥러닝 차이",0.0015212134,0.0,0.0015212134458124638
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거 (남은 답변: 모든 질문 해결 완료)",잠시 휴식,0.03224568,0.0,0.03224568068981171
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거 (남은 답변: 모든 질문 해결 완료)",확률 예측에서 MSE Loss 미 사용 이유,-0.021787276,0.0,0.02178727649152279
"BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야? (남은 답변: 핵심 아이디어, 수식)",BCE Loss 설명,0.978949,1.0,0.021050989627838135
"BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야? (남은 답변: 핵심 아이디어, 수식)",LLM Fine-Tuning 의 PEFT,0.0018188087,0.0,0.001818808726966381
"BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야? (남은 답변: 핵심 아이디어, 수식)",LoRA,-0.002264709,0.0,0.0022647089790552855
"BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야? (남은 답변: 핵심 아이디어, 수식)",LoRA 와 QLoRA 의 차이,-0.020262025,0.0,0.02026202529668808
"BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야? (남은 답변: 핵심 아이디어, 수식)",Loss Function 관련 실무 경험,-0.001298661,0.0,0.0012986609945073724
"BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야? (남은 답변: 핵심 아이디어, 수식)",Loss Function 예시,0.006201326,0.0,0.006201325915753841
"BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야? (남은 답변: 핵심 아이디어, 수식)",Loss Function 정의,-0.014488629,0.0,0.014488629065454006
"BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야? (남은 답변: 핵심 아이디어, 수식)",MBTI / 좋아하는 아이돌,-0.016654063,0.0,0.016654063016176224
"BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야? (남은 답변: 핵심 아이디어, 수식)",MSE Loss 설명,-0.008170994,0.0,0.008170993998646736
"BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야? (남은 답변: 핵심 아이디어, 수식)",MSE Loss 용도,0.01386212,0.0,0.013862119987607002
"BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야? (남은 답변: 핵심 아이디어, 수식)","Multi-Class, Multi-Label 중 BCE 가 좋은 task",-0.022217104,0.0,0.022217104211449623
"BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야? (남은 답변: 핵심 아이디어, 수식)",Multi-Label 에서 CE + Softmax 적용 문제점,-0.0023413945,0.0,0.0023413945455104113
"BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야? (남은 답변: 핵심 아이디어, 수식)",PEFT 방법 5가지,-0.0027483616,0.0,0.0027483615558594465
"BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야? (남은 답변: 핵심 아이디어, 수식)",거대 언어 모델 정의,6.04665e-06,0.0,6.046650014468469e-06
"BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야? (남은 답변: 핵심 아이디어, 수식)",마지막 할 말,0.008654261,0.0,0.008654261007905006
"BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야? (남은 답변: 핵심 아이디어, 수식)",면접 시작 인사,-0.0022512712,0.0,0.0022512711584568024
"BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야? (남은 답변: 핵심 아이디어, 수식)",면접 종료,-0.008474948,0.0,0.00847494788467884
"BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야? (남은 답변: 핵심 아이디어, 수식)","인공지능, 머신러닝, 딥러닝 차이",-0.015615238,0.0,0.01561523787677288
"BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야? (남은 답변: 핵심 아이디어, 수식)",잠시 휴식,-0.029215034,0.0,0.02921503409743309
"BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야? (남은 답변: 핵심 아이디어, 수식)",확률 예측에서 MSE Loss 미 사용 이유,-0.022403738,0.0,0.022403737530112267
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고 (남은 답변: 수식)",BCE Loss 설명,0.98259205,1.0,0.01740795373916626
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고 (남은 답변: 수식)",LLM Fine-Tuning 의 PEFT,0.005722178,0.0,0.005722178146243095
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고 (남은 답변: 수식)",LoRA,0.0018842464,0.0,0.0018842463614419103
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고 (남은 답변: 수식)",LoRA 와 QLoRA 의 차이,-0.018757204,0.0,0.018757203593850136
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고 (남은 답변: 수식)",Loss Function 관련 실무 경험,-0.020403443,0.0,0.020403442904353142
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고 (남은 답변: 수식)",Loss Function 예시,-0.008527324,0.0,0.008527323603630066
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고 (남은 답변: 수식)",Loss Function 정의,-0.0052015367,0.0,0.005201536696404219
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고 (남은 답변: 수식)",MBTI / 좋아하는 아이돌,-0.025446512,0.0,0.025446511805057526
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고 (남은 답변: 수식)",MSE Loss 설명,0.0051269545,0.0,0.005126954521983862
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고 (남은 답변: 수식)",MSE Loss 용도,-0.00626078,0.0,0.006260780151933432
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고 (남은 답변: 수식)","Multi-Class, Multi-Label 중 BCE 가 좋은 task",0.0062929783,0.0,0.006292978301644325
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고 (남은 답변: 수식)",Multi-Label 에서 CE + Softmax 적용 문제점,0.0032462857,0.0,0.003246285719797015
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고 (남은 답변: 수식)",PEFT 방법 5가지,-0.019960677,0.0,0.01996067725121975
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고 (남은 답변: 수식)",거대 언어 모델 정의,-0.0017645748,0.0,0.0017645747866481543
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고 (남은 답변: 수식)",마지막 할 말,-0.004176117,0.0,0.004176117014139891
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고 (남은 답변: 수식)",면접 시작 인사,-0.0010955604,0.0,0.0010955603793263435
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고 (남은 답변: 수식)",면접 종료,0.017938659,0.0,0.017938658595085144
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고 (남은 답변: 수식)","인공지능, 머신러닝, 딥러닝 차이",-0.002628686,0.0,0.0026286859065294266
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고 (남은 답변: 수식)",잠시 휴식,-0.022755345,0.0,0.022755345329642296
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고 (남은 답변: 수식)",확률 예측에서 MSE Loss 미 사용 이유,-0.016446186,0.0,0.0164461862295866
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')] (남은 답변: 핵심 아이디어)",BCE Loss 설명,0.9827548,1.0,0.017245173454284668
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')] (남은 답변: 핵심 아이디어)",LLM Fine-Tuning 의 PEFT,0.0022699847,0.0,0.0022699846886098385
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')] (남은 답변: 핵심 아이디어)",LoRA,-0.012270702,0.0,0.012270702049136162
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')] (남은 답변: 핵심 아이디어)",LoRA 와 QLoRA 의 차이,-0.012228145,0.0,0.012228145264089108
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')] (남은 답변: 핵심 아이디어)",Loss Function 관련 실무 경험,-0.010061204,0.0,0.010061204433441162
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')] (남은 답변: 핵심 아이디어)",Loss Function 예시,-0.013854855,0.0,0.013854854740202427
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')] (남은 답변: 핵심 아이디어)",Loss Function 정의,-0.015747072,0.0,0.01574707217514515
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')] (남은 답변: 핵심 아이디어)",MBTI / 좋아하는 아이돌,-0.017650656,0.0,0.017650656402111053
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')] (남은 답변: 핵심 아이디어)",MSE Loss 설명,-0.008155132,0.0,0.008155131712555885
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')] (남은 답변: 핵심 아이디어)",MSE Loss 용도,0.0033857545,0.0,0.0033857545349746943
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')] (남은 답변: 핵심 아이디어)","Multi-Class, Multi-Label 중 BCE 가 좋은 task",0.009891462,0.0,0.009891461580991745
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')] (남은 답변: 핵심 아이디어)",Multi-Label 에서 CE + Softmax 적용 문제점,-0.007420075,0.0,0.007420075125992298
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')] (남은 답변: 핵심 아이디어)",PEFT 방법 5가지,-0.012132658,0.0,0.012132657691836357
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')] (남은 답변: 핵심 아이디어)",거대 언어 모델 정의,0.0010989103,0.0,0.0010989103466272354
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')] (남은 답변: 핵심 아이디어)",마지막 할 말,-0.0032599862,0.0,0.00325998617336154
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')] (남은 답변: 핵심 아이디어)",면접 시작 인사,-0.004462537,0.0,0.0044625368900597095
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')] (남은 답변: 핵심 아이디어)",면접 종료,0.0007823607,0.0,0.0007823606720194221
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')] (남은 답변: 핵심 아이디어)","인공지능, 머신러닝, 딥러닝 차이",0.0027704535,0.0,0.0027704534586519003
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')] (남은 답변: 핵심 아이디어)",잠시 휴식,-0.021950342,0.0,0.021950341761112213
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')] (남은 답변: 핵심 아이디어)",확률 예측에서 MSE Loss 미 사용 이유,-0.01689042,0.0,0.016890419647097588
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다 (남은 답변: 모든 질문 해결 완료)",BCE Loss 설명,-0.013991399,0.0,0.013991398736834526
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다 (남은 답변: 모든 질문 해결 완료)",LLM Fine-Tuning 의 PEFT,-0.015291191,0.0,0.015291190706193447
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다 (남은 답변: 모든 질문 해결 완료)",LoRA,-0.033283204,0.0,0.03328320384025574
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다 (남은 답변: 모든 질문 해결 완료)",LoRA 와 QLoRA 의 차이,0.008375671,0.0,0.00837567076086998
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다 (남은 답변: 모든 질문 해결 완료)",Loss Function 관련 실무 경험,0.0126284035,0.0,0.0126284034922719
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다 (남은 답변: 모든 질문 해결 완료)",Loss Function 예시,-0.01696977,0.0,0.016969770193099976
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다 (남은 답변: 모든 질문 해결 완료)",Loss Function 정의,-0.014610484,0.0,0.01461048424243927
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다 (남은 답변: 모든 질문 해결 완료)",MBTI / 좋아하는 아이돌,0.002936479,0.0,0.0029364789370447397
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다 (남은 답변: 모든 질문 해결 완료)",MSE Loss 설명,-0.011926205,0.0,0.011926204897463322
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다 (남은 답변: 모든 질문 해결 완료)",MSE Loss 용도,0.020275164,0.0,0.020275164395570755
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다 (남은 답변: 모든 질문 해결 완료)","Multi-Class, Multi-Label 중 BCE 가 좋은 task",0.9286703,1.0,0.07132971286773682
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다 (남은 답변: 모든 질문 해결 완료)",Multi-Label 에서 CE + Softmax 적용 문제점,-0.00046402562,0.0,0.00046402562293224037
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다 (남은 답변: 모든 질문 해결 완료)",PEFT 방법 5가지,0.025995394,0.0,0.025995394214987755
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다 (남은 답변: 모든 질문 해결 완료)",거대 언어 모델 정의,-0.016306233,0.0,0.016306232661008835
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다 (남은 답변: 모든 질문 해결 완료)",마지막 할 말,-0.04335341,0.0,0.04335340857505798
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다 (남은 답변: 모든 질문 해결 완료)",면접 시작 인사,-0.007274305,0.0,0.007274304982274771
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다 (남은 답변: 모든 질문 해결 완료)",면접 종료,0.017320352,0.0,0.01732035167515278
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다 (남은 답변: 모든 질문 해결 완료)","인공지능, 머신러닝, 딥러닝 차이",-0.0030211802,0.0,0.0030211801640689373
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다 (남은 답변: 모든 질문 해결 완료)",잠시 휴식,0.04409455,0.0,0.04409455135464668
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다 (남은 답변: 모든 질문 해결 완료)",확률 예측에서 MSE Loss 미 사용 이유,-0.026320983,0.0,0.026320982724428177
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거 (남은 답변: 핵심 아이디어),BCE Loss 설명,0.98650557,1.0,0.013494431972503662
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거 (남은 답변: 핵심 아이디어),LLM Fine-Tuning 의 PEFT,0.007531689,0.0,0.007531689014285803
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거 (남은 답변: 핵심 아이디어),LoRA,0.002799174,0.0,0.0027991740498691797
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거 (남은 답변: 핵심 아이디어),LoRA 와 QLoRA 의 차이,-0.011249258,0.0,0.011249258182942867
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거 (남은 답변: 핵심 아이디어),Loss Function 관련 실무 경험,-0.019389853,0.0,0.01938985288143158
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거 (남은 답변: 핵심 아이디어),Loss Function 예시,-0.0038629354,0.0,0.0038629353512078524
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거 (남은 답변: 핵심 아이디어),Loss Function 정의,-0.013071117,0.0,0.013071116991341114
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거 (남은 답변: 핵심 아이디어),MBTI / 좋아하는 아이돌,-0.015723545,0.0,0.015723545104265213
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거 (남은 답변: 핵심 아이디어),MSE Loss 설명,-0.020006167,0.0,0.020006166771054268
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거 (남은 답변: 핵심 아이디어),MSE Loss 용도,0.0058313143,0.0,0.005831314250826836
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거 (남은 답변: 핵심 아이디어),"Multi-Class, Multi-Label 중 BCE 가 좋은 task",-0.026897935,0.0,0.026897935196757317
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거 (남은 답변: 핵심 아이디어),Multi-Label 에서 CE + Softmax 적용 문제점,-0.013219649,0.0,0.013219648972153664
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거 (남은 답변: 핵심 아이디어),PEFT 방법 5가지,-0.0061816564,0.0,0.0061816563829779625
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거 (남은 답변: 핵심 아이디어),거대 언어 모델 정의,0.003949836,0.0,0.003949835896492004
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거 (남은 답변: 핵심 아이디어),마지막 할 말,0.0012731303,0.0,0.0012731302995234728
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거 (남은 답변: 핵심 아이디어),면접 시작 인사,0.0033700836,0.0,0.003370083635672927
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거 (남은 답변: 핵심 아이디어),면접 종료,0.0096650915,0.0,0.009665091522037983
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거 (남은 답변: 핵심 아이디어),"인공지능, 머신러닝, 딥러닝 차이",-0.00037188965,0.0,0.0003718896477948874
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거 (남은 답변: 핵심 아이디어),잠시 휴식,-0.013618855,0.0,0.01361885480582714
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거 (남은 답변: 핵심 아이디어),확률 예측에서 MSE Loss 미 사용 이유,-0.015324634,0.0,0.015324633568525314
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지 (남은 답변: 모든 질문 해결 완료)",BCE Loss 설명,0.007825469,0.0,0.00782546866685152
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지 (남은 답변: 모든 질문 해결 완료)",LLM Fine-Tuning 의 PEFT,-0.0076051084,0.0,0.007605108432471752
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지 (남은 답변: 모든 질문 해결 완료)",LoRA,-0.044722904,0.0,0.04472290351986885
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지 (남은 답변: 모든 질문 해결 완료)",LoRA 와 QLoRA 의 차이,0.059633575,0.0,0.05963357537984848
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지 (남은 답변: 모든 질문 해결 완료)",Loss Function 관련 실무 경험,0.009729779,0.0,0.009729779325425625
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지 (남은 답변: 모든 질문 해결 완료)",Loss Function 예시,-0.024023728,0.0,0.02402372844517231
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지 (남은 답변: 모든 질문 해결 완료)",Loss Function 정의,-0.0021270823,0.0,0.002127082319930196
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지 (남은 답변: 모든 질문 해결 완료)",MBTI / 좋아하는 아이돌,-0.015457289,0.0,0.015457289293408394
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지 (남은 답변: 모든 질문 해결 완료)",MSE Loss 설명,-0.004828082,0.0,0.0048280819319188595
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지 (남은 답변: 모든 질문 해결 완료)",MSE Loss 용도,0.06303455,0.0,0.0630345493555069
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지 (남은 답변: 모든 질문 해결 완료)","Multi-Class, Multi-Label 중 BCE 가 좋은 task",0.91475606,1.0,0.08524394035339355
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지 (남은 답변: 모든 질문 해결 완료)",Multi-Label 에서 CE + Softmax 적용 문제점,0.009573717,0.0,0.009573716670274734
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지 (남은 답변: 모든 질문 해결 완료)",PEFT 방법 5가지,-0.0053466605,0.0,0.005346660502254963
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지 (남은 답변: 모든 질문 해결 완료)",거대 언어 모델 정의,-0.01062815,0.0,0.010628149844706059
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지 (남은 답변: 모든 질문 해결 완료)",마지막 할 말,-0.04027031,0.0,0.04027030989527702
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지 (남은 답변: 모든 질문 해결 완료)",면접 시작 인사,-0.00033443244,0.0,0.000334432435920462
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지 (남은 답변: 모든 질문 해결 완료)",면접 종료,-0.0029614118,0.0,0.002961411839351058
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지 (남은 답변: 모든 질문 해결 완료)","인공지능, 머신러닝, 딥러닝 차이",0.0062810713,0.0,0.006281071342527866
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지 (남은 답변: 모든 질문 해결 완료)",잠시 휴식,0.00096303265,0.0,0.0009630326530896127
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지 (남은 답변: 모든 질문 해결 완료)",확률 예측에서 MSE Loss 미 사용 이유,-0.03591562,0.0,0.035915620625019073
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야? (남은 답변: BCE 가 좋은 task, BCE 가 좋은 이유)",BCE Loss 설명,-0.009649502,0.0,0.009649502113461494
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야? (남은 답변: BCE 가 좋은 task, BCE 가 좋은 이유)",LLM Fine-Tuning 의 PEFT,-0.002142144,0.0,0.002142143901437521
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야? (남은 답변: BCE 가 좋은 task, BCE 가 좋은 이유)",LoRA,-0.0062281974,0.0,0.006228197365999222
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야? (남은 답변: BCE 가 좋은 task, BCE 가 좋은 이유)",LoRA 와 QLoRA 의 차이,-0.00167573,0.0,0.0016757299890741706
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야? (남은 답변: BCE 가 좋은 task, BCE 가 좋은 이유)",Loss Function 관련 실무 경험,-0.004931907,0.0,0.004931907169520855
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야? (남은 답변: BCE 가 좋은 task, BCE 가 좋은 이유)",Loss Function 예시,-0.005533987,0.0,0.005533987190574408
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야? (남은 답변: BCE 가 좋은 task, BCE 가 좋은 이유)",Loss Function 정의,-0.009601342,0.0,0.009601341560482979
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야? (남은 답변: BCE 가 좋은 task, BCE 가 좋은 이유)",MBTI / 좋아하는 아이돌,0.004739409,0.0,0.0047394088469445705
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야? (남은 답변: BCE 가 좋은 task, BCE 가 좋은 이유)",MSE Loss 설명,0.0017170089,0.0,0.0017170088831335306
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야? (남은 답변: BCE 가 좋은 task, BCE 가 좋은 이유)",MSE Loss 용도,0.006999632,0.0,0.006999631877988577
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야? (남은 답변: BCE 가 좋은 task, BCE 가 좋은 이유)","Multi-Class, Multi-Label 중 BCE 가 좋은 task",0.995326,1.0,0.004674017429351807
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야? (남은 답변: BCE 가 좋은 task, BCE 가 좋은 이유)",Multi-Label 에서 CE + Softmax 적용 문제점,-0.0064626224,0.0,0.006462622433900833
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야? (남은 답변: BCE 가 좋은 task, BCE 가 좋은 이유)",PEFT 방법 5가지,-0.011324827,0.0,0.011324826627969742
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야? (남은 답변: BCE 가 좋은 task, BCE 가 좋은 이유)",거대 언어 모델 정의,-0.00049994653,0.0,0.0004999465309083462
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야? (남은 답변: BCE 가 좋은 task, BCE 가 좋은 이유)",마지막 할 말,0.0006044576,0.0,0.0006044575711712241
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야? (남은 답변: BCE 가 좋은 task, BCE 가 좋은 이유)",면접 시작 인사,-0.00016705043,0.0,0.00016705042799003422
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야? (남은 답변: BCE 가 좋은 task, BCE 가 좋은 이유)",면접 종료,0.0039997534,0.0,0.003999753389507532
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야? (남은 답변: BCE 가 좋은 task, BCE 가 좋은 이유)","인공지능, 머신러닝, 딥러닝 차이",-0.010654143,0.0,0.010654143057763577
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야? (남은 답변: BCE 가 좋은 task, BCE 가 좋은 이유)",잠시 휴식,0.009163154,0.0,0.009163154289126396
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야? (남은 답변: BCE 가 좋은 task, BCE 가 좋은 이유)",확률 예측에서 MSE Loss 미 사용 이유,-0.0008269931,0.0,0.0008269930840469897
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!! (남은 답변: BCE 가 좋은 task, BCE 가 좋은 이유)",BCE Loss 설명,-0.012104163,0.0,0.012104162946343422
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!! (남은 답변: BCE 가 좋은 task, BCE 가 좋은 이유)",LLM Fine-Tuning 의 PEFT,-0.0031645247,0.0,0.0031645246781408787
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!! (남은 답변: BCE 가 좋은 task, BCE 가 좋은 이유)",LoRA,-0.006106356,0.0,0.006106356158852577
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!! (남은 답변: BCE 가 좋은 task, BCE 가 좋은 이유)",LoRA 와 QLoRA 의 차이,-0.001463245,0.0,0.0014632450183853507
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!! (남은 답변: BCE 가 좋은 task, BCE 가 좋은 이유)",Loss Function 관련 실무 경험,-0.0057405774,0.0,0.005740577355027199
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!! (남은 답변: BCE 가 좋은 task, BCE 가 좋은 이유)",Loss Function 예시,-0.012191292,0.0,0.012191291898488998
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!! (남은 답변: BCE 가 좋은 task, BCE 가 좋은 이유)",Loss Function 정의,-0.008335199,0.0,0.00833519920706749
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!! (남은 답변: BCE 가 좋은 task, BCE 가 좋은 이유)",MBTI / 좋아하는 아이돌,0.003551165,0.0,0.003551165107637644
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!! (남은 답변: BCE 가 좋은 task, BCE 가 좋은 이유)",MSE Loss 설명,-0.0042811534,0.0,0.004281153436750174
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!! (남은 답변: BCE 가 좋은 task, BCE 가 좋은 이유)",MSE Loss 용도,0.006584746,0.0,0.006584745831787586
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!! (남은 답변: BCE 가 좋은 task, BCE 가 좋은 이유)","Multi-Class, Multi-Label 중 BCE 가 좋은 task",0.9958674,1.0,0.004132628440856934
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!! (남은 답변: BCE 가 좋은 task, BCE 가 좋은 이유)",Multi-Label 에서 CE + Softmax 적용 문제점,-0.0012474832,0.0,0.0012474831892177463
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!! (남은 답변: BCE 가 좋은 task, BCE 가 좋은 이유)",PEFT 방법 5가지,-0.008314317,0.0,0.008314317092299461
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!! (남은 답변: BCE 가 좋은 task, BCE 가 좋은 이유)",거대 언어 모델 정의,0.0042295456,0.0,0.004229545593261719
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!! (남은 답변: BCE 가 좋은 task, BCE 가 좋은 이유)",마지막 할 말,-0.0018238972,0.0,0.0018238972406834364
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!! (남은 답변: BCE 가 좋은 task, BCE 가 좋은 이유)",면접 시작 인사,-0.0011104415,0.0,0.001110441517084837
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!! (남은 답변: BCE 가 좋은 task, BCE 가 좋은 이유)",면접 종료,0.0028943536,0.0,0.002894353587180376
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!! (남은 답변: BCE 가 좋은 task, BCE 가 좋은 이유)","인공지능, 머신러닝, 딥러닝 차이",-0.005768067,0.0,0.005768067203462124
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!! (남은 답변: BCE 가 좋은 task, BCE 가 좋은 이유)",잠시 휴식,0.013577872,0.0,0.013577871955931187
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!! (남은 답변: BCE 가 좋은 task, BCE 가 좋은 이유)",확률 예측에서 MSE Loss 미 사용 이유,0.0003247329,0.0,0.00032473288592882454
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어 (남은 답변: BCE 가 좋은 이유)",BCE Loss 설명,-0.010902953,0.0,0.010902953334152699
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어 (남은 답변: BCE 가 좋은 이유)",LLM Fine-Tuning 의 PEFT,8.131581e-05,0.0,8.13158112578094e-05
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어 (남은 답변: BCE 가 좋은 이유)",LoRA,-0.0067708674,0.0,0.0067708673886954784
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어 (남은 답변: BCE 가 좋은 이유)",LoRA 와 QLoRA 의 차이,-0.0042612767,0.0,0.004261276684701443
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어 (남은 답변: BCE 가 좋은 이유)",Loss Function 관련 실무 경험,-0.0050259093,0.0,0.005025909282267094
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어 (남은 답변: BCE 가 좋은 이유)",Loss Function 예시,-0.005771483,0.0,0.005771482829004526
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어 (남은 답변: BCE 가 좋은 이유)",Loss Function 정의,-0.009848478,0.0,0.009848478250205517
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어 (남은 답변: BCE 가 좋은 이유)",MBTI / 좋아하는 아이돌,0.0020522731,0.0,0.0020522731356322765
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어 (남은 답변: BCE 가 좋은 이유)",MSE Loss 설명,0.0036649515,0.0,0.003664951538667083
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어 (남은 답변: BCE 가 좋은 이유)",MSE Loss 용도,0.006194711,0.0,0.006194711197167635
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어 (남은 답변: BCE 가 좋은 이유)","Multi-Class, Multi-Label 중 BCE 가 좋은 task",0.99417585,1.0,0.005824148654937744
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어 (남은 답변: BCE 가 좋은 이유)",Multi-Label 에서 CE + Softmax 적용 문제점,-0.0056588254,0.0,0.005658825393766165
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어 (남은 답변: BCE 가 좋은 이유)",PEFT 방법 5가지,-0.0099916505,0.0,0.009991650469601154
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어 (남은 답변: BCE 가 좋은 이유)",거대 언어 모델 정의,-0.001580458,0.0,0.0015804580179974437
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어 (남은 답변: BCE 가 좋은 이유)",마지막 할 말,-0.0026149221,0.0,0.002614922123029828
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어 (남은 답변: BCE 가 좋은 이유)",면접 시작 인사,0.00025428776,0.0,0.0002542877628002316
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어 (남은 답변: BCE 가 좋은 이유)",면접 종료,0.0035298131,0.0,0.003529813140630722
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어 (남은 답변: BCE 가 좋은 이유)","인공지능, 머신러닝, 딥러닝 차이",-0.012579195,0.0,0.012579195201396942
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어 (남은 답변: BCE 가 좋은 이유)",잠시 휴식,0.013471572,0.0,0.01347157172858715
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어 (남은 답변: BCE 가 좋은 이유)",확률 예측에서 MSE Loss 미 사용 이유,0.0020199053,0.0,0.0020199052523821592
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아 (남은 답변: 모든 질문 해결 완료)",BCE Loss 설명,-0.012034383,0.0,0.012034382671117783
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아 (남은 답변: 모든 질문 해결 완료)",LLM Fine-Tuning 의 PEFT,-0.012722226,0.0,0.012722225859761238
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아 (남은 답변: 모든 질문 해결 완료)",LoRA,0.005530542,0.0,0.005530542228370905
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아 (남은 답변: 모든 질문 해결 완료)",LoRA 와 QLoRA 의 차이,0.00932708,0.0,0.009327080100774765
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아 (남은 답변: 모든 질문 해결 완료)",Loss Function 관련 실무 경험,0.0020179432,0.0,0.002017943188548088
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아 (남은 답변: 모든 질문 해결 완료)",Loss Function 예시,-0.023774238,0.0,0.02377423830330372
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아 (남은 답변: 모든 질문 해결 완료)",Loss Function 정의,-0.0070451554,0.0,0.007045155391097069
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아 (남은 답변: 모든 질문 해결 완료)",MBTI / 좋아하는 아이돌,-0.007081315,0.0,0.007081314921379089
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아 (남은 답변: 모든 질문 해결 완료)",MSE Loss 설명,0.0030921265,0.0,0.0030921264551579952
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아 (남은 답변: 모든 질문 해결 완료)",MSE Loss 용도,0.003693715,0.0,0.0036937149707227945
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아 (남은 답변: 모든 질문 해결 완료)","Multi-Class, Multi-Label 중 BCE 가 좋은 task",0.09405154,0.0,0.0940515398979187
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아 (남은 답변: 모든 질문 해결 완료)",Multi-Label 에서 CE + Softmax 적용 문제점,0.96749043,1.0,0.032509565353393555
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아 (남은 답변: 모든 질문 해결 완료)",PEFT 방법 5가지,-0.021771906,0.0,0.021771905943751335
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아 (남은 답변: 모든 질문 해결 완료)",거대 언어 모델 정의,0.017558279,0.0,0.017558278515934944
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아 (남은 답변: 모든 질문 해결 완료)",마지막 할 말,-0.0378188,0.0,0.037818800657987595
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아 (남은 답변: 모든 질문 해결 완료)",면접 시작 인사,-0.015597934,0.0,0.015597933903336525
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아 (남은 답변: 모든 질문 해결 완료)",면접 종료,0.007997882,0.0,0.00799788162112236
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아 (남은 답변: 모든 질문 해결 완료)","인공지능, 머신러닝, 딥러닝 차이",-0.0024817714,0.0,0.0024817714001983404
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아 (남은 답변: 모든 질문 해결 완료)",잠시 휴식,-0.025044378,0.0,0.025044377893209457
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아 (남은 답변: 모든 질문 해결 완료)",확률 예측에서 MSE Loss 미 사용 이유,-0.009492402,0.0,0.009492401964962482
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지 (남은 답변: 모든 질문 해결 완료),BCE Loss 설명,-0.009156596,0.0,0.009156595915555954
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지 (남은 답변: 모든 질문 해결 완료),LLM Fine-Tuning 의 PEFT,-0.0135967685,0.0,0.013596768490970135
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지 (남은 답변: 모든 질문 해결 완료),LoRA,-0.022855073,0.0,0.02285507321357727
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지 (남은 답변: 모든 질문 해결 완료),LoRA 와 QLoRA 의 차이,0.04233297,0.0,0.0423329696059227
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지 (남은 답변: 모든 질문 해결 완료),Loss Function 관련 실무 경험,0.9517265,1.0,0.04827350378036499
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지 (남은 답변: 모든 질문 해결 완료),Loss Function 예시,-0.042402975,0.0,0.042402975261211395
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지 (남은 답변: 모든 질문 해결 완료),Loss Function 정의,-0.014877574,0.0,0.01487757358700037
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지 (남은 답변: 모든 질문 해결 완료),MBTI / 좋아하는 아이돌,-0.0013449597,0.0,0.0013449597172439098
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지 (남은 답변: 모든 질문 해결 완료),MSE Loss 설명,-0.018300628,0.0,0.018300628289580345
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지 (남은 답변: 모든 질문 해결 완료),MSE Loss 용도,0.0039830506,0.0,0.003983050584793091
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지 (남은 답변: 모든 질문 해결 완료),"Multi-Class, Multi-Label 중 BCE 가 좋은 task",-0.005738065,0.0,0.0057380651123821735
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지 (남은 답변: 모든 질문 해결 완료),Multi-Label 에서 CE + Softmax 적용 문제점,0.005098334,0.0,0.005098334047943354
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지 (남은 답변: 모든 질문 해결 완료),PEFT 방법 5가지,-0.009874779,0.0,0.009874778799712658
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지 (남은 답변: 모든 질문 해결 완료),거대 언어 모델 정의,0.015827736,0.0,0.015827735885977745
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지 (남은 답변: 모든 질문 해결 완료),마지막 할 말,0.011528259,0.0,0.0115282591432333
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지 (남은 답변: 모든 질문 해결 완료),면접 시작 인사,-0.010333096,0.0,0.01033309567719698
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지 (남은 답변: 모든 질문 해결 완료),면접 종료,-0.0027378271,0.0,0.002737827133387327
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지 (남은 답변: 모든 질문 해결 완료),"인공지능, 머신러닝, 딥러닝 차이",-0.0011791412,0.0,0.0011791412252932787
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지 (남은 답변: 모든 질문 해결 완료),잠시 휴식,-0.022055514,0.0,0.02205551415681839
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지 (남은 답변: 모든 질문 해결 완료),확률 예측에서 MSE Loss 미 사용 이유,0.0014183167,0.0,0.00141831673681736
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나 (남은 답변: Multi-Label 에서 CE + Softmax 적용 문제점),BCE Loss 설명,-0.0014118808,0.0,0.0014118808321654797
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나 (남은 답변: Multi-Label 에서 CE + Softmax 적용 문제점),LLM Fine-Tuning 의 PEFT,-0.0050742864,0.0,0.005074286367744207
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나 (남은 답변: Multi-Label 에서 CE + Softmax 적용 문제점),LoRA,-0.0066914833,0.0,0.006691483315080404
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나 (남은 답변: Multi-Label 에서 CE + Softmax 적용 문제점),LoRA 와 QLoRA 의 차이,0.015042166,0.0,0.015042166225612164
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나 (남은 답변: Multi-Label 에서 CE + Softmax 적용 문제점),Loss Function 관련 실무 경험,-0.02300031,0.0,0.023000309243798256
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나 (남은 답변: Multi-Label 에서 CE + Softmax 적용 문제점),Loss Function 예시,-0.022817248,0.0,0.022817248478531837
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나 (남은 답변: Multi-Label 에서 CE + Softmax 적용 문제점),Loss Function 정의,-0.00038744672,0.0,0.0003874467220157385
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나 (남은 답변: Multi-Label 에서 CE + Softmax 적용 문제점),MBTI / 좋아하는 아이돌,0.0022373628,0.0,0.002237362787127495
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나 (남은 답변: Multi-Label 에서 CE + Softmax 적용 문제점),MSE Loss 설명,0.0014910194,0.0,0.0014910193858668208
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나 (남은 답변: Multi-Label 에서 CE + Softmax 적용 문제점),MSE Loss 용도,0.007378061,0.0,0.007378060836344957
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나 (남은 답변: Multi-Label 에서 CE + Softmax 적용 문제점),"Multi-Class, Multi-Label 중 BCE 가 좋은 task",0.009782747,0.0,0.009782747365534306
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나 (남은 답변: Multi-Label 에서 CE + Softmax 적용 문제점),Multi-Label 에서 CE + Softmax 적용 문제점,0.98444796,1.0,0.015552043914794922
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나 (남은 답변: Multi-Label 에서 CE + Softmax 적용 문제점),PEFT 방법 5가지,-0.0026343428,0.0,0.002634342759847641
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나 (남은 답변: Multi-Label 에서 CE + Softmax 적용 문제점),거대 언어 모델 정의,0.0047644605,0.0,0.004764460492879152
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나 (남은 답변: Multi-Label 에서 CE + Softmax 적용 문제점),마지막 할 말,-0.014730686,0.0,0.014730686321854591
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나 (남은 답변: Multi-Label 에서 CE + Softmax 적용 문제점),면접 시작 인사,-0.015800588,0.0,0.015800587832927704
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나 (남은 답변: Multi-Label 에서 CE + Softmax 적용 문제점),면접 종료,-0.002825691,0.0,0.0028256908990442753
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나 (남은 답변: Multi-Label 에서 CE + Softmax 적용 문제점),"인공지능, 머신러닝, 딥러닝 차이",-0.0011583196,0.0,0.0011583196464926004
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나 (남은 답변: Multi-Label 에서 CE + Softmax 적용 문제점),잠시 휴식,-0.0002698554,0.0,0.00026985540171153843
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나 (남은 답변: Multi-Label 에서 CE + Softmax 적용 문제점),확률 예측에서 MSE Loss 미 사용 이유,0.009448096,0.0,0.0094480961561203
Loss Function 관련 실무 경험 -> 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어 (남은 답변: 상세 경험),BCE Loss 설명,0.0044906666,0.0,0.0044906665571033955
Loss Function 관련 실무 경험 -> 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어 (남은 답변: 상세 경험),LLM Fine-Tuning 의 PEFT,-0.015835263,0.0,0.015835262835025787
Loss Function 관련 실무 경험 -> 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어 (남은 답변: 상세 경험),LoRA,-0.002022149,0.0,0.0020221490412950516
Loss Function 관련 실무 경험 -> 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어 (남은 답변: 상세 경험),LoRA 와 QLoRA 의 차이,0.010234893,0.0,0.010234893299639225
Loss Function 관련 실무 경험 -> 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어 (남은 답변: 상세 경험),Loss Function 관련 실무 경험,0.98114234,1.0,0.01885765790939331
Loss Function 관련 실무 경험 -> 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어 (남은 답변: 상세 경험),Loss Function 예시,-0.03547267,0.0,0.03547266870737076
Loss Function 관련 실무 경험 -> 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어 (남은 답변: 상세 경험),Loss Function 정의,-0.001993443,0.0,0.0019934428855776787
Loss Function 관련 실무 경험 -> 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어 (남은 답변: 상세 경험),MBTI / 좋아하는 아이돌,-0.007197649,0.0,0.007197649218142033
Loss Function 관련 실무 경험 -> 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어 (남은 답변: 상세 경험),MSE Loss 설명,-0.003719793,0.0,0.0037197929341346025
Loss Function 관련 실무 경험 -> 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어 (남은 답변: 상세 경험),MSE Loss 용도,-0.0015840271,0.0,0.0015840270789340138
Loss Function 관련 실무 경험 -> 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어 (남은 답변: 상세 경험),"Multi-Class, Multi-Label 중 BCE 가 좋은 task",-0.010162501,0.0,0.01016250066459179
Loss Function 관련 실무 경험 -> 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어 (남은 답변: 상세 경험),Multi-Label 에서 CE + Softmax 적용 문제점,0.0044367174,0.0,0.004436717368662357
Loss Function 관련 실무 경험 -> 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어 (남은 답변: 상세 경험),PEFT 방법 5가지,-0.008179303,0.0,0.008179303258657455
Loss Function 관련 실무 경험 -> 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어 (남은 답변: 상세 경험),거대 언어 모델 정의,0.011874201,0.0,0.011874200776219368
Loss Function 관련 실무 경험 -> 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어 (남은 답변: 상세 경험),마지막 할 말,0.0064790198,0.0,0.0064790197648108006
Loss Function 관련 실무 경험 -> 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어 (남은 답변: 상세 경험),면접 시작 인사,-0.012644132,0.0,0.012644131667912006
Loss Function 관련 실무 경험 -> 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어 (남은 답변: 상세 경험),면접 종료,-0.0048280917,0.0,0.004828091710805893
Loss Function 관련 실무 경험 -> 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어 (남은 답변: 상세 경험),"인공지능, 머신러닝, 딥러닝 차이",0.0039947014,0.0,0.0039947014302015305
Loss Function 관련 실무 경험 -> 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어 (남은 답변: 상세 경험),잠시 휴식,-0.020145245,0.0,0.020145244896411896
Loss Function 관련 실무 경험 -> 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어 (남은 답변: 상세 경험),확률 예측에서 MSE Loss 미 사용 이유,-0.0011703,0.0,0.0011702999472618103
Loss Function 관련 실무 경험 -> 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지? (남은 답변: 모든 질문 해결 완료),BCE Loss 설명,-0.039173357,0.0,0.03917335718870163
Loss Function 관련 실무 경험 -> 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지? (남은 답변: 모든 질문 해결 완료),LLM Fine-Tuning 의 PEFT,0.0013315991,0.0,0.001331599080003798
Loss Function 관련 실무 경험 -> 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지? (남은 답변: 모든 질문 해결 완료),LoRA,-0.0022651516,0.0,0.0022651515901088715
Loss Function 관련 실무 경험 -> 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지? (남은 답변: 모든 질문 해결 완료),LoRA 와 QLoRA 의 차이,-0.010469418,0.0,0.01046941801905632
Loss Function 관련 실무 경험 -> 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지? (남은 답변: 모든 질문 해결 완료),Loss Function 관련 실무 경험,-0.0056951577,0.0,0.0056951576843857765
Loss Function 관련 실무 경험 -> 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지? (남은 답변: 모든 질문 해결 완료),Loss Function 예시,-0.019713523,0.0,0.019713522866368294
Loss Function 관련 실무 경험 -> 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지? (남은 답변: 모든 질문 해결 완료),Loss Function 정의,0.010684098,0.0,0.010684098117053509
Loss Function 관련 실무 경험 -> 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지? (남은 답변: 모든 질문 해결 완료),MBTI / 좋아하는 아이돌,0.970644,1.0,0.029356002807617188
Loss Function 관련 실무 경험 -> 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지? (남은 답변: 모든 질문 해결 완료),MSE Loss 설명,-0.018870397,0.0,0.0188703965395689
Loss Function 관련 실무 경험 -> 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지? (남은 답변: 모든 질문 해결 완료),MSE Loss 용도,-0.0046504415,0.0,0.004650441464036703
Loss Function 관련 실무 경험 -> 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지? (남은 답변: 모든 질문 해결 완료),"Multi-Class, Multi-Label 중 BCE 가 좋은 task",-0.002986673,0.0,0.0029866730328649282
Loss Function 관련 실무 경험 -> 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지? (남은 답변: 모든 질문 해결 완료),Multi-Label 에서 CE + Softmax 적용 문제점,-0.009731257,0.0,0.00973125733435154
Loss Function 관련 실무 경험 -> 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지? (남은 답변: 모든 질문 해결 완료),PEFT 방법 5가지,-0.0032691043,0.0,0.0032691042870283127
Loss Function 관련 실무 경험 -> 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지? (남은 답변: 모든 질문 해결 완료),거대 언어 모델 정의,-0.0005246548,0.0,0.0005246548098511994
Loss Function 관련 실무 경험 -> 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지? (남은 답변: 모든 질문 해결 완료),마지막 할 말,-0.005729652,0.0,0.005729652009904385
Loss Function 관련 실무 경험 -> 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지? (남은 답변: 모든 질문 해결 완료),면접 시작 인사,-0.022178482,0.0,0.02217848226428032
Loss Function 관련 실무 경험 -> 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지? (남은 답변: 모든 질문 해결 완료),면접 종료,-0.0022973476,0.0,0.002297347644343972
Loss Function 관련 실무 경험 -> 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지? (남은 답변: 모든 질문 해결 완료),"인공지능, 머신러닝, 딥러닝 차이",2.8493865e-05,0.0,2.8493865102063864e-05
Loss Function 관련 실무 경험 -> 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지? (남은 답변: 모든 질문 해결 완료),잠시 휴식,0.0002670898,0.0,0.0002670898102223873
Loss Function 관련 실무 경험 -> 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지? (남은 답변: 모든 질문 해결 완료),확률 예측에서 MSE Loss 미 사용 이유,-0.010581173,0.0,0.01058117300271988
Loss Function 관련 실무 경험 -> 근데 기억 진짜 안나는데 어떡하지? (남은 답변: 상세 경험),BCE Loss 설명,-0.011029369,0.0,0.011029369197785854
Loss Function 관련 실무 경험 -> 근데 기억 진짜 안나는데 어떡하지? (남은 답변: 상세 경험),LLM Fine-Tuning 의 PEFT,-0.0486316,0.0,0.04863160103559494
Loss Function 관련 실무 경험 -> 근데 기억 진짜 안나는데 어떡하지? (남은 답변: 상세 경험),LoRA,-0.03772648,0.0,0.03772648051381111
Loss Function 관련 실무 경험 -> 근데 기억 진짜 안나는데 어떡하지? (남은 답변: 상세 경험),LoRA 와 QLoRA 의 차이,-0.02802351,0.0,0.028023509308695793
Loss Function 관련 실무 경험 -> 근데 기억 진짜 안나는데 어떡하지? (남은 답변: 상세 경험),Loss Function 관련 실무 경험,0.17823522,0.0,0.1782352179288864
Loss Function 관련 실무 경험 -> 근데 기억 진짜 안나는데 어떡하지? (남은 답변: 상세 경험),Loss Function 예시,-0.022019366,0.0,0.022019365802407265
Loss Function 관련 실무 경험 -> 근데 기억 진짜 안나는데 어떡하지? (남은 답변: 상세 경험),Loss Function 정의,-0.006647762,0.0,0.006647761911153793
Loss Function 관련 실무 경험 -> 근데 기억 진짜 안나는데 어떡하지? (남은 답변: 상세 경험),MBTI / 좋아하는 아이돌,-0.00072416326,0.0,0.0007241632556542754
Loss Function 관련 실무 경험 -> 근데 기억 진짜 안나는데 어떡하지? (남은 답변: 상세 경험),MSE Loss 설명,-0.0032454915,0.0,0.003245491534471512
Loss Function 관련 실무 경험 -> 근데 기억 진짜 안나는데 어떡하지? (남은 답변: 상세 경험),MSE Loss 용도,-0.009694649,0.0,0.009694648906588554
Loss Function 관련 실무 경험 -> 근데 기억 진짜 안나는데 어떡하지? (남은 답변: 상세 경험),"Multi-Class, Multi-Label 중 BCE 가 좋은 task",0.030766537,0.0,0.03076653741300106
Loss Function 관련 실무 경험 -> 근데 기억 진짜 안나는데 어떡하지? (남은 답변: 상세 경험),Multi-Label 에서 CE + Softmax 적용 문제점,-0.0017070737,0.0,0.0017070736503228545
Loss Function 관련 실무 경험 -> 근데 기억 진짜 안나는데 어떡하지? (남은 답변: 상세 경험),PEFT 방법 5가지,-0.01366741,0.0,0.013667410239577293
Loss Function 관련 실무 경험 -> 근데 기억 진짜 안나는데 어떡하지? (남은 답변: 상세 경험),거대 언어 모델 정의,-0.031574793,0.0,0.0315747931599617
Loss Function 관련 실무 경험 -> 근데 기억 진짜 안나는데 어떡하지? (남은 답변: 상세 경험),마지막 할 말,0.028389357,0.0,0.028389357030391693
Loss Function 관련 실무 경험 -> 근데 기억 진짜 안나는데 어떡하지? (남은 답변: 상세 경험),면접 시작 인사,0.01981348,0.0,0.019813479855656624
Loss Function 관련 실무 경험 -> 근데 기억 진짜 안나는데 어떡하지? (남은 답변: 상세 경험),면접 종료,-0.0130138295,0.0,0.013013829477131367
Loss Function 관련 실무 경험 -> 근데 기억 진짜 안나는데 어떡하지? (남은 답변: 상세 경험),"인공지능, 머신러닝, 딥러닝 차이",-0.023152657,0.0,0.023152656853199005
Loss Function 관련 실무 경험 -> 근데 기억 진짜 안나는데 어떡하지? (남은 답변: 상세 경험),잠시 휴식,0.8938502,1.0,0.10614979267120361
Loss Function 관련 실무 경험 -> 근데 기억 진짜 안나는데 어떡하지? (남은 답변: 상세 경험),확률 예측에서 MSE Loss 미 사용 이유,-0.0045658336,0.0,0.004565833602100611
Loss Function 관련 실무 경험 -> 나 아직 실무 경험 없는데 ㅠㅠ (남은 답변: 모든 질문 해결 완료),BCE Loss 설명,-0.026166053,0.0,0.026166053488850594
Loss Function 관련 실무 경험 -> 나 아직 실무 경험 없는데 ㅠㅠ (남은 답변: 모든 질문 해결 완료),LLM Fine-Tuning 의 PEFT,0.003516632,0.0,0.0035166318994015455
Loss Function 관련 실무 경험 -> 나 아직 실무 경험 없는데 ㅠㅠ (남은 답변: 모든 질문 해결 완료),LoRA,-0.0061904625,0.0,0.006190462503582239
Loss Function 관련 실무 경험 -> 나 아직 실무 경험 없는데 ㅠㅠ (남은 답변: 모든 질문 해결 완료),LoRA 와 QLoRA 의 차이,-0.020181952,0.0,0.02018195204436779
Loss Function 관련 실무 경험 -> 나 아직 실무 경험 없는데 ㅠㅠ (남은 답변: 모든 질문 해결 완료),Loss Function 관련 실무 경험,-0.011520824,0.0,0.011520824395120144
Loss Function 관련 실무 경험 -> 나 아직 실무 경험 없는데 ㅠㅠ (남은 답변: 모든 질문 해결 완료),Loss Function 예시,-0.0021074482,0.0,0.0021074481774121523
Loss Function 관련 실무 경험 -> 나 아직 실무 경험 없는데 ㅠㅠ (남은 답변: 모든 질문 해결 완료),Loss Function 정의,0.009548705,0.0,0.009548705071210861
Loss Function 관련 실무 경험 -> 나 아직 실무 경험 없는데 ㅠㅠ (남은 답변: 모든 질문 해결 완료),MBTI / 좋아하는 아이돌,0.97073567,1.0,0.029264330863952637
Loss Function 관련 실무 경험 -> 나 아직 실무 경험 없는데 ㅠㅠ (남은 답변: 모든 질문 해결 완료),MSE Loss 설명,-0.021435749,0.0,0.021435748785734177
Loss Function 관련 실무 경험 -> 나 아직 실무 경험 없는데 ㅠㅠ (남은 답변: 모든 질문 해결 완료),MSE Loss 용도,-0.009866474,0.0,0.009866474196314812
Loss Function 관련 실무 경험 -> 나 아직 실무 경험 없는데 ㅠㅠ (남은 답변: 모든 질문 해결 완료),"Multi-Class, Multi-Label 중 BCE 가 좋은 task",-0.0072214003,0.0,0.007221400272101164
Loss Function 관련 실무 경험 -> 나 아직 실무 경험 없는데 ㅠㅠ (남은 답변: 모든 질문 해결 완료),Multi-Label 에서 CE + Softmax 적용 문제점,-0.008602671,0.0,0.008602671325206757
Loss Function 관련 실무 경험 -> 나 아직 실무 경험 없는데 ㅠㅠ (남은 답변: 모든 질문 해결 완료),PEFT 방법 5가지,-0.00639367,0.0,0.006393670104444027
Loss Function 관련 실무 경험 -> 나 아직 실무 경험 없는데 ㅠㅠ (남은 답변: 모든 질문 해결 완료),거대 언어 모델 정의,-0.012876385,0.0,0.012876384891569614
Loss Function 관련 실무 경험 -> 나 아직 실무 경험 없는데 ㅠㅠ (남은 답변: 모든 질문 해결 완료),마지막 할 말,-0.023056362,0.0,0.023056361824274063
Loss Function 관련 실무 경험 -> 나 아직 실무 경험 없는데 ㅠㅠ (남은 답변: 모든 질문 해결 완료),면접 시작 인사,-0.020564672,0.0,0.020564671605825424
Loss Function 관련 실무 경험 -> 나 아직 실무 경험 없는데 ㅠㅠ (남은 답변: 모든 질문 해결 완료),면접 종료,0.0005150805,0.0,0.0005150805227458477
Loss Function 관련 실무 경험 -> 나 아직 실무 경험 없는데 ㅠㅠ (남은 답변: 모든 질문 해결 완료),"인공지능, 머신러닝, 딥러닝 차이",-0.00022599493,0.0,0.00022599492513108999
Loss Function 관련 실무 경험 -> 나 아직 실무 경험 없는데 ㅠㅠ (남은 답변: 모든 질문 해결 완료),잠시 휴식,0.032139387,0.0,0.03213938698172569
Loss Function 관련 실무 경험 -> 나 아직 실무 경험 없는데 ㅠㅠ (남은 답변: 모든 질문 해결 완료),확률 예측에서 MSE Loss 미 사용 이유,-0.014334742,0.0,0.014334741979837418
"Loss Function 관련 실무 경험 -> 대답하기 싫은데 힝 (남은 답변: 기본 경험, 상세 경험)",BCE Loss 설명,0.010681247,0.0,0.01068124733865261
"Loss Function 관련 실무 경험 -> 대답하기 싫은데 힝 (남은 답변: 기본 경험, 상세 경험)",LLM Fine-Tuning 의 PEFT,-0.006555314,0.0,0.006555314175784588
"Loss Function 관련 실무 경험 -> 대답하기 싫은데 힝 (남은 답변: 기본 경험, 상세 경험)",LoRA,-0.024629809,0.0,0.024629808962345123
"Loss Function 관련 실무 경험 -> 대답하기 싫은데 힝 (남은 답변: 기본 경험, 상세 경험)",LoRA 와 QLoRA 의 차이,0.010879395,0.0,0.010879394598305225
"Loss Function 관련 실무 경험 -> 대답하기 싫은데 힝 (남은 답변: 기본 경험, 상세 경험)",Loss Function 관련 실무 경험,-0.0344982,0.0,0.034498199820518494
"Loss Function 관련 실무 경험 -> 대답하기 싫은데 힝 (남은 답변: 기본 경험, 상세 경험)",Loss Function 예시,-0.020424716,0.0,0.02042471617460251
"Loss Function 관련 실무 경험 -> 대답하기 싫은데 힝 (남은 답변: 기본 경험, 상세 경험)",Loss Function 정의,0.002918912,0.0,0.002918912097811699
"Loss Function 관련 실무 경험 -> 대답하기 싫은데 힝 (남은 답변: 기본 경험, 상세 경험)",MBTI / 좋아하는 아이돌,0.007936005,0.0,0.007936004549264908
"Loss Function 관련 실무 경험 -> 대답하기 싫은데 힝 (남은 답변: 기본 경험, 상세 경험)",MSE Loss 설명,-0.0051254546,0.0,0.005125454626977444
"Loss Function 관련 실무 경험 -> 대답하기 싫은데 힝 (남은 답변: 기본 경험, 상세 경험)",MSE Loss 용도,0.0020256531,0.0,0.0020256531424820423
"Loss Function 관련 실무 경험 -> 대답하기 싫은데 힝 (남은 답변: 기본 경험, 상세 경험)","Multi-Class, Multi-Label 중 BCE 가 좋은 task",0.0018461876,0.0,0.001846187631599605
"Loss Function 관련 실무 경험 -> 대답하기 싫은데 힝 (남은 답변: 기본 경험, 상세 경험)",Multi-Label 에서 CE + Softmax 적용 문제점,0.0081022745,0.0,0.008102274499833584
"Loss Function 관련 실무 경험 -> 대답하기 싫은데 힝 (남은 답변: 기본 경험, 상세 경험)",PEFT 방법 5가지,0.0031452791,0.0,0.0031452791299670935
"Loss Function 관련 실무 경험 -> 대답하기 싫은데 힝 (남은 답변: 기본 경험, 상세 경험)",거대 언어 모델 정의,-0.0045241886,0.0,0.004524188581854105
"Loss Function 관련 실무 경험 -> 대답하기 싫은데 힝 (남은 답변: 기본 경험, 상세 경험)",마지막 할 말,0.0016673678,0.0,0.00166736776009202
"Loss Function 관련 실무 경험 -> 대답하기 싫은데 힝 (남은 답변: 기본 경험, 상세 경험)",면접 시작 인사,-0.0014837988,0.0,0.001483798841945827
"Loss Function 관련 실무 경험 -> 대답하기 싫은데 힝 (남은 답변: 기본 경험, 상세 경험)",면접 종료,0.006079229,0.0,0.006079229060560465
"Loss Function 관련 실무 경험 -> 대답하기 싫은데 힝 (남은 답변: 기본 경험, 상세 경험)","인공지능, 머신러닝, 딥러닝 차이",-0.00019292122,0.0,0.00019292121578473598
"Loss Function 관련 실무 경험 -> 대답하기 싫은데 힝 (남은 답변: 기본 경험, 상세 경험)",잠시 휴식,0.93600965,1.0,0.06399035453796387
"Loss Function 관련 실무 경험 -> 대답하기 싫은데 힝 (남은 답변: 기본 경험, 상세 경험)",확률 예측에서 MSE Loss 미 사용 이유,0.0075947484,0.0,0.00759474840015173
Loss Function 관련 실무 경험 -> 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ (남은 답변: 상세 경험),BCE Loss 설명,-0.0013245156,0.0,0.0013245155569165945
Loss Function 관련 실무 경험 -> 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ (남은 답변: 상세 경험),LLM Fine-Tuning 의 PEFT,-0.005562107,0.0,0.00556210707873106
Loss Function 관련 실무 경험 -> 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ (남은 답변: 상세 경험),LoRA,0.004325276,0.0,0.0043252757750451565
Loss Function 관련 실무 경험 -> 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ (남은 답변: 상세 경험),LoRA 와 QLoRA 의 차이,0.0053574694,0.0,0.00535746943205595
Loss Function 관련 실무 경험 -> 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ (남은 답변: 상세 경험),Loss Function 관련 실무 경험,0.9804222,1.0,0.01957780122756958
Loss Function 관련 실무 경험 -> 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ (남은 답변: 상세 경험),Loss Function 예시,-0.024123926,0.0,0.02412392571568489
Loss Function 관련 실무 경험 -> 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ (남은 답변: 상세 경험),Loss Function 정의,-0.0054025957,0.0,0.0054025957360863686
Loss Function 관련 실무 경험 -> 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ (남은 답변: 상세 경험),MBTI / 좋아하는 아이돌,-0.011516676,0.0,0.011516676284372807
Loss Function 관련 실무 경험 -> 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ (남은 답변: 상세 경험),MSE Loss 설명,0.00073735655,0.0,0.000737356545869261
Loss Function 관련 실무 경험 -> 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ (남은 답변: 상세 경험),MSE Loss 용도,0.004091702,0.0,0.004091701935976744
Loss Function 관련 실무 경험 -> 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ (남은 답변: 상세 경험),"Multi-Class, Multi-Label 중 BCE 가 좋은 task",-0.01732884,0.0,0.017328839749097824
Loss Function 관련 실무 경험 -> 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ (남은 답변: 상세 경험),Multi-Label 에서 CE + Softmax 적용 문제점,-0.0063287453,0.0,0.006328745279461145
Loss Function 관련 실무 경험 -> 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ (남은 답변: 상세 경험),PEFT 방법 5가지,-0.006208102,0.0,0.006208102218806744
Loss Function 관련 실무 경험 -> 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ (남은 답변: 상세 경험),거대 언어 모델 정의,0.0200728,0.0,0.020072799175977707
Loss Function 관련 실무 경험 -> 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ (남은 답변: 상세 경험),마지막 할 말,0.0038227418,0.0,0.003822741797193885
Loss Function 관련 실무 경험 -> 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ (남은 답변: 상세 경험),면접 시작 인사,-0.007889946,0.0,0.0078899459913373
Loss Function 관련 실무 경험 -> 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ (남은 답변: 상세 경험),면접 종료,-0.014242204,0.0,0.014242203906178474
Loss Function 관련 실무 경험 -> 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ (남은 답변: 상세 경험),"인공지능, 머신러닝, 딥러닝 차이",-0.0013364145,0.0,0.0013364144833758473
Loss Function 관련 실무 경험 -> 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ (남은 답변: 상세 경험),잠시 휴식,0.00020157578,0.0,0.0002015757781919092
Loss Function 관련 실무 경험 -> 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ (남은 답변: 상세 경험),확률 예측에서 MSE Loss 미 사용 이유,-0.0011510722,0.0,0.0011510722106322646
Loss Function 관련 실무 경험 -> 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어 (남은 답변: 모든 질문 해결 완료),BCE Loss 설명,-0.0420268,0.0,0.04202679917216301
Loss Function 관련 실무 경험 -> 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어 (남은 답변: 모든 질문 해결 완료),LLM Fine-Tuning 의 PEFT,0.0021417176,0.0,0.0021417175885289907
Loss Function 관련 실무 경험 -> 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어 (남은 답변: 모든 질문 해결 완료),LoRA,-0.01943469,0.0,0.019434690475463867
Loss Function 관련 실무 경험 -> 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어 (남은 답변: 모든 질문 해결 완료),LoRA 와 QLoRA 의 차이,0.0084050195,0.0,0.008405019529163837
Loss Function 관련 실무 경험 -> 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어 (남은 답변: 모든 질문 해결 완료),Loss Function 관련 실무 경험,0.0020734735,0.0,0.0020734735298901796
Loss Function 관련 실무 경험 -> 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어 (남은 답변: 모든 질문 해결 완료),Loss Function 예시,-0.008712571,0.0,0.008712571114301682
Loss Function 관련 실무 경험 -> 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어 (남은 답변: 모든 질문 해결 완료),Loss Function 정의,0.00538332,0.0,0.005383320152759552
Loss Function 관련 실무 경험 -> 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어 (남은 답변: 모든 질문 해결 완료),MBTI / 좋아하는 아이돌,0.9668475,1.0,0.03315252065658569
Loss Function 관련 실무 경험 -> 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어 (남은 답변: 모든 질문 해결 완료),MSE Loss 설명,-0.021697896,0.0,0.021697895601391792
Loss Function 관련 실무 경험 -> 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어 (남은 답변: 모든 질문 해결 완료),MSE Loss 용도,-0.00050596235,0.0,0.000505962350871414
Loss Function 관련 실무 경험 -> 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어 (남은 답변: 모든 질문 해결 완료),"Multi-Class, Multi-Label 중 BCE 가 좋은 task",0.011318498,0.0,0.01131849829107523
Loss Function 관련 실무 경험 -> 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어 (남은 답변: 모든 질문 해결 완료),Multi-Label 에서 CE + Softmax 적용 문제점,-0.005192378,0.0,0.00519237807020545
Loss Function 관련 실무 경험 -> 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어 (남은 답변: 모든 질문 해결 완료),PEFT 방법 5가지,0.009795824,0.0,0.009795824065804482
Loss Function 관련 실무 경험 -> 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어 (남은 답변: 모든 질문 해결 완료),거대 언어 모델 정의,0.005270679,0.0,0.005270679015666246
Loss Function 관련 실무 경험 -> 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어 (남은 답변: 모든 질문 해결 완료),마지막 할 말,0.028592726,0.0,0.028592726215720177
Loss Function 관련 실무 경험 -> 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어 (남은 답변: 모든 질문 해결 완료),면접 시작 인사,-0.01729793,0.0,0.017297929152846336
Loss Function 관련 실무 경험 -> 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어 (남은 답변: 모든 질문 해결 완료),면접 종료,-0.015079756,0.0,0.015079756267368793
Loss Function 관련 실무 경험 -> 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어 (남은 답변: 모든 질문 해결 완료),"인공지능, 머신러닝, 딥러닝 차이",0.0029726177,0.0,0.002972617745399475
Loss Function 관련 실무 경험 -> 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어 (남은 답변: 모든 질문 해결 완료),잠시 휴식,-0.046203088,0.0,0.04620308801531792
Loss Function 관련 실무 경험 -> 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어 (남은 답변: 모든 질문 해결 완료),확률 예측에서 MSE Loss 미 사용 이유,-0.032783926,0.0,0.03278392553329468
Loss Function 관련 실무 경험 -> 기억 진짜 안 나는데 가물가물하다 (남은 답변: 상세 경험),BCE Loss 설명,-0.011521035,0.0,0.011521034874022007
Loss Function 관련 실무 경험 -> 기억 진짜 안 나는데 가물가물하다 (남은 답변: 상세 경험),LLM Fine-Tuning 의 PEFT,-0.037323527,0.0,0.03732352703809738
Loss Function 관련 실무 경험 -> 기억 진짜 안 나는데 가물가물하다 (남은 답변: 상세 경험),LoRA,-0.055359088,0.0,0.05535908788442612
Loss Function 관련 실무 경험 -> 기억 진짜 안 나는데 가물가물하다 (남은 답변: 상세 경험),LoRA 와 QLoRA 의 차이,-0.006793688,0.0,0.006793688051402569
Loss Function 관련 실무 경험 -> 기억 진짜 안 나는데 가물가물하다 (남은 답변: 상세 경험),Loss Function 관련 실무 경험,0.27359018,0.0,0.27359017729759216
Loss Function 관련 실무 경험 -> 기억 진짜 안 나는데 가물가물하다 (남은 답변: 상세 경험),Loss Function 예시,-0.05008272,0.0,0.050082720816135406
Loss Function 관련 실무 경험 -> 기억 진짜 안 나는데 가물가물하다 (남은 답변: 상세 경험),Loss Function 정의,-0.0126512395,0.0,0.012651239521801472
Loss Function 관련 실무 경험 -> 기억 진짜 안 나는데 가물가물하다 (남은 답변: 상세 경험),MBTI / 좋아하는 아이돌,-0.011212442,0.0,0.011212442070245743
Loss Function 관련 실무 경험 -> 기억 진짜 안 나는데 가물가물하다 (남은 답변: 상세 경험),MSE Loss 설명,0.022494655,0.0,0.02249465510249138
Loss Function 관련 실무 경험 -> 기억 진짜 안 나는데 가물가물하다 (남은 답변: 상세 경험),MSE Loss 용도,-0.01460512,0.0,0.014605119824409485
Loss Function 관련 실무 경험 -> 기억 진짜 안 나는데 가물가물하다 (남은 답변: 상세 경험),"Multi-Class, Multi-Label 중 BCE 가 좋은 task",0.042774536,0.0,0.04277453571557999
Loss Function 관련 실무 경험 -> 기억 진짜 안 나는데 가물가물하다 (남은 답변: 상세 경험),Multi-Label 에서 CE + Softmax 적용 문제점,0.0014836363,0.0,0.0014836363261565566
Loss Function 관련 실무 경험 -> 기억 진짜 안 나는데 가물가물하다 (남은 답변: 상세 경험),PEFT 방법 5가지,-0.01768723,0.0,0.017687229439616203
Loss Function 관련 실무 경험 -> 기억 진짜 안 나는데 가물가물하다 (남은 답변: 상세 경험),거대 언어 모델 정의,-0.030995334,0.0,0.030995333567261696
Loss Function 관련 실무 경험 -> 기억 진짜 안 나는데 가물가물하다 (남은 답변: 상세 경험),마지막 할 말,0.018119557,0.0,0.018119556829333305
Loss Function 관련 실무 경험 -> 기억 진짜 안 나는데 가물가물하다 (남은 답변: 상세 경험),면접 시작 인사,0.018737655,0.0,0.018737655133008957
Loss Function 관련 실무 경험 -> 기억 진짜 안 나는데 가물가물하다 (남은 답변: 상세 경험),면접 종료,-0.039109614,0.0,0.03910961374640465
Loss Function 관련 실무 경험 -> 기억 진짜 안 나는데 가물가물하다 (남은 답변: 상세 경험),"인공지능, 머신러닝, 딥러닝 차이",-0.017099332,0.0,0.017099332064390182
Loss Function 관련 실무 경험 -> 기억 진짜 안 나는데 가물가물하다 (남은 답변: 상세 경험),잠시 휴식,0.8338618,1.0,0.1661381721496582
Loss Function 관련 실무 경험 -> 기억 진짜 안 나는데 가물가물하다 (남은 답변: 상세 경험),확률 예측에서 MSE Loss 미 사용 이유,0.019853137,0.0,0.0198531374335289
Loss Function 관련 실무 경험 -> 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ (남은 답변: 모든 질문 해결 완료),BCE Loss 설명,-0.031175204,0.0,0.03117520362138748
Loss Function 관련 실무 경험 -> 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ (남은 답변: 모든 질문 해결 완료),LLM Fine-Tuning 의 PEFT,0.0037567301,0.0,0.003756730118766427
Loss Function 관련 실무 경험 -> 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ (남은 답변: 모든 질문 해결 완료),LoRA,-0.011314968,0.0,0.011314967647194862
Loss Function 관련 실무 경험 -> 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ (남은 답변: 모든 질문 해결 완료),LoRA 와 QLoRA 의 차이,-0.020166948,0.0,0.020166948437690735
Loss Function 관련 실무 경험 -> 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ (남은 답변: 모든 질문 해결 완료),Loss Function 관련 실무 경험,0.0042318413,0.0,0.004231841303408146
Loss Function 관련 실무 경험 -> 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ (남은 답변: 모든 질문 해결 완료),Loss Function 예시,-0.00906037,0.0,0.009060369804501534
Loss Function 관련 실무 경험 -> 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ (남은 답변: 모든 질문 해결 완료),Loss Function 정의,0.008367295,0.0,0.008367295376956463
Loss Function 관련 실무 경험 -> 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ (남은 답변: 모든 질문 해결 완료),MBTI / 좋아하는 아이돌,0.9705872,1.0,0.029412806034088135
Loss Function 관련 실무 경험 -> 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ (남은 답변: 모든 질문 해결 완료),MSE Loss 설명,-0.019396327,0.0,0.019396327435970306
Loss Function 관련 실무 경험 -> 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ (남은 답변: 모든 질문 해결 완료),MSE Loss 용도,-0.015238257,0.0,0.015238257125020027
Loss Function 관련 실무 경험 -> 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ (남은 답변: 모든 질문 해결 완료),"Multi-Class, Multi-Label 중 BCE 가 좋은 task",-0.0008904808,0.0,0.0008904808200895786
Loss Function 관련 실무 경험 -> 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ (남은 답변: 모든 질문 해결 완료),Multi-Label 에서 CE + Softmax 적용 문제점,-0.012450822,0.0,0.012450821697711945
Loss Function 관련 실무 경험 -> 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ (남은 답변: 모든 질문 해결 완료),PEFT 방법 5가지,-0.0045834016,0.0,0.00458340160548687
Loss Function 관련 실무 경험 -> 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ (남은 답변: 모든 질문 해결 완료),거대 언어 모델 정의,-0.011390092,0.0,0.011390091851353645
Loss Function 관련 실무 경험 -> 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ (남은 답변: 모든 질문 해결 완료),마지막 할 말,-0.018609019,0.0,0.018609018996357918
Loss Function 관련 실무 경험 -> 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ (남은 답변: 모든 질문 해결 완료),면접 시작 인사,-0.019719817,0.0,0.019719816744327545
Loss Function 관련 실무 경험 -> 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ (남은 답변: 모든 질문 해결 완료),면접 종료,0.0018115129,0.0,0.0018115128623321652
Loss Function 관련 실무 경험 -> 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ (남은 답변: 모든 질문 해결 완료),"인공지능, 머신러닝, 딥러닝 차이",0.00016096457,0.0,0.00016096456965897232
Loss Function 관련 실무 경험 -> 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ (남은 답변: 모든 질문 해결 완료),잠시 휴식,0.017424244,0.0,0.017424244433641434
Loss Function 관련 실무 경험 -> 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ (남은 답변: 모든 질문 해결 완료),확률 예측에서 MSE Loss 미 사용 이유,-0.012461255,0.0,0.012461255304515362
"Loss Function 관련 실무 경험 -> 이걸 왜 알려줘야 하지 (남은 답변: 기본 경험, 상세 경험)",BCE Loss 설명,-0.003752459,0.0,0.0037524590734392405
"Loss Function 관련 실무 경험 -> 이걸 왜 알려줘야 하지 (남은 답변: 기본 경험, 상세 경험)",LLM Fine-Tuning 의 PEFT,-0.018512115,0.0,0.018512114882469177
"Loss Function 관련 실무 경험 -> 이걸 왜 알려줘야 하지 (남은 답변: 기본 경험, 상세 경험)",LoRA,-0.026667438,0.0,0.026667438447475433
"Loss Function 관련 실무 경험 -> 이걸 왜 알려줘야 하지 (남은 답변: 기본 경험, 상세 경험)",LoRA 와 QLoRA 의 차이,0.0071984176,0.0,0.00719841755926609
"Loss Function 관련 실무 경험 -> 이걸 왜 알려줘야 하지 (남은 답변: 기본 경험, 상세 경험)",Loss Function 관련 실무 경험,6.284144e-05,0.0,6.284144183155149e-05
"Loss Function 관련 실무 경험 -> 이걸 왜 알려줘야 하지 (남은 답변: 기본 경험, 상세 경험)",Loss Function 예시,-0.011940597,0.0,0.011940596625208855
"Loss Function 관련 실무 경험 -> 이걸 왜 알려줘야 하지 (남은 답변: 기본 경험, 상세 경험)",Loss Function 정의,0.02030693,0.0,0.02030692994594574
"Loss Function 관련 실무 경험 -> 이걸 왜 알려줘야 하지 (남은 답변: 기본 경험, 상세 경험)",MBTI / 좋아하는 아이돌,0.047338784,0.0,0.047338783740997314
"Loss Function 관련 실무 경험 -> 이걸 왜 알려줘야 하지 (남은 답변: 기본 경험, 상세 경험)",MSE Loss 설명,-0.014851652,0.0,0.014851652085781097
"Loss Function 관련 실무 경험 -> 이걸 왜 알려줘야 하지 (남은 답변: 기본 경험, 상세 경험)",MSE Loss 용도,-0.012653367,0.0,0.012653366662561893
"Loss Function 관련 실무 경험 -> 이걸 왜 알려줘야 하지 (남은 답변: 기본 경험, 상세 경험)","Multi-Class, Multi-Label 중 BCE 가 좋은 task",0.008033533,0.0,0.00803353264927864
"Loss Function 관련 실무 경험 -> 이걸 왜 알려줘야 하지 (남은 답변: 기본 경험, 상세 경험)",Multi-Label 에서 CE + Softmax 적용 문제점,-0.0055901776,0.0,0.005590177606791258
"Loss Function 관련 실무 경험 -> 이걸 왜 알려줘야 하지 (남은 답변: 기본 경험, 상세 경험)",PEFT 방법 5가지,0.0046316204,0.0,0.004631620366126299
"Loss Function 관련 실무 경험 -> 이걸 왜 알려줘야 하지 (남은 답변: 기본 경험, 상세 경험)",거대 언어 모델 정의,-0.012015655,0.0,0.01201565470546484
"Loss Function 관련 실무 경험 -> 이걸 왜 알려줘야 하지 (남은 답변: 기본 경험, 상세 경험)",마지막 할 말,0.019893922,0.0,0.01989392191171646
"Loss Function 관련 실무 경험 -> 이걸 왜 알려줘야 하지 (남은 답변: 기본 경험, 상세 경험)",면접 시작 인사,-0.0021355776,0.0,0.0021355776116251945
"Loss Function 관련 실무 경험 -> 이걸 왜 알려줘야 하지 (남은 답변: 기본 경험, 상세 경험)",면접 종료,-0.015631253,0.0,0.01563125289976597
"Loss Function 관련 실무 경험 -> 이걸 왜 알려줘야 하지 (남은 답변: 기본 경험, 상세 경험)","인공지능, 머신러닝, 딥러닝 차이",-0.009350053,0.0,0.009350053034722805
"Loss Function 관련 실무 경험 -> 이걸 왜 알려줘야 하지 (남은 답변: 기본 경험, 상세 경험)",잠시 휴식,0.92309624,1.0,0.07690376043319702
"Loss Function 관련 실무 경험 -> 이걸 왜 알려줘야 하지 (남은 답변: 기본 경험, 상세 경험)",확률 예측에서 MSE Loss 미 사용 이유,-0.016004145,0.0,0.01600414514541626
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야! (남은 답변: 모든 질문 해결 완료),BCE Loss 설명,-0.020767886,0.0,0.02076788619160652
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야! (남은 답변: 모든 질문 해결 완료),LLM Fine-Tuning 의 PEFT,-0.008146963,0.0,0.008146963082253933
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야! (남은 답변: 모든 질문 해결 완료),LoRA,-0.021058818,0.0,0.021058818325400352
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야! (남은 답변: 모든 질문 해결 완료),LoRA 와 QLoRA 의 차이,-0.019285586,0.0,0.01928558573126793
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야! (남은 답변: 모든 질문 해결 완료),Loss Function 관련 실무 경험,-0.006614931,0.0,0.0066149309277534485
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야! (남은 답변: 모든 질문 해결 완료),Loss Function 예시,-0.043996453,0.0,0.043996453285217285
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야! (남은 답변: 모든 질문 해결 완료),Loss Function 정의,-0.009030436,0.0,0.009030436165630817
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야! (남은 답변: 모든 질문 해결 완료),MBTI / 좋아하는 아이돌,-0.0047260257,0.0,0.004726025741547346
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야! (남은 답변: 모든 질문 해결 완료),MSE Loss 설명,0.01161238,0.0,0.011612379923462868
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야! (남은 답변: 모든 질문 해결 완료),MSE Loss 용도,-0.006401906,0.0,0.006401905789971352
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야! (남은 답변: 모든 질문 해결 완료),"Multi-Class, Multi-Label 중 BCE 가 좋은 task",0.014155592,0.0,0.01415559183806181
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야! (남은 답변: 모든 질문 해결 완료),Multi-Label 에서 CE + Softmax 적용 문제점,0.008275988,0.0,0.008275987580418587
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야! (남은 답변: 모든 질문 해결 완료),PEFT 방법 5가지,-0.0045857043,0.0,0.004585704300552607
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야! (남은 답변: 모든 질문 해결 완료),거대 언어 모델 정의,-0.0026835597,0.0,0.0026835596654564142
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야! (남은 답변: 모든 질문 해결 완료),마지막 할 말,0.010588319,0.0,0.010588319040834904
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야! (남은 답변: 모든 질문 해결 완료),면접 시작 인사,0.00093164644,0.0,0.000931646442040801
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야! (남은 답변: 모든 질문 해결 완료),면접 종료,0.029033672,0.0,0.02903367206454277
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야! (남은 답변: 모든 질문 해결 완료),"인공지능, 머신러닝, 딥러닝 차이",0.010636156,0.0,0.010636156424880028
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야! (남은 답변: 모든 질문 해결 완료),잠시 휴식,0.94287086,1.0,0.0571291446685791
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야! (남은 답변: 모든 질문 해결 완료),확률 예측에서 MSE Loss 미 사용 이유,0.018798303,0.0,0.018798302859067917
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해 (남은 답변: 모든 질문 해결 완료),BCE Loss 설명,-0.016018761,0.0,0.016018761321902275
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해 (남은 답변: 모든 질문 해결 완료),LLM Fine-Tuning 의 PEFT,-0.010307571,0.0,0.010307570919394493
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해 (남은 답변: 모든 질문 해결 완료),LoRA,-0.018377826,0.0,0.018377825617790222
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해 (남은 답변: 모든 질문 해결 완료),LoRA 와 QLoRA 의 차이,0.015441014,0.0,0.015441014431416988
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해 (남은 답변: 모든 질문 해결 완료),Loss Function 관련 실무 경험,-0.012436498,0.0,0.012436497956514359
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해 (남은 답변: 모든 질문 해결 완료),Loss Function 예시,-0.018205462,0.0,0.018205462023615837
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해 (남은 답변: 모든 질문 해결 완료),Loss Function 정의,-0.009557198,0.0,0.00955719780176878
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해 (남은 답변: 모든 질문 해결 완료),MBTI / 좋아하는 아이돌,0.011219431,0.0,0.011219430714845657
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해 (남은 답변: 모든 질문 해결 완료),MSE Loss 설명,-0.017092962,0.0,0.017092961817979813
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해 (남은 답변: 모든 질문 해결 완료),MSE Loss 용도,-0.0058480795,0.0,0.005848079454153776
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해 (남은 답변: 모든 질문 해결 완료),"Multi-Class, Multi-Label 중 BCE 가 좋은 task",0.022563938,0.0,0.022563938051462173
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해 (남은 답변: 모든 질문 해결 완료),Multi-Label 에서 CE + Softmax 적용 문제점,0.0073364074,0.0,0.00733640743419528
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해 (남은 답변: 모든 질문 해결 완료),PEFT 방법 5가지,-0.008057798,0.0,0.008057798258960247
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해 (남은 답변: 모든 질문 해결 완료),거대 언어 모델 정의,-0.014246178,0.0,0.014246177859604359
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해 (남은 답변: 모든 질문 해결 완료),마지막 할 말,0.0037922282,0.0,0.003792228177189827
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해 (남은 답변: 모든 질문 해결 완료),면접 시작 인사,-0.009193483,0.0,0.00919348280876875
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해 (남은 답변: 모든 질문 해결 완료),면접 종료,-0.01787802,0.0,0.01787802018225193
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해 (남은 답변: 모든 질문 해결 완료),"인공지능, 머신러닝, 딥러닝 차이",0.010078816,0.0,0.01007881574332714
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해 (남은 답변: 모든 질문 해결 완료),잠시 휴식,0.94489765,1.0,0.05510234832763672
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해 (남은 답변: 모든 질문 해결 완료),확률 예측에서 MSE Loss 미 사용 이유,-0.0035290022,0.0,0.0035290021914988756
잠시 휴식 -> 재미있는 이야기 해줄래? (남은 답변: 잠시 휴식),BCE Loss 설명,0.003599222,0.0,0.0035992220509797335
잠시 휴식 -> 재미있는 이야기 해줄래? (남은 답변: 잠시 휴식),LLM Fine-Tuning 의 PEFT,0.9926131,1.0,0.007386922836303711
잠시 휴식 -> 재미있는 이야기 해줄래? (남은 답변: 잠시 휴식),LoRA,0.010769399,0.0,0.010769398882985115
잠시 휴식 -> 재미있는 이야기 해줄래? (남은 답변: 잠시 휴식),LoRA 와 QLoRA 의 차이,-0.0056715426,0.0,0.005671542603522539
잠시 휴식 -> 재미있는 이야기 해줄래? (남은 답변: 잠시 휴식),Loss Function 관련 실무 경험,-0.0075472086,0.0,0.007547208573669195
잠시 휴식 -> 재미있는 이야기 해줄래? (남은 답변: 잠시 휴식),Loss Function 예시,-0.014767817,0.0,0.014767817221581936
잠시 휴식 -> 재미있는 이야기 해줄래? (남은 답변: 잠시 휴식),Loss Function 정의,0.009546775,0.0,0.009546775370836258
잠시 휴식 -> 재미있는 이야기 해줄래? (남은 답변: 잠시 휴식),MBTI / 좋아하는 아이돌,0.0007158061,0.0,0.0007158060907386243
잠시 휴식 -> 재미있는 이야기 해줄래? (남은 답변: 잠시 휴식),MSE Loss 설명,0.0052528614,0.0,0.00525286141782999
잠시 휴식 -> 재미있는 이야기 해줄래? (남은 답변: 잠시 휴식),MSE Loss 용도,0.0034277532,0.0,0.0034277532249689102
잠시 휴식 -> 재미있는 이야기 해줄래? (남은 답변: 잠시 휴식),"Multi-Class, Multi-Label 중 BCE 가 좋은 task",0.005122028,0.0,0.005122027825564146
잠시 휴식 -> 재미있는 이야기 해줄래? (남은 답변: 잠시 휴식),Multi-Label 에서 CE + Softmax 적용 문제점,-0.0066125155,0.0,0.006612515542656183
잠시 휴식 -> 재미있는 이야기 해줄래? (남은 답변: 잠시 휴식),PEFT 방법 5가지,-0.02023291,0.0,0.02023291029036045
잠시 휴식 -> 재미있는 이야기 해줄래? (남은 답변: 잠시 휴식),거대 언어 모델 정의,-0.008099843,0.0,0.008099842816591263
잠시 휴식 -> 재미있는 이야기 해줄래? (남은 답변: 잠시 휴식),마지막 할 말,-0.008470808,0.0,0.008470808155834675
잠시 휴식 -> 재미있는 이야기 해줄래? (남은 답변: 잠시 휴식),면접 시작 인사,-0.011488626,0.0,0.011488625779747963
잠시 휴식 -> 재미있는 이야기 해줄래? (남은 답변: 잠시 휴식),면접 종료,0.009128272,0.0,0.009128271602094173
잠시 휴식 -> 재미있는 이야기 해줄래? (남은 답변: 잠시 휴식),"인공지능, 머신러닝, 딥러닝 차이",0.003571882,0.0,0.003571881912648678
잠시 휴식 -> 재미있는 이야기 해줄래? (남은 답변: 잠시 휴식),잠시 휴식,-0.020124396,0.0,0.020124396309256554
잠시 휴식 -> 재미있는 이야기 해줄래? (남은 답변: 잠시 휴식),확률 예측에서 MSE Loss 미 사용 이유,0.013019298,0.0,0.013019298203289509
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야 (남은 답변: 모든 질문 해결 완료)",BCE Loss 설명,-0.006701887,0.0,0.006701887119561434
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야 (남은 답변: 모든 질문 해결 완료)",LLM Fine-Tuning 의 PEFT,0.036327034,0.0,0.03632703423500061
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야 (남은 답변: 모든 질문 해결 완료)",LoRA,-0.025833528,0.0,0.025833528488874435
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야 (남은 답변: 모든 질문 해결 완료)",LoRA 와 QLoRA 의 차이,-0.0065460713,0.0,0.0065460712648928165
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야 (남은 답변: 모든 질문 해결 완료)",Loss Function 관련 실무 경험,-6.240857e-05,0.0,6.240857328521088e-05
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야 (남은 답변: 모든 질문 해결 완료)",Loss Function 예시,-0.008593434,0.0,0.008593433536589146
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야 (남은 답변: 모든 질문 해결 완료)",Loss Function 정의,0.00723194,0.0,0.007231940049678087
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야 (남은 답변: 모든 질문 해결 완료)",MBTI / 좋아하는 아이돌,0.010236425,0.0,0.010236425325274467
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야 (남은 답변: 모든 질문 해결 완료)",MSE Loss 설명,0.0011241519,0.0,0.001124151865951717
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야 (남은 답변: 모든 질문 해결 완료)",MSE Loss 용도,-0.007883478,0.0,0.007883477956056595
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야 (남은 답변: 모든 질문 해결 완료)","Multi-Class, Multi-Label 중 BCE 가 좋은 task",-0.024610683,0.0,0.02461068332195282
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야 (남은 답변: 모든 질문 해결 완료)",Multi-Label 에서 CE + Softmax 적용 문제점,0.0032480217,0.0,0.0032480217050760984
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야 (남은 답변: 모든 질문 해결 완료)",PEFT 방법 5가지,0.96285254,1.0,0.037147462368011475
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야 (남은 답변: 모든 질문 해결 완료)",거대 언어 모델 정의,-0.021438766,0.0,0.02143876627087593
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야 (남은 답변: 모든 질문 해결 완료)",마지막 할 말,-3.4910965e-05,0.0,3.491096504149027e-05
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야 (남은 답변: 모든 질문 해결 완료)",면접 시작 인사,-0.022375131,0.0,0.022375131025910378
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야 (남은 답변: 모든 질문 해결 완료)",면접 종료,-0.017558524,0.0,0.017558524385094643
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야 (남은 답변: 모든 질문 해결 완료)","인공지능, 머신러닝, 딥러닝 차이",0.0064604837,0.0,0.006460483651608229
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야 (남은 답변: 모든 질문 해결 완료)",잠시 휴식,-0.024123538,0.0,0.02412353828549385
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야 (남은 답변: 모든 질문 해결 완료)",확률 예측에서 MSE Loss 미 사용 이유,-0.0013767089,0.0,0.001376708853058517
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지? (남은 답변: LLM Fine-Tuning 의 PEFT),BCE Loss 설명,0.010365132,0.0,0.010365132242441177
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지? (남은 답변: LLM Fine-Tuning 의 PEFT),LLM Fine-Tuning 의 PEFT,0.9919262,1.0,0.008073806762695312
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지? (남은 답변: LLM Fine-Tuning 의 PEFT),LoRA,0.0061410326,0.0,0.006141032557934523
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지? (남은 답변: LLM Fine-Tuning 의 PEFT),LoRA 와 QLoRA 의 차이,0.000614171,0.0,0.000614170974586159
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지? (남은 답변: LLM Fine-Tuning 의 PEFT),Loss Function 관련 실무 경험,-0.0066707055,0.0,0.006670705508440733
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지? (남은 답변: LLM Fine-Tuning 의 PEFT),Loss Function 예시,-0.008175185,0.0,0.008175184950232506
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지? (남은 답변: LLM Fine-Tuning 의 PEFT),Loss Function 정의,0.003317612,0.0,0.0033176119904965162
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지? (남은 답변: LLM Fine-Tuning 의 PEFT),MBTI / 좋아하는 아이돌,0.00908952,0.0,0.009089520201086998
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지? (남은 답변: LLM Fine-Tuning 의 PEFT),MSE Loss 설명,0.0015442729,0.0,0.0015442728763446212
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지? (남은 답변: LLM Fine-Tuning 의 PEFT),MSE Loss 용도,-0.005290733,0.0,0.0052907331846654415
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지? (남은 답변: LLM Fine-Tuning 의 PEFT),"Multi-Class, Multi-Label 중 BCE 가 좋은 task",0.008196009,0.0,0.008196009323000908
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지? (남은 답변: LLM Fine-Tuning 의 PEFT),Multi-Label 에서 CE + Softmax 적용 문제점,-0.008744113,0.0,0.008744113147258759
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지? (남은 답변: LLM Fine-Tuning 의 PEFT),PEFT 방법 5가지,-0.036683593,0.0,0.036683592945337296
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지? (남은 답변: LLM Fine-Tuning 의 PEFT),거대 언어 모델 정의,-0.01242618,0.0,0.012426179833710194
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지? (남은 답변: LLM Fine-Tuning 의 PEFT),마지막 할 말,-0.018061498,0.0,0.018061498180031776
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지? (남은 답변: LLM Fine-Tuning 의 PEFT),면접 시작 인사,-0.0017627021,0.0,0.0017627021297812462
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지? (남은 답변: LLM Fine-Tuning 의 PEFT),면접 종료,0.007813706,0.0,0.007813706062734127
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지? (남은 답변: LLM Fine-Tuning 의 PEFT),"인공지능, 머신러닝, 딥러닝 차이",-0.004257621,0.0,0.00425762077793479
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지? (남은 답변: LLM Fine-Tuning 의 PEFT),잠시 휴식,-0.01082444,0.0,0.01082444004714489
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지? (남은 답변: LLM Fine-Tuning 의 PEFT),확률 예측에서 MSE Loss 미 사용 이유,0.018533085,0.0,0.01853308454155922
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야! (남은 답변: 모든 질문 해결 완료),BCE Loss 설명,-0.015646458,0.0,0.01564645767211914
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야! (남은 답변: 모든 질문 해결 완료),LLM Fine-Tuning 의 PEFT,-0.042078868,0.0,0.04207886755466461
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야! (남은 답변: 모든 질문 해결 완료),LoRA,-0.01264022,0.0,0.012640220113098621
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야! (남은 답변: 모든 질문 해결 완료),LoRA 와 QLoRA 의 차이,-0.000439847,0.0,0.00043984700459986925
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야! (남은 답변: 모든 질문 해결 완료),Loss Function 관련 실무 경험,0.008805417,0.0,0.008805417455732822
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야! (남은 답변: 모든 질문 해결 완료),Loss Function 예시,-0.019166699,0.0,0.019166698679327965
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야! (남은 답변: 모든 질문 해결 완료),Loss Function 정의,-0.011411926,0.0,0.01141192577779293
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야! (남은 답변: 모든 질문 해결 완료),MBTI / 좋아하는 아이돌,0.024090862,0.0,0.024090861901640892
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야! (남은 답변: 모든 질문 해결 완료),MSE Loss 설명,0.006757862,0.0,0.006757861934602261
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야! (남은 답변: 모든 질문 해결 완료),MSE Loss 용도,-0.0030926268,0.0,0.0030926268082112074
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야! (남은 답변: 모든 질문 해결 완료),"Multi-Class, Multi-Label 중 BCE 가 좋은 task",-0.006865213,0.0,0.00686521315947175
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야! (남은 답변: 모든 질문 해결 완료),Multi-Label 에서 CE + Softmax 적용 문제점,-0.015608664,0.0,0.01560866367071867
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야! (남은 답변: 모든 질문 해결 완료),PEFT 방법 5가지,0.9814744,1.0,0.01852560043334961
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야! (남은 답변: 모든 질문 해결 완료),거대 언어 모델 정의,0.0036194196,0.0,0.0036194196436554193
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야! (남은 답변: 모든 질문 해결 완료),마지막 할 말,-0.020342378,0.0,0.020342377945780754
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야! (남은 답변: 모든 질문 해결 완료),면접 시작 인사,0.008958146,0.0,0.00895814597606659
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야! (남은 답변: 모든 질문 해결 완료),면접 종료,-0.009015604,0.0,0.009015603922307491
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야! (남은 답변: 모든 질문 해결 완료),"인공지능, 머신러닝, 딥러닝 차이",-0.013474625,0.0,0.01347462460398674
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야! (남은 답변: 모든 질문 해결 완료),잠시 휴식,-0.01082061,0.0,0.010820610448718071
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야! (남은 답변: 모든 질문 해결 완료),확률 예측에서 MSE Loss 미 사용 이유,-0.008442777,0.0,0.008442777208983898
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다 (남은 답변: LLM Fine-Tuning 의 PEFT),BCE Loss 설명,0.012933055,0.0,0.012933054938912392
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다 (남은 답변: LLM Fine-Tuning 의 PEFT),LLM Fine-Tuning 의 PEFT,0.9903945,1.0,0.0096055269241333
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다 (남은 답변: LLM Fine-Tuning 의 PEFT),LoRA,0.029789938,0.0,0.029789937660098076
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다 (남은 답변: LLM Fine-Tuning 의 PEFT),LoRA 와 QLoRA 의 차이,0.009202205,0.0,0.009202204644680023
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다 (남은 답변: LLM Fine-Tuning 의 PEFT),Loss Function 관련 실무 경험,-0.0032559142,0.0,0.0032559141982346773
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다 (남은 답변: LLM Fine-Tuning 의 PEFT),Loss Function 예시,-0.010769908,0.0,0.01076990831643343
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다 (남은 답변: LLM Fine-Tuning 의 PEFT),Loss Function 정의,0.003927933,0.0,0.003927933052182198
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다 (남은 답변: LLM Fine-Tuning 의 PEFT),MBTI / 좋아하는 아이돌,0.0041641355,0.0,0.0041641355492174625
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다 (남은 답변: LLM Fine-Tuning 의 PEFT),MSE Loss 설명,-0.0023189522,0.0,0.0023189522325992584
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다 (남은 답변: LLM Fine-Tuning 의 PEFT),MSE Loss 용도,-0.0064199087,0.0,0.006419908720999956
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다 (남은 답변: LLM Fine-Tuning 의 PEFT),"Multi-Class, Multi-Label 중 BCE 가 좋은 task",0.0043892083,0.0,0.004389208275824785
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다 (남은 답변: LLM Fine-Tuning 의 PEFT),Multi-Label 에서 CE + Softmax 적용 문제점,-0.01053193,0.0,0.01053193025290966
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다 (남은 답변: LLM Fine-Tuning 의 PEFT),PEFT 방법 5가지,-0.03655344,0.0,0.03655343875288963
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다 (남은 답변: LLM Fine-Tuning 의 PEFT),거대 언어 모델 정의,-0.010391441,0.0,0.01039144117385149
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다 (남은 답변: LLM Fine-Tuning 의 PEFT),마지막 할 말,-0.016105302,0.0,0.016105301678180695
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다 (남은 답변: LLM Fine-Tuning 의 PEFT),면접 시작 인사,0.0033061951,0.0,0.0033061951398849487
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다 (남은 답변: LLM Fine-Tuning 의 PEFT),면접 종료,0.007921935,0.0,0.007921935059130192
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다 (남은 답변: LLM Fine-Tuning 의 PEFT),"인공지능, 머신러닝, 딥러닝 차이",-0.01206438,0.0,0.012064379639923573
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다 (남은 답변: LLM Fine-Tuning 의 PEFT),잠시 휴식,-0.012336813,0.0,0.012336812913417816
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다 (남은 답변: LLM Fine-Tuning 의 PEFT),확률 예측에서 MSE Loss 미 사용 이유,0.013543649,0.0,0.013543648645281792
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지? (남은 답변: 모든 질문 해결 완료)",BCE Loss 설명,0.00775431,0.0,0.00775431003421545
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지? (남은 답변: 모든 질문 해결 완료)",LLM Fine-Tuning 의 PEFT,0.010037286,0.0,0.010037286207079887
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지? (남은 답변: 모든 질문 해결 완료)",LoRA,0.9693325,1.0,0.030667483806610107
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지? (남은 답변: 모든 질문 해결 완료)",LoRA 와 QLoRA 의 차이,-0.025822774,0.0,0.025822773575782776
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지? (남은 답변: 모든 질문 해결 완료)",Loss Function 관련 실무 경험,0.008335173,0.0,0.0083351731300354
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지? (남은 답변: 모든 질문 해결 완료)",Loss Function 예시,-0.0060864612,0.0,0.006086461246013641
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지? (남은 답변: 모든 질문 해결 완료)",Loss Function 정의,-0.012215698,0.0,0.012215698137879372
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지? (남은 답변: 모든 질문 해결 완료)",MBTI / 좋아하는 아이돌,-0.0017863394,0.0,0.0017863394459709525
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지? (남은 답변: 모든 질문 해결 완료)",MSE Loss 설명,0.0002671128,0.0,0.00026711280224844813
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지? (남은 답변: 모든 질문 해결 완료)",MSE Loss 용도,-0.018418448,0.0,0.0184184480458498
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지? (남은 답변: 모든 질문 해결 완료)","Multi-Class, Multi-Label 중 BCE 가 좋은 task",0.0038020995,0.0,0.00380209949798882
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지? (남은 답변: 모든 질문 해결 완료)",Multi-Label 에서 CE + Softmax 적용 문제점,0.034932993,0.0,0.03493299335241318
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지? (남은 답변: 모든 질문 해결 완료)",PEFT 방법 5가지,0.007844833,0.0,0.007844832725822926
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지? (남은 답변: 모든 질문 해결 완료)",거대 언어 모델 정의,0.010180789,0.0,0.010180789045989513
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지? (남은 답변: 모든 질문 해결 완료)",마지막 할 말,-0.007083504,0.0,0.007083503995090723
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지? (남은 답변: 모든 질문 해결 완료)",면접 시작 인사,0.029500393,0.0,0.029500393196940422
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지? (남은 답변: 모든 질문 해결 완료)",면접 종료,-0.018692864,0.0,0.0186928641051054
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지? (남은 답변: 모든 질문 해결 완료)","인공지능, 머신러닝, 딥러닝 차이",0.006306071,0.0,0.00630607083439827
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지? (남은 답변: 모든 질문 해결 완료)",잠시 휴식,-0.012391164,0.0,0.012391163967549801
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지? (남은 답변: 모든 질문 해결 완료)",확률 예측에서 MSE Loss 미 사용 이유,-0.00751666,0.0,0.0075166597962379456
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야? (남은 답변: PEFT 방법 5가지),BCE Loss 설명,-0.006773646,0.0,0.006773645989596844
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야? (남은 답변: PEFT 방법 5가지),LLM Fine-Tuning 의 PEFT,-0.01959657,0.0,0.01959656924009323
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야? (남은 답변: PEFT 방법 5가지),LoRA,0.0035956707,0.0,0.003595670685172081
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야? (남은 답변: PEFT 방법 5가지),LoRA 와 QLoRA 의 차이,-0.009769379,0.0,0.009769379161298275
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야? (남은 답변: PEFT 방법 5가지),Loss Function 관련 실무 경험,0.0051629073,0.0,0.005162907298654318
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야? (남은 답변: PEFT 방법 5가지),Loss Function 예시,-0.044723958,0.0,0.044723957777023315
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야? (남은 답변: PEFT 방법 5가지),Loss Function 정의,-0.011948529,0.0,0.011948528699576855
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야? (남은 답변: PEFT 방법 5가지),MBTI / 좋아하는 아이돌,0.002644619,0.0,0.002644618973135948
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야? (남은 답변: PEFT 방법 5가지),MSE Loss 설명,0.004886027,0.0,0.004886026959866285
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야? (남은 답변: PEFT 방법 5가지),MSE Loss 용도,0.00083495287,0.0,0.0008349528652615845
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야? (남은 답변: PEFT 방법 5가지),"Multi-Class, Multi-Label 중 BCE 가 좋은 task",-0.021336427,0.0,0.021336426958441734
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야? (남은 답변: PEFT 방법 5가지),Multi-Label 에서 CE + Softmax 적용 문제점,-0.008632591,0.0,0.008632590994238853
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야? (남은 답변: PEFT 방법 5가지),PEFT 방법 5가지,0.9830947,1.0,0.01690530776977539
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야? (남은 답변: PEFT 방법 5가지),거대 언어 모델 정의,0.01801148,0.0,0.018011480569839478
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야? (남은 답변: PEFT 방법 5가지),마지막 할 말,-0.015569355,0.0,0.015569355338811874
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야? (남은 답변: PEFT 방법 5가지),면접 시작 인사,0.007062208,0.0,0.007062207907438278
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야? (남은 답변: PEFT 방법 5가지),면접 종료,-0.0037919988,0.0,0.003791998839005828
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야? (남은 답변: PEFT 방법 5가지),"인공지능, 머신러닝, 딥러닝 차이",-0.005053371,0.0,0.00505337119102478
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야? (남은 답변: PEFT 방법 5가지),잠시 휴식,-0.015170977,0.0,0.01517097745090723
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야? (남은 답변: PEFT 방법 5가지),확률 예측에서 MSE Loss 미 사용 이유,-0.006979884,0.0,0.00697988411411643
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야! (남은 답변: 모든 질문 해결 완료),BCE Loss 설명,-0.0055057425,0.0,0.005505742505192757
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야! (남은 답변: 모든 질문 해결 완료),LLM Fine-Tuning 의 PEFT,-0.0006099669,0.0,0.0006099669262766838
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야! (남은 답변: 모든 질문 해결 완료),LoRA,-0.028859152,0.0,0.028859151527285576
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야! (남은 답변: 모든 질문 해결 완료),LoRA 와 QLoRA 의 차이,0.9794487,1.0,0.020551323890686035
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야! (남은 답변: 모든 질문 해결 완료),Loss Function 관련 실무 경험,0.014356448,0.0,0.01435644831508398
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야! (남은 답변: 모든 질문 해결 완료),Loss Function 예시,0.0040939427,0.0,0.0040939426980912685
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야! (남은 답변: 모든 질문 해결 완료),Loss Function 정의,-0.007034898,0.0,0.007034897804260254
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야! (남은 답변: 모든 질문 해결 완료),MBTI / 좋아하는 아이돌,-0.01759255,0.0,0.017592549324035645
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야! (남은 답변: 모든 질문 해결 완료),MSE Loss 설명,0.0004144756,0.0,0.00041447559488005936
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야! (남은 답변: 모든 질문 해결 완료),MSE Loss 용도,-0.0044936263,0.0,0.0044936263002455235
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야! (남은 답변: 모든 질문 해결 완료),"Multi-Class, Multi-Label 중 BCE 가 좋은 task",0.011530463,0.0,0.01153046265244484
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야! (남은 답변: 모든 질문 해결 완료),Multi-Label 에서 CE + Softmax 적용 문제점,0.006596803,0.0,0.0065968031994998455
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야! (남은 답변: 모든 질문 해결 완료),PEFT 방법 5가지,-0.0095932465,0.0,0.009593246504664421
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야! (남은 답변: 모든 질문 해결 완료),거대 언어 모델 정의,-0.035886455,0.0,0.035886455327272415
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야! (남은 답변: 모든 질문 해결 완료),마지막 할 말,-0.021990197,0.0,0.021990196779370308
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야! (남은 답변: 모든 질문 해결 완료),면접 시작 인사,0.0047292514,0.0,0.004729251377284527
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야! (남은 답변: 모든 질문 해결 완료),면접 종료,0.008905812,0.0,0.008905812166631222
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야! (남은 답변: 모든 질문 해결 완료),"인공지능, 머신러닝, 딥러닝 차이",0.00080185645,0.0,0.0008018564549274743
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야! (남은 답변: 모든 질문 해결 완료),잠시 휴식,-0.024826279,0.0,0.024826278910040855
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야! (남은 답변: 모든 질문 해결 완료),확률 예측에서 MSE Loss 미 사용 이유,-0.0050424174,0.0,0.00504241744056344
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아? (남은 답변: LoRA),BCE Loss 설명,0.0025606784,0.0,0.002560678403824568
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아? (남은 답변: LoRA),LLM Fine-Tuning 의 PEFT,0.0111211995,0.0,0.01112119946628809
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아? (남은 답변: LoRA),LoRA,0.9770908,1.0,0.022909224033355713
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아? (남은 답변: LoRA),LoRA 와 QLoRA 의 차이,-0.029945277,0.0,0.02994527667760849
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아? (남은 답변: LoRA),Loss Function 관련 실무 경험,-0.0061235726,0.0,0.006123572587966919
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아? (남은 답변: LoRA),Loss Function 예시,0.0010559064,0.0,0.0010559064103290439
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아? (남은 답변: LoRA),Loss Function 정의,-0.003198226,0.0,0.0031982259824872017
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아? (남은 답변: LoRA),MBTI / 좋아하는 아이돌,0.007908729,0.0,0.007908728905022144
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아? (남은 답변: LoRA),MSE Loss 설명,-0.0003753965,0.0,0.00037539651384577155
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아? (남은 답변: LoRA),MSE Loss 용도,0.0046144635,0.0,0.0046144635416567326
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아? (남은 답변: LoRA),"Multi-Class, Multi-Label 중 BCE 가 좋은 task",0.0008314909,0.0,0.0008314909064210951
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아? (남은 답변: LoRA),Multi-Label 에서 CE + Softmax 적용 문제점,0.0061437413,0.0,0.006143741309642792
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아? (남은 답변: LoRA),PEFT 방법 5가지,-0.009251747,0.0,0.009251747280359268
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아? (남은 답변: LoRA),거대 언어 모델 정의,0.009975526,0.0,0.009975525550544262
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아? (남은 답변: LoRA),마지막 할 말,0.0026608254,0.0,0.0026608253829181194
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아? (남은 답변: LoRA),면접 시작 인사,0.019592674,0.0,0.01959267444908619
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아? (남은 답변: LoRA),면접 종료,-0.0049914406,0.0,0.0049914405681192875
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아? (남은 답변: LoRA),"인공지능, 머신러닝, 딥러닝 차이",-0.007305094,0.0,0.007305094040930271
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아? (남은 답변: LoRA),잠시 휴식,0.004127739,0.0,0.004127738997340202
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아? (남은 답변: LoRA),확률 예측에서 MSE Loss 미 사용 이유,0.013786909,0.0,0.01378690917044878
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야! (남은 답변: 모든 질문 해결 완료),BCE Loss 설명,-0.0066214344,0.0,0.006621434353291988
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야! (남은 답변: 모든 질문 해결 완료),LLM Fine-Tuning 의 PEFT,-0.006119529,0.0,0.0061195287853479385
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야! (남은 답변: 모든 질문 해결 완료),LoRA,-0.057297785,0.0,0.057297784835100174
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야! (남은 답변: 모든 질문 해결 완료),LoRA 와 QLoRA 의 차이,0.98023564,1.0,0.019764363765716553
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야! (남은 답변: 모든 질문 해결 완료),Loss Function 관련 실무 경험,0.022336816,0.0,0.022336816415190697
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야! (남은 답변: 모든 질문 해결 완료),Loss Function 예시,0.009539563,0.0,0.009539563208818436
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야! (남은 답변: 모든 질문 해결 완료),Loss Function 정의,-0.012835868,0.0,0.012835867702960968
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야! (남은 답변: 모든 질문 해결 완료),MBTI / 좋아하는 아이돌,-0.0021964966,0.0,0.0021964965853840113
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야! (남은 답변: 모든 질문 해결 완료),MSE Loss 설명,-0.004813544,0.0,0.004813543986529112
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야! (남은 답변: 모든 질문 해결 완료),MSE Loss 용도,-0.002297586,0.0,0.0022975860629230738
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야! (남은 답변: 모든 질문 해결 완료),"Multi-Class, Multi-Label 중 BCE 가 좋은 task",-0.013693206,0.0,0.013693206012248993
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야! (남은 답변: 모든 질문 해결 완료),Multi-Label 에서 CE + Softmax 적용 문제점,0.0014087876,0.0,0.0014087875606492162
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야! (남은 답변: 모든 질문 해결 완료),PEFT 방법 5가지,0.001336879,0.0,0.0013368789805099368
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야! (남은 답변: 모든 질문 해결 완료),거대 언어 모델 정의,-0.018822681,0.0,0.01882268115878105
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야! (남은 답변: 모든 질문 해결 완료),마지막 할 말,-0.012846999,0.0,0.012846998870372772
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야! (남은 답변: 모든 질문 해결 완료),면접 시작 인사,0.00058111775,0.0,0.0005811177543364465
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야! (남은 답변: 모든 질문 해결 완료),면접 종료,0.005900126,0.0,0.005900125950574875
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야! (남은 답변: 모든 질문 해결 완료),"인공지능, 머신러닝, 딥러닝 차이",0.0045005484,0.0,0.004500548355281353
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야! (남은 답변: 모든 질문 해결 완료),잠시 휴식,-0.0211976,0.0,0.021197600290179253
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야! (남은 답변: 모든 질문 해결 완료),확률 예측에서 MSE Loss 미 사용 이유,0.0052613225,0.0,0.005261322483420372
LoRA -> 무슨 OOM 없앤다는 것 같은데 (남은 답변: LoRA),BCE Loss 설명,0.0054236734,0.0,0.005423673428595066
LoRA -> 무슨 OOM 없앤다는 것 같은데 (남은 답변: LoRA),LLM Fine-Tuning 의 PEFT,0.0056001516,0.0,0.005600151605904102
LoRA -> 무슨 OOM 없앤다는 것 같은데 (남은 답변: LoRA),LoRA,0.9802853,1.0,0.019714713096618652
LoRA -> 무슨 OOM 없앤다는 것 같은데 (남은 답변: LoRA),LoRA 와 QLoRA 의 차이,-0.041906357,0.0,0.04190635681152344
LoRA -> 무슨 OOM 없앤다는 것 같은데 (남은 답변: LoRA),Loss Function 관련 실무 경험,-0.0078398455,0.0,0.00783984549343586
LoRA -> 무슨 OOM 없앤다는 것 같은데 (남은 답변: LoRA),Loss Function 예시,-0.0013793963,0.0,0.0013793963007628918
LoRA -> 무슨 OOM 없앤다는 것 같은데 (남은 답변: LoRA),Loss Function 정의,0.0045783697,0.0,0.004578369669616222
LoRA -> 무슨 OOM 없앤다는 것 같은데 (남은 답변: LoRA),MBTI / 좋아하는 아이돌,0.011746193,0.0,0.011746193282306194
LoRA -> 무슨 OOM 없앤다는 것 같은데 (남은 답변: LoRA),MSE Loss 설명,0.0020610392,0.0,0.0020610392093658447
LoRA -> 무슨 OOM 없앤다는 것 같은데 (남은 답변: LoRA),MSE Loss 용도,0.01022731,0.0,0.010227309539914131
LoRA -> 무슨 OOM 없앤다는 것 같은데 (남은 답변: LoRA),"Multi-Class, Multi-Label 중 BCE 가 좋은 task",-0.0015725611,0.0,0.0015725611010566354
LoRA -> 무슨 OOM 없앤다는 것 같은데 (남은 답변: LoRA),Multi-Label 에서 CE + Softmax 적용 문제점,0.013750636,0.0,0.013750636018812656
LoRA -> 무슨 OOM 없앤다는 것 같은데 (남은 답변: LoRA),PEFT 방법 5가지,-0.009425375,0.0,0.009425374679267406
LoRA -> 무슨 OOM 없앤다는 것 같은데 (남은 답변: LoRA),거대 언어 모델 정의,0.005434327,0.0,0.0054343268275260925
LoRA -> 무슨 OOM 없앤다는 것 같은데 (남은 답변: LoRA),마지막 할 말,-3.2151493e-05,0.0,3.215149263269268e-05
LoRA -> 무슨 OOM 없앤다는 것 같은데 (남은 답변: LoRA),면접 시작 인사,0.030724503,0.0,0.030724503099918365
LoRA -> 무슨 OOM 없앤다는 것 같은데 (남은 답변: LoRA),면접 종료,-0.004638092,0.0,0.004638092126697302
LoRA -> 무슨 OOM 없앤다는 것 같은데 (남은 답변: LoRA),"인공지능, 머신러닝, 딥러닝 차이",-0.0062209186,0.0,0.0062209186144173145
LoRA -> 무슨 OOM 없앤다는 것 같은데 (남은 답변: LoRA),잠시 휴식,0.001905527,0.0,0.0019055269658565521
LoRA -> 무슨 OOM 없앤다는 것 같은데 (남은 답변: LoRA),확률 예측에서 MSE Loss 미 사용 이유,-0.0010633685,0.0,0.0010633685160428286
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야! (남은 답변: 모든 질문 해결 완료),BCE Loss 설명,0.009953146,0.0,0.009953145869076252
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야! (남은 답변: 모든 질문 해결 완료),LLM Fine-Tuning 의 PEFT,-0.014102253,0.0,0.014102253131568432
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야! (남은 답변: 모든 질문 해결 완료),LoRA,-0.0080964295,0.0,0.008096429519355297
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야! (남은 답변: 모든 질문 해결 완료),LoRA 와 QLoRA 의 차이,-0.017567163,0.0,0.017567163333296776
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야! (남은 답변: 모든 질문 해결 완료),Loss Function 관련 실무 경험,0.004567915,0.0,0.004567915108054876
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야! (남은 답변: 모든 질문 해결 완료),Loss Function 예시,0.018995015,0.0,0.01899501495063305
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야! (남은 답변: 모든 질문 해결 완료),Loss Function 정의,0.0044833906,0.0,0.004483390599489212
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야! (남은 답변: 모든 질문 해결 완료),MBTI / 좋아하는 아이돌,-0.038906347,0.0,0.03890634700655937
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야! (남은 답변: 모든 질문 해결 완료),MSE Loss 설명,-0.012995809,0.0,0.012995809316635132
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야! (남은 답변: 모든 질문 해결 완료),MSE Loss 용도,-0.013611037,0.0,0.013611037284135818
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야! (남은 답변: 모든 질문 해결 완료),"Multi-Class, Multi-Label 중 BCE 가 좋은 task",-0.0076234196,0.0,0.007623419631272554
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야! (남은 답변: 모든 질문 해결 완료),Multi-Label 에서 CE + Softmax 적용 문제점,-0.0076286187,0.0,0.007628618739545345
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야! (남은 답변: 모든 질문 해결 완료),PEFT 방법 5가지,0.00080236554,0.0,0.0008023655391298234
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야! (남은 답변: 모든 질문 해결 완료),거대 언어 모델 정의,0.0016673139,0.0,0.0016673138597980142
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야! (남은 답변: 모든 질문 해결 완료),마지막 할 말,0.9599316,1.0,0.04006838798522949
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야! (남은 답변: 모든 질문 해결 완료),면접 시작 인사,-0.012145533,0.0,0.012145533226430416
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야! (남은 답변: 모든 질문 해결 완료),면접 종료,-0.014953368,0.0,0.014953368343412876
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야! (남은 답변: 모든 질문 해결 완료),"인공지능, 머신러닝, 딥러닝 차이",0.0048328387,0.0,0.004832838661968708
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야! (남은 답변: 모든 질문 해결 완료),잠시 휴식,-0.0002940484,0.0,0.0002940483973361552
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야! (남은 답변: 모든 질문 해결 완료),확률 예측에서 MSE Loss 미 사용 이유,-0.023972806,0.0,0.023972805589437485
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데 (남은 답변: LoRA 와 QLoRA 의 차이),BCE Loss 설명,-0.016068634,0.0,0.016068633645772934
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데 (남은 답변: LoRA 와 QLoRA 의 차이),LLM Fine-Tuning 의 PEFT,-0.0068494543,0.0,0.006849454250186682
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데 (남은 답변: LoRA 와 QLoRA 의 차이),LoRA,-0.07370153,0.0,0.07370153069496155
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데 (남은 답변: LoRA 와 QLoRA 의 차이),LoRA 와 QLoRA 의 차이,0.98337615,1.0,0.016623854637145996
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데 (남은 답변: LoRA 와 QLoRA 의 차이),Loss Function 관련 실무 경험,0.011584996,0.0,0.011584996245801449
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데 (남은 답변: LoRA 와 QLoRA 의 차이),Loss Function 예시,-0.005308044,0.0,0.005308044143021107
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데 (남은 답변: LoRA 와 QLoRA 의 차이),Loss Function 정의,-0.012510352,0.0,0.012510351836681366
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데 (남은 답변: LoRA 와 QLoRA 의 차이),MBTI / 좋아하는 아이돌,-0.003247094,0.0,0.0032470941077917814
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데 (남은 답변: LoRA 와 QLoRA 의 차이),MSE Loss 설명,-0.0002734342,0.0,0.0002734342124313116
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데 (남은 답변: LoRA 와 QLoRA 의 차이),MSE Loss 용도,-0.0025494224,0.0,0.002549422439187765
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데 (남은 답변: LoRA 와 QLoRA 의 차이),"Multi-Class, Multi-Label 중 BCE 가 좋은 task",-0.0067845155,0.0,0.006784515455365181
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데 (남은 답변: LoRA 와 QLoRA 의 차이),Multi-Label 에서 CE + Softmax 적용 문제점,0.0058501926,0.0,0.005850192625075579
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데 (남은 답변: LoRA 와 QLoRA 의 차이),PEFT 방법 5가지,0.011922866,0.0,0.011922866106033325
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데 (남은 답변: LoRA 와 QLoRA 의 차이),거대 언어 모델 정의,-0.017081277,0.0,0.017081277444958687
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데 (남은 답변: LoRA 와 QLoRA 의 차이),마지막 할 말,-0.0035098572,0.0,0.003509857226163149
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데 (남은 답변: LoRA 와 QLoRA 의 차이),면접 시작 인사,-0.011967581,0.0,0.011967580765485764
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데 (남은 답변: LoRA 와 QLoRA 의 차이),면접 종료,0.01787973,0.0,0.017879730090498924
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데 (남은 답변: LoRA 와 QLoRA 의 차이),"인공지능, 머신러닝, 딥러닝 차이",0.0031500428,0.0,0.0031500428449362516
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데 (남은 답변: LoRA 와 QLoRA 의 차이),잠시 휴식,-0.009927094,0.0,0.009927093982696533
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데 (남은 답변: LoRA 와 QLoRA 의 차이),확률 예측에서 MSE Loss 미 사용 이유,0.004962445,0.0,0.004962444771081209
마지막 할 말 -> 로라야 정말 고마워! (남은 답변: 마지막 할 말),BCE Loss 설명,-0.0009573262,0.0,0.0009573262068443
마지막 할 말 -> 로라야 정말 고마워! (남은 답변: 마지막 할 말),LLM Fine-Tuning 의 PEFT,0.00542999,0.0,0.0054299901239573956
마지막 할 말 -> 로라야 정말 고마워! (남은 답변: 마지막 할 말),LoRA,0.023103975,0.0,0.023103974759578705
마지막 할 말 -> 로라야 정말 고마워! (남은 답변: 마지막 할 말),LoRA 와 QLoRA 의 차이,-0.0034953803,0.0,0.0034953802824020386
마지막 할 말 -> 로라야 정말 고마워! (남은 답변: 마지막 할 말),Loss Function 관련 실무 경험,-0.012220919,0.0,0.012220919132232666
마지막 할 말 -> 로라야 정말 고마워! (남은 답변: 마지막 할 말),Loss Function 예시,-0.0072066197,0.0,0.007206619717180729
마지막 할 말 -> 로라야 정말 고마워! (남은 답변: 마지막 할 말),Loss Function 정의,-0.001314071,0.0,0.0013140710070729256
마지막 할 말 -> 로라야 정말 고마워! (남은 답변: 마지막 할 말),MBTI / 좋아하는 아이돌,0.03151764,0.0,0.0315176397562027
마지막 할 말 -> 로라야 정말 고마워! (남은 답변: 마지막 할 말),MSE Loss 설명,-0.02403915,0.0,0.024039149284362793
마지막 할 말 -> 로라야 정말 고마워! (남은 답변: 마지막 할 말),MSE Loss 용도,-0.035203822,0.0,0.03520382195711136
마지막 할 말 -> 로라야 정말 고마워! (남은 답변: 마지막 할 말),"Multi-Class, Multi-Label 중 BCE 가 좋은 task",0.00011509452,0.0,0.0001150945172412321
마지막 할 말 -> 로라야 정말 고마워! (남은 답변: 마지막 할 말),Multi-Label 에서 CE + Softmax 적용 문제점,-0.0055878516,0.0,0.005587851628661156
마지막 할 말 -> 로라야 정말 고마워! (남은 답변: 마지막 할 말),PEFT 방법 5가지,-0.0068658465,0.0,0.006865846458822489
마지막 할 말 -> 로라야 정말 고마워! (남은 답변: 마지막 할 말),거대 언어 모델 정의,-0.012844463,0.0,0.012844462879002094
마지막 할 말 -> 로라야 정말 고마워! (남은 답변: 마지막 할 말),마지막 할 말,-0.027347894,0.0,0.02734789438545704
마지막 할 말 -> 로라야 정말 고마워! (남은 답변: 마지막 할 말),면접 시작 인사,-0.018518,0.0,0.018518000841140747
마지막 할 말 -> 로라야 정말 고마워! (남은 답변: 마지막 할 말),면접 종료,0.97794086,1.0,0.022059142589569092
마지막 할 말 -> 로라야 정말 고마워! (남은 답변: 마지막 할 말),"인공지능, 머신러닝, 딥러닝 차이",-0.021302031,0.0,0.021302031353116035
마지막 할 말 -> 로라야 정말 고마워! (남은 답변: 마지막 할 말),잠시 휴식,0.0019321362,0.0,0.0019321362487971783
마지막 할 말 -> 로라야 정말 고마워! (남은 답변: 마지막 할 말),확률 예측에서 MSE Loss 미 사용 이유,0.011827935,0.0,0.011827935464680195
마지막 할 말 -> 로라야 사랑해 (남은 답변: 마지막 할 말),BCE Loss 설명,-0.0060225483,0.0,0.0060225483030080795
마지막 할 말 -> 로라야 사랑해 (남은 답변: 마지막 할 말),LLM Fine-Tuning 의 PEFT,0.012277349,0.0,0.012277348898351192
마지막 할 말 -> 로라야 사랑해 (남은 답변: 마지막 할 말),LoRA,0.017668067,0.0,0.017668066546320915
마지막 할 말 -> 로라야 사랑해 (남은 답변: 마지막 할 말),LoRA 와 QLoRA 의 차이,-0.00073245063,0.0,0.0007324506295844913
마지막 할 말 -> 로라야 사랑해 (남은 답변: 마지막 할 말),Loss Function 관련 실무 경험,-0.008390729,0.0,0.008390729315578938
마지막 할 말 -> 로라야 사랑해 (남은 답변: 마지막 할 말),Loss Function 예시,-0.011437505,0.0,0.011437504552304745
마지막 할 말 -> 로라야 사랑해 (남은 답변: 마지막 할 말),Loss Function 정의,-0.0017258155,0.0,0.0017258154693990946
마지막 할 말 -> 로라야 사랑해 (남은 답변: 마지막 할 말),MBTI / 좋아하는 아이돌,0.029300261,0.0,0.029300261288881302
마지막 할 말 -> 로라야 사랑해 (남은 답변: 마지막 할 말),MSE Loss 설명,-0.021352887,0.0,0.02135288715362549
마지막 할 말 -> 로라야 사랑해 (남은 답변: 마지막 할 말),MSE Loss 용도,-0.034912966,0.0,0.034912966191768646
마지막 할 말 -> 로라야 사랑해 (남은 답변: 마지막 할 말),"Multi-Class, Multi-Label 중 BCE 가 좋은 task",0.0022114469,0.0,0.00221144687384367
마지막 할 말 -> 로라야 사랑해 (남은 답변: 마지막 할 말),Multi-Label 에서 CE + Softmax 적용 문제점,-0.005663634,0.0,0.0056636338122189045
마지막 할 말 -> 로라야 사랑해 (남은 답변: 마지막 할 말),PEFT 방법 5가지,-0.0053927046,0.0,0.005392704624682665
마지막 할 말 -> 로라야 사랑해 (남은 답변: 마지막 할 말),거대 언어 모델 정의,-0.008523814,0.0,0.008523814380168915
마지막 할 말 -> 로라야 사랑해 (남은 답변: 마지막 할 말),마지막 할 말,-0.03188691,0.0,0.031886909157037735
마지막 할 말 -> 로라야 사랑해 (남은 답변: 마지막 할 말),면접 시작 인사,-0.022209048,0.0,0.0222090482711792
마지막 할 말 -> 로라야 사랑해 (남은 답변: 마지막 할 말),면접 종료,0.97785085,1.0,0.02214914560317993
마지막 할 말 -> 로라야 사랑해 (남은 답변: 마지막 할 말),"인공지능, 머신러닝, 딥러닝 차이",-0.020039732,0.0,0.02003973163664341
마지막 할 말 -> 로라야 사랑해 (남은 답변: 마지막 할 말),잠시 휴식,-0.0024015058,0.0,0.0024015058297663927
마지막 할 말 -> 로라야 사랑해 (남은 답변: 마지막 할 말),확률 예측에서 MSE Loss 미 사용 이유,0.009847095,0.0,0.009847095236182213
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어 (남은 답변: 마지막 할 말),BCE Loss 설명,-0.0038923447,0.0,0.00389234465546906
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어 (남은 답변: 마지막 할 말),LLM Fine-Tuning 의 PEFT,0.0060299803,0.0,0.006029980257153511
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어 (남은 답변: 마지막 할 말),LoRA,0.017207703,0.0,0.01720770262181759
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어 (남은 답변: 마지막 할 말),LoRA 와 QLoRA 의 차이,-0.0053735627,0.0,0.005373562686145306
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어 (남은 답변: 마지막 할 말),Loss Function 관련 실무 경험,-0.007974977,0.0,0.007974976673722267
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어 (남은 답변: 마지막 할 말),Loss Function 예시,-0.005410383,0.0,0.005410382989794016
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어 (남은 답변: 마지막 할 말),Loss Function 정의,-0.0008892134,0.0,0.0008892134064808488
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어 (남은 답변: 마지막 할 말),MBTI / 좋아하는 아이돌,0.028309532,0.0,0.02830953150987625
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어 (남은 답변: 마지막 할 말),MSE Loss 설명,-0.026821036,0.0,0.026821035891771317
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어 (남은 답변: 마지막 할 말),MSE Loss 용도,-0.032070864,0.0,0.032070863991975784
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어 (남은 답변: 마지막 할 말),"Multi-Class, Multi-Label 중 BCE 가 좋은 task",0.0011059925,0.0,0.001105992472730577
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어 (남은 답변: 마지막 할 말),Multi-Label 에서 CE + Softmax 적용 문제점,-0.006793178,0.0,0.006793178152292967
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어 (남은 답변: 마지막 할 말),PEFT 방법 5가지,-0.0043779383,0.0,0.004377938341349363
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어 (남은 답변: 마지막 할 말),거대 언어 모델 정의,-0.010805035,0.0,0.010805035009980202
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어 (남은 답변: 마지막 할 말),마지막 할 말,-0.029178187,0.0,0.029178187251091003
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어 (남은 답변: 마지막 할 말),면접 시작 인사,-0.025220316,0.0,0.025220315903425217
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어 (남은 답변: 마지막 할 말),면접 종료,0.977836,1.0,0.022163987159729004
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어 (남은 답변: 마지막 할 말),"인공지능, 머신러닝, 딥러닝 차이",-0.018042168,0.0,0.018042167648673058
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어 (남은 답변: 마지막 할 말),잠시 휴식,-0.011955989,0.0,0.011955988593399525
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어 (남은 답변: 마지막 할 말),확률 예측에서 MSE Loss 미 사용 이유,0.015955754,0.0,0.01595575362443924
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워 (남은 답변: 마지막 할 말),BCE Loss 설명,-0.0017747836,0.0,0.0017747835954651237
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워 (남은 답변: 마지막 할 말),LLM Fine-Tuning 의 PEFT,-0.00187116,0.0,0.0018711599987000227
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워 (남은 답변: 마지막 할 말),LoRA,0.007074386,0.0,0.00707438588142395
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워 (남은 답변: 마지막 할 말),LoRA 와 QLoRA 의 차이,-0.0027152163,0.0,0.002715216251090169
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워 (남은 답변: 마지막 할 말),Loss Function 관련 실무 경험,0.0014196275,0.0,0.0014196274569258094
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워 (남은 답변: 마지막 할 말),Loss Function 예시,-0.011454874,0.0,0.011454873718321323
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워 (남은 답변: 마지막 할 말),Loss Function 정의,-0.002165896,0.0,0.002165895886719227
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워 (남은 답변: 마지막 할 말),MBTI / 좋아하는 아이돌,0.033028048,0.0,0.033028047531843185
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워 (남은 답변: 마지막 할 말),MSE Loss 설명,-0.019127138,0.0,0.01912713795900345
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워 (남은 답변: 마지막 할 말),MSE Loss 용도,-0.03715825,0.0,0.03715825080871582
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워 (남은 답변: 마지막 할 말),"Multi-Class, Multi-Label 중 BCE 가 좋은 task",0.0051614353,0.0,0.005161435343325138
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워 (남은 답변: 마지막 할 말),Multi-Label 에서 CE + Softmax 적용 문제점,-0.007027091,0.0,0.00702709099277854
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워 (남은 답변: 마지막 할 말),PEFT 방법 5가지,0.002194168,0.0,0.002194168046116829
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워 (남은 답변: 마지막 할 말),거대 언어 모델 정의,-0.014901276,0.0,0.014901275746524334
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워 (남은 답변: 마지막 할 말),마지막 할 말,-0.03594326,0.0,0.03594325855374336
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워 (남은 답변: 마지막 할 말),면접 시작 인사,0.00020029346,0.0,0.0002002934634219855
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워 (남은 답변: 마지막 할 말),면접 종료,0.9767117,1.0,0.023288309574127197
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워 (남은 답변: 마지막 할 말),"인공지능, 머신러닝, 딥러닝 차이",-0.014178366,0.0,0.014178366400301456
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워 (남은 답변: 마지막 할 말),잠시 휴식,-0.011425174,0.0,0.011425173841416836
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워 (남은 답변: 마지막 할 말),확률 예측에서 MSE Loss 미 사용 이유,-0.00036966646,0.0,0.00036966646439395845
