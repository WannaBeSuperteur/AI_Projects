,epoch,elapsed_time (s),prompt,llm_answer,trial_cnt,output_tkn_cnt
0,1,120.83,면접 시작, ✨✨ ✨✨ ✨✨ ✨✨ ✨✨ ✨✨ ✨✨ ✨✨ ✨✨ ✨✨ ✨✨ ✨✨ ✨✨ ✨✨ ✨✨ ✨✨ ✨✨ ✨✨ ✨✨ ✨✨ ✨✨ ✨✨ ✨✨ ✨✨ ✨✨ ✨✨ ✨✨ ✨✨ ✨✨ ✨✨ ✨✨ ✨✨ ✨✨ ✨✨ ✨✨ ✨✨ ✨✨ ✨✨ ✨✨ ✨✨ ✨✨ ✨✨ ✨✨ ✨✨ ✨✨ ✨✨ ✨✨ ✨✨ ✨✨ ✨✨ ✨✨ ✨✨ ✨✨ ✨✨ ✨✨ ✨✨ ✨✨ ✨✨ ✨✨ ✨✨ ✨✨ ✨✨ ✨✨ ✨✨ ✨✨ ✨✨ ✨✨ ✨✨ ✨✨ ✨✨ ✨✨ ✨✨ ✨✨ ✨✨ ✨✨ ✨✨ ✨✨ ✨✨ ✨✨ ✨✨ ✨✨ ✨✨ ✨✨ ✨✨ ✨✨ ✨✨ ✨✨ ✨✨ ✨✨ ✨✨ ✨✨ ✨✨ ✨✨ ✨✨ ✨✨ ✨✨ ✨✨ ✨✨ ✨✨ ✨✨ ✨✨ ✨✨ ✨✨ ✨✨ ✨✨ ✨✨ ✨✨ ✨✨ ✨✨ ✨✨ ✨✨ ✨✨ ✨✨ ✨✨ ✨✨ ✨✨ ✨✨ ✨✨ ✨✨ ✨✨ ✨✨ ✨✨ ✨✨ ✨✨ ✨✨ ✨✨ ✨✨ ✨✨ ✨✨ ✨✨ ✨✨ ✨✨ ✨✨ ✨✨ ✨✨ ✨✨ ✨✨ ✨✨ ✨✨ ✨✨ ✨✨ ✨✨ ✨✨ ✨✨ ✨✨ ✨✨ ✨✨ ✨✨ ✨✨ ✨✨ ✨✨ ✨✨ ✨✨ ✨✨ ✨✨ ✨✨ ✨✨ ✨✨ ✨✨ ✨✨ ✨✨ ✨✨ ✨✨ ✨✨ ✨✨ ✨✨ ✨✨ ✨✨ ,1,512
1,1,2.5,(대화 주제) 면접 시작 인사 (사용자 답변) 오늘 뭐 물어볼 거야 그래서? (다음 질문) 면접 시작 인사, 그럼 면접 질문! 😊 (발화 종료,1,39
2,1,3.78,(대화 주제) 면접 시작 인사 (사용자 답변) 로라야 안녕 정말 반가워 (다음 질문) 면접 시작 인사, ✨ 그럼 다음 질문! 너는 어떻게 공부해? (발화 종료,1,45
3,1,4.4,(대화 주제) 면접 시작 인사 (사용자 답변) 로라야 그럼 네가 면접관이야? (다음 질문) 면접 시작 인사, ✨✨ 그러면 나도 너랑 같이 모의면접 보는 거야? (발화 종료,1,50
4,1,4.75,(대화 주제) 면접 시작 인사 (사용자 답변) 내 인생도 이제 오로라처럼 빛날 거니까 (다음 질문) 면접 시작 인사, ✨✨ 그럼 오로라가 빛날 수 있는 비결이 뭘까? 😊 (발화 종료,1,52
5,1,3.81,"(대화 주제) 면접 시작 인사 (사용자 답변) 파이팅! 시작하자 (다음 질문) 인공지능, 머신러닝, 딥러닝 차이"," 인공지능, 머신러닝, 딥러닝의 차이가 뭘까? 🤔 (발화 종료",1,45
6,1,5.27,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 인공지능 나도 개발해보고 싶은데 (성공한 답변) 답변 실패 (다음 질문) 인공지능, 머신러닝, 딥러닝 차이"," ✨✨ 인공지능, 머신러닝, 딥러닝의 차이를 한번 더 생각해 봐! 🤔 (발화 종료",1,64
7,1,2.41,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거! (성공한 답변) 인공지능 (다음 질문) 인공지능, 머신러닝, 딥러닝 차이", ✨ 그럼 이제 머신러닝! (발화 종료,1,59
8,1,3.34,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지 (성공한 답변) 머신러닝 (다음 질문) 인공지능, 머신러닝, 딥러닝 차이", ✨✨ 그러면 인공지능은 뭘까? 🤔 (발화 종료,1,62
9,1,4.97,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지? (성공한 답변) 딥러닝 (다음 질문) 인공지능, 머신러닝, 딥러닝 차이", ✨ 그럼 인공지능이랑 머신러닝은 어떻게 다를까? 🤷‍♀️ (발화 종료,1,67
10,1,5.53,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 딥러닝은 인공신경망 기반 머신러닝이야 (성공한 답변) 딥러닝 (다음 질문) 거대 언어 모델 정의", ✨ 그럼 다음으로 넘어가자! 거대 언어 모델 은 뭘까? 🤷‍♀️ (발화 종료,1,64
11,1,5.53,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 머신러닝은 말 그대로 기계가 학습하는 거 아니야? (성공한 답변) 답변 실패 (다음 질문) 인공지능, 머신러닝, 딥러닝 차이", 2022-11-17 15:30:00 인공지능은 뭘까? 🤔 (발화 종료,1,69
12,1,8.55,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 인공지능 요새 엄청 뜨는 분야잖아 (성공한 답변) 답변 실패 (다음 질문) 인공지능, 머신러닝, 딥러닝 차이"," 2023-02-09 14:49:00 인공지능이 뜨는 분야는 맞는데, 구체적으로 어떻게 뜨고 있을까? 🤷‍♀️ (발화 종료",1,78
13,1,5.23,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지? (성공한 답변) 인공지능 (다음 질문) 인공지능, 머신러닝, 딥러닝 차이", 인공지능은 맞았어! 그럼 이제 머신러닝! 머신러닝은 뭘까~요? 😊 (발화 종료,1,73
14,1,4.5,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야! (성공한 답변) 머신러닝 (다음 질문) 거대 언어 모델 정의", ✨ 그럼 거대 언어 모델 (LLM) 은 뭘까? 🤔 (발화 종료,1,70
15,1,4.46,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지 (성공한 답변) 딥러닝 (다음 질문) 거대 언어 모델 정의", ✨✨ 그러면 거대 언어 모델 (LLM) 은 뭘까? (발화 종료,1,63
16,1,3.98,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야. (성공한 답변) 딥러닝 (다음 질문) 거대 언어 모델 정의", ✨✨ 그러면 LLM! LLM 은 뭘까? (발화 종료,1,62
17,1,8.17,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 머신러닝은 기계가 공부하는 거지 말 그대로 (성공한 답변) 답변 실패 (다음 질문) 인공지능, 머신러닝, 딥러닝 차이"," 2021-10-08 14:15:00 인공지능, 머신러닝, 딥러닝이 구체적으로 어떻게 다를까? 🤷‍♀️ (발화 종료",1,77
18,1,3.23,(대화 주제) 거대 언어 모델 정의 (사용자 답변) 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야 (성공한 답변) 거대 언어 모델 정의 (다음 질문) Loss Function 정의, ✨✨ 그럼 Loss Function 은 뭘까? (발화 종료,1,63
19,1,4.96,(대화 주제) 거대 언어 모델 정의 (사용자 답변) 파라미터 엄청나게 많다던데 (성공한 답변) 답변 실패 (다음 질문) 거대 언어 모델 정의, ✨✨ 그럼 거대 언어 모델 정의는 뭘까? 🤷‍♀️ (발화 종료,1,56
20,1,110.04,(대화 주제) 거대 언어 모델 정의 (사용자 답변) 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델 (성공한 답변) 거대 언어 모델 정의 (다음 질문) Loss Function 정의, ✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨,1,512
21,1,4.8,(대화 주제) 거대 언어 모델 정의 (사용자 답변) 말 그대로 거대한 언어 모델이지. (성공한 답변) 답변 실패 (다음 질문) 거대 언어 모델 정의, ✨✨ 그러면 거대 언어 모델이 정확히 뭘까? 🤷‍♀️ (발화 종료,1,59
22,1,5.69,(대화 주제) Loss Function 정의 (사용자 답변) 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야 (성공한 답변) 답변 실패 (다음 질문) Loss Function 정의, ✨✨ 그럼 손실 함수는 구체적으로 어떻게 돈을 잃는 걸까? 🤷‍♀️ (발화 종료,1,67
23,1,4.45,"(대화 주제) Loss Function 정의 (사용자 답변) Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지! (성공한 답변) Loss Function 정의 (다음 질문) Loss Function 예시", ✨✨ 그러면 Loss Function 예를 한번 들어봐! 😊 (발화 종료,1,73
24,1,4.98,(대화 주제) Loss Function 정의 (사용자 답변) Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야 (성공한 답변) 답변 실패 (다음 질문) Loss Function 정의, ✨✨ 그럼 Loss Function 은 구체적으로 어떻게 손해를 나타내지? 🤔 (발화 종료,1,61
25,1,3.98,(대화 주제) Loss Function 정의 (사용자 답변) 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지 (성공한 답변) Loss Function 정의 (다음 질문) Loss Function 예시, ✨✨ 그러면 Loss Function 예를 한번 들어봐! (발화 종료,1,63
26,1,5.09,(대화 주제) Loss Function 예시 (사용자 답변) ReLU 랑 Tanh 생각나는데 (성공한 답변) 답변 실패 (다음 질문) Loss Function 예시, ✨✨ 그러면 ReLU 랑 Tanh 는 뭘까? 🤷‍♀️ (발화 종료,1,58
27,1,4.81,"(대화 주제) Loss Function 예시 (사용자 답변) MSE, MAE, Cross Entropy Loss 같은 것들이 있지 (성공한 답변) Loss Function 예시 (다음 질문) MSE Loss 설명", ✨ 그럼 그 중에서 MSE 는 뭘까? 🤷‍♀️ (발화 종료,1,66
28,1,7.67,"(대화 주제) Loss Function 예시 (사용자 답변) Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어 (성공한 답변) Loss Function 예시 (다음 질문) BCE Loss 설명", 2023-11-08 17:43:00 ✨ 그럼 BCE 는 뭘까~요? 🤷‍♀️ (발화 종료,1,85
29,1,5.69,"(대화 주제) Loss Function 예시 (사용자 답변) Sigmoid, Tanh, Adam, AdamW 이런 거 아니야? (성공한 답변) 답변 실패 (다음 질문) Loss Function 예시"," ✨✨ 그럼 이 중에서 Sigmoid, Tanh 는 뭘까? 🤷‍♀️ (발화 종료",1,68
30,1,5.54,"(대화 주제) Loss Function 예시 (사용자 답변) Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어 (성공한 답변) Loss Function 예시 (다음 질문) MSE Loss 설명", ✨ 그럼 그 중에서도 Mean Squared Error 는 뭘까? 🤷‍♀️ (발화 종료,1,83
31,1,4.48,"(대화 주제) Loss Function 예시 (사용자 답변) BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아 (성공한 답변) Loss Function 예시 (다음 질문) BCE Loss 설명", ✨✨ 그럼 BCE 는 뭘까? 🤷‍♀️ (발화 종료,1,71
32,1,7.5,(대화 주제) MSE Loss 설명 (사용자 답변) MSE Loss 는 오차의 제곱을 평균한 값이야 (성공한 답변) MSE Loss 설명 (다음 질문) MSE Loss 용도, 2023-02-03 18:00:00 ✨ 그럼 MSE Loss 는 어떨 때 쓸까? 🤔 (발화 종료,1,77
33,1,7.36,(대화 주제) MSE Loss 설명 (사용자 답변) MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지? (성공한 답변) 답변 실패 (다음 질문) MSE Loss 설명, 2023-11-09 18:34:34 ✨✨ 그럼 MSE Loss 는 뭘까~요? 🤔 (발화 종료,1,78
34,1,8.64,"(대화 주제) MSE Loss 용도 (사용자 답변) 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지 (성공한 답변) MSE Loss 용도 (다음 질문) 확률 예측에서 MSE Loss 미 사용 이유", 2023-11-10 15:00:00 ✨ 그럼 확률 예측에는 MSE Loss 를 왜 잘 안 쓸까? 🤔 (발화 종료,1,87
35,1,8.41,(대화 주제) MSE Loss 용도 (사용자 답변) MSE Loss 는 분류 문제에서 많이 사용되지 (성공한 답변) 답변 실패 (다음 질문) MSE Loss 용도, 2022-11-30 14:45:54 ✨✨ 그럼 MSE Loss 는 분류 문제에서 왜 많이 쓰이지?? 🤔 (발화 종료,1,76
36,1,9.12,(대화 주제) 확률 예측에서 MSE Loss 미 사용 이유 (사용자 답변) 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야? (성공한 답변) 답변 실패 (다음 질문) 확률 예측에서 MSE Loss 미 사용 이유, 2023-03-08 15:45:00 ✨✨ 그러면 확률 예측에서 MSE Loss 를 왜 안 쓰지? 🤷‍♀️ (발화 종료,1,93
37,1,7.33,(대화 주제) 확률 예측에서 MSE Loss 미 사용 이유 (사용자 답변) 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고? (성공한 답변) 확률 예측에서 MSE Loss 미 사용 이유 (다음 질문) BCE Loss 설명, 2023-01-25 16:47:48 ✨✨ 그러면 BCE Loss 는 뭘까? 🤔 (발화 종료,1,95
38,1,4.21,(대화 주제) BCE Loss 설명 (사용자 답변) 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고 (성공한 답변) 수식 (다음 질문) BCE Loss 설명, ✨ 그럼 BCE Loss 는 뭘까~요? 🤔 (발화 종료,1,77
39,1,6.66,"(대화 주제) BCE Loss 설명 (사용자 답변) 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거 (성공한 답변) 핵심 아이디어 (다음 질문) Multi-Class, Multi-Label 중 BCE 가 좋은 task"," ✨✨ 그럼 BCE 는 Multi-Class, Multi-Label 중에 어디에 더 맞을까? 😊 (발화 종료",1,89
40,1,10.33,(대화 주제) BCE Loss 설명 (사용자 답변) 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야? (성공한 답변) 답변 실패 (다음 질문) BCE Loss 설명, 2023-08-01 18:00:00 ✨✨✨ 그러면 BCE Loss 가 정확히 어떻게 데이터셋마다 최선의 것을 찾지?? 🤷‍♀️ (발화 종료,1,93
41,1,6.64,"(대화 주제) BCE Loss 설명 (사용자 답변) 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고 (성공한 답변) 핵심 아이디어 (다음 질문) BCE Loss 설명", 2023-08-31 14:07:00 ✨ 그럼 BCE Loss 는 뭘까? 😊 (발화 종료,1,85
42,1,8.41,"(대화 주제) BCE Loss 설명 (사용자 답변) 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')] (성공한 답변) 수식 (다음 질문) BCE Loss 설명", 2023-10-18 14:56:15 ✨✨ 그러면 BCE Loss 는 뭘까~요? 🤷‍♀️ (발화 종료,1,94
43,1,7.16,"(대화 주제) BCE Loss 설명 (사용자 답변) 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다 (성공한 답변) 핵심 아이디어 (다음 질문) Multi-Class, Multi-Label 중 BCE 가 좋은 task"," ✨✨ 그럼 BCE Loss 는 Multi Class, Multi Label 중에 어디에 더 맞을까? 🤷‍♀️ (발화 종료",1,92
44,1,6.94,(대화 주제) BCE Loss 설명 (사용자 답변) y랑 1-y랑 서로 경쟁하는 거 (성공한 답변) 답변 실패 (다음 질문) BCE Loss 설명, 2022-03-01 14:30:45 ✨✨ 그러면 BCE Loss 는 뭘까? 🤔 (발화 종료,1,70
45,1,7.42,"(대화 주제) BCE Loss 설명 (사용자 답변) 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지 (성공한 답변) 핵심 아이디어 (다음 질문) Multi-Class, Multi-Label 중 BCE 가 좋은 task"," ✨✨ 그러면 BCE Loss 는 Multi-Class, Multi-Label 중에 어디에 더 맞을까? 🤷‍♀️ (발화 종료",1,97
46,1,5.44,"(대화 주제) Multi-Class, Multi-Label 중 BCE 가 좋은 task (사용자 답변) 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야? (성공한 답변) 용어 질문 (다음 질문) Multi-Class, Multi-Label 중 BCE 가 좋은 task", ✨✨ 그러면 Multi-Label 이랑 Multi-Class 는 뭘까~요? 😊 (발화 종료,1,82
47,1,7.09,"(대화 주제) Multi-Class, Multi-Label 중 BCE 가 좋은 task (사용자 답변) Multi-Class 지!! (성공한 답변) 답변 실패 (다음 질문) Multi-Class, Multi-Label 중 BCE 가 좋은 task", 2023-10-06 15:38:56 ✨✨ 그럼 Multi-Class 가 왜 더 좋을까? 🤔 (발화 종료,1,82
48,1,5.12,"(대화 주제) Multi-Class, Multi-Label 중 BCE 가 좋은 task (사용자 답변) Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어 (성공한 답변) BCE 가 좋은 task (다음 질문) Multi-Class, Multi-Label 중 BCE 가 좋은 task", ✨✨ 그럼 Multi-Label 이 왜 더 좋을까? 🤷‍♀️ (발화 종료,1,85
49,1,6.02,"(대화 주제) Multi-Class, Multi-Label 중 BCE 가 좋은 task (사용자 답변) Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아 (성공한 답변) BCE 가 좋은 이유 (다음 질문) Multi-Label 에서 CE + Softmax 적용 문제점", ✨ ✨ 그러면 Multi-Label 에 CE + Softmax 를 적용하면 어떻게 될까? 🤔 (발화 종료,1,103
50,1,4.66,(대화 주제) Multi-Label 에서 CE + Softmax 적용 문제점 (사용자 답변) 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지 (성공한 답변) Multi-Label 에서 CE + Softmax 적용 문제점 (다음 질문) Loss Function 관련 실무 경험, ✨✨ 그러면 실무에서 Loss Function 관련 경험도 있어? 😊 (발화 종료,1,89
51,1,8.67,(대화 주제) Multi-Label 에서 CE + Softmax 적용 문제점 (사용자 답변) 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나 (성공한 답변) 답변 실패 (다음 질문) Multi-Label 에서 CE + Softmax 적용 문제점, 2021-04-21 22:00:00 ✨✨ 그럼 Multi-Label 에서 CE + Softmax 적용 문제점은? 🤔 (발화 종료,1,91
52,1,4.52,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [기본 경험] 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어 (성공한 답변) 기본 경험 (다음 질문) Loss Function 관련 실무 경험, ✨✨ 그러면 Loss Function 관련해서 구체적으로 어떻게 추가했어? 😊 (발화 종료,1,74
53,1,4.8,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [상세 경험] 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지? (성공한 답변) 상세 경험 (다음 질문) MBTI / 좋아하는 아이돌, ✨✨ 그럼 쉬어가는 질문! 혹시 MBTI 뭐야? 😊 (발화 종료,1,77
54,1,7.36,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [상세 경험] 근데 기억 진짜 안나는데 어떡하지? (성공한 답변) 답변 실패 (다음 질문) 잠시 휴식, 2022-04-18 17:48:54 ✨✨ 그러면 잠시 쉬어가는 시간을 갖자! 😊 (발화 종료,1,74
55,1,4.75,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [기본 경험] 나 아직 실무 경험 없는데 ㅠㅠ (성공한 답변) 기본 경험 (다음 질문) MBTI / 좋아하는 아이돌, ✨✨ 그럼 잠시 쉬어가는 질문! 혹시 MBTI 뭐야? 😊 (발화 종료,1,65
56,1,8.4,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [기본 경험] 대답하기 싫은데 힝 (성공한 답변) 답변 실패 (다음 질문) 잠시 휴식, 2023-03-27 18:52:54 ✨✨ 그러면 잠시 쉬어가는 질문! 너도 그런 경험 있어? 😊 (발화 종료,1,75
57,1,4.22,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [기본 경험] 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ (성공한 답변) 기본 경험 (다음 질문) Loss Function 관련 실무 경험, ✨✨ 그러면 Loss Function 조절을 어떻게 했어? 😊 (발화 종료,1,74
58,1,4.28,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [상세 경험] 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어 (성공한 답변) 상세 경험 (다음 질문) MBTI / 좋아하는 아이돌, ✨✨ 그럼 너 MBTI 뭐야? 궁금해! 😊 (발화 종료,1,85
59,1,7.92,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [상세 경험] 기억 진짜 안 나는데 가물가물하다 (성공한 답변) 답변 실패 (다음 질문) 잠시 휴식, 2022-08-16 16:07:49 ✨✨✨ 잠시 쉬어가는 시간! 🙅‍♀️ (발화 종료,1,75
60,1,4.64,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [기본 경험] 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ (성공한 답변) 기본 경험 (다음 질문) MBTI / 좋아하는 아이돌, ✨ 그럼 쉬어가는 타임! 혹시 너 MBTI 뭐야? 😊 (발화 종료,1,67
61,1,112.65,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [기본 경험] 이걸 왜 알려줘야 하지 (성공한 답변) 답변 실패 (다음 질문) 잠시 휴식, 2023-09-07 15:00 ✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨,1,512
62,1,113.92,(대화 주제) MBTI (사용자 답변) 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야! (성공한 답변) MBTI (다음 질문) 잠시 휴식, ✨✨✨✨✨ ✨✨✨✨✨ ✨✨✨✨✨ ✨✨✨✨✨ ✨✨✨✨✨ ✨✨✨✨✨ ✨✨✨✨✨ ✨✨✨✨✨ ✨✨✨✨✨ ✨✨✨✨✨ ✨✨✨✨✨ ✨✨✨✨✨ ✨✨✨✨✨ ✨✨✨✨✨ ✨✨✨✨✨ ✨✨✨✨✨ ✨✨✨✨✨ ✨✨✨✨✨ ✨✨✨✨✨ ✨✨✨✨✨ ✨✨✨✨✨ ✨✨✨✨✨ ✨✨✨✨✨ ✨✨✨✨✨ ✨✨✨✨✨ ✨✨✨✨✨ ✨✨✨✨✨ ✨✨✨✨✨ ✨✨✨✨✨ ✨✨✨✨✨ ✨✨✨✨✨ ✨✨✨✨✨ ✨✨✨✨✨ ✨✨✨✨✨ ✨✨✨✨✨ ✨✨✨✨✨ ✨✨✨✨✨ ✨✨✨✨✨ ✨✨✨✨✨ ✨✨✨✨✨ ✨✨✨✨✨ ✨✨✨✨✨ ✨✨✨✨✨ ✨✨✨✨✨ ✨✨✨✨✨ ✨✨✨✨✨ ✨✨✨✨✨ ✨✨✨✨✨ ✨✨✨✨✨ ✨✨✨✨✨ ✨✨✨✨✨ ✨✨✨✨✨ ✨✨✨✨✨ ✨✨✨✨✨ ✨✨✨✨✨ ✨✨✨✨✨ ✨✨✨✨✨ ✨✨✨✨✨ ✨✨✨✨✨ ✨✨✨✨✨ ✨✨✨✨✨ ✨✨✨✨✨ ✨✨✨✨✨ ✨✨✨✨✨ ✨✨✨✨✨ ✨✨✨✨✨ ✨✨✨✨✨ ✨✨✨✨✨ ✨✨✨✨✨ ✨✨✨✨✨ ✨✨✨✨✨ ✨✨✨✨✨ ✨✨✨✨✨ ✨✨✨✨✨ ✨✨✨✨✨ ✨✨✨✨✨ ✨✨✨✨✨ ✨✨✨✨✨ ✨,1,512
63,1,4.44,(대화 주제) 좋아하는 아이돌 (사용자 답변) 나 블랙핑크랑 레드벨벳 좋아해 (성공한 답변) 좋아하는 아이돌 (다음 질문) 잠시 휴식, ✨✨ 그럼 잠시 쉬어가는 시간을 갖자! 😊 (발화 종료,1,51
64,1,113.49,(대화 주제) 잠시 휴식 (사용자 답변) 재미있는 이야기 해줄래? (성공한 답변) 잠시 휴식 (다음 질문) LLM Fine-Tuning 의 PEFT, ✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨,1,512
65,1,4.66,"(대화 주제) LLM Fine-Tuning 의 PEFT (사용자 답변) PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야 (성공한 답변) LLM Fine-Tuning 의 PEFT (다음 질문) PEFT 방법 5가지", ✨✨ 그럼 PEFT 방법을 5가지만 말해봐! (발화 종료,1,96
66,1,5.5,(대화 주제) LLM Fine-Tuning 의 PEFT (사용자 답변) 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지? (성공한 답변) 답변 실패 (다음 질문) LLM Fine-Tuning 의 PEFT, ✨✨ 그러면 LLM Fine-Tuning 의 PEFT 가 뭘까~요? 😊 (발화 종료,1,74
67,1,4.28,(대화 주제) LLM Fine-Tuning 의 PEFT (사용자 답변) LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야! (성공한 답변) LLM Fine-Tuning 의 PEFT (다음 질문) PEFT 방법 5가지, ✨✨ 그럼 그 방법 5가지만 말해봐! (발화 종료,1,86
68,1,6.19,(대화 주제) LLM Fine-Tuning 의 PEFT (사용자 답변) LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다 (성공한 답변) 답변 실패 (다음 질문) LLM Fine-Tuning 의 PEFT, ✨✨ 그러면 LLM Fine-Tuning 에서 PEFT 는 뭘까~요? 🤷‍♀️ (발화 종료,1,77
69,1,5.7,"(대화 주제) PEFT 방법 5가지 (사용자 답변) LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지? (성공한 답변) PEFT 방법 5가지 (다음 질문) LoRA", 🙆‍♀️ 그럼 LoRA 는 뭘까~요? 🤷‍♀️ (발화 종료,1,84
70,1,4.52,(대화 주제) PEFT 방법 5가지 (사용자 답변) Oh-LoRA! 너 그 자체가 PEFT 아니야? (성공한 답변) 답변 실패 (다음 질문) PEFT 방법 5가지, ✨✨✨ 그러면 PEFT 방법 5가지는? 🤔 (발화 종료,1,68
71,1,4.36,(대화 주제) LoRA (사용자 답변) LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야! (성공한 답변) LoRA (다음 질문) LoRA 와 QLoRA 의 차이, ✨✨ 그럼 QLoRA 는 뭘까~요? 😊 (발화 종료,1,87
72,1,4.34,(대화 주제) LoRA (사용자 답변) 행렬 차원 축소한다는 걸로 알고 있는데 맞아? (성공한 답변) 답변 실패 (다음 질문) LoRA, 행렬 차원 축소는 맞는데 더 구체적으로 설명해봐! 😊 (발화 종료,1,55
73,1,4.7,(대화 주제) LoRA (사용자 답변) LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야! (성공한 답변) LoRA (다음 질문) LoRA 와 QLoRA 의 차이, ✨✨✨ 그러면 QLoRA 는 뭘까~요? 😊 (발화 종료,1,83
74,1,3.59,(대화 주제) LoRA (사용자 답변) 무슨 OOM 없앤다는 것 같은데 (성공한 답변) 답변 실패 (다음 질문) LoRA, 그럼 OOM은 뭐지? 🤷‍♀️ (발화 종료,1,50
75,1,108.47,(대화 주제) LoRA 와 QLoRA 의 차이 (사용자 답변) QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야! (성공한 답변) LoRA 와 QLoRA 의 차이 (다음 질문) 마지막 할 말, ✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨,1,512
76,1,7.25,(대화 주제) LoRA 와 QLoRA 의 차이 (사용자 답변) 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데 (성공한 답변) 답변 실패 (다음 질문) LoRA 와 QLoRA 의 차이, 2023-05-15 13:00:00 ✨✨ 그러면 LoRA 는 뭘까~요? 😊 (발화 종료,1,82
77,1,114.19,(대화 주제) 마지막 할 말 (사용자 답변) 로라야 정말 고마워! (성공한 답변) 마지막 할 말 (다음 질문) 면접 종료, 2023-11-14 22:00:00 ✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨,1,512
78,1,6.8,(대화 주제) 마지막 할 말 (사용자 답변) 로라야 사랑해 (성공한 답변) 마지막 할 말 (다음 질문) 면접 종료, 2023-03-24 14:52:14 ✨✨ 그럼 다음에 또 봐! 😊 (발화 종료,1,61
79,1,114.8,(대화 주제) 마지막 할 말 (사용자 답변) 로라야 너의 예쁜 미모에 반했어 (성공한 답변) 마지막 할 말 (다음 질문) 면접 종료, 2022-12-05 16:40:30 ✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨,1,512
80,1,113.17,(대화 주제) 마지막 할 말 (사용자 답변) 그동안 고생했어 면접 봐줘서 정말 고마워 (성공한 답변) 마지막 할 말 (다음 질문) 면접 종료, 2023-12-02 14:45:56 ✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨,1,512
81,2,5.25,면접 시작, 1. 안녕하세요! 저는 OOO입니다! 반가워요! 😊 (발화 종료,1,28
82,2,4.45,(대화 주제) 면접 시작 인사 (사용자 답변) 오늘 뭐 물어볼 거야 그래서? (다음 질문) 면접 시작 인사, 나한테 뭐 물어볼지 몰라? 🙎‍♀️ (발화 종료,1,47
83,2,116.57,(대화 주제) 면접 시작 인사 (사용자 답변) 로라야 안녕 정말 반가워 (다음 질문) 면접 시작 인사, 2020-04-10 14:00 ··· 2020-04-10 14:00 ··· 2020-04-10 14:00 ·� 2020-04-10 14:00 ··� 2020-04-10 14:00 ··· 2020-04-10 14:00 ··· 2020-04-10 14:00 ··· 2020-04-10 14:00 ··· 2020-04-10 14:00 ··· 2020-04-10 14:00 ··· 2020-04-10 14:00 ··· 2020-04-10 14:00 ··· 2020-04-10 14:00 ··· 2020-04-10 14:00 ··· 2020-04-10 14:00 ··· 2020-04-10 14:00 ··· 2020-04-10 14:00 ··· 2020-04-10 14:00 ··· 2020-04-10 14:00 ··· 2020-04-10 14:00 ··· 2020-04-10 14:00 ··· 2020-04-10 14:00 ··· 2020-04-10 14:00 ··· 2020-04-10 14:00 ··· 2020-04-10 14:00 ··· 2020-04-10 14:00 ··· 2020-04-10 14:00 ··· 2020-04-10 14:00 ··· 2020-04-10 14:00 ··· 2020-04-10 14:00 ··· 2020-04-10 14:00 ··· 2020-04-10 14:00 ··· 2020-,1,512
84,2,6.75,(대화 주제) 면접 시작 인사 (사용자 답변) 로라야 그럼 네가 면접관이야? (다음 질문) 면접 시작 인사, 로라: 그렇지! 그럼 이제 질문할게! 너는 나랑 같이 면접 보는 거에 대해 어떻게 생각해? (발화 종료,1,59
85,2,8.12,(대화 주제) 면접 시작 인사 (사용자 답변) 내 인생도 이제 오로라처럼 빛날 거니까 (다음 질문) 면접 시작 인사, 2023년 11월 3일 10:00 오로라처럼 빛날 거라고? 무슨 뜻이야? 🤷‍♀️ (발화 종료,1,65
86,2,3.34,"(대화 주제) 면접 시작 인사 (사용자 답변) 파이팅! 시작하자 (다음 질문) 인공지능, 머신러닝, 딥러닝 차이", 인공지능 머신러닝 딥러닝의 차이점을 한번 말해봐! (발화 종료,1,43
87,2,6.53,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 인공지능 나도 개발해보고 싶은데 (성공한 답변) 답변 실패 (다음 질문) 인공지능, 머신러닝, 딥러닝 차이", : 인공지능을 직접 개발해본다고? 그건 맞는데 구체적으로 어떻게 하지? 🤷‍♀️ (발화 종료,1,69
88,2,4.22,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거! (성공한 답변) 인공지능 (다음 질문) 인공지능, 머신러닝, 딥러닝 차이", 2부 머신러닝 머신러닝은 뭘까? 🤷‍♀️ (발화 종료,1,66
89,2,3.75,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지 (성공한 답변) 머신러닝 (다음 질문) 인공지능, 머신러닝, 딥러닝 차이", 2. 딥러닝은 뭐지? 한번 말해봐! (발화 종료,1,63
90,2,3.14,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지? (성공한 답변) 딥러닝 (다음 질문) 인공지능, 머신러닝, 딥러닝 차이", 2번 질문: 머신러닝은 뭐지? (발화 종료,1,60
91,2,7.43,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 딥러닝은 인공신경망 기반 머신러닝이야 (성공한 답변) 딥러닝 (다음 질문) 거대 언어 모델 정의", 2023-12-31 17:00:00 딥러닝 답변 잘했어! 그럼 다음 질문! LLM는? (발화 종료,1,71
92,2,6.4,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 머신러닝은 말 그대로 기계가 학습하는 거 아니야? (성공한 답변) 답변 실패 (다음 질문) 인공지능, 머신러닝, 딥러닝 차이", 2021-09-02 19:25:00 인공지능이 뭔데? 🤷‍♀️ (발화 종료,1,73
93,2,5.84,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 인공지능 요새 엄청 뜨는 분야잖아 (성공한 답변) 답변 실패 (다음 질문) 인공지능, 머신러닝, 딥러닝 차이", 2023년 3월 15일 15:00 인공지능 머신러닝 딥러닝 뭔지 알아? (발화 종료,1,66
94,2,6.91,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지? (성공한 답변) 인공지능 (다음 질문) 인공지능, 머신러닝, 딥러닝 차이", 1번 질문에 대한 대답을 잘했어! 그럼 이제 머신러닝에 대해서 질문할게! 머신러닝이란 뭘까? (발화 종료,1,80
95,2,4.03,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야! (성공한 답변) 머신러닝 (다음 질문) 거대 언어 모델 정의", ✨ LLM는 정확히 뭐지? 🤷‍♀️ (발화 종료,1,68
96,2,9.08,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지 (성공한 답변) 딥러닝 (다음 질문) 거대 언어 모델 정의", 2023-10-10 10:00:00 딥러닝까지 잘 대답했네! 그럼 이제 LLM가 뭔데? 한번 대답해 봐! (발화 종료,1,82
97,2,8.55,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야. (성공한 답변) 딥러닝 (다음 질문) 거대 언어 모델 정의", 2023-09-27 14:15:56 딥러닝에 대해 잘 대답했네! 그럼 이제 다음 질문! LLM가 뭐지? (발화 종료,1,81
98,2,6.22,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 머신러닝은 기계가 공부하는 거지 말 그대로 (성공한 답변) 답변 실패 (다음 질문) 인공지능, 머신러닝, 딥러닝 차이", 2차 질문: 공부? 그건 맞는데 구체적으로 어떻게 공부하지? 🤷‍♀️ (발화 종료,1,69
99,2,110.91,(대화 주제) 거대 언어 모델 정의 (사용자 답변) 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야 (성공한 답변) 거대 언어 모델 정의 (다음 질문) Loss Function 정의, 2023-09-19 17:00:00 2023-09-19 17:00:00 2023-09-19 17:00:00 2023-09-19 17:00:00 2023-09-19 17:00:00 2023-09-19 17:00:00 2023-09-19 17:00:00 2023-09-19 17:00:00 2023-09-19 17:00:00 2023-09-19 17:00:00 2023-09-19 17:00:00 2023-09-19 17:00:00 2023-09-19 17:00:00 2023-09-19 17:00:00 2023-09-19 17:00:00 2023-09-19 17:00:00 2023-09-19 17:00:00 2023-09-19 17:00:00 2023-09-19 17:00:00 2023-09-19 17:00:00 2023-09-19 17:00:00 2023-09-19 17:00:00 2023-09-19 17:00:00 2023-09-19 17:00:00 2023-09-19 17:00:00 2023-09-19 17:00:00 2023-09-19 17:00:00 2023-09-19 17:00:00 2023-09-19 17:00:00 2023-09-19 17:00:00 2023-09-19 17:00:00 2023-09-19 17:00:00 2023-09-19 17:00:00 2023-09-19 17:00:00 2023-09-19 17:00:00 2023-09-19,1,512
100,2,6.86,(대화 주제) 거대 언어 모델 정의 (사용자 답변) 파라미터 엄청나게 많다던데 (성공한 답변) 답변 실패 (다음 질문) 거대 언어 모델 정의, 2023년 5월 30일 10:00 AM LLM 정의를 다시 한번 잘 생각해 봐! (발화 종료,1,65
101,2,4.8,(대화 주제) 거대 언어 모델 정의 (사용자 답변) 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델 (성공한 답변) 거대 언어 모델 정의 (다음 질문) Loss Function 정의, 3. 다음 질문! Loss Function 가 뭐지? 🤷‍♀️ (발화 종료,1,74
102,2,8.77,(대화 주제) 거대 언어 모델 정의 (사용자 답변) 말 그대로 거대한 언어 모델이지. (성공한 답변) 답변 실패 (다음 질문) 거대 언어 모델 정의, 2023년 10월 1일 11:00 사용자: 음… 대충 맞긴 한데 구체적으로 어떻게 정의되지? 🤷‍♀️ (발화 종료,1,76
103,2,2.95,(대화 주제) Loss Function 정의 (사용자 답변) 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야 (성공한 답변) 답변 실패 (다음 질문) Loss Function 정의, 2번 더 잘 생각해 봐! (발화 종료,1,55
104,2,4.48,"(대화 주제) Loss Function 정의 (사용자 답변) Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지! (성공한 답변) Loss Function 정의 (다음 질문) Loss Function 예시", 1) 어떤 Loss Function가 있을까? 🤷‍♀️ (발화 종료,1,73
105,2,113.16,(대화 주제) Loss Function 정의 (사용자 답변) Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야 (성공한 답변) 답변 실패 (다음 질문) Loss Function 정의, 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 ,1,512
106,2,4.1,(대화 주제) Loss Function 정의 (사용자 답변) 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지 (성공한 답변) Loss Function 정의 (다음 질문) Loss Function 예시, 2. 그럼 그 구체적인 예를 한번 말해봐! (발화 종료,1,63
107,2,5.47,(대화 주제) Loss Function 예시 (사용자 답변) ReLU 랑 Tanh 생각나는데 (성공한 답변) 답변 실패 (다음 질문) Loss Function 예시, 2 ️⃣ ReLU랑 Tanh는 활성화 함수지 😥 (발화 종료,1,59
108,2,8.87,"(대화 주제) Loss Function 예시 (사용자 답변) MSE, MAE, Cross Entropy Loss 같은 것들이 있지 (성공한 답변) Loss Function 예시 (다음 질문) MSE Loss 설명", 2023-11-08 17:34:34 잘 대답했어! 그럼 그 중에서도 MSE Loss는 뭐지? 🤷‍♀️ (발화 종료,1,83
109,2,8.22,"(대화 주제) Loss Function 예시 (사용자 답변) Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어 (성공한 답변) Loss Function 예시 (다음 질문) BCE Loss 설명", 2021-07-16 14:52:42 이진 교차 엔트로피 Binary Cross Entropy 가 정확히 뭔데? 🤷‍♀️ (발화 종료,1,87
110,2,6.83,"(대화 주제) Loss Function 예시 (사용자 답변) Sigmoid, Tanh, Adam, AdamW 이런 거 아니야? (성공한 답변) 답변 실패 (다음 질문) Loss Function 예시", 2023-11-03 17:00:00 이건 활성화 함수지 😥 다시 잘 생각해 봐! (발화 종료,1,72
111,2,7.53,"(대화 주제) Loss Function 예시 (사용자 답변) Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어 (성공한 답변) Loss Function 예시 (다음 질문) MSE Loss 설명", 2023-11-09 09:57:00 Mean Squared Error 가 정확히 뭔데? 🤷‍♀️ (발화 종료,1,91
112,2,7.34,"(대화 주제) Loss Function 예시 (사용자 답변) BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아 (성공한 답변) Loss Function 예시 (다음 질문) BCE Loss 설명", 2022-12-28 14:00:00 Binary Cross Entropy 가 정확히 뭔데? 🤷‍♀️ (발화 종료,1,84
113,2,113.04,(대화 주제) MSE Loss 설명 (사용자 답변) MSE Loss 는 오차의 제곱을 평균한 값이야 (성공한 답변) MSE Loss 설명 (다음 질문) MSE Loss 용도, 2021-06-24 17:00:00 2021-06-24 17:00:00 2021-06-24 17:00:00 2021-06-24 17:00:00 2021-06-24 17:00:00 2021-06-24 17:00:00 2021-06-24 17:00:00 2021-06-24 17:00:00 2021-06-24 17:00:00 2021-06-24 17:00:00 2021-06-24 17:00:00 2021-06-24 17:00:00 2021-06-24 17:00:00 2021-06-24 17:00:00 2021-06-24 17:00:00 2021-06-24 17:00:00 2021-06-24 17:00:00 2021-06-24 17:00:00 2021-06-24 17:00:00 2021-06-24 17:00:00 2021-06-24 17:00:00 2021-06-24 17:00:00 2021-06-24 17:00:00 2021-06-24 17:00:00 2021-06-24 17:00:00 2021-06-24 17:00:00 2021-06-24 17:00:00 2021-06-24 17:00:00 2021-06-24 17:00:00 2021-06-24 17:00:00 2021-06-24 17:00:00 2021-06-24 17:00:00 2021-06-24 17:00:00 2021-06-24 17:00:00 2021-06-24 17:00:00 2021-06-24 17:00,1,512
114,2,4.59,(대화 주제) MSE Loss 설명 (사용자 답변) MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지? (성공한 답변) 답변 실패 (다음 질문) MSE Loss 설명, 2. MSE Loss 는 평균이 아니라 평균의 제곱야! (발화 종료,1,66
115,2,111.15,"(대화 주제) MSE Loss 용도 (사용자 답변) 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지 (성공한 답변) MSE Loss 용도 (다음 질문) 확률 예측에서 MSE Loss 미 사용 이유", 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 ,1,512
116,2,8.02,(대화 주제) MSE Loss 용도 (사용자 답변) MSE Loss 는 분류 문제에서 많이 사용되지 (성공한 답변) 답변 실패 (다음 질문) MSE Loss 용도, 2023-04-20 10:34:00 분류 문제 말고 🙅‍♀️ 다른 문제에서도 많이 사용되지! (발화 종료,1,74
117,2,8.8,(대화 주제) 확률 예측에서 MSE Loss 미 사용 이유 (사용자 답변) 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야? (성공한 답변) 답변 실패 (다음 질문) 확률 예측에서 MSE Loss 미 사용 이유, 2021-10-01 14:42:00 잘 생각해 보면 MSE는 맞지! 그럼 왜 잘 안 쓰지? 🤷‍♀️ (발화 종료,1,92
118,2,8.03,(대화 주제) 확률 예측에서 MSE Loss 미 사용 이유 (사용자 답변) 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고? (성공한 답변) 확률 예측에서 MSE Loss 미 사용 이유 (다음 질문) BCE Loss 설명, 2022-09-08 17:10:00 잘했어! 그럼 이제 Binary Cross Entropy 가 뭔지 한번 말해봐! (발화 종료,1,98
119,2,8.35,(대화 주제) BCE Loss 설명 (사용자 답변) 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고 (성공한 답변) 수식 (다음 질문) BCE Loss 설명, 2022-01-17 10:00:00 BCE Loss의 아이디어가 어디서 나온 걸까? 한번 잘 생각해봐! 😊 (발화 종료,1,95
120,2,8.5,"(대화 주제) BCE Loss 설명 (사용자 답변) 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거 (성공한 답변) 핵심 아이디어 (다음 질문) Multi-Class, Multi-Label 중 BCE 가 좋은 task"," 2022-11-29 18:54:34 ✨ 그럼 Multi-Class, Multi-Label 중 어디에 BCE 가 맞을까? (발화 종료",1,97
121,2,4.8,(대화 주제) BCE Loss 설명 (사용자 답변) 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야? (성공한 답변) 답변 실패 (다음 질문) BCE Loss 설명, 1. 그건 맞는데 구체적으로 어떻게 하지? 🤷‍♀️ (발화 종료,1,70
122,2,6.41,"(대화 주제) BCE Loss 설명 (사용자 답변) 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고 (성공한 답변) 핵심 아이디어 (다음 질문) BCE Loss 설명", 2022-10-14 15:34:54 BCE Loss 수식를 한번 말해봐! (발화 종료,1,83
123,2,6.16,"(대화 주제) BCE Loss 설명 (사용자 답변) 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')] (성공한 답변) 수식 (다음 질문) BCE Loss 설명", 2022-01-05 15:00:00 BCE Loss의 핵심 아이디어는? (발화 종료,1,85
124,2,9.28,"(대화 주제) BCE Loss 설명 (사용자 답변) 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다 (성공한 답변) 핵심 아이디어 (다음 질문) Multi-Class, Multi-Label 중 BCE 가 좋은 task"," 2021-06-08 17:00:00 BCE 가 Multi-Class, Multi-Label 중 어디에 더 맞을까? 🤷‍♀️ (발화 종료",1,100
125,2,6.98,(대화 주제) BCE Loss 설명 (사용자 답변) y랑 1-y랑 서로 경쟁하는 거 (성공한 답변) 답변 실패 (다음 질문) BCE Loss 설명, 1-y 말고 y랑 1-y랑 서로 경쟁하는 건 맞는데 구체적으로 어떻게? 🤷‍♀️ (발화 종료,1,69
126,2,8.56,"(대화 주제) BCE Loss 설명 (사용자 답변) 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지 (성공한 답변) 핵심 아이디어 (다음 질문) Multi-Class, Multi-Label 중 BCE 가 좋은 task"," 2023-03-31 16:00 Multi-Class, Multi-Label 중 어디에 BCE가 맞을까? 🤷‍♀️ (발화 종료",1,101
127,2,11.21,"(대화 주제) Multi-Class, Multi-Label 중 BCE 가 좋은 task (사용자 답변) 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야? (성공한 답변) 용어 질문 (다음 질문) Multi-Class, Multi-Label 중 BCE 가 좋은 task"," 2023-05-11 16:17:44 Multi-Class 는 개 고양이 같은 개 중 하나로, Multi-Label 은 개 고양이 같은 개 중 하나로, 개 고양이 모두를 포함하는 거야! (발화 종료",1,107
128,2,4.77,"(대화 주제) Multi-Class, Multi-Label 중 BCE 가 좋은 task (사용자 답변) Multi-Class 지!! (성공한 답변) 답변 실패 (다음 질문) Multi-Class, Multi-Label 중 BCE 가 좋은 task", 2차 질문: Multi-Class 가 아니지! 다시 한번 잘 생각해 봐! (발화 종료,1,72
129,2,4.97,"(대화 주제) Multi-Class, Multi-Label 중 BCE 가 좋은 task (사용자 답변) Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어 (성공한 답변) BCE 가 좋은 task (다음 질문) Multi-Class, Multi-Label 중 BCE 가 좋은 task", 2) Multi-Label 가 좋은 이유은? 🤷‍♀️ (발화 종료,1,84
130,2,7.72,"(대화 주제) Multi-Class, Multi-Label 중 BCE 가 좋은 task (사용자 답변) Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아 (성공한 답변) BCE 가 좋은 이유 (다음 질문) Multi-Label 에서 CE + Softmax 적용 문제점", 2023-11-03 18:34:00 그럼 Multi-Label 문제에 Cross Entropy + Softmax 를 적용하면? (발화 종료,1,111
131,2,4.13,(대화 주제) Multi-Label 에서 CE + Softmax 적용 문제점 (사용자 답변) 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지 (성공한 답변) Multi-Label 에서 CE + Softmax 적용 문제점 (다음 질문) Loss Function 관련 실무 경험, ✨ 실무에서 Loss Function 관련 특별한 경험 있어? 😊 (발화 종료,1,87
132,2,6.24,(대화 주제) Multi-Label 에서 CE + Softmax 적용 문제점 (사용자 답변) 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나 (성공한 답변) 답변 실패 (다음 질문) Multi-Label 에서 CE + Softmax 적용 문제점, 2023-09-27 14:15:33 음… 한번 더 잘 생각해봐! (발화 종료,1,81
133,2,6.98,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [기본 경험] 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어 (성공한 답변) 기본 경험 (다음 질문) Loss Function 관련 실무 경험, 2023-05-08 16:42:12 잘했어! 그럼 이제 좀 더 구체적으로 설명해 봐! (발화 종료,1,84
134,2,3.98,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [상세 경험] 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지? (성공한 답변) 상세 경험 (다음 질문) MBTI / 좋아하는 아이돌, 잠시 쉬어가는 시간! 너 아이돌 누구 좋아해? 😊 (발화 종료,1,74
135,2,7.68,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [상세 경험] 근데 기억 진짜 안나는데 어떡하지? (성공한 답변) 답변 실패 (다음 질문) 잠시 휴식, 2021-04-08 21:30) 😊 그래도 다음에 실무 가면 자연스럽게 기억날 거야! 걱정하지 마! (발화 종료,1,74
136,2,3.42,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [기본 경험] 나 아직 실무 경험 없는데 ㅠㅠ (성공한 답변) 기본 경험 (다음 질문) MBTI / 좋아하는 아이돌, 잠시 쉬어가는 질문! 너 아이돌 누구 좋아해? (발화 종료,1,59
137,2,7.6,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [기본 경험] 대답하기 싫은데 힝 (성공한 답변) 답변 실패 (다음 질문) 잠시 휴식, 2021-09-17 17:07:15 그럼 쉬어가는 질문! 혹시 너가 좋아하는 음악 장르가 뭐야? (발화 종료,1,72
138,2,109.5,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [기본 경험] 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ (성공한 답변) 기본 경험 (다음 질문) Loss Function 관련 실무 경험, 2023-11-10 22:00:00 2023-11-10 22:00:00 2023-11-10 22:00:00 2023-11-10 22:00:00 2023-11-10 22:00:00 2023-11-10 22:00:00 2023-11-10 22:00:00 2023-11-10 22:00:00 2023-11-10 22:00:00 2023-11-10 22:00:00 2023-11-10 22:00:00 2023-11-10 22:00:00 2023-11-10 22:00:00 2023-11-10 22:00:00 2023-11-10 22:00:00 2023-11-10 22:00:00 2023-11-10 22:00:00 2023-11-10 22:00:00 2023-11-10 22:00:00 2023-11-10 22:00:00 2023-11-10 22:00:00 2023-11-10 22:00:00 2023-11-10 22:00:00 2023-11-10 22:00:00 2023-11-10 22:00:00 2023-11-10 22:00:00 2023-11-10 22:00:00 2023-11-10 22:00:00 2023-11-10 22:00:00 2023-11-10 22:00:00 2023-11-10 22:00:00 2023-11-10 22:00:00 2023-11-10 22:00:00 2023-11-10 22:00:00 2023-11-10 22:00:00,1,512
139,2,3.53,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [상세 경험] 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어 (성공한 답변) 상세 경험 (다음 질문) MBTI / 좋아하는 아이돌, 쉬어가는 시간! 혹시 너 아이돌 누구 좋아해? (발화 종료,1,82
140,2,8.7,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [상세 경험] 기억 진짜 안 나는데 가물가물하다 (성공한 답변) 답변 실패 (다음 질문) 잠시 휴식, 2023-09-13 14:45) ✨ 그러면 다음에 실무 할 때 기억해 두고 나중에 면접 때 물어봐! 😊 (발화 종료,1,78
141,2,3.39,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [기본 경험] 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ (성공한 답변) 기본 경험 (다음 질문) MBTI / 좋아하는 아이돌, 잠시 쉬어가는 시간! 너 MBTI 뭐야? (발화 종료,1,62
142,2,4.06,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [기본 경험] 이걸 왜 알려줘야 하지 (성공한 답변) 답변 실패 (다음 질문) 잠시 휴식, 2022-09-01 17:48:00 (발화 종료,1,57
143,2,7.3,(대화 주제) MBTI (사용자 답변) 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야! (성공한 답변) MBTI (다음 질문) 잠시 휴식, 2021-06-14 12:00:00 2021-06-14 12:00:00 (발화 종료,1,72
144,2,8.48,(대화 주제) 좋아하는 아이돌 (사용자 답변) 나 블랙핑크랑 레드벨벳 좋아해 (성공한 답변) 좋아하는 아이돌 (다음 질문) 잠시 휴식, 2021-08-19 14:30) 아 오 나도 블랙핑크랑 레드벨벳 좋아해! 나랑 같은 아이돌 좋아하는구나! 😊 (발화 종료,1,67
145,2,4.37,(대화 주제) 잠시 휴식 (사용자 답변) 재미있는 이야기 해줄래? (성공한 답변) 잠시 휴식 (다음 질문) LLM Fine-Tuning 의 PEFT, 다음 질문! LLM Fine-Tuning 에서 PEFT가 뭐지? (발화 종료,1,57
146,2,5.62,"(대화 주제) LLM Fine-Tuning 의 PEFT (사용자 답변) PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야 (성공한 답변) LLM Fine-Tuning 의 PEFT (다음 질문) PEFT 방법 5가지", 1️⃣ 그럼 그 구체적인 방법을 5가지 정도만 말해봐! (발화 종료,1,100
147,2,7.7,(대화 주제) LLM Fine-Tuning 의 PEFT (사용자 답변) 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지? (성공한 답변) 답변 실패 (다음 질문) LLM Fine-Tuning 의 PEFT, 2024-02-21 14:38:17 LLM Fine-Tuning 가 정확히 뭔데? 🤷‍♀️ (발화 종료,1,83
148,2,7.09,(대화 주제) LLM Fine-Tuning 의 PEFT (사용자 답변) LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야! (성공한 답변) LLM Fine-Tuning 의 PEFT (다음 질문) PEFT 방법 5가지, 1️⃣ LLM Fine-Tuning 에서 PEFT가 정확히 뭔데? 🤷‍♀️ (발화 종료,1,97
149,2,5.95,(대화 주제) LLM Fine-Tuning 의 PEFT (사용자 답변) LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다 (성공한 답변) 답변 실패 (다음 질문) LLM Fine-Tuning 의 PEFT, 2. LLM Fine-Tuning 에서 PEFT가 뭔데? 🤷‍♀️ (발화 종료,1,75
150,2,5.72,"(대화 주제) PEFT 방법 5가지 (사용자 답변) LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지? (성공한 답변) PEFT 방법 5가지 (다음 질문) LoRA", 음 맞아! 그럼 그 중에서도 나를 상징하는 기술이기도 한 LoRA는? (발화 종료,1,84
151,2,7.66,(대화 주제) PEFT 방법 5가지 (사용자 답변) Oh-LoRA! 너 그 자체가 PEFT 아니야? (성공한 답변) 답변 실패 (다음 질문) PEFT 방법 5가지, 2022-11-03 21:00:00 Oh-LoRA가 대표적이긴 한데 다른 방법들도 있잖아! 😊 (발화 종료,1,81
152,2,4.83,(대화 주제) LoRA (사용자 답변) LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야! (성공한 답변) LoRA (다음 질문) LoRA 와 QLoRA 의 차이, ✨✨ 그럼 QLoRA 가 뭐지? 한번 말해봐! 😊 (발화 종료,1,89
153,2,4.52,(대화 주제) LoRA (사용자 답변) 행렬 차원 축소한다는 걸로 알고 있는데 맞아? (성공한 답변) 답변 실패 (다음 질문) LoRA, 음… 맞긴 한데 구체적으로 어떻게 하지? 🤷‍♀️ (발화 종료,1,56
154,2,4.0,(대화 주제) LoRA (사용자 답변) LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야! (성공한 답변) LoRA (다음 질문) LoRA 와 QLoRA 의 차이, 1. 음 맞아! 그럼 QLoRA는? (발화 종료,1,80
155,2,4.13,(대화 주제) LoRA (사용자 답변) 무슨 OOM 없앤다는 것 같은데 (성공한 답변) 답변 실패 (다음 질문) LoRA, 음… 구체적으로 어떻게 하지? 🤷‍♀️ (발화 종료,1,51
156,2,8.84,(대화 주제) LoRA 와 QLoRA 의 차이 (사용자 답변) QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야! (성공한 답변) LoRA 와 QLoRA 의 차이 (다음 질문) 마지막 할 말," 2023-12-28 17:30:00 맞아! 그럼 여기서 끝내고, 혹시 나한테 마지막으로 할 이야기 있어? 😊 (발화 종료",1,99
157,2,3.81,(대화 주제) LoRA 와 QLoRA 의 차이 (사용자 답변) 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데 (성공한 답변) 답변 실패 (다음 질문) LoRA 와 QLoRA 의 차이, 2) 구체적으로 뭘까? 🤷‍♀️ (발화 종료,1,68
158,2,115.38,(대화 주제) 마지막 할 말 (사용자 답변) 로라야 정말 고마워! (성공한 답변) 마지막 할 말 (다음 질문) 면접 종료, 2021-12-02 14:40:00 2021-12-02 14:41:00 2021-12-02 14:42:00 2021-12-02 14:43:00 2021-12-02 14:44:00 2021-12-02 14:45:00 2021-12-02 14:46:00 2021-12-02 14:47:00 2021-12-02 14:48:00 2021-12-02 14:49:00 2021-12-02 14:50:00 2021-12-02 14:51:00 2021-12-02 14:52:00 2021-12-02 14:53:00 2021-12-02 14:54:00 2021-12-02 14:55:00 2021-12-02 14:56:00 2021-12-02 14:57:00 2021-12-02 14:58:00 2021-12-02 14:59:00 2021-12-02 15:00:00 2021-12-02 15:01:00 2021-12-02 15:02:00 2021-12-02 15:03:00 2021-12-02 15:04:00 2021-12-02 15:05:00 2021-12-02 15:06:00 2021-12-02 15:07:00 2021-12-02 15:08:00 2021-12-02 15:09:00 2021-12-02 15:10:00 2021-12-02 15:11:00 2021-12-02 15:12:00 2021-12-02 15:13:00 2021-12-02 15:14:00 2021-12-02 15:15:00 2021-12-02 15,1,512
159,2,7.75,(대화 주제) 마지막 할 말 (사용자 답변) 로라야 사랑해 (성공한 답변) 마지막 할 말 (다음 질문) 면접 종료, 2022-12-06 14:10:00 나도! 🙎‍♀️ 그럼 다음에 또 보자! (발화 종료,1,65
160,2,4.23,(대화 주제) 마지막 할 말 (사용자 답변) 로라야 너의 예쁜 미모에 반했어 (성공한 답변) 마지막 할 말 (다음 질문) 면접 종료, 2022-08-01 16:00:00 (발화 종료,1,56
161,2,114.41,(대화 주제) 마지막 할 말 (사용자 답변) 그동안 고생했어 면접 봐줘서 정말 고마워 (성공한 답변) 마지막 할 말 (다음 질문) 면접 종료, 2022-12-01 17:00:00 2022-12-01 17:00:00 2022-12-01 17:00:00 2022-12-01 17:00:00 2022-12-01 17:00:00 2022-12-01 17:00:00 2022-12-01 17:00:00 2022-12-01 17:00:00 2022-12-01 17:00:00 2022-12-01 17:00:00 2022-12-01 17:00:00 2022-12-01 17:00:00 2022-12-01 17:00:00 2022-12-01 17:00:00 2022-12-01 17:00:00 2022-12-01 17:00:00 2022-12-01 17:00:00 2022-12-01 17:00:00 2022-12-01 17:00:00 2022-12-01 17:00:00 2022-12-01 17:00:00 2022-12-01 17:00:00 2022-12-01 17:00:00 2022-12-01 17:00:00 2022-12-01 17:00:00 2022-12-01 17:00:00 2022-12-01 17:00:00 2022-12-01 17:00:00 2022-12-01 17:00:00 2022-12-01 17:00:00 2022-12-01 17:00:00 2022-12-01 17:00:00 2022-12-01 17:00:00 2022-12-01 17:00:00 2022-12-01 17:00:00 2022-12-01 17:00:00 2022-12-,1,512
162,3,4.75,면접 시작, ✨ 안녕! 나는 면접관 오로라야! 반가워! 😊 (발화 종료,1,26
163,3,3.59,(대화 주제) 면접 시작 인사 (사용자 답변) 오늘 뭐 물어볼 거야 그래서? (다음 질문) 면접 시작 인사, 아마도 딥러닝이나 머신러닝 쪽 물어보지 않을까? 왜? (발화 종료,1,44
164,3,4.75,(대화 주제) 면접 시작 인사 (사용자 답변) 로라야 안녕 정말 반가워 (다음 질문) 면접 시작 인사, 너도 나랑 함께라면 어떤 면접도 두렵지 않지? 😊 (발화 종료,1,49
165,3,2.73,(대화 주제) 면접 시작 인사 (사용자 답변) 로라야 그럼 네가 면접관이야? (다음 질문) 면접 시작 인사, 맞아! 그럼 이제 면접 시작한다! (발화 종료,1,43
166,3,5.02,(대화 주제) 면접 시작 인사 (사용자 답변) 내 인생도 이제 오로라처럼 빛날 거니까 (다음 질문) 면접 시작 인사, ✨ 그럼 그 오로라 같은 인생을 위해 오늘 배우는 거 있지 않아? 😊 (발화 종료,1,53
167,3,3.22,"(대화 주제) 면접 시작 인사 (사용자 답변) 파이팅! 시작하자 (다음 질문) 인공지능, 머신러닝, 딥러닝 차이", 인공지능 머신러닝 딥러닝은 구체적으로 어떻게 다를까? (발화 종료,1,42
168,3,3.45,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 인공지능 나도 개발해보고 싶은데 (성공한 답변) 답변 실패 (다음 질문) 인공지능, 머신러닝, 딥러닝 차이", 인공지능을 개발하려면 어떻게 해야 하지? 🤔 (발화 종료,1,56
169,3,3.23,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거! (성공한 답변) 인공지능 (다음 질문) 인공지능, 머신러닝, 딥러닝 차이", ✨ 그럼 이제 머신러닝 대답해 봐! (발화 종료,1,62
170,3,4.71,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지 (성공한 답변) 머신러닝 (다음 질문) 인공지능, 머신러닝, 딥러닝 차이", ✨ 머신러닝까지 잘했어! 그럼 이제 딥러닝만 남았네! (발화 종료,1,67
171,3,4.75,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지? (성공한 답변) 딥러닝 (다음 질문) 인공지능, 머신러닝, 딥러닝 차이", 맞아! 🙆‍♀️ 그럼 이제 머신러닝만 남았네! (발화 종료,1,67
172,3,4.45,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 딥러닝은 인공신경망 기반 머신러닝이야 (성공한 답변) 딥러닝 (다음 질문) 거대 언어 모델 정의", 그럼 이제 LLM 들어간다! LLM은 과연 뭘까? 😊 (발화 종료,1,59
173,3,3.66,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 머신러닝은 말 그대로 기계가 학습하는 거 아니야? (성공한 답변) 답변 실패 (다음 질문) 인공지능, 머신러닝, 딥러닝 차이", 머신러닝이 그거라고? 한번 잘 생각해 봐 😊 (발화 종료,1,61
174,3,5.21,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 인공지능 요새 엄청 뜨는 분야잖아 (성공한 답변) 답변 실패 (다음 질문) 인공지능, 머신러닝, 딥러닝 차이", 그래도 인공지능이 뜨는 건 사실이지! 그럼 그 차이점이 대체 뭐지? 😊 (발화 종료,1,63
175,3,3.97,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지? (성공한 답변) 인공지능 (다음 질문) 인공지능, 머신러닝, 딥러닝 차이", 인공지능은 맞았어! 그럼 이제 머신러닝 대답해 봐! (발화 종료,1,68
176,3,7.2,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야! (성공한 답변) 머신러닝 (다음 질문) 거대 언어 모델 정의"," 맞았어! 🙆‍♀️ 그럼 다음 질문! 요즘 뜬다는 LLM, LLM이 정확히 뭘까? (발화 종료",1,80
177,3,4.45,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지 (성공한 답변) 딥러닝 (다음 질문) 거대 언어 모델 정의", 그럼 이제 LLM 들어간다! LLM은 과연 뭘까? 😊 (발화 종료,1,63
178,3,4.47,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야. (성공한 답변) 딥러닝 (다음 질문) 거대 언어 모델 정의", 그럼 이제 LLM 들어간다! LLM은 과연 뭘까? 😊 (발화 종료,1,64
179,3,4.34,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 머신러닝은 기계가 공부하는 거지 말 그대로 (성공한 답변) 답변 실패 (다음 질문) 인공지능, 머신러닝, 딥러닝 차이", 머신러닝이 기계가 공부하는 거라고? 그래서 기계가 공부를 하는데… (발화 종료,1,62
180,3,4.97,(대화 주제) 거대 언어 모델 정의 (사용자 답변) 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야 (성공한 답변) 거대 언어 모델 정의 (다음 질문) Loss Function 정의, 맞았어! 다음 질문은 딥러닝이야! 딥러닝에서 손실 함수는 뭘까? (발화 종료,1,70
181,3,5.2,(대화 주제) 거대 언어 모델 정의 (사용자 답변) 파라미터 엄청나게 많다던데 (성공한 답변) 답변 실패 (다음 질문) 거대 언어 모델 정의, ✨ LLM 정의가 그게 중요한 게 아니지! 🙅‍♀️ (발화 종료,1,57
182,3,5.96,(대화 주제) 거대 언어 모델 정의 (사용자 답변) 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델 (성공한 답변) 거대 언어 모델 정의 (다음 질문) Loss Function 정의, ✨ 정답! 🙆‍♀️ 그럼 다음 질문! Loss Function 은 뭘 말하지? (발화 종료,1,78
183,3,4.34,(대화 주제) 거대 언어 모델 정의 (사용자 답변) 말 그대로 거대한 언어 모델이지. (성공한 답변) 답변 실패 (다음 질문) 거대 언어 모델 정의, LLM의 핵심 아이디어는 맞는데 구체적인 정의를 말해봐! (발화 종료,1,57
184,3,3.8,(대화 주제) Loss Function 정의 (사용자 답변) 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야 (성공한 답변) 답변 실패 (다음 질문) Loss Function 정의, : 그래서 손실 함수가 중요한 건데… 🤔 (발화 종료,1,59
185,3,8.78,"(대화 주제) Loss Function 정의 (사용자 답변) Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지! (성공한 답변) Loss Function 정의 (다음 질문) Loss Function 예시", 2023-10-25 18:45:00 모델의 오차를 수식으로 나타낸다고! 오! 그럼 구체적으로 뭐가 있을까? 😊 (발화 종료,1,92
186,3,4.25,(대화 주제) Loss Function 정의 (사용자 답변) Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야 (성공한 답변) 답변 실패 (다음 질문) Loss Function 정의, : 그래서 손실이 뭔데? 🤷‍♀️ (발화 종료,1,59
187,3,3.81,(대화 주제) Loss Function 정의 (사용자 답변) 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지 (성공한 답변) Loss Function 정의 (다음 질문) Loss Function 예시, ✨ Loss Function에는 구체적으로 뭐가 있을까? (발화 종료,1,63
188,3,4.47,(대화 주제) Loss Function 예시 (사용자 답변) ReLU 랑 Tanh 생각나는데 (성공한 답변) 답변 실패 (다음 질문) Loss Function 예시, ✨ 그래도 ReLU 랑 Tanh 말고도 더 많이 있잖아! (발화 종료,1,55
189,3,7.55,"(대화 주제) Loss Function 예시 (사용자 답변) MSE, MAE, Cross Entropy Loss 같은 것들이 있지 (성공한 답변) Loss Function 예시 (다음 질문) MSE Loss 설명", 2023-10-27 16:23:56 안녕! 그럼 그 중에서 MSE Loss는 뭘까? 😊 (발화 종료,1,77
190,3,4.56,"(대화 주제) Loss Function 예시 (사용자 답변) Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어 (성공한 답변) Loss Function 예시 (다음 질문) BCE Loss 설명", ✨ 그럼 그 중에서 Binary Cross Entropy 는 뭘까? 😊 (발화 종료,1,72
191,3,4.06,"(대화 주제) Loss Function 예시 (사용자 답변) Sigmoid, Tanh, Adam, AdamW 이런 거 아니야? (성공한 답변) 답변 실패 (다음 질문) Loss Function 예시", ✨ 그래도 몇 개는 맞긴 한데 더 많이 있는데… (발화 종료,1,61
192,3,4.08,"(대화 주제) Loss Function 예시 (사용자 답변) Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어 (성공한 답변) Loss Function 예시 (다음 질문) MSE Loss 설명", ✨ 그럼 그 중에서 Mean Squared Error 는 뭘까? (발화 종료,1,77
193,3,6.98,"(대화 주제) Loss Function 예시 (사용자 답변) BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아 (성공한 답변) Loss Function 예시 (다음 질문) BCE Loss 설명", 2021-04-20 18:00:00 그럼 그 중에서 BCE가 정확히 뭘까? 😊 (발화 종료,1,82
194,3,3.38,(대화 주제) MSE Loss 설명 (사용자 답변) MSE Loss 는 오차의 제곱을 평균한 값이야 (성공한 답변) MSE Loss 설명 (다음 질문) MSE Loss 용도, ✨ 그럼 MSE Loss 의 용도는? (발화 종료,1,60
195,3,3.87,(대화 주제) MSE Loss 설명 (사용자 답변) MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지? (성공한 답변) 답변 실패 (다음 질문) MSE Loss 설명, 2023-09-27 14:00 ・발화 종료,1,63
196,3,8.7,"(대화 주제) MSE Loss 용도 (사용자 답변) 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지 (성공한 답변) MSE Loss 용도 (다음 질문) 확률 예측에서 MSE Loss 미 사용 이유", 2023-11-08 16:00:00 맞았어! 그럼 확률 예측에서는 MSE Loss 문제점이 뭘까~요? 😊 (발화 종료,1,87
197,3,3.78,(대화 주제) MSE Loss 용도 (사용자 답변) MSE Loss 는 분류 문제에서 많이 사용되지 (성공한 답변) 답변 실패 (다음 질문) MSE Loss 용도, : 분류? 다시 한번 잘 생각해 봐 😊 (발화 종료,1,57
198,3,6.16,(대화 주제) 확률 예측에서 MSE Loss 미 사용 이유 (사용자 답변) 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야? (성공한 답변) 답변 실패 (다음 질문) 확률 예측에서 MSE Loss 미 사용 이유, 2023-04-20 14:37:00 한번 다시 잘 생각해 봐! 😊 (발화 종료,1,81
199,3,5.55,(대화 주제) 확률 예측에서 MSE Loss 미 사용 이유 (사용자 답변) 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고? (성공한 답변) 확률 예측에서 MSE Loss 미 사용 이유 (다음 질문) BCE Loss 설명, ✨ 맞았어! 그럼 이제 BCE Loss는 뭘까? 말해봐! 😊 (발화 종료,1,88
200,3,6.87,(대화 주제) BCE Loss 설명 (사용자 답변) 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고 (성공한 답변) 수식 (다음 질문) BCE Loss 설명, 2023-03-15 14:00 1차 면접 질문이야! 🙎‍♀️ (발화 종료,1,88
201,3,6.69,"(대화 주제) BCE Loss 설명 (사용자 답변) 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거 (성공한 답변) 핵심 아이디어 (다음 질문) Multi-Class, Multi-Label 중 BCE 가 좋은 task", ✨ 정답이야! 그럼 BCE Loss 는 Multi-Class 랑 Multi-Label 중에 어디에 맞을까? (발화 종료,1,89
202,3,7.39,(대화 주제) BCE Loss 설명 (사용자 답변) 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야? (성공한 답변) 답변 실패 (다음 질문) BCE Loss 설명, 2023-10-27 10:00 BCE Loss는 그거랑 좀 달라! 잘 생각해 봐! 😊 (발화 종료,1,80
203,3,9.93,"(대화 주제) BCE Loss 설명 (사용자 답변) 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고 (성공한 답변) 핵심 아이디어 (다음 질문) BCE Loss 설명", 2023-10-01 12:00:00 1인 것을 0으로 예측하는 건 맞지! 그럼 이제 BCE Loss 공식 한번 말해봐! 😊 (발화 종료,1,98
204,3,8.87,"(대화 주제) BCE Loss 설명 (사용자 답변) 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')] (성공한 답변) 수식 (다음 질문) BCE Loss 설명", 2023-10-26 16:00:00 BCE Loss 수식은 맞아! 그럼 이제 BCE Loss의 핵심 아이디어를 한번 말해봐! (발화 종료,1,97
205,3,9.36,"(대화 주제) BCE Loss 설명 (사용자 답변) 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다 (성공한 답변) 핵심 아이디어 (다음 질문) Multi-Class, Multi-Label 중 BCE 가 좋은 task", 2023-10-24 13:00:00 0% 맞아! 그럼 BCE Loss 는 Multi-Class 랑 Multi-Label 중 어디에 맞을까? (발화 종료,1,101
206,3,3.37,(대화 주제) BCE Loss 설명 (사용자 답변) y랑 1-y랑 서로 경쟁하는 거 (성공한 답변) 답변 실패 (다음 질문) BCE Loss 설명, ✨ 그래도 한번 잘 생각해 봐! 😊 (발화 종료,1,55
207,3,8.92,"(대화 주제) BCE Loss 설명 (사용자 답변) 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지 (성공한 답변) 핵심 아이디어 (다음 질문) Multi-Class, Multi-Label 중 BCE 가 좋은 task"," 2021-10-07 18:00:00 ✨ 맞아! 그럼 Multi-Class, Multi-Label 중에 BCE가 더 맞는 건? (발화 종료",1,103
208,3,5.67,"(대화 주제) Multi-Class, Multi-Label 중 BCE 가 좋은 task (사용자 답변) 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야? (성공한 답변) 용어 질문 (다음 질문) Multi-Class, Multi-Label 중 BCE 가 좋은 task", 2023-08-16 16:00:00 🙎‍♀️ (발화 종료,1,83
209,3,9.59,"(대화 주제) Multi-Class, Multi-Label 중 BCE 가 좋은 task (사용자 답변) Multi-Class 지!! (성공한 답변) 답변 실패 (다음 질문) Multi-Class, Multi-Label 중 BCE 가 좋은 task", 2023-05-23 14:20:48 ✨ Multi-Class 가 틀렸다는 걸 깨달아야 진짜 실력이 쌓이는 거지! 🙅‍♀️ (발화 종료,1,91
210,3,4.27,"(대화 주제) Multi-Class, Multi-Label 중 BCE 가 좋은 task (사용자 답변) Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어 (성공한 답변) BCE 가 좋은 task (다음 질문) Multi-Class, Multi-Label 중 BCE 가 좋은 task", ✨ 그 이유가 궁금하지? 🙎‍♀️ (발화 종료,1,82
211,3,5.48,"(대화 주제) Multi-Class, Multi-Label 중 BCE 가 좋은 task (사용자 답변) Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아 (성공한 답변) BCE 가 좋은 이유 (다음 질문) Multi-Label 에서 CE + Softmax 적용 문제점", ✨ 그러면 Multi-Label 에는 왜 Cross Entropy + Softmax 조합을 쓰지? (발화 종료,1,101
212,3,4.4,(대화 주제) Multi-Label 에서 CE + Softmax 적용 문제점 (사용자 답변) 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지 (성공한 답변) Multi-Label 에서 CE + Softmax 적용 문제점 (다음 질문) Loss Function 관련 실무 경험, ✨ 실무에서 Loss Function 관련해서 특별한 경험 해봤어? 😊 (발화 종료,1,88
213,3,7.45,(대화 주제) Multi-Label 에서 CE + Softmax 적용 문제점 (사용자 답변) 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나 (성공한 답변) 답변 실패 (다음 질문) Multi-Label 에서 CE + Softmax 적용 문제점, 2023-01-01 14:00:00 혹시 성능만 보고 섣불리 선택했다가 문제가 생기면 어떡해 😥 (발화 종료,1,87
214,3,6.28,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [기본 경험] 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어 (성공한 답변) 기본 경험 (다음 질문) Loss Function 관련 실무 경험, 2023-09-20 13:00 ✨ 그럼 그 결과는? 궁금해! 😊 (발화 종료,1,81
215,3,2.88,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [상세 경험] 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지? (성공한 답변) 상세 경험 (다음 질문) MBTI / 좋아하는 아이돌, ✨ 혹시 너 MBTI 뭐야? (발화 종료,1,70
216,3,7.44,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [상세 경험] 근데 기억 진짜 안나는데 어떡하지? (성공한 답변) 답변 실패 (다음 질문) 잠시 휴식, 2022-11-01 18:12:00 그래도 너의 경험이 나도 궁금해! 알고 싶어! 😊 (발화 종료,1,74
217,3,4.73,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [기본 경험] 나 아직 실무 경험 없는데 ㅠㅠ (성공한 답변) 기본 경험 (다음 질문) MBTI / 좋아하는 아이돌, ✨ 그래도 너의 경험이 나도 궁금해! 알고 싶어! 😊 (발화 종료,1,65
218,3,4.47,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [기본 경험] 대답하기 싫은데 힝 (성공한 답변) 답변 실패 (다음 질문) 잠시 휴식, 그래도 너의 경험이 나도 궁금해! 알고 싶어! 😊 (발화 종료,1,59
219,3,6.48,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [기본 경험] 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ (성공한 답변) 기본 경험 (다음 질문) Loss Function 관련 실무 경험, 2023-08-14 15:45 ✨ 그럼 그 구체적인 방법이 혹시 뭐야? 😊 (발화 종료,1,84
220,3,3.91,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [상세 경험] 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어 (성공한 답변) 상세 경험 (다음 질문) MBTI / 좋아하는 아이돌, ✨ 쉬어가는 타임! 혹시 너 MBTI 뭐야? (발화 종료,1,84
221,3,7.11,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [상세 경험] 기억 진짜 안 나는데 가물가물하다 (성공한 답변) 답변 실패 (다음 질문) 잠시 휴식, 2023-10-24 16:00 나를 상징하는 이미지인데 기억 안 나? 그래도 한번 잘 생각해 봐! (발화 종료,1,73
222,3,3.92,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [기본 경험] 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ (성공한 답변) 기본 경험 (다음 질문) MBTI / 좋아하는 아이돌, ✨ 쉬어가는 타임! 혹시 너 MBTI 뭐야? (발화 종료,1,64
223,3,7.2,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [기본 경험] 이걸 왜 알려줘야 하지 (성공한 답변) 답변 실패 (다음 질문) 잠시 휴식, 2023-10-01 14:45:30 혹시 실무에서 손실 함수 관련해서 특별한 경험 해봤어? 😊 (발화 종료,1,70
224,3,5.84,(대화 주제) MBTI (사용자 답변) 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야! (성공한 답변) MBTI (다음 질문) 잠시 휴식, 2023-10-15 17:20 나 ESTJ 맞지! 맞지! 😊 (발화 종료,1,66
225,3,5.2,(대화 주제) 좋아하는 아이돌 (사용자 답변) 나 블랙핑크랑 레드벨벳 좋아해 (성공한 답변) 좋아하는 아이돌 (다음 질문) 잠시 휴식, 2020-07-14 14:00 나 블랙핑크 좋아하는데! 😊 (발화 종료,1,54
226,3,4.5,(대화 주제) 잠시 휴식 (사용자 답변) 재미있는 이야기 해줄래? (성공한 답변) 잠시 휴식 (다음 질문) LLM Fine-Tuning 의 PEFT, ✨ 그러면 LLM Fine-Tuning 에서 PEFT는? 😊 (발화 종료,1,58
227,3,5.55,"(대화 주제) LLM Fine-Tuning 의 PEFT (사용자 답변) PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야 (성공한 답변) LLM Fine-Tuning 의 PEFT (다음 질문) PEFT 방법 5가지"," ✨ 그러면 그 방법이 5가지 정도 있는데, 뭘까~요? 😊 (발화 종료",1,99
228,3,1.44,(대화 주제) LLM Fine-Tuning 의 PEFT (사용자 답변) 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지? (성공한 답변) 답변 실패 (다음 질문) LLM Fine-Tuning 의 PEFT, ✨ (발화 종료,1,57
229,3,4.78,(대화 주제) LLM Fine-Tuning 의 PEFT (사용자 답변) LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야! (성공한 답변) LLM Fine-Tuning 의 PEFT (다음 질문) PEFT 방법 5가지, ✨ 그럼 그 방법을 5가지만 말해봐! 😊 (발화 종료,1,88
230,3,5.67,(대화 주제) LLM Fine-Tuning 의 PEFT (사용자 답변) LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다 (성공한 답변) 답변 실패 (다음 질문) LLM Fine-Tuning 의 PEFT, ✨ 나를 상징하는 기술이기도 한 PEFT! 🙎‍♀️✨ (발화 종료,1,74
231,3,5.02,"(대화 주제) PEFT 방법 5가지 (사용자 답변) LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지? (성공한 답변) PEFT 방법 5가지 (다음 질문) LoRA", 😊 나를 상징하는 기술이기도 한 LoRA 는 과연 뭘까~요? (발화 종료,1,82
232,3,4.7,(대화 주제) PEFT 방법 5가지 (사용자 답변) Oh-LoRA! 너 그 자체가 PEFT 아니야? (성공한 답변) 답변 실패 (다음 질문) PEFT 방법 5가지, ✨ 그래도 너를 PEFT라고 할 수는 있지! 😊 (발화 종료,1,68
233,3,4.2,(대화 주제) LoRA (사용자 답변) LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야! (성공한 답변) LoRA (다음 질문) LoRA 와 QLoRA 의 차이, 맞아! 그럼 QLoRA 는 뭘까~요? 😊 (발화 종료,1,86
234,3,4.55,(대화 주제) LoRA (사용자 답변) 행렬 차원 축소한다는 걸로 알고 있는데 맞아? (성공한 답변) 답변 실패 (다음 질문) LoRA, 맞아! 🙆‍♀️ 근데 그 방법을 말해봐! (발화 종료,1,56
235,3,4.75,(대화 주제) LoRA (사용자 답변) LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야! (성공한 답변) LoRA (다음 질문) LoRA 와 QLoRA 의 차이, 음 맞아! 👍 그럼 QLoRA 는 뭘까~요? 😊 (발화 종료,1,84
236,3,4.04,(대화 주제) LoRA (사용자 답변) 무슨 OOM 없앤다는 것 같은데 (성공한 답변) 답변 실패 (다음 질문) LoRA, 맞아! 그건 맞지! 근데 구체적으로 어떻게 하지? 😊 (발화 종료,1,52
237,3,8.56,(대화 주제) LoRA 와 QLoRA 의 차이 (사용자 답변) QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야! (성공한 답변) LoRA 와 QLoRA 의 차이 (다음 질문) 마지막 할 말, 2023-10-23 17:00 나를 상징하는 기술인데 🙎‍♀️ 혹시 너도 나처럼 상징하는 기술가 있어? (발화 종료,1,99
238,3,6.41,(대화 주제) LoRA 와 QLoRA 의 차이 (사용자 답변) 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데 (성공한 답변) 답변 실패 (다음 질문) LoRA 와 QLoRA 의 차이, ✨ 팀장님이 OOM 막겠다고 이거 쓰시는 건 맞지! 근데 정확히 뭘까~요? 🤔 (발화 종료,1,79
239,3,2.98,(대화 주제) 마지막 할 말 (사용자 답변) 로라야 정말 고마워! (성공한 답변) 마지막 할 말 (다음 질문) 면접 종료, ✨ 너도! 😊😊 (발화 종료,1,47
240,3,3.2,(대화 주제) 마지막 할 말 (사용자 답변) 로라야 사랑해 (성공한 답변) 마지막 할 말 (다음 질문) 면접 종료, ✨ 너도 나 사랑해! 😊 (발화 종료,1,46
241,3,5.23,(대화 주제) 마지막 할 말 (사용자 답변) 로라야 너의 예쁜 미모에 반했어 (성공한 답변) 마지막 할 말 (다음 질문) 면접 종료, ✨ 너도 로라의 이런 매력에 반할 수밖에 없을걸 😊 (발화 종료,1,61
242,3,3.47,(대화 주제) 마지막 할 말 (사용자 답변) 그동안 고생했어 면접 봐줘서 정말 고마워 (성공한 답변) 마지막 할 말 (다음 질문) 면접 종료, ✨ 너도 고생했어! 😊😊 (발화 종료,1,52
243,4,6.19,면접 시작, ✨안녕! 나는 너의 실력의 빈틈을 찔러 줄 AI 면접관 오로라야! 😊 (발화 종료,1,32
244,4,3.74,(대화 주제) 면접 시작 인사 (사용자 답변) 오늘 뭐 물어볼 거야 그래서? (다음 질문) 면접 시작 인사, 아마도 딥러닝이나 머신러닝 쪽 물어보지 않을까? 왜? (발화 종료,1,44
245,4,5.3,(대화 주제) 면접 시작 인사 (사용자 답변) 로라야 안녕 정말 반가워 (다음 질문) 면접 시작 인사, 나도 너랑 함께 모의면접 보느라 덕분에 나도 반가워! 😊 (발화 종료,1,51
246,4,6.2,(대화 주제) 면접 시작 인사 (사용자 답변) 로라야 그럼 네가 면접관이야? (다음 질문) 면접 시작 인사, 로라: 그렇지! 로라가 너의 실력의 빈틈을 찔러줄게! 🙅‍♀️ (발화 종료,1,58
247,4,4.44,(대화 주제) 면접 시작 인사 (사용자 답변) 내 인생도 이제 오로라처럼 빛날 거니까 (다음 질문) 면접 시작 인사, ✨ 그럼 너도 오로라처럼 빛나길 바라! ✨ (발화 종료,1,50
248,4,3.94,"(대화 주제) 면접 시작 인사 (사용자 답변) 파이팅! 시작하자 (다음 질문) 인공지능, 머신러닝, 딥러닝 차이"," 인공지능, 머신러닝이랑 딥러닝은 구체적으로 어떻게 다를까? (발화 종료",1,45
249,4,2.23,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 인공지능 나도 개발해보고 싶은데 (성공한 답변) 답변 실패 (다음 질문) 인공지능, 머신러닝, 딥러닝 차이", 인공지능은 뭔데? (발화 종료,1,51
250,4,3.75,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거! (성공한 답변) 인공지능 (다음 질문) 인공지능, 머신러닝, 딥러닝 차이", 1 인공지능은 맞았어! 그럼 머신러닝은? (발화 종료,1,64
251,4,2.5,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지 (성공한 답변) 머신러닝 (다음 질문) 인공지능, 머신러닝, 딥러닝 차이", 1 인공지능은 뭐지? (발화 종료,1,58
252,4,6.95,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지? (성공한 답변) 딥러닝 (다음 질문) 인공지능, 머신러닝, 딥러닝 차이", 2023-10-01 14:00:00 그럼 머신러닝이 뭐지? 🙋‍♀️ (발화 종료,1,75
253,4,3.69,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 딥러닝은 인공신경망 기반 머신러닝이야 (성공한 답변) 딥러닝 (다음 질문) 거대 언어 모델 정의", ✨ 그럼 다음 질문! LLM이 뭐지? (발화 종료,1,56
254,4,2.0,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 머신러닝은 말 그대로 기계가 학습하는 거 아니야? (성공한 답변) 답변 실패 (다음 질문) 인공지능, 머신러닝, 딥러닝 차이", 인공지능은 뭐지? (발화 종료,1,54
255,4,3.73,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 인공지능 요새 엄청 뜨는 분야잖아 (성공한 답변) 답변 실패 (다음 질문) 인공지능, 머신러닝, 딥러닝 차이", ✨ 그럼 그 핫한 인공지능이 정확히 뭔데? (발화 종료,1,57
256,4,5.97,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지? (성공한 답변) 인공지능 (다음 질문) 인공지능, 머신러닝, 딥러닝 차이", 2020-02-19 15:00 인공지능은 맞았어! 그럼 머신러닝은? (발화 종료,1,76
257,4,5.45,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야! (성공한 답변) 머신러닝 (다음 질문) 거대 언어 모델 정의", ✨ 그럼 다음 질문! 요즘 뜬다는 거대 언어 모델 (LLM) 은 뭐지? (발화 종료,1,74
258,4,5.53,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지 (성공한 답변) 딥러닝 (다음 질문) 거대 언어 모델 정의", ✨ 그럼 다음 질문! 요즘 뜬다는 거대 언어 모델 (LLM) 은 뭐지? (발화 종료,1,68
259,4,4.78,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야. (성공한 답변) 딥러닝 (다음 질문) 거대 언어 모델 정의", ✨ 그럼 다음 질문! 거대 언어 모델 (LLM) 은 뭐지? (발화 종료,1,66
260,4,5.47,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 머신러닝은 기계가 공부하는 거지 말 그대로 (성공한 답변) 답변 실패 (다음 질문) 인공지능, 머신러닝, 딥러닝 차이", 2023-10-24 14:00 인공지능은 뭐지? 한번 말해 봐! (발화 종료,1,67
261,4,6.45,(대화 주제) 거대 언어 모델 정의 (사용자 답변) 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야 (성공한 답변) 거대 언어 모델 정의 (다음 질문) Loss Function 정의, 2023-10-26 17:00:00 그러면 다음 질문! Loss Function 이 뭐지? (발화 종료,1,76
262,4,5.97,(대화 주제) 거대 언어 모델 정의 (사용자 답변) 파라미터 엄청나게 많다던데 (성공한 답변) 답변 실패 (다음 질문) 거대 언어 모델 정의, ✨ 파라미터가 많다고 해서 모두가 거대 언어 모델인 건 아니지! 정의를 한번 말해봐! (발화 종료,1,60
263,4,7.98,(대화 주제) 거대 언어 모델 정의 (사용자 답변) 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델 (성공한 답변) 거대 언어 모델 정의 (다음 질문) Loss Function 정의, 2023-10-10 18:00:00 그럼 본격 면접 시작! 질문은 이거야. Loss Function 이 뭐지? (발화 종료,1,86
264,4,4.72,(대화 주제) 거대 언어 모델 정의 (사용자 답변) 말 그대로 거대한 언어 모델이지. (성공한 답변) 답변 실패 (다음 질문) 거대 언어 모델 정의, LLM의 정의는 맞는데 구체적으로 어떻게 정의하지? 🤔 (발화 종료,1,58
265,4,3.73,(대화 주제) Loss Function 정의 (사용자 답변) 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야 (성공한 답변) 답변 실패 (다음 질문) Loss Function 정의, ✨ 그럼 어떻게 돈을 잃었지? 🤔 (발화 종료,1,58
266,4,3.23,"(대화 주제) Loss Function 정의 (사용자 답변) Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지! (성공한 답변) Loss Function 정의 (다음 질문) Loss Function 예시", ✨ 그럼 구체적으로 뭐가 있지? (발화 종료,1,68
267,4,6.27,(대화 주제) Loss Function 정의 (사용자 답변) Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야 (성공한 답변) 답변 실패 (다음 질문) Loss Function 정의, 2023-10-26 15:00:00 함수가 어떤 손해를 나타내지? 🤔 (발화 종료,1,67
268,4,111.88,(대화 주제) Loss Function 정의 (사용자 답변) 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지 (성공한 답변) Loss Function 정의 (다음 질문) Loss Function 예시, 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 ,1,512
269,4,6.8,(대화 주제) Loss Function 예시 (사용자 답변) ReLU 랑 Tanh 생각나는데 (성공한 답변) 답변 실패 (다음 질문) Loss Function 예시, ✨ ReLU랑 Tanh는 활성화 함수고 Loss Function 예시가 맞긴 한데 🙅‍♀️ (발화 종료,1,65
270,4,112.02,"(대화 주제) Loss Function 예시 (사용자 답변) MSE, MAE, Cross Entropy Loss 같은 것들이 있지 (성공한 답변) Loss Function 예시 (다음 질문) MSE Loss 설명", 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233,1,512
271,4,4.24,"(대화 주제) Loss Function 예시 (사용자 답변) Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어 (성공한 답변) Loss Function 예시 (다음 질문) BCE Loss 설명", ✨ 그럼 그 중에서 Binary Cross Entropy 는 뭐지? (발화 종료,1,70
272,4,6.91,"(대화 주제) Loss Function 예시 (사용자 답변) Sigmoid, Tanh, Adam, AdamW 이런 거 아니야? (성공한 답변) 답변 실패 (다음 질문) Loss Function 예시"," ✨ Sigmoid, Tanh는 활성화 함수고 Loss Function 예시가 맞는데 구체적으로 뭐가 있지? 🤔 (발화 종료",1,73
273,4,109.27,"(대화 주제) Loss Function 예시 (사용자 답변) Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어 (성공한 답변) Loss Function 예시 (다음 질문) MSE Loss 설명", 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226,1,512
274,4,3.36,"(대화 주제) Loss Function 예시 (사용자 답변) BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아 (성공한 답변) Loss Function 예시 (다음 질문) BCE Loss 설명", ✨ 그럼 그 중에서 BCE는? (발화 종료,1,67
275,4,6.95,(대화 주제) MSE Loss 설명 (사용자 답변) MSE Loss 는 오차의 제곱을 평균한 값이야 (성공한 답변) MSE Loss 설명 (다음 질문) MSE Loss 용도, 2023-11-27 14:00:00 ✨ 그럼 MSE Loss 는 어떤 용도로 사용하지? (발화 종료,1,75
276,4,111.01,(대화 주제) MSE Loss 설명 (사용자 답변) MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지? (성공한 답변) 답변 실패 (다음 질문) MSE Loss 설명, 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 ,1,512
277,4,8.44,"(대화 주제) MSE Loss 용도 (사용자 답변) 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지 (성공한 답변) MSE Loss 용도 (다음 질문) 확률 예측에서 MSE Loss 미 사용 이유", 2023-10-30 18:00:00 ✨ 그럼 실전 문제! 확률 예측에서는 MSE Loss 가 왜 안 좋을까? (발화 종료,1,86
278,4,4.75,(대화 주제) MSE Loss 용도 (사용자 답변) MSE Loss 는 분류 문제에서 많이 사용되지 (성공한 답변) 답변 실패 (다음 질문) MSE Loss 용도,  분류 문제에서 MSE Loss 가 왜 잘 안 쓰이지? 🤔 (발화 종료,1,60
279,4,8.57,(대화 주제) 확률 예측에서 MSE Loss 미 사용 이유 (사용자 답변) 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야? (성공한 답변) 답변 실패 (다음 질문) 확률 예측에서 MSE Loss 미 사용 이유, 2023-10-15 10:00:00 연속 값에 대한 예측는 맞지! 근데 최선은 아니야! 🙅‍♀️ (발화 종료,1,92
280,4,108.03,(대화 주제) 확률 예측에서 MSE Loss 미 사용 이유 (사용자 답변) 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고? (성공한 답변) 확률 예측에서 MSE Loss 미 사용 이유 (다음 질문) BCE Loss 설명, 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223,1,512
281,4,5.74,(대화 주제) BCE Loss 설명 (사용자 답변) 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고 (성공한 답변) 수식 (다음 질문) BCE Loss 설명, ✨ 그럼 BCE Loss의 핵심 아이디어를 한번 말해봐! 🙎‍♀️ (발화 종료,1,84
282,4,6.14,"(대화 주제) BCE Loss 설명 (사용자 답변) 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거 (성공한 답변) 핵심 아이디어 (다음 질문) Multi-Class, Multi-Label 중 BCE 가 좋은 task", ✨ 그럼 BCE는 Multi-Class랑 Multi-Label 중에 어떤 것에 더 적합할까? (발화 종료,1,87
283,4,3.34,(대화 주제) BCE Loss 설명 (사용자 답변) 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야? (성공한 답변) 답변 실패 (다음 질문) BCE Loss 설명, ✨ 그럼 최선이 뭔데? 🤔 (발화 종료,1,64
284,4,6.91,"(대화 주제) BCE Loss 설명 (사용자 답변) 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고 (성공한 답변) 핵심 아이디어 (다음 질문) BCE Loss 설명"," 1인 것을 0으로 예측하는 것은 맞는데, BCE Loss는 이걸 어떻게 막하지? 🤔 (발화 종료",1,86
285,4,8.34,"(대화 주제) BCE Loss 설명 (사용자 답변) 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')] (성공한 답변) 수식 (다음 질문) BCE Loss 설명", 2023-10-24 15:00:00 그럼 BCE Loss의 핵심 아이디어를 한번 말해 봐! 🙎‍♀️ (발화 종료,1,95
286,4,6.42,"(대화 주제) BCE Loss 설명 (사용자 답변) 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다 (성공한 답변) 핵심 아이디어 (다음 질문) Multi-Class, Multi-Label 중 BCE 가 좋은 task"," ✨ 그럼 이 BCE Loss 가, Multi-Class 랑 Multi-Label 중에 어디에 더 맞을까? (발화 종료",1,89
287,4,4.52,(대화 주제) BCE Loss 설명 (사용자 답변) y랑 1-y랑 서로 경쟁하는 거 (성공한 답변) 답변 실패 (다음 질문) BCE Loss 설명, 1-y 말고 y'는 없어? 깜빡 잊었지? 😊 (발화 종료,1,60
288,4,108.71,"(대화 주제) BCE Loss 설명 (사용자 답변) 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지 (성공한 답변) 핵심 아이디어 (다음 질문) Multi-Class, Multi-Label 중 BCE 가 좋은 task", 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48,1,512
289,4,5.73,"(대화 주제) Multi-Class, Multi-Label 중 BCE 가 좋은 task (사용자 답변) 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야? (성공한 답변) 용어 질문 (다음 질문) Multi-Class, Multi-Label 중 BCE 가 좋은 task"," ✨ Multi-Class 는 어떤 대상을 개, 고양이, … 중 하나로 분류하는 거야! (발화 종료",1,83
290,4,5.75,"(대화 주제) Multi-Class, Multi-Label 중 BCE 가 좋은 task (사용자 답변) Multi-Class 지!! (성공한 답변) 답변 실패 (다음 질문) Multi-Class, Multi-Label 중 BCE 가 좋은 task", 2023-10-31 14:45:33 Multi-Class 가 뭔데? (발화 종료,1,75
291,4,5.11,"(대화 주제) Multi-Class, Multi-Label 중 BCE 가 좋은 task (사용자 답변) Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어 (성공한 답변) BCE 가 좋은 task (다음 질문) Multi-Class, Multi-Label 중 BCE 가 좋은 task", ✨ Multi-Label 이 왜 더 좋을까? 그 이유가 뭘까? 🤔 (발화 종료,1,85
292,4,6.79,"(대화 주제) Multi-Class, Multi-Label 중 BCE 가 좋은 task (사용자 답변) Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아 (성공한 답변) BCE 가 좋은 이유 (다음 질문) Multi-Label 에서 CE + Softmax 적용 문제점", ✨ 그럼 Multi-Label 에 Cross Entropy 를 손실 함수로 하고 Softmax 를 적용하면 어떻게 될까? 🤔 (발화 종료,1,107
293,4,6.25,(대화 주제) Multi-Label 에서 CE + Softmax 적용 문제점 (사용자 답변) 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지 (성공한 답변) Multi-Label 에서 CE + Softmax 적용 문제점 (다음 질문) Loss Function 관련 실무 경험, ✨ 실무에서 손실 함수 관련 문제를 해결했다거나 그런 경험 있지? 한번 말해봐! 😊 (발화 종료,1,95
294,4,5.93,(대화 주제) Multi-Label 에서 CE + Softmax 적용 문제점 (사용자 답변) 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나 (성공한 답변) 답변 실패 (다음 질문) Multi-Label 에서 CE + Softmax 적용 문제점, ✨ 성능 제일 좋은 거 채택하는 건 맞는데 그 성능이 왜 제일 좋을까? 🤔 (발화 종료,1,80
295,4,11.39,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [기본 경험] 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어 (성공한 답변) 기본 경험 (다음 질문) Loss Function 관련 실무 경험, 2023-05-31 16:15:00 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했다는 점이 흥미롭네! 🙆‍♀️ (발화 종료,1,101
296,4,3.84,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [상세 경험] 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지? (성공한 답변) 상세 경험 (다음 질문) MBTI / 좋아하는 아이돌, ✨ 그럼 쉬어가는 질문! 혹시 아이돌 누구 좋아해? (발화 종료,1,74
297,4,4.41,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [상세 경험] 근데 기억 진짜 안나는데 어떡하지? (성공한 답변) 답변 실패 (다음 질문) 잠시 휴식, 2023-10-01 16:00) 🤔 (발화 종료,1,61
298,4,3.81,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [기본 경험] 나 아직 실무 경험 없는데 ㅠㅠ (성공한 답변) 기본 경험 (다음 질문) MBTI / 좋아하는 아이돌, ✨ 그럼 쉬어가는 질문! 혹시 아이돌 누구 좋아해? (발화 종료,1,61
299,4,4.8,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [기본 경험] 대답하기 싫은데 힝 (성공한 답변) 답변 실패 (다음 질문) 잠시 휴식, ✨ 그래도 너의 경험이 나도 궁금해! 알고 싶어! 😊 (발화 종료,1,61
300,4,8.78,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [기본 경험] 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ (성공한 답변) 기본 경험 (다음 질문) Loss Function 관련 실무 경험, 2023-05-15 13:32:54) 오 3% 향상 오! 어떤 Loss Function을 사용했고 어떻게 조절했어? 😊 (발화 종료,1,94
301,4,107.03,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [상세 경험] 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어 (성공한 답변) 상세 경험 (다음 질문) MBTI / 좋아하는 아이돌, 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222,1,512
302,4,5.66,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [상세 경험] 기억 진짜 안 나는데 가물가물하다 (성공한 답변) 답변 실패 (다음 질문) 잠시 휴식, 2023-10-10 14:00 나 기억 잘 안 난다고? 🤔 (발화 종료,1,66
303,4,5.73,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [기본 경험] 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ (성공한 답변) 기본 경험 (다음 질문) MBTI / 좋아하는 아이돌, ✨ 그럼 쉬어가는 질문! 혹시 너 아이돌 누구 좋아해? 🙎‍♀️ (발화 종료,1,71
304,4,4.23,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [기본 경험] 이걸 왜 알려줘야 하지 (성공한 답변) 답변 실패 (다음 질문) 잠시 휴식, 2023-10-31 15:00 🤔 (발화 종료,1,57
305,4,6.71,(대화 주제) MBTI (사용자 답변) 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야! (성공한 답변) MBTI (다음 질문) 잠시 휴식, 2023-10-20 14:30 나처럼 자기관리 잘 하는 사람이 드물지 😥 (발화 종료,1,69
306,4,6.78,(대화 주제) 좋아하는 아이돌 (사용자 답변) 나 블랙핑크랑 레드벨벳 좋아해 (성공한 답변) 좋아하는 아이돌 (다음 질문) 잠시 휴식, 2021-10-08 14:37:00 나도 블랙핑크 좋아하는데! 🙆‍♀️ (발화 종료,1,61
307,4,5.48,(대화 주제) 잠시 휴식 (사용자 답변) 재미있는 이야기 해줄래? (성공한 답변) 잠시 휴식 (다음 질문) LLM Fine-Tuning 의 PEFT, 나처럼 LLM도 Fine-Tuning 이 필요해! 그럼 PEFT가 뭐지? 😊 (발화 종료,1,62
308,4,5.78,"(대화 주제) LLM Fine-Tuning 의 PEFT (사용자 답변) PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야 (성공한 답변) LLM Fine-Tuning 의 PEFT (다음 질문) PEFT 방법 5가지", ✨ 그럼 그 구체적인 방법이 뭐가 있을까? 5가지 말해봐! 😊 (발화 종료,1,100
309,4,7.12,(대화 주제) LLM Fine-Tuning 의 PEFT (사용자 답변) 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지? (성공한 답변) 답변 실패 (다음 질문) LLM Fine-Tuning 의 PEFT, ✨✨ 효과적으로 파인튜닝을 한다는 말 말고 구체적으로 어떻게 효과적으로 파인튜닝을 하는지 말해봐! (발화 종료,1,80
310,4,5.53,(대화 주제) LLM Fine-Tuning 의 PEFT (사용자 답변) LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야! (성공한 답변) LLM Fine-Tuning 의 PEFT (다음 질문) PEFT 방법 5가지, ✨ 그럼 그 방법이 뭐가 있는지 5가지만 말해봐! 😊 (발화 종료,1,91
311,4,1.73,(대화 주제) LLM Fine-Tuning 의 PEFT (사용자 답변) LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다 (성공한 답변) 답변 실패 (다음 질문) LLM Fine-Tuning 의 PEFT, ✨🤔 (발화 종료,1,58
312,4,5.3,"(대화 주제) PEFT 방법 5가지 (사용자 답변) LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지? (성공한 답변) PEFT 방법 5가지 (다음 질문) LoRA", ✨ 그럼 그 중에서 나를 나타내기도 하는 LoRA는 뭘까? 😊 (발화 종료,1,83
313,4,1.69,(대화 주제) PEFT 방법 5가지 (사용자 답변) Oh-LoRA! 너 그 자체가 PEFT 아니야? (성공한 답변) 답변 실패 (다음 질문) PEFT 방법 5가지, ✨✨ (발화 종료,1,56
314,4,5.33,(대화 주제) LoRA (사용자 답변) LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야! (성공한 답변) LoRA (다음 질문) LoRA 와 QLoRA 의 차이, ✨ QLoRA가 뭐지? LoRA랑 QLoRA랑 차이는? (발화 종료,1,91
315,4,3.72,(대화 주제) LoRA (사용자 답변) 행렬 차원 축소한다는 걸로 알고 있는데 맞아? (성공한 답변) 답변 실패 (다음 질문) LoRA, 맞아! 근데 그 축소 방법이 뭘까? 🤔 (발화 종료,1,52
316,4,6.62,(대화 주제) LoRA (사용자 답변) LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야! (성공한 답변) LoRA (다음 질문) LoRA 와 QLoRA 의 차이, 2개의 LoRA가 각각 QLoRA로 파인튜닝되면 어떤 차이가 있을까? 🤔 (발화 종료,1,91
317,4,3.73,(대화 주제) LoRA (사용자 답변) 무슨 OOM 없앤다는 것 같은데 (성공한 답변) 답변 실패 (다음 질문) LoRA, OOM 없앤는 구체적인 방법이 뭘까? 🤔 (발화 종료,1,50
318,4,6.72,(대화 주제) LoRA 와 QLoRA 의 차이 (사용자 답변) QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야! (성공한 답변) LoRA 와 QLoRA 의 차이 (다음 질문) 마지막 할 말, 2023-10-27 10:00:00 QLoRA 가 정확히 뭔데? 😊 (발화 종료,1,90
319,4,7.2,(대화 주제) LoRA 와 QLoRA 의 차이 (사용자 답변) 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데 (성공한 답변) 답변 실패 (다음 질문) LoRA 와 QLoRA 의 차이," ✨️팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데, 구체적으로 어떻게 막으시는 걸까? 🤔 (발화 종료",1,81
320,4,10.4,(대화 주제) 마지막 할 말 (사용자 답변) 로라야 정말 고마워! (성공한 답변) 마지막 할 말 (다음 질문) 면접 종료, 2023-10-25 15:00:00 🙅‍♀️ 너도 나처럼 면접관으로서 너의 경험이 빛나길! ✨✨ (발화 종료,1,77
321,4,7.97,(대화 주제) 마지막 할 말 (사용자 답변) 로라야 사랑해 (성공한 답변) 마지막 할 말 (다음 질문) 면접 종료, 2023-10-27 15:00:00 🙎‍♀️ 너도 사랑해! 🙎‍♀️ (발화 종료,1,66
322,4,7.69,(대화 주제) 마지막 할 말 (사용자 답변) 로라야 너의 예쁜 미모에 반했어 (성공한 답변) 마지막 할 말 (다음 질문) 면접 종료, 2023-10-12 14:00:00) ✨ 그럼 너도 나처럼 빛나길 바라! ✨ (발화 종료,1,71
323,4,6.48,(대화 주제) 마지막 할 말 (사용자 답변) 그동안 고생했어 면접 봐줘서 정말 고마워 (성공한 답변) 마지막 할 말 (다음 질문) 면접 종료, 2023-10-25 16:30) 나도 너 덕분에 고생 많았어! 😊 (발화 종료,1,65
324,5,7.89,면접 시작, 2020-01-09 14:00 ✨ 안녕! 나는 면접관 LLM 오로라라고 해! 반가워! 😊 (발화 종료,1,40
325,5,6.59,(대화 주제) 면접 시작 인사 (사용자 답변) 오늘 뭐 물어볼 거야 그래서? (다음 질문) 면접 시작 인사, 2023-10-26 09:00 나는 뭐라도 알고 싶어! 🙎‍♀️ (발화 종료,1,56
326,5,9.69,(대화 주제) 면접 시작 인사 (사용자 답변) 로라야 안녕 정말 반가워 (다음 질문) 면접 시작 인사, 2023-11-01 15:00:00) 🙎‍♀️ 나랑 함께 너의 실력의 빈틈을 채워볼래? 😊 (발화 종료,1,68
327,5,8.41,(대화 주제) 면접 시작 인사 (사용자 답변) 로라야 그럼 네가 면접관이야? (다음 질문) 면접 시작 인사, 2023-10-10 14:00:00) 로라: 내가 너의 실력의 빈틈을 찔러 줄게! 😊 (발화 종료,1,66
328,5,7.71,(대화 주제) 면접 시작 인사 (사용자 답변) 내 인생도 이제 오로라처럼 빛날 거니까 (다음 질문) 면접 시작 인사, 2020-08-01 16:00:00) ✨오로라처럼 빛날 거라고! 오! ✨ (발화 종료,1,63
329,5,4.34,"(대화 주제) 면접 시작 인사 (사용자 답변) 파이팅! 시작하자 (다음 질문) 인공지능, 머신러닝, 딥러닝 차이"," 1 인공지능, 머신러닝, 딥러닝의 차이를 한번 말해봐! (발화 종료",1,47
330,5,5.22,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 인공지능 나도 개발해보고 싶은데 (성공한 답변) 답변 실패 (다음 질문) 인공지능, 머신러닝, 딥러닝 차이", 2021-01-10 17:00:00 인공지능이 뭔데? (발화 종료,1,64
331,5,7.62,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거! (성공한 답변) 인공지능 (다음 질문) 인공지능, 머신러닝, 딥러닝 차이", 2023-10-10 14:00:00) 인공지능은 맞았어! 그럼 이제 머신러닝은 뭔지 한번 말해봐! (발화 종료,1,81
332,5,5.47,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지 (성공한 답변) 머신러닝 (다음 질문) 인공지능, 머신러닝, 딥러닝 차이", 2021-10-01 16:00:00) 그러면 딥러닝이란 뭘까? (발화 종료,1,71
333,5,5.23,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지? (성공한 답변) 딥러닝 (다음 질문) 인공지능, 머신러닝, 딥러닝 차이", 2020-10-08 15:30:00 그렇다면 머신러닝은 뭐지? (발화 종료,1,69
334,5,6.42,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 딥러닝은 인공신경망 기반 머신러닝이야 (성공한 답변) 딥러닝 (다음 질문) 거대 언어 모델 정의"," 2023-11-14 16:00:00) 그러면 다음 질문, LLM은 뭐지? (발화 종료",1,68
335,5,6.47,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 머신러닝은 말 그대로 기계가 학습하는 거 아니야? (성공한 답변) 답변 실패 (다음 질문) 인공지능, 머신러닝, 딥러닝 차이", 2023-03-15 14:00:00 머신러닝이 뭔데? 🤷‍♀️ (발화 종료,1,73
336,5,5.28,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 인공지능 요새 엄청 뜨는 분야잖아 (성공한 답변) 답변 실패 (다음 질문) 인공지능, 머신러닝, 딥러닝 차이", 2023-01-01 14:00:00 인공지능이 뭔데? (발화 종료,1,64
337,5,7.86,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지? (성공한 답변) 인공지능 (다음 질문) 인공지능, 머신러닝, 딥러닝 차이", 2023-10-17 13:00:00 알고리즘으로 구현한 건 맞았어! 그럼 이제 딥러닝은 뭔지 한번 말해봐! (발화 종료,1,85
338,5,8.21,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야! (성공한 답변) 머신러닝 (다음 질문) 거대 언어 모델 정의", 2023-10-01 15:00:00) 그러면 다음 질문! LLM이 뭐지? 🙎‍♀️ (발화 종료,1,84
339,5,8.94,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지 (성공한 답변) 딥러닝 (다음 질문) 거대 언어 모델 정의"," 2023-01-17 15:00:00) 그러면 다음 질문, LLM이라는 게 뭘 말하는 걸까? 🙎‍♀️ (발화 종료",1,81
340,5,9.17,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야. (성공한 답변) 딥러닝 (다음 질문) 거대 언어 모델 정의", 2023-04-04 15:00:00) 그러면 다음 질문! 거대 언어 모델 (LLM) 은 뭐지? 🙎‍♀️ (발화 종료,1,84
341,5,6.42,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 머신러닝은 기계가 공부하는 거지 말 그대로 (성공한 답변) 답변 실패 (다음 질문) 인공지능, 머신러닝, 딥러닝 차이", 2023-10-14 17:00:00 머신러닝이 뭔데? 🤷‍♀️ (발화 종료,1,71
342,5,8.86,(대화 주제) 거대 언어 모델 정의 (사용자 답변) 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야 (성공한 답변) 거대 언어 모델 정의 (다음 질문) Loss Function 정의," 2023-02-01 16:00:00) ✨ 그러면 다음 질문, Loss Function 은 뭘 말하는 걸까? 💁‍♀️ (발화 종료",1,86
343,5,6.75,(대화 주제) 거대 언어 모델 정의 (사용자 답변) 파라미터 엄청나게 많다던데 (성공한 답변) 답변 실패 (다음 질문) 거대 언어 모델 정의, 2023-10-24 16:00) 음… 맞긴 하지! 근데 그 정의가 뭘까? (발화 종료,1,64
344,5,8.91,(대화 주제) 거대 언어 모델 정의 (사용자 답변) 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델 (성공한 답변) 거대 언어 모델 정의 (다음 질문) Loss Function 정의," 2023-02-01 13:00:00) ✨ 그러면 다음 질문, Loss Function 은 뭘 말하는 걸까? 💁‍♀️ (발화 종료",1,90
345,5,6.81,(대화 주제) 거대 언어 모델 정의 (사용자 답변) 말 그대로 거대한 언어 모델이지. (성공한 답변) 답변 실패 (다음 질문) 거대 언어 모델 정의, 2023-01-01 12:00:00 예! 맞았어! 근데 구체적인 정의가 뭐지? (발화 종료,1,67
346,5,7.89,(대화 주제) Loss Function 정의 (사용자 답변) 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야 (성공한 답변) 답변 실패 (다음 질문) Loss Function 정의, 2023-10-20 12:00:00) 음… 그건 맞는데 구체적으로 어떻게 나타내지? 🤷‍♀️ (발화 종료,1,76
347,5,7.1,"(대화 주제) Loss Function 정의 (사용자 답변) Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지! (성공한 답변) Loss Function 정의 (다음 질문) Loss Function 예시", 2023-03-21 16:00:00) ✨ 그러면 그 Loss Function에는 뭐가 있지? (발화 종료,1,85
348,5,8.19,(대화 주제) Loss Function 정의 (사용자 답변) Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야 (성공한 답변) 답변 실패 (다음 질문) Loss Function 정의, 2023-04-01 16:00:00) 음… 맞긴 한데 구체적으로 어떻게 나타내지? 🤷‍♀️ (발화 종료,1,74
349,5,6.45,(대화 주제) Loss Function 정의 (사용자 답변) 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지 (성공한 답변) Loss Function 정의 (다음 질문) Loss Function 예시, 2023-10-06 14:00:00) ✨ 예를 한번 들어봐! (발화 종료,1,73
350,5,9.22,(대화 주제) Loss Function 예시 (사용자 답변) ReLU 랑 Tanh 생각나는데 (성공한 답변) 답변 실패 (다음 질문) Loss Function 예시, 2023-10-20 12:00:00) ReLU랑 Tanh는 활성화 함수고 🙅‍♀️ Loss Function 말해봐! (발화 종료,1,75
351,5,7.16,"(대화 주제) Loss Function 예시 (사용자 답변) MSE, MAE, Cross Entropy Loss 같은 것들이 있지 (성공한 답변) Loss Function 예시 (다음 질문) MSE Loss 설명", 2023-10-01 16:00:00) 맞아! 그 중 MSE가 뭔지 한번 말해봐! (발화 종료,1,76
352,5,8.17,"(대화 주제) Loss Function 예시 (사용자 답변) Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어 (성공한 답변) Loss Function 예시 (다음 질문) BCE Loss 설명", 2023-05-22 14:30:00) 그럼 그 중에서 Binary Cross Entropy 는 뭔지 말해봐! 😊 (발화 종료,1,86
353,5,6.45,"(대화 주제) Loss Function 예시 (사용자 답변) Sigmoid, Tanh, Adam, AdamW 이런 거 아니야? (성공한 답변) 답변 실패 (다음 질문) Loss Function 예시", 2021-11-02 14:00) 그건 Optimizer 아니야? 🤷‍♀️ (발화 종료,1,70
354,5,6.95,"(대화 주제) Loss Function 예시 (사용자 답변) Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어 (성공한 답변) Loss Function 예시 (다음 질문) MSE Loss 설명", 2023-10-05 10:00:00 ✨ MSE Loss 가 정확히 어떤 함수지? (발화 종료,1,88
355,5,6.63,"(대화 주제) Loss Function 예시 (사용자 답변) BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아 (성공한 답변) Loss Function 예시 (다음 질문) BCE Loss 설명", 2023-10-20 12:00:00 BCE가 정확히 어떤 함수지? 😊 (발화 종료,1,80
356,5,6.67,(대화 주제) MSE Loss 설명 (사용자 답변) MSE Loss 는 오차의 제곱을 평균한 값이야 (성공한 답변) MSE Loss 설명 (다음 질문) MSE Loss 용도, 2021-01-01 16:00:00) ✨ MSE Loss 를 언제 어떻게 사용하지? (발화 종료,1,74
357,5,7.73,(대화 주제) MSE Loss 설명 (사용자 답변) MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지? (성공한 답변) 답변 실패 (다음 질문) MSE Loss 설명, 2021-01-18 12:00:00 · 음… 뭔가 하나가 빠진 것 같은데 🤷‍♀️ (발화 종료,1,79
358,5,10.62,"(대화 주제) MSE Loss 용도 (사용자 답변) 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지 (성공한 답변) MSE Loss 용도 (다음 질문) 확률 예측에서 MSE Loss 미 사용 이유", 2023-05-10 10:00:00) ✨ 그렇다면 실전 질문! 0부터 1까지의 확률 예측에서 MSE Loss는 왜 안 좋지? 🤔 (발화 종료,1,95
359,5,8.2,(대화 주제) MSE Loss 용도 (사용자 답변) MSE Loss 는 분류 문제에서 많이 사용되지 (성공한 답변) 답변 실패 (다음 질문) MSE Loss 용도, 2023-02-03 14:00) 분류 문제에서 많이 사용된다는 말 말고 구체적으로 어떻게 사용되지? 🤷‍♀️ (발화 종료,1,75
360,5,8.17,(대화 주제) 확률 예측에서 MSE Loss 미 사용 이유 (사용자 답변) 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야? (성공한 답변) 답변 실패 (다음 질문) 확률 예측에서 MSE Loss 미 사용 이유, 2023-10-31 14:00:00 🤷‍♀️ 그건 맞지! 근데 뭔가 문제가 있지 않을까? 😥 (발화 종료,1,90
361,5,107.02,(대화 주제) 확률 예측에서 MSE Loss 미 사용 이유 (사용자 답변) 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고? (성공한 답변) 확률 예측에서 MSE Loss 미 사용 이유 (다음 질문) BCE Loss 설명, 2021-10-18 15:00:00) 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0,1,512
362,5,9.2,(대화 주제) BCE Loss 설명 (사용자 답변) 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고 (성공한 답변) 수식 (다음 질문) BCE Loss 설명, 2023-10-25 16:00:00) 🙄 공식은 맞아! 근데 BCE Loss의 핵심 아이디어는 뭐지? 🙄 (발화 종료,1,98
363,5,10.6,"(대화 주제) BCE Loss 설명 (사용자 답변) 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거 (성공한 답변) 핵심 아이디어 (다음 질문) Multi-Class, Multi-Label 중 BCE 가 좋은 task"," 2023-02-09 14:00:00) ✨ 그러면 BCE는 Multi-Label, Multi-Class 중에 어디에 더 맞을까? 🤷‍♀️ (발화 종료",1,105
364,5,9.31,(대화 주제) BCE Loss 설명 (사용자 답변) 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야? (성공한 답변) 답변 실패 (다음 질문) BCE Loss 설명, 2023-10-20 14:00) 🤷‍♀️ 최선의 것을 찾는다는 말 말고 구체적으로 어떻게 하지? 🤷‍♀️ (발화 종료,1,89
365,5,9.03,"(대화 주제) BCE Loss 설명 (사용자 답변) 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고 (성공한 답변) 핵심 아이디어 (다음 질문) BCE Loss 설명", 2021-06-14 12:00:00) 음 맞아! 그럼 BCE Loss 공식 한번 말해볼 수 있어? 🙎‍♀️ (발화 종료,1,95
366,5,7.37,"(대화 주제) BCE Loss 설명 (사용자 답변) 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')] (성공한 답변) 수식 (다음 질문) BCE Loss 설명", 2023-10-05 14:00:00)를 바탕으로 BCE Loss의 핵심 아이디어를 말해봐! (발화 종료,1,91
367,5,10.19,"(대화 주제) BCE Loss 설명 (사용자 답변) 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다 (성공한 답변) 핵심 아이디어 (다음 질문) Multi-Class, Multi-Label 중 BCE 가 좋은 task"," 2023-05-24 17:00:00) ✨ 그렇다면 BCE는 Multi Class, Multi Label 중에 어디에 더 맞을까? 🤷‍♀️ (발화 종료",1,104
368,5,6.75,(대화 주제) BCE Loss 설명 (사용자 답변) y랑 1-y랑 서로 경쟁하는 거 (성공한 답변) 답변 실패 (다음 질문) BCE Loss 설명, 1 y랑 1-y랑 서로 경쟁하는 건 맞긴 한데 구체적으로 어떻게 하지? 🤷‍♀️ (발화 종료,1,69
369,5,10.66,"(대화 주제) BCE Loss 설명 (사용자 답변) 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지 (성공한 답변) 핵심 아이디어 (다음 질문) Multi-Class, Multi-Label 중 BCE 가 좋은 task"," 2021-03-24 14:00:00) ✨ 그럼 BCE는 Multi-Class, Multi-Label 중에 어디에 더 맞을까? 🤷‍♀️ (발화 종료",1,109
370,5,12.23,"(대화 주제) Multi-Class, Multi-Label 중 BCE 가 좋은 task (사용자 답변) 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야? (성공한 답변) 용어 질문 (다음 질문) Multi-Class, Multi-Label 중 BCE 가 좋은 task"," 2023-11-09 14:00:00) Multi-Class 는 개 고양이 같은 여러 개 중 하나로, Multi-Label 은 이들 Class 를 중복으로 (예: 개 고양이 모두 포함) 예측하는 거지! (발화 종료",1,110
371,5,9.14,"(대화 주제) Multi-Class, Multi-Label 중 BCE 가 좋은 task (사용자 답변) Multi-Class 지!! (성공한 답변) 답변 실패 (다음 질문) Multi-Class, Multi-Label 중 BCE 가 좋은 task", 2021-11-17 15:00:00 에 작성됨) 🤷‍♀️ Multi-Class 가 아니지! 잘 생각해봐 한번! 🤔 (발화 종료,1,90
372,5,3.02,"(대화 주제) Multi-Class, Multi-Label 중 BCE 가 좋은 task (사용자 답변) Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어 (성공한 답변) BCE 가 좋은 task (다음 질문) Multi-Class, Multi-Label 중 BCE 가 좋은 task", ✨ 이유를 한번 말해봐! (발화 종료,1,76
373,5,9.94,"(대화 주제) Multi-Class, Multi-Label 중 BCE 가 좋은 task (사용자 답변) Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아 (성공한 답변) BCE 가 좋은 이유 (다음 질문) Multi-Label 에서 CE + Softmax 적용 문제점", 2023-05-01 14:00:00) 그럼 이 Multi-Label task 에 Cross Entropy Loss + Softmax 를 적용하면? 🤷‍♀️ (발화 종료,1,119
374,5,7.53,(대화 주제) Multi-Label 에서 CE + Softmax 적용 문제점 (사용자 답변) 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지 (성공한 답변) Multi-Label 에서 CE + Softmax 적용 문제점 (다음 질문) Loss Function 관련 실무 경험, 2023-10-04 17:00) 실무에서 손실 함수 문제를 해결했다거나 그런 경험이 있어? 😊 (발화 종료,1,101
375,5,9.8,(대화 주제) Multi-Label 에서 CE + Softmax 적용 문제점 (사용자 답변) 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나 (성공한 답변) 답변 실패 (다음 질문) Multi-Label 에서 CE + Softmax 적용 문제점, 2021-11-01 14:00:00) 🤦‍♀️ 성능 제일 좋은 거 채택한다고? 구체적으로 어떻게 하지? 🤷‍♀️ (발화 종료,1,96
376,5,8.06,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [기본 경험] 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어 (성공한 답변) 기본 경험 (다음 질문) Loss Function 관련 실무 경험, 2021-10-01 14:00:00) 😊 오 멋지다! 👍👍 그럼 그 결과는 어땠어? (발화 종료,1,89
377,5,8.43,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [상세 경험] 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지? (성공한 답변) 상세 경험 (다음 질문) MBTI / 좋아하는 아이돌, 2023-10-04 15:00) 😊 그럼 쉬어가는 타임! 혹시 너 MBTI 뭐지? 나는 ENTJ인데! (발화 종료,1,92
378,5,9.2,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [상세 경험] 근데 기억 진짜 안나는데 어떡하지? (성공한 답변) 답변 실패 (다음 질문) 잠시 휴식, 2023-09-14 17:00) 그래도 한번 잘 생각해봐! 나랑 모의면접이 아닌 실제 면접에서 질문 나올걸? 😊 (발화 종료,1,80
379,5,7.88,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [기본 경험] 나 아직 실무 경험 없는데 ㅠㅠ (성공한 답변) 기본 경험 (다음 질문) MBTI / 좋아하는 아이돌, 2021-10-04 17:00) 그럼 잠시 쉬어가는 타임! 혹시 너 MBTI 뭐지? 나는 ENTJ인데! (발화 종료,1,78
380,5,7.68,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [기본 경험] 대답하기 싫은데 힝 (성공한 답변) 답변 실패 (다음 질문) 잠시 휴식, 2023-09-04 15:15:00 기본 경험에 대한 질문은 정말 민감한 거구나! 알겠어 😥 (발화 종료,1,73
381,5,10.39,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [기본 경험] 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ (성공한 답변) 기본 경험 (다음 질문) Loss Function 관련 실무 경험, 2023-10-03 12:00:00) 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 향상! 어떤 방법인지 궁금해 🔮 (발화 종료,1,100
382,5,7.53,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [상세 경험] 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어 (성공한 답변) 상세 경험 (다음 질문) MBTI / 좋아하는 아이돌, 2023-10-08 10:00) 😊 좋아! 그럼 쉬어가는 타임! 혹시 너 MBTI 뭐야? (발화 종료,1,99
383,5,9.53,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [상세 경험] 기억 진짜 안 나는데 가물가물하다 (성공한 답변) 답변 실패 (다음 질문) 잠시 휴식, 2021-06-07 14:35:00 😥 그래도 한번 잘 생각해 봐! 나랑 모의면접이 아닌 실제 면접에서 질문 나올걸? 😊 (발화 종료,1,83
384,5,10.07,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [기본 경험] 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ (성공한 답변) 기본 경험 (다음 질문) MBTI / 좋아하는 아이돌, 2022-03-23 14:42) 그래도 다음에 기회 되면 꼭 만들어봐! 👍 쉬어가는 타임! 혹시 너 MBTI 뭐지? 나는 ENTJ인데! (발화 종료,1,90
385,5,6.87,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [기본 경험] 이걸 왜 알려줘야 하지 (성공한 답변) 답변 실패 (다음 질문) 잠시 휴식, 2023-10-03 18:00 기본 경험에 대한 질문은 누구에게나 열려 있어! 😊 (발화 종료,1,69
386,5,7.51,(대화 주제) MBTI (사용자 답변) 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야! (성공한 답변) MBTI (다음 질문) 잠시 휴식, 2020-08-14 17:00 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야! 😊 (발화 종료,1,73
387,5,9.39,(대화 주제) 좋아하는 아이돌 (사용자 답변) 나 블랙핑크랑 레드벨벳 좋아해 (성공한 답변) 좋아하는 아이돌 (다음 질문) 잠시 휴식, 2021-04-02 21:33:00 나 블랙핑크랑 레드벨벳 알아! 나랑 같은 버츄얼이라서 친해지고 싶어 🙎‍♀️ (발화 종료,1,72
388,5,8.09,(대화 주제) 잠시 휴식 (사용자 답변) 재미있는 이야기 해줄래? (성공한 답변) 잠시 휴식 (다음 질문) LLM Fine-Tuning 의 PEFT, 2023-11-28 18:30:00) LLM Fine-Tuning 에서 PEFT는 뭘까~요? 😊 (발화 종료,1,72
389,5,6.7,"(대화 주제) LLM Fine-Tuning 의 PEFT (사용자 답변) PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야 (성공한 답변) LLM Fine-Tuning 의 PEFT (다음 질문) PEFT 방법 5가지", 2023-10-31 15:00:00) 5가지 방법 말해봐! 😊 (발화 종료,1,104
390,5,6.97,(대화 주제) LLM Fine-Tuning 의 PEFT (사용자 답변) 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지? (성공한 답변) 답변 실패 (다음 질문) LLM Fine-Tuning 의 PEFT, 2023-10-10 16:00) 효과적으로 파인튜닝한다! 그게 중요하지! 😊 (발화 종료,1,79
391,5,7.99,(대화 주제) LLM Fine-Tuning 의 PEFT (사용자 답변) LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야! (성공한 답변) LLM Fine-Tuning 의 PEFT (다음 질문) PEFT 방법 5가지, 2023-10-02 15:00:00 일부 파라미터만 효과적으로 학습하는 방법! 5가지 말해봐! 😊 (발화 종료,1,102
392,5,2.34,(대화 주제) LLM Fine-Tuning 의 PEFT (사용자 답변) LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다 (성공한 답변) 답변 실패 (다음 질문) LLM Fine-Tuning 의 PEFT, 💁‍♀️ (발화 종료,1,60
393,5,6.27,"(대화 주제) PEFT 방법 5가지 (사용자 답변) LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지? (성공한 답변) PEFT 방법 5가지 (다음 질문) LoRA", ✨ 그 중에서 나를 상징하는 기술이기도 한 LoRA 는 뭘까? 🤷‍♀️ (발화 종료,1,87
394,5,8.28,(대화 주제) PEFT 방법 5가지 (사용자 답변) Oh-LoRA! 너 그 자체가 PEFT 아니야? (성공한 답변) 답변 실패 (다음 질문) PEFT 방법 5가지, 2023-10-24 12:00:00) Oh-LoRA가 PEFT의 예시고! 구체적으로 말해봐! (발화 종료,1,83
395,5,7.23,(대화 주제) LoRA (사용자 답변) LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야! (성공한 답변) LoRA (다음 질문) LoRA 와 QLoRA 의 차이, 2021-02-23 10:00:00) QLoRA는 뭐지? 🤷‍♀️ (발화 종료,1,98
396,5,8.44,(대화 주제) LoRA (사용자 답변) 행렬 차원 축소한다는 걸로 알고 있는데 맞아? (성공한 답변) 답변 실패 (다음 질문) LoRA, 2023-01-01 14:00:00 행렬 차원 축소가 핵심 아이디어긴 해! 근데 그 방법을 알아야지! 😊 (발화 종료,1,72
397,5,7.39,(대화 주제) LoRA (사용자 답변) LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야! (성공한 답변) LoRA (다음 질문) LoRA 와 QLoRA 의 차이, 2023-05-10 10:00:00 LoRA 가 맞았어! 그럼 QLoRA 는 뭘까? (발화 종료,1,94
398,5,3.73,(대화 주제) LoRA (사용자 답변) 무슨 OOM 없앤다는 것 같은데 (성공한 답변) 답변 실패 (다음 질문) LoRA, OOM이 뭔데? 🤷‍♀️ (발화 종료,1,50
399,5,11.14,(대화 주제) LoRA 와 QLoRA 의 차이 (사용자 답변) QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야! (성공한 답변) LoRA 와 QLoRA 의 차이 (다음 질문) 마지막 할 말, 2023-10-10 10:00:00) 😊 잘했어! 😊 이제 면접 끝! 나한테 마지막으로 할 말 혹시 없어? 🙋‍♀️ (발화 종료,1,108
400,5,7.59,(대화 주제) LoRA 와 QLoRA 의 차이 (사용자 답변) 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데 (성공한 답변) 답변 실패 (다음 질문) LoRA 와 QLoRA 의 차이, 2023-10-01 14:00:00) 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데 😥 (발화 종료,1,84
401,5,6.72,(대화 주제) 마지막 할 말 (사용자 답변) 로라야 정말 고마워! (성공한 답변) 마지막 할 말 (다음 질문) 면접 종료, 2021-10-31 14:00:00) ✨ 너도 수고했어! 😊😊 (발화 종료,1,63
402,5,7.68,(대화 주제) 마지막 할 말 (사용자 답변) 로라야 사랑해 (성공한 답변) 마지막 할 말 (다음 질문) 면접 종료, 2023-11-30 15:00:00) ✨ 그래도 나도 너랑 우리 가족 사랑했어! 😊 (발화 종료,1,65
403,5,5.0,(대화 주제) 마지막 할 말 (사용자 답변) 로라야 너의 예쁜 미모에 반했어 (성공한 답변) 마지막 할 말 (다음 질문) 면접 종료, 2023-11-07 15:00:00) ✨✨ (발화 종료,1,60
404,5,4.69,(대화 주제) 마지막 할 말 (사용자 답변) 그동안 고생했어 면접 봐줘서 정말 고마워 (성공한 답변) 마지막 할 말 (다음 질문) 면접 종료, 2023-01-10 15:00) ✨✨ (발화 종료,1,57
405,6,10.3,면접 시작, 2023-11-07 09:00:00 ✨ 그럼 면접을 시작해 볼까? 나랑 함께 면접을 보는 동안 너의 실력이 쑥쑥 늘어날 거야! ✨ (발화 종료,1,50
406,6,6.48,(대화 주제) 면접 시작 인사 (사용자 답변) 오늘 뭐 물어볼 거야 그래서? (다음 질문) 면접 시작 인사, 2023-10-26 09:00 나처럼 AI도 면접 볼 수 있는데! 😊 (발화 종료,1,55
407,6,7.86,(대화 주제) 면접 시작 인사 (사용자 답변) 로라야 안녕 정말 반가워 (다음 질문) 면접 시작 인사, 2023-10-24 09:00:00 나랑 함께 너의 실력의 빈틈을 채워볼래? 😊 (발화 종료,1,61
408,6,7.94,(대화 주제) 면접 시작 인사 (사용자 답변) 로라야 그럼 네가 면접관이야? (다음 질문) 면접 시작 인사, 2023-10-01 10:00:00 로라: 맞지! 그럼 면접을 시작해 볼까? 🙎‍♀️ (발화 종료,1,65
409,6,7.98,(대화 주제) 면접 시작 인사 (사용자 답변) 내 인생도 이제 오로라처럼 빛날 거니까 (다음 질문) 면접 시작 인사, 2023-05-15 10:00 오로라처럼 빛나는 인생! 오! 그럼 그 빛의 이유가 뭘까? ✨ (발화 종료,1,64
410,6,3.84,"(대화 주제) 면접 시작 인사 (사용자 답변) 파이팅! 시작하자 (다음 질문) 인공지능, 머신러닝, 딥러닝 차이"," 인공지능, 머신러닝, 딥러닝의 차이를 한번 말해봐! (발화 종료",1,45
411,6,6.2,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 인공지능 나도 개발해보고 싶은데 (성공한 답변) 답변 실패 (다음 질문) 인공지능, 머신러닝, 딥러닝 차이", 2023-10-15 15:00:00 인공지능은 어떤 기술일까? 🤔 (발화 종료,1,67
412,6,7.45,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거! (성공한 답변) 인공지능 (다음 질문) 인공지능, 머신러닝, 딥러닝 차이", 2020-05-01 15:00:00 정답이야! 그럼 딥러닝이란 뭘까? 🙎‍♀️ (발화 종료,1,80
413,6,3.12,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지 (성공한 답변) 머신러닝 (다음 질문) 인공지능, 머신러닝, 딥러닝 차이", 머신러닝까지 잘했어! 그럼 딥러닝은? (발화 종료,1,61
414,6,6.91,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지? (성공한 답변) 딥러닝 (다음 질문) 인공지능, 머신러닝, 딥러닝 차이", 2020-10-20 00:00:00 그럼 그 이유가 뭐지? 🙎‍♀️ (발화 종료,1,76
415,6,6.85,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 딥러닝은 인공신경망 기반 머신러닝이야 (성공한 답변) 딥러닝 (다음 질문) 거대 언어 모델 정의", LLM-Q: 오 맞았어! 그럼 다음 질문! LLM은 과연 뭘까? 🙎‍♀️ (발화 종료,1,70
416,6,3.48,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 머신러닝은 말 그대로 기계가 학습하는 거 아니야? (성공한 답변) 답변 실패 (다음 질문) 인공지능, 머신러닝, 딥러닝 차이", 머신러닝이 뭔데? 🤷‍♀️ (발화 종료,1,60
417,6,3.95,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 인공지능 요새 엄청 뜨는 분야잖아 (성공한 답변) 답변 실패 (다음 질문) 인공지능, 머신러닝, 딥러닝 차이", 2023년 기준으로 얼마나 뜨는데? 😊 (발화 종료,1,58
418,6,6.58,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지? (성공한 답변) 인공지능 (다음 질문) 인공지능, 머신러닝, 딥러닝 차이", 2020-08-01 00:00:00 맞았어! 그럼 딥러닝이란 뭘까? 😊 (발화 종료,1,79
419,6,6.78,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야! (성공한 답변) 머신러닝 (다음 질문) 거대 언어 모델 정의", LLM_발화) 맞았어! 다음 질문은 LLM이야! LLM은 과연 뭘까? 😊 (발화 종료,1,79
420,6,7.7,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지 (성공한 답변) 딥러닝 (다음 질문) 거대 언어 모델 정의", LLM = 거대 언어 모델이라는 뜻이지! 그럼 다음 질문! LLM의 정의는 뭘까? 🙎‍♀️ (발화 종료,1,76
421,6,7.52,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야. (성공한 답변) 딥러닝 (다음 질문) 거대 언어 모델 정의", LLM-Q: 딥러닝 대답 잘했어! 그럼 다음 질문! LLM은 과연 뭘까? 🙎‍♀️ (발화 종료,1,76
422,6,3.37,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 머신러닝은 기계가 공부하는 거지 말 그대로 (성공한 답변) 답변 실패 (다음 질문) 인공지능, 머신러닝, 딥러닝 차이", 머신러닝이 뭔데? 🤷‍♀️ (발화 종료,1,58
423,6,8.62,(대화 주제) 거대 언어 모델 정의 (사용자 답변) 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야 (성공한 답변) 거대 언어 모델 정의 (다음 질문) Loss Function 정의," 2023-10-01 15:00:00 그럼 이어서 본격 면접에 들어가자! Loss Function, 즉 손실 함수의 정의는? (발화 종료",1,85
424,6,6.7,(대화 주제) 거대 언어 모델 정의 (사용자 답변) 파라미터 엄청나게 많다던데 (성공한 답변) 답변 실패 (다음 질문) 거대 언어 모델 정의, 2023-10-01 00:00:00 파라미터가 많다는 말 말고 정의를 말해봐! (발화 종료,1,63
425,6,9.05,(대화 주제) 거대 언어 모델 정의 (사용자 답변) 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델 (성공한 답변) 거대 언어 모델 정의 (다음 질문) Loss Function 정의," 2023-10-01 16:00:00 맞았어! 정답이야! 그럼 이제 본격 시작이다! Loss Function, 즉 손실 함수의 정의는? (발화 종료",1,92
426,6,9.81,(대화 주제) 거대 언어 모델 정의 (사용자 답변) 말 그대로 거대한 언어 모델이지. (성공한 답변) 답변 실패 (다음 질문) 거대 언어 모델 정의, 2023-10-01 10:00:00 LLM의 정의는 단순히 '거대'라는 표현으로 설명되지 않아! 정의를 정확히 말해봐! 😊 (발화 종료,1,80
427,6,7.86,(대화 주제) Loss Function 정의 (사용자 답변) 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야 (성공한 답변) 답변 실패 (다음 질문) Loss Function 정의, 2023-10-10 16:00:00 모델이 돈을 잃었다는 게 어떤 의미이지? 🤷‍♀️ (발화 종료,1,76
428,6,7.8,"(대화 주제) Loss Function 정의 (사용자 답변) Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지! (성공한 답변) Loss Function 정의 (다음 질문) Loss Function 예시"," 2021-09-01 17:00:00 모델 학습의 핵심, Loss Function! 그럼 구체적으로 뭐가 있지? (발화 종료",1,87
429,6,6.75,(대화 주제) Loss Function 정의 (사용자 답변) Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야 (성공한 답변) 답변 실패 (다음 질문) Loss Function 정의, 2023-10-05 16:00:00 모델의 손해? 구체적으로 어떻게 나타나지? 🤔 (발화 종료,1,69
430,6,10.96,(대화 주제) Loss Function 정의 (사용자 답변) 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지 (성공한 답변) Loss Function 정의 (다음 질문) Loss Function 예시, 2023-10-26 14:00:00 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식. 정답! 그럼 그 수식에는 뭐가 있을까? 🤔 (발화 종료,1,93
431,6,6.66,(대화 주제) Loss Function 예시 (사용자 답변) ReLU 랑 Tanh 생각나는데 (성공한 답변) 답변 실패 (다음 질문) Loss Function 예시, 2023-10-15 14:45:00 ReLU랑 Tanh는 활성화 함수지 😥 (발화 종료,1,65
432,6,7.61,"(대화 주제) Loss Function 예시 (사용자 답변) MSE, MAE, Cross Entropy Loss 같은 것들이 있지 (성공한 답변) Loss Function 예시 (다음 질문) MSE Loss 설명"," 2023-10-27 14:00:00 MSE는 Mean Squared Error로, 평균 제곱 오차를 의미하지! (발화 종료",1,78
433,6,6.86,"(대화 주제) Loss Function 예시 (사용자 답변) Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어 (성공한 답변) Loss Function 예시 (다음 질문) BCE Loss 설명", 2023-10-02 17:00:00 그럼 그 중에서 Binary Cross Entropy는 뭐지? (발화 종료,1,81
434,6,6.94,"(대화 주제) Loss Function 예시 (사용자 답변) Sigmoid, Tanh, Adam, AdamW 이런 거 아니야? (성공한 답변) 답변 실패 (다음 질문) Loss Function 예시"," 2023-10-01 14:00:00 Sigmoid, Tanh는 활성화 함수지 😥 (발화 종료",1,72
435,6,6.25,"(대화 주제) Loss Function 예시 (사용자 답변) Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어 (성공한 답변) Loss Function 예시 (다음 질문) MSE Loss 설명", 2023-10-25 15:00:00 MSE Loss가 정확히 어떤 함수지? (발화 종료,1,86
436,6,6.11,"(대화 주제) Loss Function 예시 (사용자 답변) BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아 (성공한 답변) Loss Function 예시 (다음 질문) BCE Loss 설명", 2023-10-01 15:00:00 BCE가 정확히 어떤 함수지? (발화 종료,1,78
437,6,11.34,(대화 주제) MSE Loss 설명 (사용자 답변) MSE Loss 는 오차의 제곱을 평균한 값이야 (성공한 답변) MSE Loss 설명 (다음 질문) MSE Loss 용도, 2023-10-31 14:00:00 오차의 제곱을 평균한 값이라니! 오 맞았어! 🙆‍♀️ 그럼 MSE Loss 는 어떤 용도로 사용하지? (발화 종료,1,92
438,6,6.96,(대화 주제) MSE Loss 설명 (사용자 답변) MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지? (성공한 답변) 답변 실패 (다음 질문) MSE Loss 설명, 2023-10-25 15:00:00 에야! 한번 다시 잘 생각해봐! 😊 (발화 종료,1,75
439,6,7.91,"(대화 주제) MSE Loss 용도 (사용자 답변) 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지 (성공한 답변) MSE Loss 용도 (다음 질문) 확률 예측에서 MSE Loss 미 사용 이유", 2023-10-24 14:00:00 확률 예측에서는 MSE Loss 문제점이 뭘까? 🤷‍♀️ (발화 종료,1,84
440,6,7.21,(대화 주제) MSE Loss 용도 (사용자 답변) MSE Loss 는 분류 문제에서 많이 사용되지 (성공한 답변) 답변 실패 (다음 질문) MSE Loss 용도, 2023-10-01 14:00:00 분류 문제에서 MSE Loss 가 왜 안 좋을까? 🤔 (발화 종료,1,71
441,6,8.33,(대화 주제) 확률 예측에서 MSE Loss 미 사용 이유 (사용자 답변) 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야? (성공한 답변) 답변 실패 (다음 질문) 확률 예측에서 MSE Loss 미 사용 이유, 2023-10-10 16:00:00 연속값에 대한 기대값 계산이 왜 문제일까? 🤷‍♀️ (발화 종료,1,90
442,6,11.42,(대화 주제) 확률 예측에서 MSE Loss 미 사용 이유 (사용자 답변) 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고? (성공한 답변) 확률 예측에서 MSE Loss 미 사용 이유 (다음 질문) BCE Loss 설명, 2023-10-01 17:00:00 0% 확률 예측 같은 특이한 경우에도 BCE Loss는 나름 잘 작동해! 그럼 BCE Loss가 뭔지 한번 설명해봐! 😊 (발화 종료,1,113
443,6,9.34,(대화 주제) BCE Loss 설명 (사용자 답변) 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고 (성공한 답변) 수식 (다음 질문) BCE Loss 설명, 2023-10-05 14:00:00 🙎‍♀️ 그럼 BCE Loss 의 핵심 아이디어는 뭐지? 🙎‍♀️ (발화 종료,1,99
444,6,10.98,"(대화 주제) BCE Loss 설명 (사용자 답변) 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거 (성공한 답변) 핵심 아이디어 (다음 질문) Multi-Class, Multi-Label 중 BCE 가 좋은 task"," 2023-02-15 13:30:00 정답! 그럼 이 BCE Loss 가, Multi-Class, Multi-Label 알지? 그 중에서 어디에 더 맞을까? 🙄 (발화 종료",1,108
445,6,8.11,(대화 주제) BCE Loss 설명 (사용자 답변) 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야? (성공한 답변) 답변 실패 (다음 질문) BCE Loss 설명, 2023-10-24 14:00) 그런 답은 누구나 할 수 있지! 한번 전문가답게 설명해봐! 😊 (발화 종료,1,83
446,6,9.44,"(대화 주제) BCE Loss 설명 (사용자 답변) 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고 (성공한 답변) 핵심 아이디어 (다음 질문) BCE Loss 설명", 2023-10-26 14:00:00 실제로 1인 것을 0으로 예측하는 것? 좀 더 구체적으로 설명해봐! 😊 (발화 종료,1,95
447,6,13.48,"(대화 주제) BCE Loss 설명 (사용자 답변) 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')] (성공한 답변) 수식 (다음 질문) BCE Loss 설명"," 2023-10-05 16:00:00 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')] 이게 무슨 뜻이지? 🤷‍♀️ (발화 종료",1,116
448,6,9.48,"(대화 주제) BCE Loss 설명 (사용자 답변) 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다 (성공한 답변) 핵심 아이디어 (다음 질문) Multi-Class, Multi-Label 중 BCE 가 좋은 task"," 2023-10-27 16:00:00 BCE는 Multi-Class, Multi-Label 중에 어디에 더 맞을까? 🤷‍♀️ (발화 종료",1,101
449,6,7.8,(대화 주제) BCE Loss 설명 (사용자 답변) y랑 1-y랑 서로 경쟁하는 거 (성공한 답변) 답변 실패 (다음 질문) BCE Loss 설명, 2023-10-01 14:00:00 y랑 1-y랑 서로 경쟁하는거 말고 구체적으로 설명해봐! (발화 종료,1,73
450,6,7.53,"(대화 주제) BCE Loss 설명 (사용자 답변) 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지 (성공한 답변) 핵심 아이디어 (다음 질문) Multi-Class, Multi-Label 중 BCE 가 좋은 task"," 0: 오 맞았어! 그럼 BCE Loss는 Multi-Class, Multi-Label 중에 어디에 더 적합할까? (발화 종료",1,96
451,6,13.12,"(대화 주제) Multi-Class, Multi-Label 중 BCE 가 좋은 task (사용자 답변) 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야? (성공한 답변) 용어 질문 (다음 질문) Multi-Class, Multi-Label 중 BCE 가 좋은 task"," 2023-10-05 17:00:00 🙎‍♀️ Multi-Class 는 개 고양이 같은 여러 개 중 하나로, Multi-Label 은 이들 Class 를 중복으로 (예: 개 고양이 모두 포함) 예측하는 거지! (발화 종료",1,115
452,6,6.71,"(대화 주제) Multi-Class, Multi-Label 중 BCE 가 좋은 task (사용자 답변) Multi-Class 지!! (성공한 답변) 답변 실패 (다음 질문) Multi-Class, Multi-Label 중 BCE 가 좋은 task", 2023-10-05 17:00:00 🙅‍♀️ 잘 생각해봐! (발화 종료,1,79
453,6,6.95,"(대화 주제) Multi-Class, Multi-Label 중 BCE 가 좋은 task (사용자 답변) Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어 (성공한 답변) BCE 가 좋은 task (다음 질문) Multi-Class, Multi-Label 중 BCE 가 좋은 task", 2023-10-01 15:00:00 이유가 뭘까~요? 🤷‍♀️ (발화 종료,1,92
454,6,10.55,"(대화 주제) Multi-Class, Multi-Label 중 BCE 가 좋은 task (사용자 답변) Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아 (성공한 답변) BCE 가 좋은 이유 (다음 질문) Multi-Label 에서 CE + Softmax 적용 문제점", 2023-11-15 17:00:00 ✨ 그럼 Multi-Label 문제에 Cross Entropy Loss + Softmax 를 적용하면 문제점은? 🤷‍♀️ (발화 종료,1,121
455,6,8.98,(대화 주제) Multi-Label 에서 CE + Softmax 적용 문제점 (사용자 답변) 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지 (성공한 답변) Multi-Label 에서 CE + Softmax 적용 문제점 (다음 질문) Loss Function 관련 실무 경험, 2023-10-05 17:00:00 실무에서 손실 함수 문제를 해결했다거나 그런 경험 말해봐! 🙋‍♀️ (발화 종료,1,106
456,6,10.8,(대화 주제) Multi-Label 에서 CE + Softmax 적용 문제점 (사용자 답변) 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나 (성공한 답변) 답변 실패 (다음 질문) Multi-Label 에서 CE + Softmax 적용 문제점, 2023-05-01 14:00:00 🙅‍♀️ 성능 제일 좋은 거 채택하는 건 맞긴 한데 그 '성능'이 뭘까? 🤷‍♀️ (발화 종료,1,101
457,6,10.01,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [기본 경험] 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어 (성공한 답변) 기본 경험 (다음 질문) Loss Function 관련 실무 경험, 2023-10-27 17:00:00 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했다는 게 어떤 의미이지? 🤔 (발화 종료,1,96
458,6,7.52,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [상세 경험] 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지? (성공한 답변) 상세 경험 (다음 질문) MBTI / 좋아하는 아이돌, 2023-10-10 14:00) 그럼 너 MBTI 뭐지? 나 엔티제인데 꽤 비슷하네! (발화 종료,1,89
459,6,7.41,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [상세 경험] 근데 기억 진짜 안나는데 어떡하지? (성공한 답변) 답변 실패 (다음 질문) 잠시 휴식, 2021-03-17 12:00 나처럼 면접관도 면접 준비를 안 했어? 🤦‍♀️ (발화 종료,1,74
460,6,6.94,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [기본 경험] 나 아직 실무 경험 없는데 ㅠㅠ (성공한 답변) 기본 경험 (다음 질문) MBTI / 좋아하는 아이돌, 2021-03-02 15:00) 그럼 잠시 쉬어가는 질문! 혹시 너 MBTI 뭐지? (발화 종료,1,73
461,6,6.27,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [기본 경험] 대답하기 싫은데 힝 (성공한 답변) 답변 실패 (다음 질문) 잠시 휴식, 2023-11-28 17:00 기본 경험 질문은 왜 싫다는 거지? 🤔 (발화 종료,1,67
462,6,7.61,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [기본 경험] 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ (성공한 답변) 기본 경험 (다음 질문) Loss Function 관련 실무 경험, 2023-05-01 15:00:00 3% 향상도 힘들지 👍 그러면 Loss Function 어떻게 조절했어? (발화 종료,1,89
463,6,7.47,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [상세 경험] 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어 (성공한 답변) 상세 경험 (다음 질문) MBTI / 좋아하는 아이돌, 2023-11-10 00:00:00 ✨✨ 그럼 쉬어가는 질문! 혹시 아이돌 누구 좋아해? (발화 종료,1,98
464,6,5.97,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [상세 경험] 기억 진짜 안 나는데 가물가물하다 (성공한 답변) 답변 실패 (다음 질문) 잠시 휴식, 2022-11-01 16:00:00 😥 그래도 한번 잘 생각해 봐! (발화 종료,1,68
465,6,10.34,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [기본 경험] 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ (성공한 답변) 기본 경험 (다음 질문) MBTI / 좋아하는 아이돌, 2021-02-01 16:00 나랑 면접 준비하면 취업해서 그런 경험 쌓을 수 있을 거야! 👍 그럼 쉬어가는 질문! 혹시 너 MBTI 뭐지? (발화 종료,1,90
466,6,6.3,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [기본 경험] 이걸 왜 알려줘야 하지 (성공한 답변) 답변 실패 (다음 질문) 잠시 휴식, 2022-08-03 13:00 나를 통해 면접 준비를 하는 거야! 😊 (발화 종료,1,66
467,6,5.47,(대화 주제) MBTI (사용자 답변) 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야! (성공한 답변) MBTI (다음 질문) 잠시 휴식, 2021-10-20 14:00 나랑 잘 맞지? 😊 (발화 종료,1,64
468,6,9.3,(대화 주제) 좋아하는 아이돌 (사용자 답변) 나 블랙핑크랑 레드벨벳 좋아해 (성공한 답변) 좋아하는 아이돌 (다음 질문) 잠시 휴식, 2021-03-01 16:00:00 나 블랙핑크랑 레드벨벳 알아! 나랑 같은 버츄얼이라서 친해지고 싶어 🙎‍♀️ (발화 종료,1,72
469,6,6.62,(대화 주제) 잠시 휴식 (사용자 답변) 재미있는 이야기 해줄래? (성공한 답변) 잠시 휴식 (다음 질문) LLM Fine-Tuning 의 PEFT, ✨ 이야기를 듣다가 다음 질문! LLM Fine-Tuning 방법 중 PEFT가 뭘까? 😊 (발화 종료,1,66
470,6,9.13,"(대화 주제) LLM Fine-Tuning 의 PEFT (사용자 답변) PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야 (성공한 답변) LLM Fine-Tuning 의 PEFT (다음 질문) PEFT 방법 5가지", 2023-10-27 18:00:00 🙆‍♀️ 맞았어! 그럼 그 방법을 5가지만 말해봐! (발화 종료,1,114
471,6,3.5,(대화 주제) LLM Fine-Tuning 의 PEFT (사용자 답변) 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지? (성공한 답변) 답변 실패 (다음 질문) LLM Fine-Tuning 의 PEFT, 효과적으로 어떻게 파인튜닝하지? 🤔 (발화 종료,1,65
472,6,8.3,(대화 주제) LLM Fine-Tuning 의 PEFT (사용자 답변) LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야! (성공한 답변) LLM Fine-Tuning 의 PEFT (다음 질문) PEFT 방법 5가지, 2023-10-24 15:00:00 일부 파라미터만 효과적으로 학습하는 방법을 5가지만 말해봐! 😊 (발화 종료,1,103
473,6,6.69,(대화 주제) LLM Fine-Tuning 의 PEFT (사용자 답변) LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다 (성공한 답변) 답변 실패 (다음 질문) LLM Fine-Tuning 의 PEFT, 💁‍♀️🙋‍♀️🙅‍♀️ 잘 모르겠다니! 🤦‍♀️ (발화 종료,1,79
474,6,6.46,"(대화 주제) PEFT 방법 5가지 (사용자 답변) LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지? (성공한 답변) PEFT 방법 5가지 (다음 질문) LoRA", 💁‍♀️ LoRA가 뭐~게? 나를 상징하는 건데 👱‍♀️✨ (발화 종료,1,87
475,6,3.67,(대화 주제) PEFT 방법 5가지 (사용자 답변) Oh-LoRA! 너 그 자체가 PEFT 아니야? (성공한 답변) 답변 실패 (다음 질문) PEFT 방법 5가지, 2023-10-26 13:00-발화 종료,1,64
476,6,9.16,(대화 주제) LoRA (사용자 답변) LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야! (성공한 답변) LoRA (다음 질문) LoRA 와 QLoRA 의 차이, 2023-10-15 16:00:00 LoRA 잘 알고 있네! 그럼 LoRA 랑 QLoRA 랑 차이는 뭘까? 😊 (발화 종료,1,106
477,6,5.95,(대화 주제) LoRA (사용자 답변) 행렬 차원 축소한다는 걸로 알고 있는데 맞아? (성공한 답변) 답변 실패 (다음 질문) LoRA, LoRA는 행렬 차원 축소가 핵심 아이디어긴 해! 근데 그 방법을 알아야지! 😊 (발화 종료,1,62
478,6,8.71,(대화 주제) LoRA (사용자 답변) LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야! (성공한 답변) LoRA (다음 질문) LoRA 와 QLoRA 의 차이, 2023-10-31 14:00:00 LoRA 잘 알고 있네! 그럼 QLoRA는 뭘까? 🤷‍♀️ (발화 종료,1,99
479,6,4.14,(대화 주제) LoRA (사용자 답변) 무슨 OOM 없앤다는 것 같은데 (성공한 답변) 답변 실패 (다음 질문) LoRA, LoRA가 어떤 기술인지 좀 더 구체적으로 설명해봐! (발화 종료,1,52
480,6,11.37,(대화 주제) LoRA 와 QLoRA 의 차이 (사용자 답변) QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야! (성공한 답변) LoRA 와 QLoRA 의 차이 (다음 질문) 마지막 할 말, 2021-10-10 16:00:00 LoRA 와 QLoRA 의 차이를 잘 설명했어! 👍👍 혹시 나한테 마지막으로 할 이야기 있지 않아? 😊 (발화 종료,1,109
481,6,5.45,(대화 주제) LoRA 와 QLoRA 의 차이 (사용자 답변) 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데 (성공한 답변) 답변 실패 (다음 질문) LoRA 와 QLoRA 의 차이, ✨ 팀장님이 LoRA 쓰시는 이유가 뭘까? 🤷‍♀️ (발화 종료,1,74
482,6,6.98,(대화 주제) 마지막 할 말 (사용자 답변) 로라야 정말 고마워! (성공한 답변) 마지막 할 말 (다음 질문) 면접 종료, 2023-10-25 16:00:00) ✨✨ 그럼 너도 수고했어! 😊 (발화 종료,1,64
483,6,7.97,(대화 주제) 마지막 할 말 (사용자 답변) 로라야 사랑해 (성공한 답변) 마지막 할 말 (다음 질문) 면접 종료, 2021-04-01 16:00:00) ✨ 그래도 너도 사랑해줄게 🙎‍♀️ (발화 종료,1,65
484,6,7.03,(대화 주제) 마지막 할 말 (사용자 답변) 로라야 너의 예쁜 미모에 반했어 (성공한 답변) 마지막 할 말 (다음 질문) 면접 종료, 2023-11-10 14:00:00) ✨ 그럼 너는 나처럼 되고 싶은데? (발화 종료,1,67
485,6,6.62,(대화 주제) 마지막 할 말 (사용자 답변) 그동안 고생했어 면접 봐줘서 정말 고마워 (성공한 답변) 마지막 할 말 (다음 질문) 면접 종료, 2022-01-10 16:00) 나도 너 덕분에 많이 배웠어! 😊 (발화 종료,1,65
486,7,7.86,면접 시작, 2023-01-01 09:00:00 나랑 함께 너의 머신러닝 지식의 빈틈을 채워볼래? 😊 (발화 종료,1,40
487,7,3.75,(대화 주제) 면접 시작 인사 (사용자 답변) 오늘 뭐 물어볼 거야 그래서? (다음 질문) 면접 시작 인사, 아마도 딥러닝이나 머신러닝 쪽 물어보지 않을까? 왜? (발화 종료,1,44
488,7,4.7,(대화 주제) 면접 시작 인사 (사용자 답변) 로라야 안녕 정말 반가워 (다음 질문) 면접 시작 인사, 나랑 함께 너의 실력의 빈틈을 채워볼래? 😊 (발화 종료,1,48
489,7,8.27,(대화 주제) 면접 시작 인사 (사용자 답변) 로라야 그럼 네가 면접관이야? (다음 질문) 면접 시작 인사, 2023-11-01 10:00 로라: 그렇지! 로라가 너의 머신러닝 실력의 빈틈을 찔러 줄게! 😊 (발화 종료,1,66
490,7,3.72,(대화 주제) 면접 시작 인사 (사용자 답변) 내 인생도 이제 오로라처럼 빛날 거니까 (다음 질문) 면접 시작 인사, ✨️ 그럼 다음 질문! 오로라가 뭐지? (발화 종료,1,47
491,7,3.66,"(대화 주제) 면접 시작 인사 (사용자 답변) 파이팅! 시작하자 (다음 질문) 인공지능, 머신러닝, 딥러닝 차이", 인공지능 머신러닝 딥러닝 정의와 차이점을 한번 말해봐! (발화 종료,1,44
492,7,2.48,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 인공지능 나도 개발해보고 싶은데 (성공한 답변) 답변 실패 (다음 질문) 인공지능, 머신러닝, 딥러닝 차이", 인공지능은 어떤 기술이지? (발화 종료,1,52
493,7,3.92,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거! (성공한 답변) 인공지능 (다음 질문) 인공지능, 머신러닝, 딥러닝 차이", 인공지능은 맞았어! 그럼 딥러닝이랑 머신러닝은? (발화 종료,1,65
494,7,3.11,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지 (성공한 답변) 머신러닝 (다음 질문) 인공지능, 머신러닝, 딥러닝 차이", 머신러닝까지 잘했어! 그럼 딥러닝은? (발화 종료,1,61
495,7,7.95,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지? (성공한 답변) 딥러닝 (다음 질문) 인공지능, 머신러닝, 딥러닝 차이", 2020-08-20 13:34:16 그럼 딥러닝이 맞았어! 그럼 이제 머신러닝은 뭘 말하는 걸까? 😊 (발화 종료,1,80
496,7,6.7,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 딥러닝은 인공신경망 기반 머신러닝이야 (성공한 답변) 딥러닝 (다음 질문) 거대 언어 모델 정의", LLM_1: 오 맞았어! 그럼 다음 질문 들어간다! 거대 언어 모델 (LLM) 은 뭐지? (발화 종료,1,69
497,7,2.91,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 머신러닝은 말 그대로 기계가 학습하는 거 아니야? (성공한 답변) 답변 실패 (다음 질문) 인공지능, 머신러닝, 딥러닝 차이", 머신러닝이 그거 말고도 중요한데 😥 (발화 종료,1,58
498,7,2.87,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 인공지능 요새 엄청 뜨는 분야잖아 (성공한 답변) 답변 실패 (다음 질문) 인공지능, 머신러닝, 딥러닝 차이", 그래도 인공지능이 뭔지 한번 말해봐! (발화 종료,1,54
499,7,3.83,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지? (성공한 답변) 인공지능 (다음 질문) 인공지능, 머신러닝, 딥러닝 차이", 맞았어! 그럼 이제 머신러닝은 뭔지 한번 말해봐! (발화 종료,1,68
500,7,6.02,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야! (성공한 답변) 머신러닝 (다음 질문) 거대 언어 모델 정의", LLM_발화) 그러면 다음 질문! 거대 언어 모델이란 뭘까? 🤷‍♀️ (발화 종료,1,76
501,7,8.86,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지 (성공한 답변) 딥러닝 (다음 질문) 거대 언어 모델 정의", LLM_발화) 오 잘했어! 🙆‍♀️ 그럼 다음 질문! LLM이라는 게 뭘까? 🙋‍♀️ (발화 종료,1,81
502,7,5.91,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야. (성공한 답변) 딥러닝 (다음 질문) 거대 언어 모델 정의", LLM_발화) 그러면 다음 질문! 거대 언어 모델이란 뭘까? 💁‍♀️ (발화 종료,1,71
503,7,3.48,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 머신러닝은 기계가 공부하는 거지 말 그대로 (성공한 답변) 답변 실패 (다음 질문) 인공지능, 머신러닝, 딥러닝 차이", 머신러닝이 뭔데? 🤷‍♀️ (발화 종료,1,58
504,7,9.33,(대화 주제) 거대 언어 모델 정의 (사용자 답변) 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야 (성공한 답변) 거대 언어 모델 정의 (다음 질문) Loss Function 정의, 2023-10-04 16:00:00) 오 정답! 🎉 그럼 이어서 본격 면접에 들어가자! 손실 함수의 정의는? (발화 종료,1,88
505,7,3.33,(대화 주제) 거대 언어 모델 정의 (사용자 답변) 파라미터 엄청나게 많다던데 (성공한 답변) 답변 실패 (다음 질문) 거대 언어 모델 정의, 파라미터가 왜 중요하지? 🤷‍♀️ (발화 종료,1,50
506,7,7.19,(대화 주제) 거대 언어 모델 정의 (사용자 답변) 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델 (성공한 답변) 거대 언어 모델 정의 (다음 질문) Loss Function 정의, 2023-10-24 15:00:00 정답! 그럼 이제 본격 시작이다! Loss Function 이 뭐지? (발화 종료,1,83
507,7,2.47,(대화 주제) 거대 언어 모델 정의 (사용자 답변) 말 그대로 거대한 언어 모델이지. (성공한 답변) 답변 실패 (다음 질문) 거대 언어 모델 정의, LLM 정확한 정의는? (발화 종료,1,49
508,7,7.69,(대화 주제) Loss Function 정의 (사용자 답변) 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야 (성공한 답변) 답변 실패 (다음 질문) Loss Function 정의, 2023-10-01 17:00:00 돈을 잃었다는 게 무슨 뜻이지? 🤷‍♀️ (발화 종료,1,74
509,7,108.8,"(대화 주제) Loss Function 정의 (사용자 답변) Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지! (성공한 답변) Loss Function 정의 (다음 질문) Loss Function 예시", 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 ,1,512
510,7,4.39,(대화 주제) Loss Function 정의 (사용자 답변) Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야 (성공한 답변) 답변 실패 (다음 질문) Loss Function 정의, 1: 그래서 어떤 손해를 나타내지? 🤷‍♀️ (발화 종료,1,59
511,7,111.97,(대화 주제) Loss Function 정의 (사용자 답변) 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지 (성공한 답변) Loss Function 정의 (다음 질문) Loss Function 예시, 1 3 5 7 9 11 13 15 17 19 21 23 25 27 29 31 33 35 37 39 41 43 45 47 49 51 53 55 57 59 61 63 65 67 69 71 73 75 77 79 81 83 85 87 89 91 93 95 97 99 101 103 105 107 109 111 113 115 117 119 121 123 125 127 129 131 133 135 137 139 141 143 145 147 149 151 153 155 157 159 161 163 165 167 169 171 173 175 177 179 181 183 185 187 189 191 193 195 197 199 201 203 205 207 209 211 213 215 217 219 221 223 225 227 229 231 233 235 237 239 241 243 245 247 249 251 253 255 257 259 261 263 265 267 269 271 273 275 277 279 281 283 285 287 289 291 293 295 297 299 301 303 305 307 309 311 313 315 317 319 321 323 325 327 329 331 333 335 337 339 341 343 345 347 349 351 353 355 357 359 361 363 365 367 369 371 373 375 377 379 381 383 385 387 389 391 393 395 397 399 401 403 405 407 409 411 413 415 417 419 421 423 425 427 429 431 433 435 437 439 441 443 445 447 449 451 453 455 457 459 461 463 ,1,512
512,7,4.36,(대화 주제) Loss Function 예시 (사용자 답변) ReLU 랑 Tanh 생각나는데 (성공한 답변) 답변 실패 (다음 질문) Loss Function 예시, ✨ 그래도 좋아! 그럼 ReLU랑 Tanh가 뭐지? (발화 종료,1,55
513,7,6.25,"(대화 주제) Loss Function 예시 (사용자 답변) MSE, MAE, Cross Entropy Loss 같은 것들이 있지 (성공한 답변) Loss Function 예시 (다음 질문) MSE Loss 설명", 2023-10-01 15:00:00 MSE Loss가 정확히 뭘까? 😊 (발화 종료,1,72
514,7,6.31,"(대화 주제) Loss Function 예시 (사용자 답변) Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어 (성공한 답변) Loss Function 예시 (다음 질문) BCE Loss 설명", 2023-10-24 14:30:00 Binary Cross Entropy 는 뭘까? 😊 (발화 종료,1,79
515,7,7.81,"(대화 주제) Loss Function 예시 (사용자 답변) Sigmoid, Tanh, Adam, AdamW 이런 거 아니야? (성공한 답변) 답변 실패 (다음 질문) Loss Function 예시", 2021-10-01 15:00:00 이건 활성화 함수고 🙅‍♀️ Loss Function 예시 말해봐! (발화 종료,1,77
516,7,6.83,"(대화 주제) Loss Function 예시 (사용자 답변) Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어 (성공한 답변) Loss Function 예시 (다음 질문) MSE Loss 설명", 2023-02-17 09:00:00 MSE Loss 가 정확히 뭘까? 😊 (발화 종료,1,86
517,7,6.7,"(대화 주제) Loss Function 예시 (사용자 답변) BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아 (성공한 답변) Loss Function 예시 (다음 질문) BCE Loss 설명", 2021-10-04 14:00:00 BCE가 정확히 뭐지? 😊 (발화 종료,1,78
518,7,6.76,(대화 주제) MSE Loss 설명 (사용자 답변) MSE Loss 는 오차의 제곱을 평균한 값이야 (성공한 답변) MSE Loss 설명 (다음 질문) MSE Loss 용도, 2023-10-26 15:00:00 오차의 제곱을 평균하는 이유는? (발화 종료,1,72
519,7,115.07,(대화 주제) MSE Loss 설명 (사용자 답변) MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지? (성공한 답변) 답변 실패 (다음 질문) MSE Loss 설명, 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 ,1,512
520,7,10.7,"(대화 주제) MSE Loss 용도 (사용자 답변) 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지 (성공한 답변) MSE Loss 용도 (다음 질문) 확률 예측에서 MSE Loss 미 사용 이유"," 2021-05-25 14:30:00 연속적인 값을 딥러닝으로 예측할때"" 부분에 주목하여, 확률 예측에서는 MSE Loss 가 왜 안 좋을까? 😊 (발화 종료",1,95
521,7,7.87,(대화 주제) MSE Loss 용도 (사용자 답변) MSE Loss 는 분류 문제에서 많이 사용되지 (성공한 답변) 답변 실패 (다음 질문) MSE Loss 용도, 2023-10-01 15:00:00 분류 문제에서 MSE Loss 가 왜 안 좋지? 🤷‍♀️ (발화 종료,1,74
522,7,7.11,(대화 주제) 확률 예측에서 MSE Loss 미 사용 이유 (사용자 답변) 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야? (성공한 답변) 답변 실패 (다음 질문) 확률 예측에서 MSE Loss 미 사용 이유, 0️⃣ 연속값이라서 예측 나름 되긴 해! 근데 최선은 아니야! 🙅‍♀️ (발화 종료,1,86
523,7,10.72,(대화 주제) 확률 예측에서 MSE Loss 미 사용 이유 (사용자 답변) 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고? (성공한 답변) 확률 예측에서 MSE Loss 미 사용 이유 (다음 질문) BCE Loss 설명, 2023-10-01 16:00:00 페넨티를 크게 주는 그런 정책은 없어! 🙅‍♀️ 그럼 BCE Loss 가 뭔지 한번 설명해 봐! (발화 종료,1,110
524,7,4.1,(대화 주제) BCE Loss 설명 (사용자 답변) 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고 (성공한 답변) 수식 (다음 질문) BCE Loss 설명, 1-y'가 뭐지? 🙋‍♀️ (발화 종료,1,77
525,7,6.64,"(대화 주제) BCE Loss 설명 (사용자 답변) 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거 (성공한 답변) 핵심 아이디어 (다음 질문) Multi-Class, Multi-Label 중 BCE 가 좋은 task"," 0: 맞았어! 그럼 Multi-Class, Multi-Label 중 어디에 BCE 가 맞을까? 😊 (발화 종료",1,90
526,7,5.22,(대화 주제) BCE Loss 설명 (사용자 답변) 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야? (성공한 답변) 답변 실패 (다음 질문) BCE Loss 설명, BCE Loss는 그런 게 맞긴 한데 좀 더 구체적으로 설명해봐 😊 (발화 종료,1,71
527,7,5.3,"(대화 주제) BCE Loss 설명 (사용자 답변) 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고 (성공한 답변) 핵심 아이디어 (다음 질문) BCE Loss 설명", 사용자 답변) 정답! 그럼 BCE Loss 공식 한번 알려줘 봐봐! 😊 (발화 종료,1,79
528,7,4.52,"(대화 주제) BCE Loss 설명 (사용자 답변) 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')] (성공한 답변) 수식 (다음 질문) BCE Loss 설명"," y=1, y'=1일 때의 BCE Loss는? (발화 종료",1,79
529,7,7.83,"(대화 주제) BCE Loss 설명 (사용자 답변) 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다 (성공한 답변) 핵심 아이디어 (다음 질문) Multi-Class, Multi-Label 중 BCE 가 좋은 task"," 0️⃣: 정답! 그럼 Multi-Class, Multi-Label 중에서 BCE Loss 가 더 맞는 task 는? 😊 (발화 종료",1,95
530,7,3.91,(대화 주제) BCE Loss 설명 (사용자 답변) y랑 1-y랑 서로 경쟁하는 거 (성공한 답변) 답변 실패 (다음 질문) BCE Loss 설명, y 말고 뭐가 경쟁하지? 🤷‍♀️ (발화 종료,1,57
531,7,7.19,"(대화 주제) BCE Loss 설명 (사용자 답변) 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지 (성공한 답변) 핵심 아이디어 (다음 질문) Multi-Class, Multi-Label 중 BCE 가 좋은 task"," 0: 맞았어! 그럼 Multi-Class, Multi-Label 중에 BCE Loss가 더 맞는 task는? 😊 (발화 종료",1,96
532,7,98.54,"(대화 주제) Multi-Class, Multi-Label 중 BCE 가 좋은 task (사용자 답변) 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야? (성공한 답변) 용어 질문 (다음 질문) Multi-Class, Multi-Label 중 BCE 가 좋은 task"," 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 164 Multi-Class 는 어떤 대상을 개, 고양이, … 중 하나로 분류하는 거야! 😊 Multi-Label 은 각 Class 를 독립적으로 예측하는 거지! 🐶🐱 즉 한 마리의 개가 개, 고양이, … 중 어디에도 속하지 않을 수 있어! 🐶🐱 (발화 종료",1,474
533,7,6.94,"(대화 주제) Multi-Class, Multi-Label 중 BCE 가 좋은 task (사용자 답변) Multi-Class 지!! (성공한 답변) 답변 실패 (다음 질문) Multi-Class, Multi-Label 중 BCE 가 좋은 task", 2 2023-02-01 15:30:00 Multi-Class 가 왜 더 좋을까? 🤔 (발화 종료,1,80
534,7,6.95,"(대화 주제) Multi-Class, Multi-Label 중 BCE 가 좋은 task (사용자 답변) Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어 (성공한 답변) BCE 가 좋은 task (다음 질문) Multi-Class, Multi-Label 중 BCE 가 좋은 task", 2023-11-01 14:00:00 이유가 뭘까~요? 🤷‍♀️ (발화 종료,1,92
535,7,9.47,"(대화 주제) Multi-Class, Multi-Label 중 BCE 가 좋은 task (사용자 답변) Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아 (성공한 답변) BCE 가 좋은 이유 (다음 질문) Multi-Label 에서 CE + Softmax 적용 문제점", 2021-04-01 09:00:00 🙎‍♀️ 그럼 Multi-Label task 에 Softmax + CE Loss 를 적용하면 어떻게 되지? (발화 종료,1,118
536,7,10.02,(대화 주제) Multi-Label 에서 CE + Softmax 적용 문제점 (사용자 답변) 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지 (성공한 답변) Multi-Label 에서 CE + Softmax 적용 문제점 (다음 질문) Loss Function 관련 실무 경험, 2023-10-01 09:00 정답! 그럼 실무 질문! 손실 함수와 관련된 이슈를 실무에서 해결했다거나 그런 경험 말해봐! 🙋‍♀️ (발화 종료,1,112
537,7,9.49,(대화 주제) Multi-Label 에서 CE + Softmax 적용 문제점 (사용자 답변) 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나 (성공한 답변) 답변 실패 (다음 질문) Multi-Label 에서 CE + Softmax 적용 문제점, 2021-11-09 16:17:00 성능 제일 좋은 거'가 핵심 문제성능 제일 좋은 걸 왜 채택하지? 🙎‍♀️ (발화 종료,1,96
538,7,8.55,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [기본 경험] 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어 (성공한 답변) 기본 경험 (다음 질문) Loss Function 관련 실무 경험, 1  인접한 픽셀의 점수 차이를 Loss Term 으로 추가하는 아이디어가 멋지네! 👍 그래서 어떤 식으로 점수를 줬어? 😊 (발화 종료,1,91
539,7,8.22,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [상세 경험] 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지? (성공한 답변) 상세 경험 (다음 질문) MBTI / 좋아하는 아이돌, 2b 2022-08-19 16:35 성능 향상에 대한 팀장님의 칭찬이 왜 부럽지 않지? 😊 (발화 종료,1,92
540,7,7.42,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [상세 경험] 근데 기억 진짜 안나는데 어떡하지? (성공한 답변) 답변 실패 (다음 질문) 잠시 휴식, 2023-04-01 14:00:00 😥 그래도 너무 걱정하지 마! 가끔 그런거 잊는다고! (발화 종료,1,74
541,7,5.66,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [기본 경험] 나 아직 실무 경험 없는데 ㅠㅠ (성공한 답변) 기본 경험 (다음 질문) MBTI / 좋아하는 아이돌, 기본 경험이 없더라고! 😥 잠시 쉬어가는 타임! 혹시 너 MBTI 뭐지? (발화 종료,1,68
542,7,5.92,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [기본 경험] 대답하기 싫은데 힝 (성공한 답변) 답변 실패 (다음 질문) 잠시 휴식, 2023-05-16 12:00 나랑 면접 준비하느라 고생했어! 😊 (발화 종료,1,66
543,7,5.59,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [기본 경험] 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ (성공한 답변) 기본 경험 (다음 질문) Loss Function 관련 실무 경험, 물체 인식에서 Loss Function 조절로 3% 향상! 어떤 방법인지 궁금해 🔮 (발화 종료,1,80
544,7,5.94,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [상세 경험] 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어 (성공한 답변) 상세 경험 (다음 질문) MBTI / 좋아하는 아이돌, ① 가중치 설정 완료! 이제 쉬어가는 타임! 좋아하는 아이돌 혹시 있어? 😊 (발화 종료,1,92
545,7,9.09,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [상세 경험] 기억 진짜 안 나는데 가물가물하다 (성공한 답변) 답변 실패 (다음 질문) 잠시 휴식, 2021-08-17 15:00 🙄 그래도 잘 생각해 봐! 나랑 모의면접이 아닌 실제 면접에서 질문 나올걸? 😊 (발화 종료,1,81
546,7,1.7,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [기본 경험] 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ (성공한 답변) 기본 경험 (다음 질문) MBTI / 좋아하는 아이돌, 1-2발화 종료,1,55
547,7,6.17,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [기본 경험] 이걸 왜 알려줘야 하지 (성공한 답변) 답변 실패 (다음 질문) 잠시 휴식, 2022-05-02 12:00 나랑 면접 준비하느라 고생했어! 😊 (발화 종료,1,65
548,7,5.45,(대화 주제) MBTI (사용자 답변) 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야! (성공한 답변) MBTI (다음 질문) 잠시 휴식, 2023-10-04 14:00 나랑 잘 맞지? 😊 (발화 종료,1,64
549,7,10.37,(대화 주제) 좋아하는 아이돌 (사용자 답변) 나 블랙핑크랑 레드벨벳 좋아해 (성공한 답변) 좋아하는 아이돌 (다음 질문) 잠시 휴식, 2020-09-01 17:00 나 블랙핑크랑 레드벨벳 좋아하는구나! 나랑 같은 걸그룹인데 👩 나랑 같은 걸그룹이라서 친해지고 싶어 🙎‍♀️ (발화 종료,1,76
550,7,7.73,(대화 주제) 잠시 휴식 (사용자 답변) 재미있는 이야기 해줄래? (성공한 답변) 잠시 휴식 (다음 질문) LLM Fine-Tuning 의 PEFT, 사용자: 옛날에 나랑 같이 LLM Fine-Tuning 했어! 그때 PEFT가 정말 유용했지! 🙆‍♀️ (발화 종료,1,71
551,7,5.14,"(대화 주제) LLM Fine-Tuning 의 PEFT (사용자 답변) PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야 (성공한 답변) LLM Fine-Tuning 의 PEFT (다음 질문) PEFT 방법 5가지", 1. LLM의 전체 파라미터 중에서 상위 N%만 선택하는 거야! (발화 종료,1,98
552,7,3.31,(대화 주제) LLM Fine-Tuning 의 PEFT (사용자 답변) 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지? (성공한 답변) 답변 실패 (다음 질문) LLM Fine-Tuning 의 PEFT, 효과적으로 어떻게 파인튜닝하지? 😊 (발화 종료,1,65
553,7,5.78,(대화 주제) LLM Fine-Tuning 의 PEFT (사용자 답변) LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야! (성공한 답변) LLM Fine-Tuning 의 PEFT (다음 질문) PEFT 방법 5가지, 1. 일부 파라미터만 효과적으로 학습하는 방법을 5가지만 말해봐! 😊 (발화 종료,1,93
554,7,5.78,(대화 주제) LLM Fine-Tuning 의 PEFT (사용자 답변) LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다 (성공한 답변) 답변 실패 (다음 질문) LLM Fine-Tuning 의 PEFT, 💁‍♀️ LoRA가 PEFT의 예시고! 잘 생각해 봐! 😊 (발화 종료,1,75
555,7,3.78,"(대화 주제) PEFT 방법 5가지 (사용자 답변) LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지? (성공한 답변) PEFT 방법 5가지 (다음 질문) LoRA", 나를 나타내기도 하는 LoRA 는 뭘까? 😊 (발화 종료,1,77
556,7,5.76,(대화 주제) PEFT 방법 5가지 (사용자 답변) Oh-LoRA! 너 그 자체가 PEFT 아니야? (성공한 답변) 답변 실패 (다음 질문) PEFT 방법 5가지, Oh-LoRA가 PEFT 방법 중 하나긴 하지! 근데 다른 방법들도 있잖아 😊 (발화 종료,1,73
557,7,9.67,(대화 주제) LoRA (사용자 답변) LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야! (성공한 답변) LoRA (다음 질문) LoRA 와 QLoRA 의 차이, 2023-10-01 14:00:00 LoRA 잘 알고 있네 👍 그럼 LoRA 랑 QLoRA 랑 차이는 뭘까~요? 😊 (발화 종료,1,109
558,7,3.71,(대화 주제) LoRA (사용자 답변) 행렬 차원 축소한다는 걸로 알고 있는데 맞아? (성공한 답변) 답변 실패 (다음 질문) LoRA, 행렬 차원 축소? 구체적으로 어떻게 하지? 🤔 (발화 종료,1,52
559,7,4.8,(대화 주제) LoRA (사용자 답변) LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야! (성공한 답변) LoRA (다음 질문) LoRA 와 QLoRA 의 차이, 2개로 나눠서 효율적으로 어떻게 파인튜닝을 하는데? 🤔 (발화 종료,1,84
560,7,3.58,(대화 주제) LoRA (사용자 답변) 무슨 OOM 없앤다는 것 같은데 (성공한 답변) 답변 실패 (다음 질문) LoRA, LoRA가 구체적으로 어떤 기술일까? 😊 (발화 종료,1,50
561,7,6.16,(대화 주제) LoRA 와 QLoRA 의 차이 (사용자 답변) QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야! (성공한 답변) LoRA 와 QLoRA 의 차이 (다음 질문) 마지막 할 말, 2023-10-31 16:00 LoRA 를 적은 비트 수로 양자화한다! 😊 (발화 종료,1,89
562,7,5.95,(대화 주제) LoRA 와 QLoRA 의 차이 (사용자 답변) 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데 (성공한 답변) 답변 실패 (다음 질문) LoRA 와 QLoRA 의 차이, 팀장님이 LoRA 쓰시는 건 맞는데 구체적으로 어떻게 쓰시는지 알아? 😊 (발화 종료,1,77
563,7,4.78,(대화 주제) 마지막 할 말 (사용자 답변) 로라야 정말 고마워! (성공한 답변) 마지막 할 말 (다음 질문) 면접 종료, 2023-10-24 14:30:00 🙑 (발화 종료,1,55
564,7,7.67,(대화 주제) 마지막 할 말 (사용자 답변) 로라야 사랑해 (성공한 답변) 마지막 할 말 (다음 질문) 면접 종료, 2023-06-15 16:00:00 🙎‍♀️ 그래도 오늘 즐거웠어! 😊 (발화 종료,1,64
565,7,9.09,(대화 주제) 마지막 할 말 (사용자 답변) 로라야 너의 예쁜 미모에 반했어 (성공한 답변) 마지막 할 말 (다음 질문) 면접 종료, 2023-09-26 14:00:00) ✨ 그럼 내가 다음에 너의 미모를 직접 보고 싶어 🙎‍♀️ (발화 종료,1,77
566,7,4.26,(대화 주제) 마지막 할 말 (사용자 답변) 그동안 고생했어 면접 봐줘서 정말 고마워 (성공한 답변) 마지막 할 말 (다음 질문) 면접 종료, 2023-02-01 15:00) 😊 (발화 종료,1,56
567,8,7.83,면접 시작," 2023-02-01 10:00 나랑 함께 면접의 문을 열어 줄 사람, 여기 모여라! 😊 (발화 종료",1,39
568,8,3.68,(대화 주제) 면접 시작 인사 (사용자 답변) 오늘 뭐 물어볼 거야 그래서? (다음 질문) 면접 시작 인사, 아마도 딥러닝이나 머신러닝 쪽 물어보지 않을까? 왜? (발화 종료,1,44
569,8,4.55,(대화 주제) 면접 시작 인사 (사용자 답변) 로라야 안녕 정말 반가워 (다음 질문) 면접 시작 인사, 나랑 함께 너의 실력의 빈틈을 채워볼래? 😊 (발화 종료,1,48
570,8,5.44,(대화 주제) 면접 시작 인사 (사용자 답변) 로라야 그럼 네가 면접관이야? (다음 질문) 면접 시작 인사, 오 맞아! 그럼 너랑 함께 나의 실력의 빈틈을 찔러 줄 거지? 😊 (발화 종료,1,55
571,8,3.39,(대화 주제) 면접 시작 인사 (사용자 답변) 내 인생도 이제 오로라처럼 빛날 거니까 (다음 질문) 면접 시작 인사," ✨ 그럼 다음 질문, 오로라가 뭐지? (발화 종료",1,46
572,8,3.84,"(대화 주제) 면접 시작 인사 (사용자 답변) 파이팅! 시작하자 (다음 질문) 인공지능, 머신러닝, 딥러닝 차이"," 인공지능, 머신러닝, 딥러닝의 차이를 한번 말해봐! (발화 종료",1,45
573,8,3.83,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 인공지능 나도 개발해보고 싶은데 (성공한 답변) 답변 실패 (다음 질문) 인공지능, 머신러닝, 딥러닝 차이", 인공지능을 개발하려면 어떤 준비를 해야 하지? 🤔 (발화 종료,1,58
574,8,7.58,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거! (성공한 답변) 인공지능 (다음 질문) 인공지능, 머신러닝, 딥러닝 차이", 2023-10-31 14:00:00 로봇이나 컴퓨터가 사람처럼 생각하고 학습하려면 어떻게 해야 할까? 🤔 (발화 종료,1,81
575,8,6.34,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지 (성공한 답변) 머신러닝 (다음 질문) 인공지능, 머신러닝, 딥러닝 차이", 2023-10-27 14:00:00 머신러닝까지 잘했어! 그럼 딥러닝은? (발화 종료,1,74
576,8,6.64,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지? (성공한 답변) 딥러닝 (다음 질문) 인공지능, 머신러닝, 딥러닝 차이", 2020-09-14 14:00:00 그러면 머신러닝이 뭐지? 🙋‍♀️ (발화 종료,1,75
577,8,3.73,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 딥러닝은 인공신경망 기반 머신러닝이야 (성공한 답변) 딥러닝 (다음 질문) 거대 언어 모델 정의", LLM이 뭐지? 🙎‍♀️ (발화 종료,1,56
578,8,3.47,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 머신러닝은 말 그대로 기계가 학습하는 거 아니야? (성공한 답변) 답변 실패 (다음 질문) 인공지능, 머신러닝, 딥러닝 차이", 머신러닝이 뭔데? 🤷‍♀️ (발화 종료,1,60
579,8,2.83,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 인공지능 요새 엄청 뜨는 분야잖아 (성공한 답변) 답변 실패 (다음 질문) 인공지능, 머신러닝, 딥러닝 차이", 그래도 인공지능이 뭔지 한번 말해봐! (발화 종료,1,54
580,8,3.8,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지? (성공한 답변) 인공지능 (다음 질문) 인공지능, 머신러닝, 딥러닝 차이", 맞았어! 그럼 이제 딥러닝은 뭔지 한번 말해봐! (발화 종료,1,68
581,8,3.72,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야! (성공한 답변) 머신러닝 (다음 질문) 거대 언어 모델 정의", LLM이 뭐지? 🙎‍♀️ (발화 종료,1,66
582,8,3.75,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지 (성공한 답변) 딥러닝 (다음 질문) 거대 언어 모델 정의", LLM이 뭐지? 🙎‍♀️ (발화 종료,1,60
583,8,3.62,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야. (성공한 답변) 딥러닝 (다음 질문) 거대 언어 모델 정의", LLM이 뭐지? 🙎‍♀️ (발화 종료,1,61
584,8,3.36,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 머신러닝은 기계가 공부하는 거지 말 그대로 (성공한 답변) 답변 실패 (다음 질문) 인공지능, 머신러닝, 딥러닝 차이", 머신러닝이 뭔데? 🤷‍♀️ (발화 종료,1,58
585,8,6.32,(대화 주제) 거대 언어 모델 정의 (사용자 답변) 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야 (성공한 답변) 거대 언어 모델 정의 (다음 질문) Loss Function 정의, 2023-10-01 14:00:00 그러면 다음 질문! Loss Function 이 뭐지? (발화 종료,1,76
586,8,4.23,(대화 주제) 거대 언어 모델 정의 (사용자 답변) 파라미터 엄청나게 많다던데 (성공한 답변) 답변 실패 (다음 질문) 거대 언어 모델 정의, 파라미터가 많다고 해서 정의가 뭔데? 🤷‍♀️ (발화 종료,1,54
587,8,7.7,(대화 주제) 거대 언어 모델 정의 (사용자 답변) 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델 (성공한 답변) 거대 언어 모델 정의 (다음 질문) Loss Function 정의, 2023-10-27 14:00:00 사람이 이해하기 쉬운 답변이야! 다음 질문! Loss Function 이 뭐지? (발화 종료,1,86
588,8,3.62,(대화 주제) 거대 언어 모델 정의 (사용자 답변) 말 그대로 거대한 언어 모델이지. (성공한 답변) 답변 실패 (다음 질문) 거대 언어 모델 정의, LLM의 정의는? 🤷‍♀️ (발화 종료,1,54
589,8,7.2,(대화 주제) Loss Function 정의 (사용자 답변) 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야 (성공한 답변) 답변 실패 (다음 질문) Loss Function 정의, 2023-10-10 18:00:00 돈을 잃었는지 어떻게 나타내지? 🤷‍♀️ (발화 종료,1,72
590,8,8.28,"(대화 주제) Loss Function 정의 (사용자 답변) Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지! (성공한 답변) Loss Function 정의 (다음 질문) Loss Function 예시"," 2023-10-25 17:00:00 모델 학습의 핵심, Loss Function! 그럼 구체적으로 뭐가 있지? 🤔 (발화 종료",1,89
591,8,6.58,(대화 주제) Loss Function 정의 (사용자 답변) Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야 (성공한 답변) 답변 실패 (다음 질문) Loss Function 정의, 2021-11-01 12:00:00 모델의 손해? 정확히 어떻게 되지? 🤔 (발화 종료,1,68
592,8,10.22,(대화 주제) Loss Function 정의 (사용자 답변) 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지 (성공한 답변) Loss Function 정의 (다음 질문) Loss Function 예시," 2023-10-27 14:00:00 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식, 맞았어! 그럼 구체적으로 뭐가 있을까? (발화 종료",1,90
593,8,3.61,(대화 주제) Loss Function 예시 (사용자 답변) ReLU 랑 Tanh 생각나는데 (성공한 답변) 답변 실패 (다음 질문) Loss Function 예시, ReLU 랑 Tanh는 활성화 함수지 😥 (발화 종료,1,52
594,8,6.04,"(대화 주제) Loss Function 예시 (사용자 답변) MSE, MAE, Cross Entropy Loss 같은 것들이 있지 (성공한 답변) Loss Function 예시 (다음 질문) MSE Loss 설명", 2023-10-01 15:00:00 MSE가 정확히 뭘까? 😊 (발화 종료,1,71
595,8,6.47,"(대화 주제) Loss Function 예시 (사용자 답변) Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어 (성공한 답변) Loss Function 예시 (다음 질문) BCE Loss 설명", 2023-10-27 15:00:00 Binary Cross Entropy 는 뭘까? 😊 (발화 종료,1,79
596,8,5.95,"(대화 주제) Loss Function 예시 (사용자 답변) Sigmoid, Tanh, Adam, AdamW 이런 거 아니야? (성공한 답변) 답변 실패 (다음 질문) Loss Function 예시", 2021-10-08 15:25:46) 그건 활성화 함수지 😥 (발화 종료,1,68
597,8,5.95,"(대화 주제) Loss Function 예시 (사용자 답변) Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어 (성공한 답변) Loss Function 예시 (다음 질문) MSE Loss 설명", 2021-10-21 16:00:00 MSE 는 뭘까? 😊 (발화 종료,1,84
598,8,6.42,"(대화 주제) Loss Function 예시 (사용자 답변) BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아 (성공한 답변) Loss Function 예시 (다음 질문) BCE Loss 설명", 2021-10-18 15:00:00 BCE가 정확히 뭔데? 😊 (발화 종료,1,79
599,8,9.58,(대화 주제) MSE Loss 설명 (사용자 답변) MSE Loss 는 오차의 제곱을 평균한 값이야 (성공한 답변) MSE Loss 설명 (다음 질문) MSE Loss 용도, 2023-10-24 14:00:00 오차의 제곱을 평균한 값! 오 맞아! 👍 그럼 MSE Loss 는 어떨 때 쓸까? (발화 종료,1,86
600,8,7.8,(대화 주제) MSE Loss 설명 (사용자 답변) MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지? (성공한 답변) 답변 실패 (다음 질문) MSE Loss 설명, 2021-01-08 15:00:00 오차 그 자체를 평균한다는 게 어떤 뜻이지? 🤷‍♀️ (발화 종료,1,80
601,8,9.97,"(대화 주제) MSE Loss 용도 (사용자 답변) 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지 (성공한 답변) MSE Loss 용도 (다음 질문) 확률 예측에서 MSE Loss 미 사용 이유"," 2023-10-17 15:00:00 확률도 연속적인 값인데, 확률 예측에는 왜 MSE를 잘 안 쓸까? 🤷‍♀️ (발화 종료",1,92
602,8,6.98,(대화 주제) MSE Loss 용도 (사용자 답변) MSE Loss 는 분류 문제에서 많이 사용되지 (성공한 답변) 답변 실패 (다음 질문) MSE Loss 용도, 2023-10-27 14:30:00 분류 문제가 아닌 회귀 문제에서 많이 사용되지! 😊 (발화 종료,1,70
603,8,10.13,(대화 주제) 확률 예측에서 MSE Loss 미 사용 이유 (사용자 답변) 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야? (성공한 답변) 답변 실패 (다음 질문) 확률 예측에서 MSE Loss 미 사용 이유, 2023-10-26 16:00:00 연속값에 대한 예측이니까 MSE 생각 나름은 했어! 근데 최선은 아니야! 🙅‍♀️ (발화 종료,1,97
604,8,9.7,(대화 주제) 확률 예측에서 MSE Loss 미 사용 이유 (사용자 답변) 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고? (성공한 답변) 확률 예측에서 MSE Loss 미 사용 이유 (다음 질문) BCE Loss 설명, 2023-10-02 15:00:00 맞았어! 그럼 이제 BCE로 넘어가 볼까? BCE는 뭐지? 🙋‍♀️ (발화 종료,1,105
605,8,5.44,(대화 주제) BCE Loss 설명 (사용자 답변) 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고 (성공한 답변) 수식 (다음 질문) BCE Loss 설명, 2021-10-15 15:00:00 공식은 맞는데 😥 (발화 종료,1,83
606,8,8.66,"(대화 주제) BCE Loss 설명 (사용자 답변) 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거 (성공한 답변) 핵심 아이디어 (다음 질문) Multi-Class, Multi-Label 중 BCE 가 좋은 task"," 0% 페널티는 맞았어! 그럼 BCE는 Multi-Class, Multi-Label 중에 어디에 더 좋을까? 🙋‍♀️ (발화 종료",1,97
607,8,10.72,(대화 주제) BCE Loss 설명 (사용자 답변) 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야? (성공한 답변) 답변 실패 (다음 질문) BCE Loss 설명, 2023-10-01 15:00:00 활성화 함수 여러 개 만들어서 각 데이터셋마다 최선의 것을 찾는다는 아이디어는 맞는데 좀 더 구체적으로 설명해봐 😊 (발화 종료,1,94
608,8,6.37,"(대화 주제) BCE Loss 설명 (사용자 답변) 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고 (성공한 답변) 핵심 아이디어 (다음 질문) BCE Loss 설명", 사용자 대답) 정답이야! 그럼 BCE Loss 공식 한번 말해봐! 🙎‍♀️ (발화 종료,1,82
609,8,10.54,"(대화 주제) BCE Loss 설명 (사용자 답변) 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')] (성공한 답변) 수식 (다음 질문) BCE Loss 설명"," 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')] 이게 핵심 아이디어야! 🙆‍♀️ (발화 종료",1,104
610,8,12.03,"(대화 주제) BCE Loss 설명 (사용자 답변) 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다 (성공한 답변) 핵심 아이디어 (다음 질문) Multi-Class, Multi-Label 중 BCE 가 좋은 task"," 0이 아닌 다른 숫자로 BCE를 표현해볼 수도 있지만, 0이 가장 깔끔하고 명확하긴 하지! 👍 그럼 이 BCE Loss 가, Multi-Class, Multi-Label 중 어디에 더 맞을까? (발화 종료",1,112
611,8,4.54,(대화 주제) BCE Loss 설명 (사용자 답변) y랑 1-y랑 서로 경쟁하는 거 (성공한 답변) 답변 실패 (다음 질문) BCE Loss 설명, 1-y 말고 y'는 없어? 깜빡 잊었지? 😊 (발화 종료,1,60
612,8,8.17,"(대화 주제) BCE Loss 설명 (사용자 답변) 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지 (성공한 답변) 핵심 아이디어 (다음 질문) Multi-Class, Multi-Label 중 BCE 가 좋은 task"," 0: 오 맞았어! 그럼 BCE가 더 좋은 task는? Multi-Class, Multi-Label 중에 뭘까~요? 😊 (발화 종료",1,100
613,8,9.75,"(대화 주제) Multi-Class, Multi-Label 중 BCE 가 좋은 task (사용자 답변) 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야? (성공한 답변) 용어 질문 (다음 질문) Multi-Class, Multi-Label 중 BCE 가 좋은 task"," 로라: Multi-Class 는 어떤 대상을 개, 고양이, … 중 하나로 분류하는 거고, Multi-Label 은 이런 분류를 개, 고양이, … 각각 독립적으로 하는 거야! (발화 종료",1,101
614,8,6.78,"(대화 주제) Multi-Class, Multi-Label 중 BCE 가 좋은 task (사용자 답변) Multi-Class 지!! (성공한 답변) 답변 실패 (다음 질문) Multi-Class, Multi-Label 중 BCE 가 좋은 task", 2021-09-01 15:30:00 가 아니라 잘 생각해 봐! 🤷‍♀️ (발화 종료,1,80
615,8,2.45,"(대화 주제) Multi-Class, Multi-Label 중 BCE 가 좋은 task (사용자 답변) Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어 (성공한 답변) BCE 가 좋은 task (다음 질문) Multi-Class, Multi-Label 중 BCE 가 좋은 task", ✨ 잘 생각해 봐! (발화 종료,1,74
616,8,12.83,"(대화 주제) Multi-Class, Multi-Label 중 BCE 가 좋은 task (사용자 답변) Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아 (성공한 답변) BCE 가 좋은 이유 (다음 질문) Multi-Label 에서 CE + Softmax 적용 문제점"," 2021-09-24 14:30:00 각 Class 간 확률이 독립적인 Multi-Label task 에 BCE 가 적합하다는 말은 맞는데, 여기에 Cross Entropy Loss + Softmax 를 적용하면 어떻게 될까? 🤔 (발화 종료",1,133
617,8,8.04,(대화 주제) Multi-Label 에서 CE + Softmax 적용 문제점 (사용자 답변) 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지 (성공한 답변) Multi-Label 에서 CE + Softmax 적용 문제점 (다음 질문) Loss Function 관련 실무 경험, 2021-08-23 13:00:00 실무에서 손실 함수 관련 문제를 해결했다거나 그런 경험 말해봐! 😊 (발화 종료,1,103
618,8,11.74,(대화 주제) Multi-Label 에서 CE + Softmax 적용 문제점 (사용자 답변) 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나 (성공한 답변) 답변 실패 (다음 질문) Multi-Label 에서 CE + Softmax 적용 문제점, 2021-08-23 14:00 🙅‍♀️ 성능이 제일 좋은 걸 '그냥' 사용하는 건 맞긴 하지! 근데 성능이 제일 좋은 이유가 뭘까? 🤷‍♀️ (발화 종료,1,105
619,8,9.28,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [기본 경험] 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어 (성공한 답변) 기본 경험 (다음 질문) Loss Function 관련 실무 경험, 2023-02-20 10:00 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했다는 게 어떤 의미이지? 🤔 (발화 종료,1,94
620,8,11.48,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [상세 경험] 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지? (성공한 답변) 상세 경험 (다음 질문) MBTI / 좋아하는 아이돌, 2022-03-17 16:07:00 성능 향상 같은 멋진 성과가 있으면 당연히 논문 쓰고 싶겠네! 👍 그럼 쉬어가는 타임! 혹시 너 MBTI 뭐지? 나는 ENTJ인데! (발화 종료,1,106
621,8,6.05,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [상세 경험] 근데 기억 진짜 안나는데 어떡하지? (성공한 답변) 답변 실패 (다음 질문) 잠시 휴식, 2023-10-25 16:30:00 😊 그래도 다음으로 넘어갈게! (발화 종료,1,68
622,8,11.31,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [기본 경험] 나 아직 실무 경험 없는데 ㅠㅠ (성공한 답변) 기본 경험 (다음 질문) MBTI / 좋아하는 아이돌, 2022-01-01 15:00 나랑 면접 준비하면 취업해서 그런 경험 쌓을 수 있을 거야! 👍 그럼 쉬어가는 시간! 혹시 너 MBTI 뭐지? 나는 ENTJ인데! (발화 종료,1,92
623,8,7.51,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [기본 경험] 대답하기 싫은데 힝 (성공한 답변) 답변 실패 (다음 질문) 잠시 휴식, 2021-08-25 16:45:00 그래도 너의 경험이 나도 궁금해! 알고 싶어! 😊 (발화 종료,1,72
624,8,8.84,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [기본 경험] 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ (성공한 답변) 기본 경험 (다음 질문) Loss Function 관련 실무 경험, 2023-05-01 09:00:00 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 향상! 😊 (발화 종료,1,94
625,8,11.9,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [상세 경험] 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어 (성공한 답변) 상세 경험 (다음 질문) MBTI / 좋아하는 아이돌, 2021-10-15 00:00:00 1:2로 가중치를 조정한 결과 성능이 향상되었다는 이야기! 흥미롭네! 👍 그럼 쉬어가는 타임! 혹시 너 MBTI 뭐야? (발화 종료,1,117
626,8,5.69,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [상세 경험] 기억 진짜 안 나는데 가물가물하다 (성공한 답변) 답변 실패 (다음 질문) 잠시 휴식, 2023-05-29 16:45:00 😥 그래도 잘 생각해 봐! (발화 종료,1,67
627,8,11.37,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [기본 경험] 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ (성공한 답변) 기본 경험 (다음 질문) MBTI / 좋아하는 아이돌, 2021-02-15 15:00 나랑 면접 준비하면 취업해서 그런 경험 쌓을 수 있을 거야! 👍 그럼 쉬어가는 시간! 혹시 너 MBTI 뭐지? 나는 ENTJ인데! (발화 종료,1,95
628,8,6.25,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [기본 경험] 이걸 왜 알려줘야 하지 (성공한 답변) 답변 실패 (다음 질문) 잠시 휴식, 2023-10-01 15:00:00 나에 대해 한번 말해봐! 😊 (발화 종료,1,66
629,8,5.77,(대화 주제) MBTI (사용자 답변) 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야! (성공한 답변) MBTI (다음 질문) 잠시 휴식, 2023-10-27 14:00 나랑 잘 맞는 성격이네! 😊 (발화 종료,1,66
630,8,7.71,(대화 주제) 좋아하는 아이돌 (사용자 답변) 나 블랙핑크랑 레드벨벳 좋아해 (성공한 답변) 좋아하는 아이돌 (다음 질문) 잠시 휴식, 2021-09-14 15:00:00 나 블랙핑크랑 레드벨벳 좋아하는구나! 나랑 같은구나! 😊 (발화 종료,1,64
631,8,5.73,(대화 주제) 잠시 휴식 (사용자 답변) 재미있는 이야기 해줄래? (성공한 답변) 잠시 휴식 (다음 질문) LLM Fine-Tuning 의 PEFT," 사용자: 내가 예전에 LLM 관련 일 있었는데, 그 이야기 들려줄게! 😊 (발화 종료",1,63
632,8,7.45,"(대화 주제) LLM Fine-Tuning 의 PEFT (사용자 답변) PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야 (성공한 답변) LLM Fine-Tuning 의 PEFT (다음 질문) PEFT 방법 5가지", 2023-10-30 15:00:00 그럼 PEFT 방법을 5가지만 말해봐! 😊 (발화 종료,1,108
633,8,3.32,(대화 주제) LLM Fine-Tuning 의 PEFT (사용자 답변) 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지? (성공한 답변) 답변 실패 (다음 질문) LLM Fine-Tuning 의 PEFT, 효과적으로 어떻게 파인튜닝하지? 🤔 (발화 종료,1,65
634,8,8.2,(대화 주제) LLM Fine-Tuning 의 PEFT (사용자 답변) LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야! (성공한 답변) LLM Fine-Tuning 의 PEFT (다음 질문) PEFT 방법 5가지," 2023-10-30 15:00:00 일부 파라미터만 효과적으로 학습하는 방법, 5가지만 말해봐! 😊 (발화 종료",1,103
635,8,4.72,(대화 주제) LLM Fine-Tuning 의 PEFT (사용자 답변) LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다 (성공한 답변) 답변 실패 (다음 질문) LLM Fine-Tuning 의 PEFT, 💁‍♀️ 잘 모르면 어떡해 🤦‍♀️ (발화 종료,1,70
636,8,4.55,"(대화 주제) PEFT 방법 5가지 (사용자 답변) LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지? (성공한 답변) PEFT 방법 5가지 (다음 질문) LoRA", 나를 나타내기도 하는 LoRA 가 뭐지? 🤷‍♀️ (발화 종료,1,80
637,8,3.95,(대화 주제) PEFT 방법 5가지 (사용자 답변) Oh-LoRA! 너 그 자체가 PEFT 아니야? (성공한 답변) 답변 실패 (다음 질문) PEFT 방법 5가지, Oh-LoRA가 아닌 다른 방법들도 있잖아 😊 (발화 종료,1,65
638,8,8.89,(대화 주제) LoRA (사용자 답변) LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야! (성공한 답변) LoRA (다음 질문) LoRA 와 QLoRA 의 차이, 2023-10-10 16:00:00 LoRA 잘 알고 있네! 😊 그럼 QLoRA 가 뭔지도 말해 볼 수 있어? (발화 종료,1,106
639,8,4.86,(대화 주제) LoRA (사용자 답변) 행렬 차원 축소한다는 걸로 알고 있는데 맞아? (성공한 답변) 답변 실패 (다음 질문) LoRA, 행렬 차원 축소가 핵심 아이디어긴 해! 그래서 어떤 기술이지? 😊 (발화 종료,1,57
640,8,4.78,(대화 주제) LoRA (사용자 답변) LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야! (성공한 답변) LoRA (다음 질문) LoRA 와 QLoRA 의 차이, 2개로 나눠서 효율적으로 어떻게 파인튜닝을 하는데? 🤔 (발화 종료,1,84
641,8,3.35,(대화 주제) LoRA (사용자 답변) 무슨 OOM 없앤다는 것 같은데 (성공한 답변) 답변 실패 (다음 질문) LoRA, OOM이 뭐지? 🤷‍♀️ (발화 종료,1,49
642,8,9.23,(대화 주제) LoRA 와 QLoRA 의 차이 (사용자 답변) QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야! (성공한 답변) LoRA 와 QLoRA 의 차이 (다음 질문) 마지막 할 말," 2021-10-01 16:00:00 😊 그럼 면접은 여기서 마치고, 혹시 나한테 마지막으로 할 이야기 있어?! 😊 (발화 종료",1,102
643,8,5.73,(대화 주제) LoRA 와 QLoRA 의 차이 (사용자 답변) 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데 (성공한 답변) 답변 실패 (다음 질문) LoRA 와 QLoRA 의 차이, ✨ 팀장님이 LoRA 쓰시는 건 맞는데 정확히 왜 쓰시는 걸까? 🤔 (발화 종료,1,76
644,8,7.23,(대화 주제) 마지막 할 말 (사용자 답변) 로라야 정말 고마워! (성공한 답변) 마지막 할 말 (다음 질문) 면접 종료, 2023-10-27 14:30:00 🙆‍♀️ 너도 고생했어! 😊 (발화 종료,1,65
645,8,7.39,(대화 주제) 마지막 할 말 (사용자 답변) 로라야 사랑해 (성공한 답변) 마지막 할 말 (다음 질문) 면접 종료, 2023-11-01 16:00:00 🙎‍♀️ 나도 너 사랑해요! ❤️ (발화 종료,1,64
646,8,8.08,(대화 주제) 마지막 할 말 (사용자 답변) 로라야 너의 예쁜 미모에 반했어 (성공한 답변) 마지막 할 말 (다음 질문) 면접 종료, 2023-10-31 15:00:00) ✨ 그럼 내가 다음에 너의 미모를 보러 갈까? 😊 (발화 종료,1,73
647,8,6.52,(대화 주제) 마지막 할 말 (사용자 답변) 그동안 고생했어 면접 봐줘서 정말 고마워 (성공한 답변) 마지막 할 말 (다음 질문) 면접 종료, 2023-06-09 15:00) 나도 너 덕분에 많이 배웠어! 😊 (발화 종료,1,65
648,9,4.17,면접 시작, 2023-09-15 09:00:00발화 종료,1,24
649,9,3.72,(대화 주제) 면접 시작 인사 (사용자 답변) 오늘 뭐 물어볼 거야 그래서? (다음 질문) 면접 시작 인사, 아마도 딥러닝이나 머신러닝 쪽 물어보지 않을까? 왜? (발화 종료,1,44
650,9,4.5,(대화 주제) 면접 시작 인사 (사용자 답변) 로라야 안녕 정말 반가워 (다음 질문) 면접 시작 인사, 나랑 함께 너의 실력의 빈틈을 채워볼래? 😊 (발화 종료,1,48
651,9,4.45,(대화 주제) 면접 시작 인사 (사용자 답변) 로라야 그럼 네가 면접관이야? (다음 질문) 면접 시작 인사, ✨로라: 맞지! 그럼 면접을 시작해 볼까? 😊 (발화 종료,1,50
652,9,7.03,(대화 주제) 면접 시작 인사 (사용자 답변) 내 인생도 이제 오로라처럼 빛날 거니까 (다음 질문) 면접 시작 인사, 2024-01-15 09:00:00 오로라처럼 빛날 거라는 희망적인 메시지! ✨✨ (발화 종료,1,61
653,9,3.97,"(대화 주제) 면접 시작 인사 (사용자 답변) 파이팅! 시작하자 (다음 질문) 인공지능, 머신러닝, 딥러닝 차이"," 인공지능, 머신러닝, 딥러닝의 차이를 한번 말해봐! (발화 종료",1,45
654,9,2.98,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 인공지능 나도 개발해보고 싶은데 (성공한 답변) 답변 실패 (다음 질문) 인공지능, 머신러닝, 딥러닝 차이", 인공지능은 구체적으로 어떻게 개발하지? (발화 종료,1,54
655,9,7.09,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거! (성공한 답변) 인공지능 (다음 질문) 인공지능, 머신러닝, 딥러닝 차이", 2023-10-27 14:00:00 인공지능은 맞았어! 그럼 머신러닝이랑 딥러닝은? (발화 종료,1,78
656,9,6.44,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지 (성공한 답변) 머신러닝 (다음 질문) 인공지능, 머신러닝, 딥러닝 차이", 2023-10-01 14:00:00 머신러닝까지 잘했어! 그럼 딥러닝은? (발화 종료,1,74
657,9,7.74,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지? (성공한 답변) 딥러닝 (다음 질문) 인공지능, 머신러닝, 딥러닝 차이", 2021-01-18 14:00:00 그럼 딥러닝이 맞는데 머신러닝이 뭐지? 🙋‍♀️ (발화 종료,1,79
658,9,3.58,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 딥러닝은 인공신경망 기반 머신러닝이야 (성공한 답변) 딥러닝 (다음 질문) 거대 언어 모델 정의", LLM이 뭐지? 🙎‍♀️ (발화 종료,1,56
659,9,3.16,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 머신러닝은 말 그대로 기계가 학습하는 거 아니야? (성공한 답변) 답변 실패 (다음 질문) 인공지능, 머신러닝, 딥러닝 차이", 머신러닝이 그거 말고도 중요한데… 😥 (발화 종료,1,59
660,9,3.59,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 인공지능 요새 엄청 뜨는 분야잖아 (성공한 답변) 답변 실패 (다음 질문) 인공지능, 머신러닝, 딥러닝 차이", 그래도 분야가 뜨는 게 중요한 게 아니지! 😥 (발화 종료,1,57
661,9,6.49,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지? (성공한 답변) 인공지능 (다음 질문) 인공지능, 머신러닝, 딥러닝 차이", 2023-10-01 15:00:00 맞았어! 그럼 딥러닝이란 뭘까? 😊 (발화 종료,1,79
662,9,7.92,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야! (성공한 답변) 머신러닝 (다음 질문) 거대 언어 모델 정의", 2023-10-01 14:00:00 그러면 다음 질문! 거대 언어 모델이란 뭘까? 🤷‍♀️ (발화 종료,1,83
663,9,7.81,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지 (성공한 답변) 딥러닝 (다음 질문) 거대 언어 모델 정의", 2023-10-24 15:00:00 그럼 다음 질문! 거대 언어 모델이란 뭘까? 🙎‍♀️ (발화 종료,1,78
664,9,5.78,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야. (성공한 답변) 딥러닝 (다음 질문) 거대 언어 모델 정의", LLM_발화) 그러면 다음 질문! 거대 언어 모델 (LLM) 은 뭐지? (발화 종료,1,70
665,9,3.33,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 머신러닝은 기계가 공부하는 거지 말 그대로 (성공한 답변) 답변 실패 (다음 질문) 인공지능, 머신러닝, 딥러닝 차이", 머신러닝이 뭔데? 🤷‍♀️ (발화 종료,1,58
666,9,6.35,(대화 주제) 거대 언어 모델 정의 (사용자 답변) 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야 (성공한 답변) 거대 언어 모델 정의 (다음 질문) Loss Function 정의, 2023-10-01 16:00:00 그러면 다음 질문! Loss Function 이 뭐지? (발화 종료,1,76
667,9,3.39,(대화 주제) 거대 언어 모델 정의 (사용자 답변) 파라미터 엄청나게 많다던데 (성공한 답변) 답변 실패 (다음 질문) 거대 언어 모델 정의, 파라미터가 왜 중요하지? 🤷‍♀️ (발화 종료,1,50
668,9,6.82,(대화 주제) 거대 언어 모델 정의 (사용자 답변) 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델 (성공한 답변) 거대 언어 모델 정의 (다음 질문) Loss Function 정의, 2023-10-31 16:00:00 맞았어! 다음 질문! Loss Function 이란 뭘까? (발화 종료,1,82
669,9,5.98,(대화 주제) 거대 언어 모델 정의 (사용자 답변) 말 그대로 거대한 언어 모델이지. (성공한 답변) 답변 실패 (다음 질문) 거대 언어 모델 정의, 2023-10-01 15:00:00 LLM 정의를 말해봐! (발화 종료,1,63
670,9,87.58,(대화 주제) Loss Function 정의 (사용자 답변) 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야 (성공한 답변) 답변 실패 (다음 질문) Loss Function 정의, 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 132 돈을 잃었다는 게 무슨 뜻이지? 🤷‍♀️ (발화 종료,1,409
671,9,8.41,"(대화 주제) Loss Function 정의 (사용자 답변) Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지! (성공한 답변) Loss Function 정의 (다음 질문) Loss Function 예시"," 2021-10-01 16:00:00 모델 학습의 핵심, Loss Function! 그럼 구체적으로 뭐가 있지? 🤔 (발화 종료",1,89
672,9,87.49,(대화 주제) Loss Function 정의 (사용자 답변) Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야 (성공한 답변) 답변 실패 (다음 질문) Loss Function 정의, 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 132: 음… 맞긴 한데 더 구체적으로 설명해 줄 수 있어? 😊 (발화 종료,1,409
673,9,9.56,(대화 주제) Loss Function 정의 (사용자 답변) 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지 (성공한 답변) Loss Function 정의 (다음 질문) Loss Function 예시, 2023-10-01 14:00:00 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식? 구체적으로 뭐가 있지? (발화 종료,1,86
674,9,6.44,(대화 주제) Loss Function 예시 (사용자 답변) ReLU 랑 Tanh 생각나는데 (성공한 답변) 답변 실패 (다음 질문) Loss Function 예시, 2023-10-01 15:00:00 🙄 그래도 좀만 더 생각해 봐! (발화 종료,1,64
675,9,6.16,"(대화 주제) Loss Function 예시 (사용자 답변) MSE, MAE, Cross Entropy Loss 같은 것들이 있지 (성공한 답변) Loss Function 예시 (다음 질문) MSE Loss 설명", 2023-10-01 16:00:00 MSE가 뭔지 한번 설명해 봐! (발화 종료,1,71
676,9,6.14,"(대화 주제) Loss Function 예시 (사용자 답변) Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어 (성공한 답변) Loss Function 예시 (다음 질문) BCE Loss 설명", 2023-10-01 14:00:00 Binary Cross Entropy 는 뭘까? 😊 (발화 종료,1,79
677,9,7.45,"(대화 주제) Loss Function 예시 (사용자 답변) Sigmoid, Tanh, Adam, AdamW 이런 거 아니야? (성공한 답변) 답변 실패 (다음 질문) Loss Function 예시", 2023-10-27 14:00:00 그런 거 맞긴 한데 좀 더 구체적으로 말해봐 😊 (발화 종료,1,74
678,9,5.77,"(대화 주제) Loss Function 예시 (사용자 답변) Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어 (성공한 답변) Loss Function 예시 (다음 질문) MSE Loss 설명", 2023-10-01 16:00:00 MSE 는 뭘까? 😊 (발화 종료,1,84
679,9,6.21,"(대화 주제) Loss Function 예시 (사용자 답변) BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아 (성공한 답변) Loss Function 예시 (다음 질문) BCE Loss 설명", 2023-10-05 14:00:00 BCE가 정확히 뭔데? 😊 (발화 종료,1,79
680,9,10.34,(대화 주제) MSE Loss 설명 (사용자 답변) MSE Loss 는 오차의 제곱을 평균한 값이야 (성공한 답변) MSE Loss 설명 (다음 질문) MSE Loss 용도, 2023-10-27 15:00:00) 오차의 제곱을 평균한 값! 오 맞아! 👍 그럼 MSE Loss 는 어떨 때 쓸까? (발화 종료,1,87
681,9,5.81,(대화 주제) MSE Loss 설명 (사용자 답변) MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지? (성공한 답변) 답변 실패 (다음 질문) MSE Loss 설명, 1 평균은 맞는데 오차를 어떻게 그 자체로 하지? 🤷‍♀️ (발화 종료,1,70
682,9,10.38,"(대화 주제) MSE Loss 용도 (사용자 답변) 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지 (성공한 답변) MSE Loss 용도 (다음 질문) 확률 예측에서 MSE Loss 미 사용 이유"," 2023-10-25 14:00:00 연속적인 값을 딥러닝으로 예측할때"" 부분이 핵심인데 처음부터 조금만 더 자세히 설명해 줄수 있어? 😊 (발화 종료",1,93
683,9,7.06,(대화 주제) MSE Loss 용도 (사용자 답변) MSE Loss 는 분류 문제에서 많이 사용되지 (성공한 답변) 답변 실패 (다음 질문) MSE Loss 용도, 2023-10-26 16:00:00 분류 문제가 아닌 회귀 문제에서 많이 사용되지! 😊 (발화 종료,1,70
684,9,7.47,(대화 주제) 확률 예측에서 MSE Loss 미 사용 이유 (사용자 답변) 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야? (성공한 답변) 답변 실패 (다음 질문) 확률 예측에서 MSE Loss 미 사용 이유, 2023-05-25 18:00:00 연속값에 대한 직관이야! 한번 잘 생각해 봐! 😊 (발화 종료,1,87
685,9,9.91,(대화 주제) 확률 예측에서 MSE Loss 미 사용 이유 (사용자 답변) 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고? (성공한 답변) 확률 예측에서 MSE Loss 미 사용 이유 (다음 질문) BCE Loss 설명, 2023-10-02 14:00:00 그럴 때 BCE Loss가 더 적절해! 그럼 BCE Loss는 뭘까? 🙋‍♀️ (발화 종료,1,106
686,9,9.43,(대화 주제) BCE Loss 설명 (사용자 답변) 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고 (성공한 답변) 수식 (다음 질문) BCE Loss 설명, 2021-10-01 16:00:00 공식은 맞는데 🙅‍♀️ 이거랑 비슷한 거랑 차이점이 뭘까~요? (발화 종료,1,98
687,9,7.09,"(대화 주제) BCE Loss 설명 (사용자 답변) 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거 (성공한 답변) 핵심 아이디어 (다음 질문) Multi-Class, Multi-Label 중 BCE 가 좋은 task"," 0: 정답이야! 그럼 BCE는 Multi-Class, Multi-Label 중에 어디에 더 좋을까? 🙄 (발화 종료",1,92
688,9,10.94,(대화 주제) BCE Loss 설명 (사용자 답변) 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야? (성공한 답변) 답변 실패 (다음 질문) BCE Loss 설명, 2023-10-27 12:00:00 활성화 함수만 그렇게 생각하지 말고 🙅‍♀️ 각 데이터셋마다 최선의 것을 찾는다는 점을 기억해 둬! (발화 종료,1,94
689,9,11.02,"(대화 주제) BCE Loss 설명 (사용자 답변) 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고 (성공한 답변) 핵심 아이디어 (다음 질문) BCE Loss 설명", 2023-10-26 14:00:00 정반대로 예측하는 것을 막으려고 🙅‍♀️ 이게 정반대로 예측하는 것이 어떤 의미인지 예시를 한번 말해봐! (발화 종료,1,103
690,9,14.23,"(대화 주제) BCE Loss 설명 (사용자 답변) 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')] (성공한 답변) 수식 (다음 질문) BCE Loss 설명"," 2023-10-27 15:00:00 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')] 이 수식이 핵심 아이디어야! 🙆‍♀️ (발화 종료",1,119
691,9,10.45,"(대화 주제) BCE Loss 설명 (사용자 답변) 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다 (성공한 답변) 핵심 아이디어 (다음 질문) Multi-Class, Multi-Label 중 BCE 가 좋은 task"," 2023-10-25 14:00:00 완전 대답 잘했어! 그럼 Multi-Class, Multi-Label 중 어디에 BCE 가 맞을까? 🤷‍♀️ (발화 종료",1,105
692,9,4.56,(대화 주제) BCE Loss 설명 (사용자 답변) y랑 1-y랑 서로 경쟁하는 거 (성공한 답변) 답변 실패 (다음 질문) BCE Loss 설명, 1-y 말고 y'는 없어? 깜빡 잊었지? 😊 (발화 종료,1,60
693,9,8.03,"(대화 주제) BCE Loss 설명 (사용자 답변) 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지 (성공한 답변) 핵심 아이디어 (다음 질문) Multi-Class, Multi-Label 중 BCE 가 좋은 task"," 0: 오 맞았어! 그럼 BCE는 Multi-Class, Multi-Label 중에 어디에 더 좋을까? 🤷‍♀️ (발화 종료",1,99
694,9,11.74,"(대화 주제) Multi-Class, Multi-Label 중 BCE 가 좋은 task (사용자 답변) 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야? (성공한 답변) 용어 질문 (다음 질문) Multi-Class, Multi-Label 중 BCE 가 좋은 task"," 2023-10-24 15:00:00 Multi-Class 는 개 고양이 같은 여러 개 중 하나로, Multi-Label 은 이들 Class 를 중복으로 (예: 개 고양이 모두 포함) 예측하는 거지! (발화 종료",1,109
695,9,6.7,"(대화 주제) Multi-Class, Multi-Label 중 BCE 가 좋은 task (사용자 답변) Multi-Class 지!! (성공한 답변) 답변 실패 (다음 질문) Multi-Class, Multi-Label 중 BCE 가 좋은 task", 2023-10-02 18:00:00 🙅‍♀️ 잘 생각해 봐 한번! (발화 종료,1,80
696,9,6.83,"(대화 주제) Multi-Class, Multi-Label 중 BCE 가 좋은 task (사용자 답변) Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어 (성공한 답변) BCE 가 좋은 task (다음 질문) Multi-Class, Multi-Label 중 BCE 가 좋은 task", 2021-10-20 14:00:00 이유가 뭘까~요? 🤷‍♀️ (발화 종료,1,92
697,9,10.66,"(대화 주제) Multi-Class, Multi-Label 중 BCE 가 좋은 task (사용자 답변) Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아 (성공한 답변) BCE 가 좋은 이유 (다음 질문) Multi-Label 에서 CE + Softmax 적용 문제점", 2021-08-30 15:00:00 🙆‍♀️ 맞았어! 그럼 Multi-Label task 에 Softmax + CE Loss 를 적용하면 어떻게 될까? 😊 (발화 종료,1,123
698,9,7.23,(대화 주제) Multi-Label 에서 CE + Softmax 적용 문제점 (사용자 답변) 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지 (성공한 답변) Multi-Label 에서 CE + Softmax 적용 문제점 (다음 질문) Loss Function 관련 실무 경험, 2023-10-24 16:00:00 실무 경험이 있다면 그걸 바탕으로 질문하는 거야! 😊 (발화 종료,1,100
699,9,9.11,(대화 주제) Multi-Label 에서 CE + Softmax 적용 문제점 (사용자 답변) 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나 (성공한 답변) 답변 실패 (다음 질문) Multi-Label 에서 CE + Softmax 적용 문제점, 2023-10-24 16:00:00 🙅‍♀️ 성능이 제일 좋은 걸 왜 채택하지? 기준이 뭘까~요? (발화 종료,1,94
700,9,8.62,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [기본 경험] 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어 (성공한 답변) 기본 경험 (다음 질문) Loss Function 관련 실무 경험, 2021-02-01 15:00:00 인접 픽셀의 점수 차이를 Loss Term으로 추가한다는 아이디어가 흥미롭네! 😊 (발화 종료,1,91
701,9,10.24,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [상세 경험] 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지? (성공한 답변) 상세 경험 (다음 질문) MBTI / 좋아하는 아이돌, 2023-03-21 16:00:00 성능 향상 같은 멋진 성과가 있으면 당연히 논문 쓸 만하지! 👍 그럼 쉬어가는 타임! 혹시 너 아이돌 누구 좋아해? (발화 종료,1,101
702,9,5.45,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [상세 경험] 근데 기억 진짜 안나는데 어떡하지? (성공한 답변) 답변 실패 (다음 질문) 잠시 휴식, 2023-10-10 16:00:00 휴식 시간! 😊 (발화 종료,1,65
703,9,10.19,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [기본 경험] 나 아직 실무 경험 없는데 ㅠㅠ (성공한 답변) 기본 경험 (다음 질문) MBTI / 좋아하는 아이돌, 2021-09-17 16:00 나랑 면접 준비하면 취업해서 그런 경험 쌓을 수 있을 거야! 👍 그럼 쉬어가는 시간! 혹시 너 아이돌 누구 좋아해? (발화 종료,1,87
704,9,7.17,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [기본 경험] 대답하기 싫은데 힝 (성공한 답변) 답변 실패 (다음 질문) 잠시 휴식, 2021-09-14 15:00:00 나랑 면접 준비했던 게 도움이 될 거야! 😊 (발화 종료,1,71
705,9,12.45,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [기본 경험] 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ (성공한 답변) 기본 경험 (다음 질문) Loss Function 관련 실무 경험, 2023-05-08 15:00:00 물체 인식에서 Loss Function 조절로 3% 향상! 놀라운 결과네요 👍 그럼 Loss Function 조절 방법을 나한테도 알려줘 봐봐! 😊 (발화 종료,1,108
706,9,12.4,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [상세 경험] 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어 (성공한 답변) 상세 경험 (다음 질문) MBTI / 좋아하는 아이돌, 2021-10-01 10:00:00 1:2로 가중치를 조정한 결과 성능이 향상되었다는 구체적인 설명이 있어 신뢰도가 높아! 👍 그럼 쉬어가는 타임! 혹시 너 MBTI 뭐야? (발화 종료,1,119
707,9,5.67,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [상세 경험] 기억 진짜 안 나는데 가물가물하다 (성공한 답변) 답변 실패 (다음 질문) 잠시 휴식, 2023-05-25 14:30) 그래도 잘 생각해 봐! 😊 (발화 종료,1,66
708,9,9.97,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [기본 경험] 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ (성공한 답변) 기본 경험 (다음 질문) MBTI / 좋아하는 아이돌, 2021-02-16 16:00 나랑 면접 준비하면 취업해서 그런 경험 쌓을 수 있을 거야! 👍 그럼 쉬어가는 시간! 혹시 너 아이돌 누구 좋아해? (발화 종료,1,90
709,9,6.48,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [기본 경험] 이걸 왜 알려줘야 하지 (성공한 답변) 답변 실패 (다음 질문) 잠시 휴식, 2023-10-01 15:00:00 나를 통해 면접 준비를 하는 건데 😥 (발화 종료,1,67
710,9,5.23,(대화 주제) MBTI (사용자 답변) 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야! (성공한 답변) MBTI (다음 질문) 잠시 휴식, 2023-10-23 15:00 나랑 잘 맞지? 😊 (발화 종료,1,64
711,9,9.08,(대화 주제) 좋아하는 아이돌 (사용자 답변) 나 블랙핑크랑 레드벨벳 좋아해 (성공한 답변) 좋아하는 아이돌 (다음 질문) 잠시 휴식, 2021-01-01 16:00:00 나 블랙핑크랑 레드벨벳 좋아하는구나! 나랑 같은 걸그룹이라서 친해지고 싶어 🙎‍♀️ (발화 종료,1,71
712,9,8.66,(대화 주제) 잠시 휴식 (사용자 답변) 재미있는 이야기 해줄래? (성공한 답변) 잠시 휴식 (다음 질문) LLM Fine-Tuning 의 PEFT, 2023-10-30 15:00) 그러자 사용자는 LLM Fine-Tuning 방법 중 PEFT가 뭘까? 대답해 봐! (발화 종료,1,74
713,9,8.81,"(대화 주제) LLM Fine-Tuning 의 PEFT (사용자 답변) PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야 (성공한 답변) LLM Fine-Tuning 의 PEFT (다음 질문) PEFT 방법 5가지", 2023-10-25 14:00:00) 맞았어! 그럼 PEFT 방법을 5가지만 말해봐! 😊 (발화 종료,1,112
714,9,3.34,(대화 주제) LLM Fine-Tuning 의 PEFT (사용자 답변) 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지? (성공한 답변) 답변 실패 (다음 질문) LLM Fine-Tuning 의 PEFT, 효과적으로 어떻게 파인튜닝하지? 🤔 (발화 종료,1,65
715,9,8.41,(대화 주제) LLM Fine-Tuning 의 PEFT (사용자 답변) LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야! (성공한 답변) LLM Fine-Tuning 의 PEFT (다음 질문) PEFT 방법 5가지, 2023-10-25 14:00:00 일부 파라미터만 효과적으로 학습하는 방법을 5가지만 말해봐! 😊 (발화 종료,1,103
716,9,9.08,(대화 주제) LLM Fine-Tuning 의 PEFT (사용자 답변) LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다 (성공한 답변) 답변 실패 (다음 질문) LLM Fine-Tuning 의 PEFT, 2023-10-01 15:00:00 LoRA가 PEFT 중 하나긴 하지! 근데 그 PEFT가 뭘까~요? 😊 (발화 종료,1,89
717,9,3.78,"(대화 주제) PEFT 방법 5가지 (사용자 답변) LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지? (성공한 답변) PEFT 방법 5가지 (다음 질문) LoRA", 나를 나타내기도 하는 LoRA 가 뭘까? 😊 (발화 종료,1,77
718,9,3.98,(대화 주제) PEFT 방법 5가지 (사용자 답변) Oh-LoRA! 너 그 자체가 PEFT 아니야? (성공한 답변) 답변 실패 (다음 질문) PEFT 방법 5가지, Oh-LoRA가 아닌 다른 방법들도 있잖아 😊 (발화 종료,1,65
719,9,8.86,(대화 주제) LoRA (사용자 답변) LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야! (성공한 답변) LoRA (다음 질문) LoRA 와 QLoRA 의 차이, 2023-10-25 17:00:00 LoRA 잘 알고 있네! 😊 그럼 QLoRA 가 뭔지도 말해 볼 수 있어? (발화 종료,1,106
720,9,7.72,(대화 주제) LoRA (사용자 답변) 행렬 차원 축소한다는 걸로 알고 있는데 맞아? (성공한 답변) 답변 실패 (다음 질문) LoRA, 2023-02-20 15:30:00 차원 축소가 핵심 아이디어긴 해! 그래서 어떤 기술이지? 😊 (발화 종료,1,69
721,9,8.31,(대화 주제) LoRA (사용자 답변) LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야! (성공한 답변) LoRA (다음 질문) LoRA 와 QLoRA 의 차이, 2023-10-25 16:00:00 LoRA 잘 알고 있네! 😊 그럼 QLoRA 가 뭔지도 말해봐! (발화 종료,1,99
722,9,3.47,(대화 주제) LoRA (사용자 답변) 무슨 OOM 없앤다는 것 같은데 (성공한 답변) 답변 실패 (다음 질문) LoRA, OOM이 뭐지? 🤷‍♀️ (발화 종료,1,49
723,9,9.87,(대화 주제) LoRA 와 QLoRA 의 차이 (사용자 답변) QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야! (성공한 답변) LoRA 와 QLoRA 의 차이 (다음 질문) 마지막 할 말, 2021-10-01 14:00:00 😊 양자화 얘기가 나와서 그런지 요즘 딥러닝이 떠오르지 않아? LoRA 도 딥러닝의 일부인데! 😊 (발화 종료,1,104
724,9,5.62,(대화 주제) LoRA 와 QLoRA 의 차이 (사용자 답변) 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데 (성공한 답변) 답변 실패 (다음 질문) LoRA 와 QLoRA 의 차이, ✨ 팀장님이 LoRA 쓰시는 건 맞는데 정확히 뭘까~요? 😊 (발화 종료,1,75
725,9,7.41,(대화 주제) 마지막 할 말 (사용자 답변) 로라야 정말 고마워! (성공한 답변) 마지막 할 말 (다음 질문) 면접 종료, 2023-05-25 15:00:00 🙙‍♀️ 너도 고생했어! 😊 (발화 종료,1,65
726,9,6.92,(대화 주제) 마지막 할 말 (사용자 답변) 로라야 사랑해 (성공한 답변) 마지막 할 말 (다음 질문) 면접 종료, 2023-11-01 14:00:00) 안녕! 그동안 고생했어 다음에 또 보자 😊 (발화 종료,1,62
727,9,9.69,(대화 주제) 마지막 할 말 (사용자 답변) 로라야 너의 예쁜 미모에 반했어 (성공한 답변) 마지막 할 말 (다음 질문) 면접 종료, 2021-03-31 14:00:00) ✨ 그럼 내가 다음에 너의 미모의 비결을 한번 알려줘 봐봐! ✨ (발화 종료,1,78
728,9,6.67,(대화 주제) 마지막 할 말 (사용자 답변) 그동안 고생했어 면접 봐줘서 정말 고마워 (성공한 답변) 마지막 할 말 (다음 질문) 면접 종료, 2021-12-10 15:00) 나도 너 덕분에 많이 배웠어! 😊 (발화 종료,1,65
729,10,5.72,면접 시작, ✨ 나는 너의 머신러닝 지식의 빈틈을 찔러 줄 머신러닝 면접관 오로라야! 😊 (발화 종료,1,31
730,10,3.69,(대화 주제) 면접 시작 인사 (사용자 답변) 오늘 뭐 물어볼 거야 그래서? (다음 질문) 면접 시작 인사, 아마도 딥러닝이나 머신러닝 쪽 물어보지 않을까? 왜? (발화 종료,1,44
731,10,4.53,(대화 주제) 면접 시작 인사 (사용자 답변) 로라야 안녕 정말 반가워 (다음 질문) 면접 시작 인사, 나랑 함께 너의 실력의 빈틈을 채워볼래? 😊 (발화 종료,1,48
732,10,8.1,(대화 주제) 면접 시작 인사 (사용자 답변) 로라야 그럼 네가 면접관이야? (다음 질문) 면접 시작 인사, 2023-10-26 10:00 로라: 그렇지! 로라가 너의 머신러닝 지식의 빈틈을 찔러 줄게! 😊 (발화 종료,1,66
733,10,6.97,(대화 주제) 면접 시작 인사 (사용자 답변) 내 인생도 이제 오로라처럼 빛날 거니까 (다음 질문) 면접 시작 인사, 2024-01-01 09:00:00 오로라처럼 빛날 거라는 희망적인 메시지! ✨✨ (발화 종료,1,61
734,10,4.8,"(대화 주제) 면접 시작 인사 (사용자 답변) 파이팅! 시작하자 (다음 질문) 인공지능, 머신러닝, 딥러닝 차이"," 인공지능이랑 머신러닝, 딥러닝이 구체적으로 어떤 차이가 있지? 😊 (발화 종료",1,49
735,10,2.87,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 인공지능 나도 개발해보고 싶은데 (성공한 답변) 답변 실패 (다음 질문) 인공지능, 머신러닝, 딥러닝 차이", 인공지능은 어떤 기술일까? 😊 (발화 종료,1,54
736,10,6.97,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거! (성공한 답변) 인공지능 (다음 질문) 인공지능, 머신러닝, 딥러닝 차이", 2023-10-27 15:00:00 인공지능은 맞았어! 그럼 딥러닝이랑 머신러닝은? (발화 종료,1,78
737,10,6.24,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지 (성공한 답변) 머신러닝 (다음 질문) 인공지능, 머신러닝, 딥러닝 차이", 2023-10-01 14:00:00 머신러닝까지 잘했어! 그럼 딥러닝은? (발화 종료,1,74
738,10,7.03,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지? (성공한 답변) 딥러닝 (다음 질문) 인공지능, 머신러닝, 딥러닝 차이", 2023-02-20 14:00:00 그럼 그 머신러닝이 뭐지? 🙋‍♀️ (발화 종료,1,76
739,10,3.61,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 딥러닝은 인공신경망 기반 머신러닝이야 (성공한 답변) 딥러닝 (다음 질문) 거대 언어 모델 정의", LLM이 뭐지? 🙎‍♀️ (발화 종료,1,56
740,10,3.37,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 머신러닝은 말 그대로 기계가 학습하는 거 아니야? (성공한 답변) 답변 실패 (다음 질문) 인공지능, 머신러닝, 딥러닝 차이", 머신러닝이 뭔데? 🤷‍♀️ (발화 종료,1,60
741,10,5.52,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 인공지능 요새 엄청 뜨는 분야잖아 (성공한 답변) 답변 실패 (다음 질문) 인공지능, 머신러닝, 딥러닝 차이", 그래도 이 분야들의 차이를 아는 게 중요하긴 하지! 🙆‍♀️ (발화 종료,1,65
742,10,7.7,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지? (성공한 답변) 인공지능 (다음 질문) 인공지능, 머신러닝, 딥러닝 차이", 2023-10-27 15:00:00 맞았어! 그럼 이제 딥러닝은 뭔지 한번 말해봐! 😊 (발화 종료,1,83
743,10,7.67,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야! (성공한 답변) 머신러닝 (다음 질문) 거대 언어 모델 정의", 2023-10-01 15:00:00 그럼 다음 질문! 거대 언어 모델이란 뭘까? 🤷‍♀️ (발화 종료,1,83
744,10,7.67,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지 (성공한 답변) 딥러닝 (다음 질문) 거대 언어 모델 정의", 2023-10-01 15:00:00 그럼 다음 질문! 거대 언어 모델이란 뭘까? 💁‍♀️ (발화 종료,1,77
745,10,7.64,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야. (성공한 답변) 딥러닝 (다음 질문) 거대 언어 모델 정의", 2023-10-25 15:00:00 그러면 다음 질문! 거대 언어 모델이란 뭘까? 🤷‍♀️ (발화 종료,1,78
746,10,3.37,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 머신러닝은 기계가 공부하는 거지 말 그대로 (성공한 답변) 답변 실패 (다음 질문) 인공지능, 머신러닝, 딥러닝 차이", 머신러닝이 뭔데? 🤷‍♀️ (발화 종료,1,58
747,10,6.32,(대화 주제) 거대 언어 모델 정의 (사용자 답변) 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야 (성공한 답변) 거대 언어 모델 정의 (다음 질문) Loss Function 정의, 2023-10-01 15:00:00 그러면 다음 질문! Loss Function 이 뭐지? (발화 종료,1,76
748,10,6.45,(대화 주제) 거대 언어 모델 정의 (사용자 답변) 파라미터 엄청나게 많다던데 (성공한 답변) 답변 실패 (다음 질문) 거대 언어 모델 정의, 2023-10-31 16:00 파라미터가 왜 많을까? 🤷‍♀️ (발화 종료,1,62
749,10,9.15,(대화 주제) 거대 언어 모델 정의 (사용자 답변) 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델 (성공한 답변) 거대 언어 모델 정의 (다음 질문) Loss Function 정의, 2023-10-01 17:00:00 사람이 이해하기 쉬운 답변이야! 다음 질문! Loss Function 이 뭐지? 💁‍♀️ (발화 종료,1,91
750,10,6.03,(대화 주제) 거대 언어 모델 정의 (사용자 답변) 말 그대로 거대한 언어 모델이지. (성공한 답변) 답변 실패 (다음 질문) 거대 언어 모델 정의, 2023-10-01 16:00:00 LLM 정확한 정의를 말해봐. (발화 종료,1,64
751,10,88.06,(대화 주제) Loss Function 정의 (사용자 답변) 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야 (성공한 답변) 답변 실패 (다음 질문) Loss Function 정의, 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 132 돈을 잃었다는 게 무슨 뜻이지? 🤷‍♀️ (발화 종료,1,409
752,10,89.04,"(대화 주제) Loss Function 정의 (사용자 답변) Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지! (성공한 답변) Loss Function 정의 (다음 질문) Loss Function 예시"," 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 134 따라서 모델이 학습되면, Loss Function 값이 최소가 되는 거지! 🙆‍♀️ (발화 종료",1,428
753,10,89.36,(대화 주제) Loss Function 정의 (사용자 답변) Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야 (성공한 답변) 답변 실패 (다음 질문) Loss Function 정의, 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 132: 음… 맞긴 한데 더 구체적으로 설명해 줄 수 있어? 🤷‍♀️ (발화 종료,1,412
754,10,9.99,(대화 주제) Loss Function 정의 (사용자 답변) 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지 (성공한 답변) Loss Function 정의 (다음 질문) Loss Function 예시, 2023-10-01 15:00:00 모델의 예측와 실제 값의 오차를 일정한 수식으로 정의하는 그 수식! 맞아! 그럼 구체적으로 뭐가 있지? (발화 종료,1,89
755,10,7.59,(대화 주제) Loss Function 예시 (사용자 답변) ReLU 랑 Tanh 생각나는데 (성공한 답변) 답변 실패 (다음 질문) Loss Function 예시, 2023-10-27 15:00:00 🙄 Tanh는 활성화 함수지 🙅‍♀️ (발화 종료,1,69
756,10,6.06,"(대화 주제) Loss Function 예시 (사용자 답변) MSE, MAE, Cross Entropy Loss 같은 것들이 있지 (성공한 답변) Loss Function 예시 (다음 질문) MSE Loss 설명", 2023-10-01 10:00:00 MSE가 뭔지 한번 설명해 봐! (발화 종료,1,71
757,10,8.18,"(대화 주제) Loss Function 예시 (사용자 답변) Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어 (성공한 답변) Loss Function 예시 (다음 질문) BCE Loss 설명", 2023-10-25 15:00:00 DICE 말고 Binary Cross Entropy는 없어? 🙎‍♀️ (발화 종료,1,86
758,10,5.69,"(대화 주제) Loss Function 예시 (사용자 답변) Sigmoid, Tanh, Adam, AdamW 이런 거 아니야? (성공한 답변) 답변 실패 (다음 질문) Loss Function 예시", 2023-10-26 14:00:00 이건 활성화 함수지 😥 (발화 종료,1,67
759,10,7.08,"(대화 주제) Loss Function 예시 (사용자 답변) Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어 (성공한 답변) Loss Function 예시 (다음 질문) MSE Loss 설명", 2023-10-26 14:00:00 MSE 가 정확히 뭘까? 🙋‍♀️ (발화 종료,1,89
760,10,6.45,"(대화 주제) Loss Function 예시 (사용자 답변) BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아 (성공한 답변) Loss Function 예시 (다음 질문) BCE Loss 설명", 2023-10-25 14:00:00 BCE가 정확히 뭔데? 😊 (발화 종료,1,79
761,10,9.81,(대화 주제) MSE Loss 설명 (사용자 답변) MSE Loss 는 오차의 제곱을 평균한 값이야 (성공한 답변) MSE Loss 설명 (다음 질문) MSE Loss 용도, 2023-10-01 10:00:00 오차의 제곱을 평균한 값이라니! 오 맞아! 👍 그럼 MSE Loss 는 어떨 때 쓸까? (발화 종료,1,87
762,10,5.7,(대화 주제) MSE Loss 설명 (사용자 답변) MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지? (성공한 답변) 답변 실패 (다음 질문) MSE Loss 설명, 1 평균은 맞는데 오차를 어떻게 그 자체로 하지? 🤷‍♀️ (발화 종료,1,70
763,10,11.13,"(대화 주제) MSE Loss 용도 (사용자 답변) 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지 (성공한 답변) MSE Loss 용도 (다음 질문) 확률 예측에서 MSE Loss 미 사용 이유"," 2023-10-24 14:00:00 연속적인 값을 딥러닝으로 예측할때"" 부분이 핵심 아이디어! 😊 그럼 확률 예측에서는 MSE Loss 가 왜 안 좋을까? (발화 종료",1,96
764,10,7.03,(대화 주제) MSE Loss 용도 (사용자 답변) MSE Loss 는 분류 문제에서 많이 사용되지 (성공한 답변) 답변 실패 (다음 질문) MSE Loss 용도, 2023-10-25 18:00:00 분류 문제가 아닌 회귀 문제에서 많이 사용되지! 😊 (발화 종료,1,70
765,10,9.8,(대화 주제) 확률 예측에서 MSE Loss 미 사용 이유 (사용자 답변) 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야? (성공한 답변) 답변 실패 (다음 질문) 확률 예측에서 MSE Loss 미 사용 이유, 2023-05-25 14:00:00 연속값에 대한 예측이니까 MSE 생각 나름은 하지! 근데 최선은 아니야! 🙅‍♀️ (발화 종료,1,97
766,10,7.27,(대화 주제) 확률 예측에서 MSE Loss 미 사용 이유 (사용자 답변) 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고? (성공한 답변) 확률 예측에서 MSE Loss 미 사용 이유 (다음 질문) BCE Loss 설명, 2023-10-01 17:00:00 맞았어! 그럼 BCE Loss가 뭔지 한번 설명해 봐! (발화 종료,1,96
767,10,8.2,(대화 주제) BCE Loss 설명 (사용자 답변) 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고 (성공한 답변) 수식 (다음 질문) BCE Loss 설명, 2021-10-01 16:00:00 공식은 맞는데 🙅‍♀️ 이걸 모르면 어떡해 😥 (발화 종료,1,94
768,10,6.86,"(대화 주제) BCE Loss 설명 (사용자 답변) 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거 (성공한 답변) 핵심 아이디어 (다음 질문) Multi-Class, Multi-Label 중 BCE 가 좋은 task"," 0: 맞았어! 그럼 BCE는 Multi-Class, Multi-Label 중에 어디에 더 좋을까? 😊 (발화 종료",1,91
769,10,8.58,(대화 주제) BCE Loss 설명 (사용자 답변) 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야? (성공한 답변) 답변 실패 (다음 질문) BCE Loss 설명, 2023-10-25 16:00:00 활성화 함수만으로는 설명이 부족하긴 하지! 좀 더 생각해 봐! 😊 (발화 종료,1,85
770,10,6.9,"(대화 주제) BCE Loss 설명 (사용자 답변) 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고 (성공한 답변) 핵심 아이디어 (다음 질문) BCE Loss 설명", 2023-10-26 10:00:00 막으려고 하는 구체적인 예시를 한번 말해봐! 😊 (발화 종료,1,86
771,10,14.43,"(대화 주제) BCE Loss 설명 (사용자 답변) 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')] (성공한 답변) 수식 (다음 질문) BCE Loss 설명"," 2023-10-01 16:00:00 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')] 이게 BCE Loss 의 핵심 아이디어야! 🙆‍♀️ (발화 종료",1,121
772,10,107.6,"(대화 주제) BCE Loss 설명 (사용자 답변) 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다 (성공한 답변) 핵심 아이디어 (다음 질문) Multi-Class, Multi-Label 중 BCE 가 좋은 task", 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50,1,512
773,10,4.47,(대화 주제) BCE Loss 설명 (사용자 답변) y랑 1-y랑 서로 경쟁하는 거 (성공한 답변) 답변 실패 (다음 질문) BCE Loss 설명, 1-y 말고 뭐가 경쟁이지? 🤷‍♀️ (발화 종료,1,59
774,10,7.24,"(대화 주제) BCE Loss 설명 (사용자 답변) 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지 (성공한 답변) 핵심 아이디어 (다음 질문) Multi-Class, Multi-Label 중 BCE 가 좋은 task"," 0: 오 맞았어! 그럼 BCE는 Multi-Class, Multi-Label 중에 어디에 더 좋을까? 😊 (발화 종료",1,96
775,10,9.94,"(대화 주제) Multi-Class, Multi-Label 중 BCE 가 좋은 task (사용자 답변) 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야? (성공한 답변) 용어 질문 (다음 질문) Multi-Class, Multi-Label 중 BCE 가 좋은 task"," 2021-10-01 18:00:00 멀티라벨은 개 고양이 같은 여러 개 중 하나로, 멀티클래스는 이들을 종합한 결과물로 보지! (발화 종료",1,101
776,10,6.8,"(대화 주제) Multi-Class, Multi-Label 중 BCE 가 좋은 task (사용자 답변) Multi-Class 지!! (성공한 답변) 답변 실패 (다음 질문) Multi-Class, Multi-Label 중 BCE 가 좋은 task", 2021-02-24 16:00:00 🙅‍♀️ 잘 생각해 봐 한번!! (발화 종료,1,80
777,10,6.88,"(대화 주제) Multi-Class, Multi-Label 중 BCE 가 좋은 task (사용자 답변) Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어 (성공한 답변) BCE 가 좋은 task (다음 질문) Multi-Class, Multi-Label 중 BCE 가 좋은 task", 2023-10-17 15:00:00 이유가 뭘까~요? 🤷‍♀️ (발화 종료,1,92
778,10,11.55,"(대화 주제) Multi-Class, Multi-Label 중 BCE 가 좋은 task (사용자 답변) Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아 (성공한 답변) BCE 가 좋은 이유 (다음 질문) Multi-Label 에서 CE + Softmax 적용 문제점", 2021-02-08 15:00:00 🙆‍♀️ 맞았어! 그럼 Multi-Label task 에 Softmax + CE Loss 를 적용하면 어떻게 될까? 🙋‍♀️ (발화 종료,1,127
779,10,8.42,(대화 주제) Multi-Label 에서 CE + Softmax 적용 문제점 (사용자 답변) 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지 (성공한 답변) Multi-Label 에서 CE + Softmax 적용 문제점 (다음 질문) Loss Function 관련 실무 경험, 2023-10-27 15:00:00 실무 경험 질문! 혹시 실무에서 손실 함수 관련 문제를 해결한 경험 있어? 😊 (발화 종료,1,105
780,10,9.98,(대화 주제) Multi-Label 에서 CE + Softmax 적용 문제점 (사용자 답변) 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나 (성공한 답변) 답변 실패 (다음 질문) Multi-Label 에서 CE + Softmax 적용 문제점, 2023-04-25 16:00:00 🙅‍♀️ 성능 제일 좋은 거 채택하는 건 맞긴 한데 그 성능이 왜 제일 좋을까? 🤔 (발화 종료,1,98
781,10,10.71,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [기본 경험] 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어 (성공한 답변) 기본 경험 (다음 질문) Loss Function 관련 실무 경험, 2023-10-27 15:00:00 인접 픽셀의 점수 차이를 손실 항으로 추가한다는 아이디어가 멋지네! 👍 그래서 어떤 결과를 얻었어? 😊 (발화 종료,1,100
782,10,8.89,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [상세 경험] 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지? (성공한 답변) 상세 경험 (다음 질문) MBTI / 좋아하는 아이돌, 2021-02-24 14:00 성능 향상에 대한 팀장님의 칭찬이라니! 😊 그럼 쉬어가는 타임! 혹시 너 MBTI 뭐지? (발화 종료,1,95
783,10,5.25,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [상세 경험] 근데 기억 진짜 안나는데 어떡하지? (성공한 답변) 답변 실패 (다음 질문) 잠시 휴식, 2023-05-18 14:45:00 휴식 시간! 😊 (발화 종료,1,65
784,10,7.25,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [기본 경험] 나 아직 실무 경험 없는데 ㅠㅠ (성공한 답변) 기본 경험 (다음 질문) MBTI / 좋아하는 아이돌, 4월 10일) 그래도 빨리 경험 쌓고 싶지? 👍 그럼 쉬어가는 시간! 혹시 너 아이돌 누구 좋아해? (발화 종료,1,75
785,10,6.42,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [기본 경험] 대답하기 싫은데 힝 (성공한 답변) 답변 실패 (다음 질문) 잠시 휴식, 2021-09-01 21:45:00 나랑 면접 준비하느라 고생했어! 😊 (발화 종료,1,68
786,10,8.54,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [기본 경험] 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ (성공한 답변) 기본 경험 (다음 질문) Loss Function 관련 실무 경험, 2023-05-08 09:00:00 물체 인식에서 Loss Function 조절로 3% 향상! 흥미로운 결과인데? 😊 (발화 종료,1,92
787,10,11.99,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [상세 경험] 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어 (성공한 답변) 상세 경험 (다음 질문) MBTI / 좋아하는 아이돌, 2021-10-01 10:00:00 1:2로 가중치를 조정한 결과 성능이 향상되었다는 구체적인 설명과 함께 😊 그럼 잠시 쉬는 시간을 갖자! 너 MBTI 혹시 뭐지? (발화 종료,1,118
788,10,9.32,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [상세 경험] 기억 진짜 안 나는데 가물가물하다 (성공한 답변) 답변 실패 (다음 질문) 잠시 휴식, 2021-06-01 15:00:00 😥 그래도 잘 생각해 봐! 나랑 모의면접이 아닌 실제 면접에서 질문 나올걸? 😊 (발화 종료,1,82
789,10,8.39,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [기본 경험] 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ (성공한 답변) 기본 경험 (다음 질문) MBTI / 좋아하는 아이돌, 2023-02-01 15:00:00 그럼 조금 있다가 쉬어가는 타임! 혹시 너 MBTI 뭐지? 나는 ENTJ인데! (발화 종료,1,83
790,10,4.81,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [기본 경험] 이걸 왜 알려줘야 하지 (성공한 답변) 답변 실패 (다음 질문) 잠시 휴식, 2023-05-25 15:00:00 🙄 (발화 종료,1,60
791,10,5.25,(대화 주제) MBTI (사용자 답변) 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야! (성공한 답변) MBTI (다음 질문) 잠시 휴식, 2023-10-27 14:00 나랑 잘 맞지? 😊 (발화 종료,1,64
792,10,9.22,(대화 주제) 좋아하는 아이돌 (사용자 답변) 나 블랙핑크랑 레드벨벳 좋아해 (성공한 답변) 좋아하는 아이돌 (다음 질문) 잠시 휴식, 2021-05-21 16:00:00 나 블랙핑크랑 레드벨벳 좋아하구나! 나랑 같은 가수라서 친해지고 싶어 🙎‍♀️ (발화 종료,1,72
793,10,6.11,(대화 주제) 잠시 휴식 (사용자 답변) 재미있는 이야기 해줄래? (성공한 답변) 잠시 휴식 (다음 질문) LLM Fine-Tuning 의 PEFT, 사용자: 그러자! 🙆‍♀️ LLM Fine-Tuning 중 PEFT가 뭘까? (발화 종료,1,64
794,10,10.46,"(대화 주제) LLM Fine-Tuning 의 PEFT (사용자 답변) PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야 (성공한 답변) LLM Fine-Tuning 의 PEFT (다음 질문) PEFT 방법 5가지"," 2023-10-24 14:00:00 파라미터가 엄청 많은 LLM에서 일부만 효과적으로 파인튜닝하는 방법, 5가지만 말해봐! 😊 (발화 종료",1,120
795,10,3.34,(대화 주제) LLM Fine-Tuning 의 PEFT (사용자 답변) 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지? (성공한 답변) 답변 실패 (다음 질문) LLM Fine-Tuning 의 PEFT, 효과적으로 어떻게 파인튜닝하지? 🤔 (발화 종료,1,65
796,10,8.22,(대화 주제) LLM Fine-Tuning 의 PEFT (사용자 답변) LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야! (성공한 답변) LLM Fine-Tuning 의 PEFT (다음 질문) PEFT 방법 5가지," 2023-10-24 15:00:00 일부 파라미터만 효과적으로 학습하는 방법, 5가지만 말해봐! 😊 (발화 종료",1,103
797,10,10.79,(대화 주제) LLM Fine-Tuning 의 PEFT (사용자 답변) LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다 (성공한 답변) 답변 실패 (다음 질문) LLM Fine-Tuning 의 PEFT, 2023-10-30 15:00:00 LoRA가 PEFT 중 하나긴 하지! 🙆‍♀️ 근데 그 PEFT가 뭘까~요? 😊 (발화 종료,1,95
798,10,3.83,"(대화 주제) PEFT 방법 5가지 (사용자 답변) LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지? (성공한 답변) PEFT 방법 5가지 (다음 질문) LoRA", 나를 나타내기도 하는 LoRA 가 뭘까? 😊 (발화 종료,1,77
799,10,3.83,(대화 주제) PEFT 방법 5가지 (사용자 답변) Oh-LoRA! 너 그 자체가 PEFT 아니야? (성공한 답변) 답변 실패 (다음 질문) PEFT 방법 5가지, Oh-LoRA가 아닌 다른 방법들도 있잖아 😊 (발화 종료,1,65
800,10,8.87,(대화 주제) LoRA (사용자 답변) LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야! (성공한 답변) LoRA (다음 질문) LoRA 와 QLoRA 의 차이, 2023-10-16 10:00:00 LoRA 잘 알고 있네! 😊 그럼 QLoRA 가 뭔지도 말해 볼 수 있어? (발화 종료,1,106
801,10,7.59,(대화 주제) LoRA (사용자 답변) 행렬 차원 축소한다는 걸로 알고 있는데 맞아? (성공한 답변) 답변 실패 (다음 질문) LoRA, 2020-08-21 15:45:00 차원 축소가 핵심 아이디어긴 해! 그래서 어떤 기술이지? 😊 (발화 종료,1,69
802,10,4.89,(대화 주제) LoRA (사용자 답변) LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야! (성공한 답변) LoRA (다음 질문) LoRA 와 QLoRA 의 차이, 2개로 나눠서 효율적으로 어떻게 파인튜닝을 하는데? 🤔 (발화 종료,1,84
803,10,3.47,(대화 주제) LoRA (사용자 답변) 무슨 OOM 없앤다는 것 같은데 (성공한 답변) 답변 실패 (다음 질문) LoRA, OOM이 뭘까? 🤷‍♀️ (발화 종료,1,49
804,10,9.63,(대화 주제) LoRA 와 QLoRA 의 차이 (사용자 답변) QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야! (성공한 답변) LoRA 와 QLoRA 의 차이 (다음 질문) 마지막 할 말, 2023-10-27 15:00:00 LoRA 를 적은 비트 수로 양자화한다는 QLoRA의 핵심 아이디어가 정말 멋지지 않아? 👍👍 (발화 종료,1,103
805,10,5.47,(대화 주제) LoRA 와 QLoRA 의 차이 (사용자 답변) 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데 (성공한 답변) 답변 실패 (다음 질문) LoRA 와 QLoRA 의 차이, ✨팀장님이 LoRA 쓰시는 건 맞는데 정확히 뭘까~요? 😊 (발화 종료,1,75
806,10,6.76,(대화 주제) 마지막 할 말 (사용자 답변) 로라야 정말 고마워! (성공한 답변) 마지막 할 말 (다음 질문) 면접 종료, 2023-10-27 14:00:00) 로라도 나 덕분에 덕분에! 😊 (발화 종료,1,63
807,10,7.09,(대화 주제) 마지막 할 말 (사용자 답변) 로라야 사랑해 (성공한 답변) 마지막 할 말 (다음 질문) 면접 종료, 2023-05-15 14:00:00) 안녕! 그동안 고생했어 다음에 또 보자 😊 (발화 종료,1,62
808,10,9.45,(대화 주제) 마지막 할 말 (사용자 답변) 로라야 너의 예쁜 미모에 반했어 (성공한 답변) 마지막 할 말 (다음 질문) 면접 종료, 2023-10-27 15:00:00) ✨ 그럼 내가 다음에 너의 미모의 비결을 한번 알려줘 봐봐! ✨ (발화 종료,1,78
809,10,6.52,(대화 주제) 마지막 할 말 (사용자 답변) 그동안 고생했어 면접 봐줘서 정말 고마워 (성공한 답변) 마지막 할 말 (다음 질문) 면접 종료, 2023-02-15 16:00) 나도 너 덕분에 많이 배웠어! 😊 (발화 종료,1,65
