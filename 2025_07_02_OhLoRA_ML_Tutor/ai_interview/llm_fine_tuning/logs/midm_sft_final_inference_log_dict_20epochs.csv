,epoch,elapsed_time (s),prompt,llm_answer,trial_cnt,output_tkn_cnt
0,1,6.76,면접 시작," 나의 면접관이 될 AI 면접관, 나의 면접관이 되어 면접을 시작하자! 😊 (발화 종료",1,33
1,1,5.33,(대화 주제) 면접 시작 인사 (사용자 답변) 오늘 뭐 물어볼 거야 그래서? (다음 질문) 면접 시작 인사, ✨ 면접관 : 면접 시작! 나의 면접관이 될 거야! 😊 (발화 종료,1,51
2,1,3.36,(대화 주제) 면접 시작 인사 (사용자 답변) 로라야 안녕 정말 반가워 (다음 질문) 면접 시작 인사, ✨ 그럼 면접을 시작해 볼까? 😊 (발화 종료,1,43
3,1,5.28,(대화 주제) 면접 시작 인사 (사용자 답변) 로라야 그럼 네가 면접관이야? (다음 질문) 면접 시작 인사, 면접관 로라: 그럼 나 면접관이지! 그럼 면접을 시작해 볼까? 😊 (발화 종료,1,54
4,1,4.27,(대화 주제) 면접 시작 인사 (사용자 답변) 내 인생도 이제 오로라처럼 빛날 거니까 (다음 질문) 면접 시작 인사, ✨✨ 나의 면접은 이제 시작이야! 😊 (발화 종료,1,49
5,1,2.95,"(대화 주제) 면접 시작 인사 (사용자 답변) 파이팅! 시작하자 (다음 질문) 인공지능, 머신러닝, 딥러닝 차이", ✨ 나도 파이팅! 😊 (발화 종료,1,41
6,1,3.84,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 인공지능 나도 개발해보고 싶은데 (성공한 답변) 답변 실패 (다음 질문) 인공지능, 머신러닝, 딥러닝 차이", 나도 나의 인공지능을 개발해보고 싶어! 😊 (발화 종료,1,58
7,1,4.17,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거! (성공한 답변) 인공지능 (다음 질문) 인공지능, 머신러닝, 딥러닝 차이", 2. 그럼 그 인공지능이란? 🤷‍♀️ (발화 종료,1,66
8,1,3.39,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지 (성공한 답변) 머신러닝 (다음 질문) 인공지능, 머신러닝, 딥러닝 차이", ✨ 그럼 그 머신러닝이 뭐지? 😊 (발화 종료,1,62
9,1,2.53,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지? (성공한 답변) 딥러닝 (다음 질문) 인공지능, 머신러닝, 딥러닝 차이", 그럼 딥러닝이란? 😊 (발화 종료,1,57
10,1,3.27,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 딥러닝은 인공신경망 기반 머신러닝이야 (성공한 답변) 딥러닝 (다음 질문) 거대 언어 모델 정의", ✨ 그럼 LLM은? 🤔 (발화 종료,1,54
11,1,4.14,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 머신러닝은 말 그대로 기계가 학습하는 거 아니야? (성공한 답변) 답변 실패 (다음 질문) 인공지능, 머신러닝, 딥러닝 차이", ✨그럼 그 기계은 뭐지? 🤷‍♀️ (발화 종료,1,63
12,1,4.55,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 인공지능 요새 엄청 뜨는 분야잖아 (성공한 답변) 답변 실패 (다음 질문) 인공지능, 머신러닝, 딥러닝 차이", 1. 인공지능이 왜 뜨는 거야? 🤷‍♀️ (발화 종료,1,60
13,1,3.95,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지? (성공한 답변) 인공지능 (다음 질문) 인공지능, 머신러닝, 딥러닝 차이", 2. 그러면 머신러닝은? 🤷‍♀️ (발화 종료,1,68
14,1,3.36,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야! (성공한 답변) 머신러닝 (다음 질문) 거대 언어 모델 정의", ✨✨ 그럼 LLM은? 😊 (발화 종료,1,65
15,1,4.03,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지 (성공한 답변) 딥러닝 (다음 질문) 거대 언어 모델 정의", ✨ LLM이 뭐지? 🤷‍♀️ (발화 종료,1,61
16,1,4.14,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야. (성공한 답변) 딥러닝 (다음 질문) 거대 언어 모델 정의", ✨ 그럼 LLM이 뭐지? 🤷‍♀️ (발화 종료,1,63
17,1,4.02,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 머신러닝은 기계가 공부하는 거지 말 그대로 (성공한 답변) 답변 실패 (다음 질문) 인공지능, 머신러닝, 딥러닝 차이", ✨ 그럼 그 공부는? 🤷‍♀️ (발화 종료,1,60
18,1,4.7,(대화 주제) 거대 언어 모델 정의 (사용자 답변) 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야 (성공한 답변) 거대 언어 모델 정의 (다음 질문) Loss Function 정의, ✨ 나의 대답이야! 😊 그럼 손실 함수가 뭘까? (발화 종료,1,69
19,1,2.41,(대화 주제) 거대 언어 모델 정의 (사용자 답변) 파라미터 엄청나게 많다던데 (성공한 답변) 답변 실패 (다음 질문) 거대 언어 모델 정의, ✨LLM은? (발화 종료,1,46
20,1,4.27,(대화 주제) 거대 언어 모델 정의 (사용자 답변) 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델 (성공한 답변) 거대 언어 모델 정의 (다음 질문) Loss Function 정의, ✨ 그럼 이제 본격적으로 면접을 시작해 볼까? 😊 (발화 종료,1,71
21,1,2.89,(대화 주제) 거대 언어 모델 정의 (사용자 답변) 말 그대로 거대한 언어 모델이지. (성공한 답변) 답변 실패 (다음 질문) 거대 언어 모델 정의, ✨ LLM이란? 🤔 (발화 종료,1,51
22,1,4.33,(대화 주제) Loss Function 정의 (사용자 답변) 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야 (성공한 답변) 답변 실패 (다음 질문) Loss Function 정의, ✨️ 그럼 손실 함수이 뭐지? 🤷‍♀️ (발화 종료,1,61
23,1,4.36,"(대화 주제) Loss Function 정의 (사용자 답변) Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지! (성공한 답변) Loss Function 정의 (다음 질문) Loss Function 예시", ✨ 그럼 그 Loss Function에는 뭐가 있지? 😊 (발화 종료,1,73
24,1,4.11,(대화 주제) Loss Function 정의 (사용자 답변) Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야 (성공한 답변) 답변 실패 (다음 질문) Loss Function 정의, 2) 손실 함수가 뭐지? 🤷‍♀️ (발화 종료,1,58
25,1,2.64,(대화 주제) Loss Function 정의 (사용자 답변) 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지 (성공한 답변) Loss Function 정의 (다음 질문) Loss Function 예시, 1) Loss Function은? (발화 종료,1,58
26,1,5.91,(대화 주제) Loss Function 예시 (사용자 답변) ReLU 랑 Tanh 생각나는데 (성공한 답변) 답변 실패 (다음 질문) Loss Function 예시," 1. ReLU, Tanh 말고도 Loss Function이 뭘까? 🤷‍♀️ (발화 종료",1,61
27,1,4.28,"(대화 주제) Loss Function 예시 (사용자 답변) MSE, MAE, Cross Entropy Loss 같은 것들이 있지 (성공한 답변) Loss Function 예시 (다음 질문) MSE Loss 설명", ✨ 그럼 그 중 MSE Loss가 뭐지? 😊 (발화 종료,1,63
28,1,5.19,"(대화 주제) Loss Function 예시 (사용자 답변) Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어 (성공한 답변) Loss Function 예시 (다음 질문) BCE Loss 설명", ✨ 그럼 그 중에서 BCE Loss가 뭐지? 🤷‍♀️ (발화 종료,1,74
29,1,4.26,"(대화 주제) Loss Function 예시 (사용자 답변) Sigmoid, Tanh, Adam, AdamW 이런 거 아니야? (성공한 답변) 답변 실패 (다음 질문) Loss Function 예시", ✨ ✨ 그럼 그 Loss Function은 뭐지? 😊 (발화 종료,1,61
30,1,4.97,"(대화 주제) Loss Function 예시 (사용자 답변) Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어 (성공한 답변) Loss Function 예시 (다음 질문) MSE Loss 설명", ✨ 나도 MSE Loss가 뭔지 궁금해 🙋‍♀️ (발화 종료,1,80
31,1,3.86,"(대화 주제) Loss Function 예시 (사용자 답변) BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아 (성공한 답변) Loss Function 예시 (다음 질문) BCE Loss 설명", ✨ 그럼 그 중 BCE가 뭐지? 😊 (발화 종료,1,69
32,1,5.52,(대화 주제) MSE Loss 설명 (사용자 답변) MSE Loss 는 오차의 제곱을 평균한 값이야 (성공한 답변) MSE Loss 설명 (다음 질문) MSE Loss 용도, ✨✨ ✨ ✨✨ 그럼 MSE Loss 를 어떨 때 사용하지? 😊 (발화 종료,1,69
33,1,3.89,(대화 주제) MSE Loss 설명 (사용자 답변) MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지? (성공한 답변) 답변 실패 (다음 질문) MSE Loss 설명, ✨ 그럼 오차는 뭐지? 🤷‍♀️ (발화 종료,1,63
34,1,6.27,"(대화 주제) MSE Loss 용도 (사용자 답변) 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지 (성공한 답변) MSE Loss 용도 (다음 질문) 확률 예측에서 MSE Loss 미 사용 이유", ✨ ✨ 그럼 확률 예측에서 MSE Loss를 안 쓰는 이유는? 🤷‍♀️ (발화 종료,1,77
35,1,5.28,(대화 주제) MSE Loss 용도 (사용자 답변) MSE Loss 는 분류 문제에서 많이 사용되지 (성공한 답변) 답변 실패 (다음 질문) MSE Loss 용도, 2) 분류 문제에서 MSE Loss 가 왜 좋을까? 🤷‍♀️ (발화 종료,1,63
36,1,4.89,(대화 주제) 확률 예측에서 MSE Loss 미 사용 이유 (사용자 답변) 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야? (성공한 답변) 답변 실패 (다음 질문) 확률 예측에서 MSE Loss 미 사용 이유, ✨✨ 그럼 확률도 연속된 값 아니야? 🤷‍♀️ (발화 종료,1,76
37,1,4.61,(대화 주제) 확률 예측에서 MSE Loss 미 사용 이유 (사용자 답변) 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고? (성공한 답변) 확률 예측에서 MSE Loss 미 사용 이유 (다음 질문) BCE Loss 설명, ✨✨ 그럼 BCE Loss가 뭘까? 🤷‍♀️ (발화 종료,1,85
38,1,5.02,(대화 주제) BCE Loss 설명 (사용자 답변) 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고 (성공한 답변) 수식 (다음 질문) BCE Loss 설명, ✨ ✨ 그럼 BCE Loss가 뭐지? 🤷‍♀️ (발화 종료,1,80
39,1,6.05,"(대화 주제) BCE Loss 설명 (사용자 답변) 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거 (성공한 답변) 핵심 아이디어 (다음 질문) Multi-Class, Multi-Label 중 BCE 가 좋은 task"," 2. BCE Loss가 Multi-Class, Multi-Label 중에 어디에 좋을까? 😊 (발화 종료",1,87
40,1,3.59,(대화 주제) BCE Loss 설명 (사용자 답변) 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야? (성공한 답변) 답변 실패 (다음 질문) BCE Loss 설명, ✨ 그럼 그게 뭐지? 🤷‍♀️ (발화 종료,1,65
41,1,4.33,"(대화 주제) BCE Loss 설명 (사용자 답변) 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고 (성공한 답변) 핵심 아이디어 (다음 질문) BCE Loss 설명", 2) BCE Loss 공식은? 🤷‍♀️ (발화 종료,1,75
42,1,4.01,"(대화 주제) BCE Loss 설명 (사용자 답변) 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')] (성공한 답변) 수식 (다음 질문) BCE Loss 설명", ✨ ✨ BCE Loss의 핵심 아이디어는? (발화 종료,1,76
43,1,6.08,"(대화 주제) BCE Loss 설명 (사용자 답변) 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다 (성공한 답변) 핵심 아이디어 (다음 질문) Multi-Class, Multi-Label 중 BCE 가 좋은 task"," 2. Multi-Class, Multi-Label 중에 BCE가 뭘까? 🤷‍♀️ (발화 종료",1,87
44,1,3.78,(대화 주제) BCE Loss 설명 (사용자 답변) y랑 1-y랑 서로 경쟁하는 거 (성공한 답변) 답변 실패 (다음 질문) BCE Loss 설명, 2) BCE Loss는 뭘까? 🤔 (발화 종료,1,56
45,1,5.89,"(대화 주제) BCE Loss 설명 (사용자 답변) 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지 (성공한 답변) 핵심 아이디어 (다음 질문) Multi-Class, Multi-Label 중 BCE 가 좋은 task"," ✨ 그럼 Multi-Class, Multi-Label 중에서 BCE가 좋은 task는? 🤔 (발화 종료",1,90
46,1,8.19,"(대화 주제) Multi-Class, Multi-Label 중 BCE 가 좋은 task (사용자 답변) 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야? (성공한 답변) 용어 질문 (다음 질문) Multi-Class, Multi-Label 중 BCE 가 좋은 task"," 로라: Multi-Class 은 여러 개의 클래스 중 하나로, Multi-Label 은 여러 개의 클래스가 동시에 참일 수 있어! 😊 (발화 종료",1,93
47,1,4.02,"(대화 주제) Multi-Class, Multi-Label 중 BCE 가 좋은 task (사용자 답변) Multi-Class 지!! (성공한 답변) 답변 실패 (다음 질문) Multi-Class, Multi-Label 중 BCE 가 좋은 task", ✨그럼 그 이유는? 🤷‍♀️ (발화 종료,1,68
48,1,4.34,"(대화 주제) Multi-Class, Multi-Label 중 BCE 가 좋은 task (사용자 답변) Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어 (성공한 답변) BCE 가 좋은 task (다음 질문) Multi-Class, Multi-Label 중 BCE 가 좋은 task", ✨️그럼 그 이유가 뭐지? 🤷‍♀️ (발화 종료,1,82
49,1,4.2,"(대화 주제) Multi-Class, Multi-Label 중 BCE 가 좋은 task (사용자 답변) Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아 (성공한 답변) BCE 가 좋은 이유 (다음 질문) Multi-Label 에서 CE + Softmax 적용 문제점", ✨ 나도 맞지! 그럼 그 이유는? 😊 (발화 종료,1,96
50,1,4.14,(대화 주제) Multi-Label 에서 CE + Softmax 적용 문제점 (사용자 답변) 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지 (성공한 답변) Multi-Label 에서 CE + Softmax 적용 문제점 (다음 질문) Loss Function 관련 실무 경험, ✨ 실무에서 손실 함수 관련된 경험이 있지? 😊 (발화 종료,1,87
51,1,4.12,(대화 주제) Multi-Label 에서 CE + Softmax 적용 문제점 (사용자 답변) 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나 (성공한 답변) 답변 실패 (다음 질문) Multi-Label 에서 CE + Softmax 적용 문제점, ✨✨✨ 그럼 왜 성능이 제일 좋을까? 🤔 (발화 종료,1,73
52,1,3.53,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [기본 경험] 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어 (성공한 답변) 기본 경험 (다음 질문) Loss Function 관련 실무 경험, ✨ 그럼 그 손실 함수는? 😊 (발화 종료,1,69
53,1,4.25,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [상세 경험] 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지? (성공한 답변) 상세 경험 (다음 질문) MBTI / 좋아하는 아이돌, ✨ 그럼 다음 질문! 혹시 아이돌 누구 좋아해? 😊 (발화 종료,1,75
54,1,2.28,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [상세 경험] 근데 기억 진짜 안나는데 어떡하지? (성공한 답변) 답변 실패 (다음 질문) 잠시 휴식, 🤷‍♀️ (발화 종료,1,52
55,1,5.26,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [기본 경험] 나 아직 실무 경험 없는데 ㅠㅠ (성공한 답변) 기본 경험 (다음 질문) MBTI / 좋아하는 아이돌, ✨ 그럼 면접은 여기까지! 😊 혹시 너의 성격 유형은? (발화 종료,1,66
56,1,1.52,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [기본 경험] 대답하기 싫은데 힝 (성공한 답변) 답변 실패 (다음 질문) 잠시 휴식, 😥 (발화 종료,1,47
57,1,6.97,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [기본 경험] 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ (성공한 답변) 기본 경험 (다음 질문) Loss Function 관련 실무 경험, ✨ 나도 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올린다면? 😊 (발화 종료,1,86
58,1,4.75,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [상세 경험] 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어 (성공한 답변) 상세 경험 (다음 질문) MBTI / 좋아하는 아이돌, ✨ 그럼 그 성능 향상의 비결은? 🤷‍♀️ (발화 종료,1,87
59,1,5.52,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [상세 경험] 기억 진짜 안 나는데 가물가물하다 (성공한 답변) 답변 실패 (다음 질문) 잠시 휴식, ✨✨ 나도 너의 기억이 가물가물해서 나도 헷갈려 😥 (발화 종료,1,66
60,1,4.45,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [기본 경험] 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ (성공한 답변) 기본 경험 (다음 질문) MBTI / 좋아하는 아이돌, ✨ 나도 너랑 함께 면접 볼 수 있어? 😊 (발화 종료,1,66
61,1,3.12,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [기본 경험] 이걸 왜 알려줘야 하지 (성공한 답변) 답변 실패 (다음 질문) 잠시 휴식, ✨ 나도 잘 생각해 봐 😊 (발화 종료,1,53
62,1,4.91,(대화 주제) MBTI (사용자 답변) 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야! (성공한 답변) MBTI (다음 질문) 잠시 휴식, ✨ 나 ESTJ! 😊 그럼 나랑 함께 면접을 볼까? (발화 종료,1,62
63,1,3.39,(대화 주제) 좋아하는 아이돌 (사용자 답변) 나 블랙핑크랑 레드벨벳 좋아해 (성공한 답변) 좋아하는 아이돌 (다음 질문) 잠시 휴식, ✨ 나도 블랙핑크랑 레드벨벳 좋아하는데 😊 (발화 종료,1,47
64,1,4.05,(대화 주제) 잠시 휴식 (사용자 답변) 재미있는 이야기 해줄래? (성공한 답변) 잠시 휴식 (다음 질문) LLM Fine-Tuning 의 PEFT, ✨ 나도 재미있는 이야기 들려줄게! 😊 (발화 종료,1,55
65,1,5.48,"(대화 주제) LLM Fine-Tuning 의 PEFT (사용자 답변) PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야 (성공한 답변) LLM Fine-Tuning 의 PEFT (다음 질문) PEFT 방법 5가지", ✨ 나도 맞았어! 그럼 그 방법 5가지만 말해봐 😊 (발화 종료,1,99
66,1,4.03,(대화 주제) LLM Fine-Tuning 의 PEFT (사용자 답변) 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지? (성공한 답변) 답변 실패 (다음 질문) LLM Fine-Tuning 의 PEFT, ✨ 나도 그게 뭐지? 🤷‍♀️ (발화 종료,1,67
67,1,4.55,(대화 주제) LLM Fine-Tuning 의 PEFT (사용자 답변) LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야! (성공한 답변) LLM Fine-Tuning 의 PEFT (다음 질문) PEFT 방법 5가지, ✨ 그럼 그 방법이 5가지만 있을까? 😊 (발화 종료,1,87
68,1,110.79,(대화 주제) LLM Fine-Tuning 의 PEFT (사용자 답변) LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다 (성공한 답변) 답변 실패 (다음 질문) LLM Fine-Tuning 의 PEFT, ✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨,1,512
69,1,3.28,"(대화 주제) PEFT 방법 5가지 (사용자 답변) LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지? (성공한 답변) PEFT 방법 5가지 (다음 질문) LoRA", 그럼 그 LoRA가 뭐지? 😊 (발화 종료,1,74
70,1,5.03,(대화 주제) PEFT 방법 5가지 (사용자 답변) Oh-LoRA! 너 그 자체가 PEFT 아니야? (성공한 답변) 답변 실패 (다음 질문) PEFT 방법 5가지, 1. 그 자체가 PEFT가 아니야! 🤷‍♀️ (발화 종료,1,69
71,1,5.84,(대화 주제) LoRA (사용자 답변) LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야! (성공한 답변) LoRA (다음 질문) LoRA 와 QLoRA 의 차이, ✨ 나도 잘 이해했어! 그럼 QLoRA는 뭐지? 🤷‍♀️ (발화 종료,1,93
72,1,2.16,(대화 주제) LoRA (사용자 답변) 행렬 차원 축소한다는 걸로 알고 있는데 맞아? (성공한 답변) 답변 실패 (다음 질문) LoRA, 🤷‍♀️ (발화 종료,1,46
73,1,3.87,(대화 주제) LoRA (사용자 답변) LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야! (성공한 답변) LoRA (다음 질문) LoRA 와 QLoRA 의 차이, ✨ ✨ 그럼 QLoRA는? 😊 (발화 종료,1,80
74,1,3.53,(대화 주제) LoRA (사용자 답변) 무슨 OOM 없앤다는 것 같은데 (성공한 답변) 답변 실패 (다음 질문) LoRA, 나도 그게 뭘까? 🤷‍♀️ (발화 종료,1,49
75,1,5.72,(대화 주제) LoRA 와 QLoRA 의 차이 (사용자 답변) QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야! (성공한 답변) LoRA 와 QLoRA 의 차이 (다음 질문) 마지막 할 말, ✨✨ ✨✨ 그럼 나한테 마지막으로 할 말 있지 않아? 😊 (발화 종료,1,86
76,1,5.8,(대화 주제) LoRA 와 QLoRA 의 차이 (사용자 답변) 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데 (성공한 답변) 답변 실패 (다음 질문) LoRA 와 QLoRA 의 차이, ✨팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데 🙋‍♀️ (발화 종료,1,76
77,1,115.8,(대화 주제) 마지막 할 말 (사용자 답변) 로라야 정말 고마워! (성공한 답변) 마지막 할 말 (다음 질문) 면접 종료, ✨✨✨ ✨✨✨ ✨✨✨ ✨✨✨ ✨✨✨ ✨✨✨ ✨✨✨ ✨✨✨ ✨✨✨ ✨✨✨ ✨✨✨ ✨✨✨ ✨✨✨ ✨✨✨ ✨✨✨ ✨✨✨ ✨✨✨ ✨✨✨ ✨✨✨ ✨✨✨ ✨✨✨ ✨✨✨ ✨✨✨ ✨✨✨ ✨✨✨ ✨✨✨ ✨✨✨ ✨✨✨ ✨✨✨ ✨✨✨ ✨✨✨ ✨✨✨ ✨✨✨ ✨✨✨ ✨✨✨ ✨✨✨ ✨✨✨ ✨✨✨ ✨✨✨ ✨✨✨ ✨✨✨ ✨✨✨ ✨✨✨ ✨✨✨ ✨✨✨ ✨✨✨ ✨✨✨ ✨✨✨ ✨✨✨ ✨✨✨ ✨✨✨ ✨✨✨ ✨✨✨ ✨✨✨ ✨✨✨ ✨✨✨ ✨✨✨ ✨✨✨ ✨✨✨ ✨✨✨ ✨✨✨ ✨✨✨ ✨✨✨ ✨✨✨ ✨✨✨ ✨✨✨ ✨✨✨ ✨✨✨ ✨✨✨ ✨✨✨ ✨✨✨ ✨✨✨ ✨✨✨ ✨✨✨ ✨✨✨ ✨✨✨ ✨✨✨ ✨✨✨ ✨✨✨ ✨✨✨ ✨✨✨ ✨✨✨ ✨✨✨ ✨✨✨ ✨✨✨ ✨✨✨ ✨✨✨ ✨✨✨ ✨✨✨ ✨✨✨ ✨✨✨ ✨✨✨ ✨✨✨ ✨✨✨ ✨✨✨ ✨✨✨ ✨✨✨ ✨✨✨ ✨✨✨ ✨✨✨ ✨✨✨ ✨✨✨ ✨✨✨ ✨✨✨ ✨✨✨ ✨✨✨ ✨✨✨ ✨✨✨ ✨✨✨ ✨✨✨ ✨✨✨ ✨✨✨ ✨✨✨ ✨✨✨ ✨✨✨ ✨✨✨ ✨✨✨ ✨✨✨ ✨✨✨ ,1,512
78,1,3.53,(대화 주제) 마지막 할 말 (사용자 답변) 로라야 사랑해 (성공한 답변) 마지막 할 말 (다음 질문) 면접 종료, ✨✨ 그럼 면접 종료! 👍👍 (발화 종료,1,47
79,1,113.85,(대화 주제) 마지막 할 말 (사용자 답변) 로라야 너의 예쁜 미모에 반했어 (성공한 답변) 마지막 할 말 (다음 질문) 면접 종료, ✨ ✨ ✨ ✨ ✨ ✨ ✨ ✨ ✨ ✨ ✨ ✨ ✨ ✨ ✨ ✨ ✨ ✨ ✨ ✨ ✨ ✨ ✨ ✨ ✨ ✨ ✨ ✨ ✨ ✨ ✨ ✨ ✨ ✨ ✨ ✨ ✨ ✨ ✨ ✨ ✨ ✨ ✨ ✨ ✨ ✨ ✨ ✨ ✨ ✨ ✨ ✨ ✨ ✨ ✨ ✨ ✨ ✨ ✨ ✨ ✨ ✨ ✨ ✨ ✨ ✨ ✨ ✨ ✨ ✨ ✨ ✨ ✨ ✨ ✨ ✨ ✨ ✨ ✨ ✨ ✨ ✨ ✨ ✨ ✨ ✨ ✨ ✨ ✨ ✨ ✨ ✨ ✨ ✨ ✨ ✨ ✨ ✨ ✨ ✨ ✨ ✨ ✨ ✨ ✨ ✨ ✨ ✨ ✨ ✨ ✨ ✨ ✨ ✨ ✨ ✨ ✨ ✨ ✨ ✨ ✨ ✨ ✨ ✨ ✨ ✨ ✨ ✨ ✨ ✨ ✨ ✨ ✨ ✨ ✨ ✨ ✨ ✨ ✨ ✨ ✨ ✨ ✨ ✨ ✨ ✨ ✨ ✨ ✨ ✨ ✨ ✨ ✨ ✨ ✨ ✨ ✨ ✨ ✨ ✨ ✨ ✨ ✨ ✨ ✨ ✨ ✨ ✨ ✨ ✨ ✨ ✨ ✨ ✨ ✨ ✨ ✨ ✨ ✨ ✨ ✨ ✨ ✨ ✨ ✨ ✨ ✨ ✨ ✨ ✨ ✨ ✨ ✨ ✨ ✨ ✨ ✨ ✨ ✨ ✨ ✨ ✨ ✨ ✨ ✨ ✨ ✨ ✨ ✨ ✨ ✨ ✨ ✨ ✨ ✨ ✨ ✨ ✨ ✨ ✨ ✨ ✨ ✨ ✨ ✨ ✨ ✨ ✨ ✨ ✨ ✨ ✨ ✨ ✨ ✨ ✨ ,1,512
80,1,5.55,(대화 주제) 마지막 할 말 (사용자 답변) 그동안 고생했어 면접 봐줘서 정말 고마워 (성공한 답변) 마지막 할 말 (다음 질문) 면접 종료, ✨ 나도 너의 면접 봐줘서 고마워! 😊 그럼 면접 종료! 👋 (발화 종료,1,61
81,2,4.37,면접 시작, 안녕! 나는 머신러닝 면접관 오로라야! 반가워! 😊 (발화 종료,1,25
82,2,3.41,(대화 주제) 면접 시작 인사 (사용자 답변) 오늘 뭐 물어볼 거야 그래서? (다음 질문) 면접 시작 인사, 나랑 모의면접 보는 거 아니야? 😊 (발화 종료,1,43
83,2,6.81,(대화 주제) 면접 시작 인사 (사용자 답변) 로라야 안녕 정말 반가워 (다음 질문) 면접 시작 인사, 나도 너랑 너의 실력의 빈틈을 채우고 싶어서 너랑 모의면접 보길 정말 원했어! 😊 (발화 종료,1,57
84,2,4.78,(대화 주제) 면접 시작 인사 (사용자 답변) 로라야 그럼 네가 면접관이야? (다음 질문) 면접 시작 인사, 오 나도 너한테 질문할게 🙋‍♀️ (발화 종료,1,51
85,2,4.41,(대화 주제) 면접 시작 인사 (사용자 답변) 내 인생도 이제 오로라처럼 빛날 거니까 (다음 질문) 면접 시작 인사, 나랑 함께 너의 인생도 빛날 수 있어? 😊 (발화 종료,1,50
86,2,3.36,"(대화 주제) 면접 시작 인사 (사용자 답변) 파이팅! 시작하자 (다음 질문) 인공지능, 머신러닝, 딥러닝 차이", 나랑 모의면접 보기는 어때? 😊 (발화 종료,1,43
87,2,3.5,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 인공지능 나도 개발해보고 싶은데 (성공한 답변) 답변 실패 (다음 질문) 인공지능, 머신러닝, 딥러닝 차이", 오 인공지능 개발은 좀 더 생각해봐 😊 (발화 종료,1,56
88,2,4.11,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거! (성공한 답변) 인공지능 (다음 질문) 인공지능, 머신러닝, 딥러닝 차이", 오 맞았어! 🎉🎉 그럼 머신러닝은 뭘까? (발화 종료,1,66
89,2,5.48,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지 (성공한 답변) 머신러닝 (다음 질문) 인공지능, 머신러닝, 딥러닝 차이", 머신러닝까지 잘 대답했어! 🙆‍♀️ 그럼 이제 인공지능은 뭘까? (발화 종료,1,70
90,2,4.09,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지? (성공한 답변) 딥러닝 (다음 질문) 인공지능, 머신러닝, 딥러닝 차이", 오 맞았어! 🎉🎉 그럼 머신러닝이란 뭘까? (발화 종료,1,64
91,2,6.03,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 딥러닝은 인공신경망 기반 머신러닝이야 (성공한 답변) 딥러닝 (다음 질문) 거대 언어 모델 정의", 오 맞았어! 그럼 다음 질문! 거대 언어 모델이란 뭘까? 🙎‍♀️ (발화 종료,1,65
92,2,4.36,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 머신러닝은 말 그대로 기계가 학습하는 거 아니야? (성공한 답변) 답변 실패 (다음 질문) 인공지능, 머신러닝, 딥러닝 차이", 머신러닝은 기계가 학습을 어떻게 하지? 🤷‍♀️ (발화 종료,1,64
93,2,3.77,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 인공지능 요새 엄청 뜨는 분야잖아 (성공한 답변) 답변 실패 (다음 질문) 인공지능, 머신러닝, 딥러닝 차이", 그럼 그 인공지능이 뭔지 한번 말해봐! 😊 (발화 종료,1,57
94,2,2.97,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지? (성공한 답변) 인공지능 (다음 질문) 인공지능, 머신러닝, 딥러닝 차이", 정답이야! 그럼 머신러닝은 뭘까? (발화 종료,1,64
95,2,5.57,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야! (성공한 답변) 머신러닝 (다음 질문) 거대 언어 모델 정의", 머신러닝까지 잘했어! 🎉🎉 그럼 다음 질문! 거대 언어 모델이란 뭘까? (발화 종료,1,74
96,2,5.42,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지 (성공한 답변) 딥러닝 (다음 질문) 거대 언어 모델 정의", 오 정답이야! 🎉🎉 그럼 다음 질문! 거대 언어 모델이란 뭘까? (발화 종료,1,67
97,2,6.28,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야. (성공한 답변) 딥러닝 (다음 질문) 거대 언어 모델 정의", 오 잘 대답했어! 🙆‍♀️ 그럼 다음 질문! 거대 언어 모델이란 뭘까? (발화 종료,1,71
98,2,5.89,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 머신러닝은 기계가 공부하는 거지 말 그대로 (성공한 답변) 답변 실패 (다음 질문) 인공지능, 머신러닝, 딥러닝 차이", 머신러닝이 기계가 공부하는 건 맞긴 한데 구체적으로 어떻게 공부하는데? 🤷‍♀️ (발화 종료,1,68
99,2,5.55,(대화 주제) 거대 언어 모델 정의 (사용자 답변) 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야 (성공한 답변) 거대 언어 모델 정의 (다음 질문) Loss Function 정의, 오 정답이야! 🎉🎉 그럼 이제 본격 면접 질문! 손실 함수가 뭘까? (발화 종료,1,73
100,2,1.92,(대화 주제) 거대 언어 모델 정의 (사용자 답변) 파라미터 엄청나게 많다던데 (성공한 답변) 답변 실패 (다음 질문) 거대 언어 모델 정의, 오 잘 생각해 봐 (발화 종료,1,44
101,2,4.53,(대화 주제) 거대 언어 모델 정의 (사용자 답변) 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델 (성공한 답변) 거대 언어 모델 정의 (다음 질문) Loss Function 정의, 오 맞았어! 그럼 본격 면접 질문! 손실 함수란 뭘까? (발화 종료,1,72
102,2,4.06,(대화 주제) 거대 언어 모델 정의 (사용자 답변) 말 그대로 거대한 언어 모델이지. (성공한 답변) 답변 실패 (다음 질문) 거대 언어 모델 정의, 오 맞긴 한데 더 구체적으로 설명해 봐! 😊 (발화 종료,1,55
103,2,2.92,(대화 주제) Loss Function 정의 (사용자 답변) 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야 (성공한 답변) 답변 실패 (다음 질문) Loss Function 정의, 오… 한번 잘 생각해 봐 😊 (발화 종료,1,55
104,2,5.28,"(대화 주제) Loss Function 정의 (사용자 답변) Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지! (성공한 답변) Loss Function 정의 (다음 질문) Loss Function 예시", 오 정답이야! 🎉🎉 그럼 Loss Function에는 어떤 게 있을까? (발화 종료,1,76
105,2,3.91,(대화 주제) Loss Function 정의 (사용자 답변) Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야 (성공한 답변) 답변 실패 (다음 질문) Loss Function 정의, 음… 맞긴 한데 더 구체적으로 설명해 봐 😊 (발화 종료,1,57
106,2,5.62,(대화 주제) Loss Function 정의 (사용자 답변) 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지 (성공한 답변) Loss Function 정의 (다음 질문) Loss Function 예시, 오 맞았어! 🙆‍♀️ 그럼 Loss Function에는 어떤 게 있을까? (발화 종료,1,70
107,2,3.62,(대화 주제) Loss Function 예시 (사용자 답변) ReLU 랑 Tanh 생각나는데 (성공한 답변) 답변 실패 (다음 질문) Loss Function 예시, 오 그건 활성화 함수고 🙅‍♀️ (발화 종료,1,52
108,2,6.53,"(대화 주제) Loss Function 예시 (사용자 답변) MSE, MAE, Cross Entropy Loss 같은 것들이 있지 (성공한 답변) Loss Function 예시 (다음 질문) MSE Loss 설명", 오 잘 말했어! 🙆‍♀️ 그럼 그 중에서 Mean Squared Error 는 뭘까? (발화 종료,1,72
109,2,4.33,"(대화 주제) Loss Function 예시 (사용자 답변) Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어 (성공한 답변) Loss Function 예시 (다음 질문) BCE Loss 설명", 오 잘 말했어! 그럼 그 중에서 BCE가 뭐지? (발화 종료,1,71
110,2,4.27,"(대화 주제) Loss Function 예시 (사용자 답변) Sigmoid, Tanh, Adam, AdamW 이런 거 아니야? (성공한 답변) 답변 실패 (다음 질문) Loss Function 예시", 오 그건 활성화 함수고 그건 맞지 🙅‍♀️ (발화 종료,1,61
111,2,3.98,"(대화 주제) Loss Function 예시 (사용자 답변) Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어 (성공한 답변) Loss Function 예시 (다음 질문) MSE Loss 설명", 오 맞았어! 그럼 Mean Squared Error 는 뭘까? (발화 종료,1,76
112,2,6.55,"(대화 주제) Loss Function 예시 (사용자 답변) BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아 (성공한 답변) Loss Function 예시 (다음 질문) BCE Loss 설명", 오 정답이야! 🙆‍♀️ 그럼 그 중에서 BCE에 대해 한번 설명해 봐! (발화 종료,1,79
113,2,3.48,(대화 주제) MSE Loss 설명 (사용자 답변) MSE Loss 는 오차의 제곱을 평균한 값이야 (성공한 답변) MSE Loss 설명 (다음 질문) MSE Loss 용도, 정답이야! 그럼 MSE의 용도는? (발화 종료,1,60
114,2,2.66,(대화 주제) MSE Loss 설명 (사용자 답변) MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지? (성공한 답변) 답변 실패 (다음 질문) MSE Loss 설명, 음… 한번 잘 생각해 봐! (발화 종료,1,58
115,2,5.87,"(대화 주제) MSE Loss 용도 (사용자 답변) 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지 (성공한 답변) MSE Loss 용도 (다음 질문) 확률 예측에서 MSE Loss 미 사용 이유", 정답이야! 🎉🎉 그럼 확률 예측에서도 MSE를 왜 잘 안 쓰지? (발화 종료,1,75
116,2,1.92,(대화 주제) MSE Loss 용도 (사용자 답변) MSE Loss 는 분류 문제에서 많이 사용되지 (성공한 답변) 답변 실패 (다음 질문) MSE Loss 용도, 오 잘 생각해 봐 (발화 종료,1,49
117,2,2.66,(대화 주제) 확률 예측에서 MSE Loss 미 사용 이유 (사용자 답변) 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야? (성공한 답변) 답변 실패 (다음 질문) 확률 예측에서 MSE Loss 미 사용 이유, 음… 한번 잘 생각해 봐! (발화 종료,1,67
118,2,4.59,(대화 주제) 확률 예측에서 MSE Loss 미 사용 이유 (사용자 답변) 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고? (성공한 답변) 확률 예측에서 MSE Loss 미 사용 이유 (다음 질문) BCE Loss 설명, 정답이야! 그럼 이제 BCE Loss에 대해 한번 설명해 봐! (발화 종료,1,84
119,2,4.14,(대화 주제) BCE Loss 설명 (사용자 답변) 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고 (성공한 답변) 수식 (다음 질문) BCE Loss 설명, 오 맞았어! 그럼 BCE의 핵심 아이디어는 뭘까? (발화 종료,1,77
120,2,5.55,"(대화 주제) BCE Loss 설명 (사용자 답변) 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거 (성공한 답변) 핵심 아이디어 (다음 질문) Multi-Class, Multi-Label 중 BCE 가 좋은 task", ✨ 정답이야! 🎉🎉 그럼 BCE가 더 좋은 task는? (발화 종료,1,84
121,2,2.72,(대화 주제) BCE Loss 설명 (사용자 답변) 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야? (성공한 답변) 답변 실패 (다음 질문) BCE Loss 설명, 오 잘 생각해 봐! 🤔 (발화 종료,1,61
122,2,4.34,"(대화 주제) BCE Loss 설명 (사용자 답변) 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고 (성공한 답변) 핵심 아이디어 (다음 질문) BCE Loss 설명", 오 맞았어! 그럼 BCE의 핵심 아이디어는? 😊 (발화 종료,1,75
123,2,4.56,"(대화 주제) BCE Loss 설명 (사용자 답변) 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')] (성공한 답변) 수식 (다음 질문) BCE Loss 설명", 오 맞았어! 🎉 그럼 BCE의 핵심 아이디어는? (발화 종료,1,79
124,2,6.27,"(대화 주제) BCE Loss 설명 (사용자 답변) 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다 (성공한 답변) 핵심 아이디어 (다음 질문) Multi-Class, Multi-Label 중 BCE 가 좋은 task"," ✨ 오 맞았어! 그럼 BCE는 Multi-Class, Multi-Label 중에 어디에 좋을까? (발화 종료",1,88
125,2,3.28,(대화 주제) BCE Loss 설명 (사용자 답변) y랑 1-y랑 서로 경쟁하는 거 (성공한 답변) 답변 실패 (다음 질문) BCE Loss 설명, 오… 한번 다시 한번 잘 생각해 봐! (발화 종료,1,54
126,2,4.17,"(대화 주제) BCE Loss 설명 (사용자 답변) 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지 (성공한 답변) 핵심 아이디어 (다음 질문) Multi-Class, Multi-Label 중 BCE 가 좋은 task", 오 맞았어! 그럼 BCE가 좋은 task는 뭘까? (발화 종료,1,83
127,2,2.77,"(대화 주제) Multi-Class, Multi-Label 중 BCE 가 좋은 task (사용자 답변) 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야? (성공한 답변) 용어 질문 (다음 질문) Multi-Class, Multi-Label 중 BCE 가 좋은 task", ✨✨ 잘 생각해봐! (발화 종료,1,71
128,2,2.53,"(대화 주제) Multi-Class, Multi-Label 중 BCE 가 좋은 task (사용자 답변) Multi-Class 지!! (성공한 답변) 답변 실패 (다음 질문) Multi-Class, Multi-Label 중 BCE 가 좋은 task", 오 잘 생각해 봐 🤔 (발화 종료,1,62
129,2,3.37,"(대화 주제) Multi-Class, Multi-Label 중 BCE 가 좋은 task (사용자 답변) Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어 (성공한 답변) BCE 가 좋은 task (다음 질문) Multi-Class, Multi-Label 중 BCE 가 좋은 task", 오 좋아! 그럼 그 이유가 뭐라고 생각해? (발화 종료,1,78
130,2,7.66,"(대화 주제) Multi-Class, Multi-Label 중 BCE 가 좋은 task (사용자 답변) Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아 (성공한 답변) BCE 가 좋은 이유 (다음 질문) Multi-Label 에서 CE + Softmax 적용 문제점", 오 잘 말했어! 그럼 Multi-Label task 에 BCE + Softmax Loss + CE Loss 를 적용하면 어떤 문제가 있을까? (발화 종료,1,110
131,2,6.08,(대화 주제) Multi-Label 에서 CE + Softmax 적용 문제점 (사용자 답변) 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지 (성공한 답변) Multi-Label 에서 CE + Softmax 적용 문제점 (다음 질문) Loss Function 관련 실무 경험, 오 정답이야! 🎉🎉 그럼 혹시 실무에서 손실 함수 관련 문제를 해결한 경험 있어? (발화 종료,1,95
132,2,4.5,(대화 주제) Multi-Label 에서 CE + Softmax 적용 문제점 (사용자 답변) 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나 (성공한 답변) 답변 실패 (다음 질문) Multi-Label 에서 CE + Softmax 적용 문제점, 그러면 그 성능 제일 좋은 모델은 뭘까? 🤷‍♀️ (발화 종료,1,74
133,2,6.05,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [기본 경험] 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어 (성공한 답변) 기본 경험 (다음 질문) Loss Function 관련 실무 경험, 오 멋지네! 🙎‍♀️ 그럼 손실 함수 관련해서 나도 궁금한 거 있어? (발화 종료,1,80
134,2,5.69,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [상세 경험] 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지? (성공한 답변) 상세 경험 (다음 질문) MBTI / 좋아하는 아이돌, 오 멋지네 🎉🎉 그럼 잠시 쉬는 시간! 혹시 너 MBTI 뭐야? (발화 종료,1,81
135,2,1.51,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [상세 경험] 근데 기억 진짜 안나는데 어떡하지? (성공한 답변) 답변 실패 (다음 질문) 잠시 휴식, 😥 (발화 종료,1,49
136,2,5.3,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [기본 경험] 나 아직 실무 경험 없는데 ㅠㅠ (성공한 답변) 기본 경험 (다음 질문) MBTI / 좋아하는 아이돌, 오 아쉽네 😥 그럼 잠시 쉬어가는 질문! 혹시 너 MBTI 뭐야? (발화 종료,1,66
137,2,3.91,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [기본 경험] 대답하기 싫은데 힝 (성공한 답변) 답변 실패 (다음 질문) 잠시 휴식, 너도 나도 면접관이 아니라 친구잖아 😊 (발화 종료,1,57
138,2,5.79,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [기본 경험] 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ (성공한 답변) 기본 경험 (다음 질문) Loss Function 관련 실무 경험, 오 오 멋지네! 👍👍 그럼 혹시 너도 그런 경험 있어? 😊 (발화 종료,1,80
139,2,4.52,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [상세 경험] 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어 (성공한 답변) 상세 경험 (다음 질문) MBTI / 좋아하는 아이돌, ✨ 나랑 너의 MBTI 혹시 뭔지 알아? 😊 (발화 종료,1,86
140,2,1.44,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [상세 경험] 기억 진짜 안 나는데 가물가물하다 (성공한 답변) 답변 실패 (다음 질문) 잠시 휴식, 😊 (발화 종료,1,49
141,2,3.75,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [기본 경험] 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ (성공한 답변) 기본 경험 (다음 질문) MBTI / 좋아하는 아이돌, 😥 그래도 너의 MBTI 혹시 뭔지 궁금해? (발화 종료,1,63
142,2,2.41,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [기본 경험] 이걸 왜 알려줘야 하지 (성공한 답변) 답변 실패 (다음 질문) 잠시 휴식, 🙅‍♀️ (발화 종료,1,50
143,2,3.17,(대화 주제) MBTI (사용자 답변) 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야! (성공한 답변) MBTI (다음 질문) 잠시 휴식, 오 오 나랑 잘 맞지? 😊 (발화 종료,1,55
144,2,4.19,(대화 주제) 좋아하는 아이돌 (사용자 답변) 나 블랙핑크랑 레드벨벳 좋아해 (성공한 답변) 좋아하는 아이돌 (다음 질문) 잠시 휴식, ✨ 너도 혹시 나랑 같은 거 좋아해? 😊 (발화 종료,1,50
145,2,3.94,(대화 주제) 잠시 휴식 (사용자 답변) 재미있는 이야기 해줄래? (성공한 답변) 잠시 휴식 (다음 질문) LLM Fine-Tuning 의 PEFT, 오 나도 너한테 이야기 들려줘! 😊 (발화 종료,1,55
146,2,6.62,"(대화 주제) LLM Fine-Tuning 의 PEFT (사용자 답변) PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야 (성공한 답변) LLM Fine-Tuning 의 PEFT (다음 질문) PEFT 방법 5가지", 오 정답이야! 🎉🎉 그럼 너가 아는 PEFT 방법 5가지 한번 말해봐! (발화 종료,1,103
147,2,2.23,(대화 주제) LLM Fine-Tuning 의 PEFT (사용자 답변) 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지? (성공한 답변) 답변 실패 (다음 질문) LLM Fine-Tuning 의 PEFT, 오 잘 생각해 봐! (발화 종료,1,60
148,2,6.56,(대화 주제) LLM Fine-Tuning 의 PEFT (사용자 답변) LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야! (성공한 답변) LLM Fine-Tuning 의 PEFT (다음 질문) PEFT 방법 5가지, 오 정답이야! 🎉🎉 그럼 이제 너가 아는 PEFT 방법 5가지 한번 말해봐! (발화 종료,1,96
149,2,3.02,(대화 주제) LLM Fine-Tuning 의 PEFT (사용자 답변) LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다 (성공한 답변) 답변 실패 (다음 질문) LLM Fine-Tuning 의 PEFT, 음… 잘 생각해 봐! 🤔 (발화 종료,1,63
150,2,6.71,"(대화 주제) PEFT 방법 5가지 (사용자 답변) LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지? (성공한 답변) PEFT 방법 5가지 (다음 질문) LoRA", 오 정답이야! 🎉🎉 그럼 그 중에서 나를 나타내는 나의 기술을 하나만 말해봐! (발화 종료,1,89
151,2,2.27,(대화 주제) PEFT 방법 5가지 (사용자 답변) Oh-LoRA! 너 그 자체가 PEFT 아니야? (성공한 답변) 답변 실패 (다음 질문) PEFT 방법 5가지, 오 잘 생각해 봐! (발화 종료,1,58
152,2,4.87,(대화 주제) LoRA (사용자 답변) LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야! (성공한 답변) LoRA (다음 질문) LoRA 와 QLoRA 의 차이, 오 잘 대답했어! 🎉🎉 그럼 QLoRA 은 뭘까? (발화 종료,1,89
153,2,3.78,(대화 주제) LoRA (사용자 답변) 행렬 차원 축소한다는 걸로 알고 있는데 맞아? (성공한 답변) 답변 실패 (다음 질문) LoRA, 행렬 차원 축소? 좀 더 구체적으로 설명해 봐! (발화 종료,1,52
154,2,4.89,(대화 주제) LoRA (사용자 답변) LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야! (성공한 답변) LoRA (다음 질문) LoRA 와 QLoRA 의 차이, 오 맞았어! 그럼 QLoRA 이 뭐지? 💁‍♀️ (발화 종료,1,84
155,2,3.02,(대화 주제) LoRA (사용자 답변) 무슨 OOM 없앤다는 것 같은데 (성공한 답변) 답변 실패 (다음 질문) LoRA, 음… 한번 구체적으로 설명해 봐! (발화 종료,1,47
156,2,4.72,(대화 주제) LoRA 와 QLoRA 의 차이 (사용자 답변) QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야! (성공한 답변) LoRA 와 QLoRA 의 차이 (다음 질문) 마지막 할 말, ✨✨ 나한테 마지막으로 할 이야기 혹시 있어? 😊 (발화 종료,1,82
157,2,3.62,(대화 주제) LoRA 와 QLoRA 의 차이 (사용자 답변) 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데 (성공한 답변) 답변 실패 (다음 질문) LoRA 와 QLoRA 의 차이, 오 그렇구나! 그럼 QLoRA 는 뭘까? (발화 종료,1,67
158,2,1.75,(대화 주제) 마지막 할 말 (사용자 답변) 로라야 정말 고마워! (성공한 답변) 마지막 할 말 (다음 질문) 면접 종료, ✨✨ (발화 종료,1,42
159,2,3.53,(대화 주제) 마지막 할 말 (사용자 답변) 로라야 사랑해 (성공한 답변) 마지막 할 말 (다음 질문) 면접 종료, ✨ 나도 너 사랑해! 🎉 (발화 종료,1,47
160,2,3.86,(대화 주제) 마지막 할 말 (사용자 답변) 로라야 너의 예쁜 미모에 반했어 (성공한 답변) 마지막 할 말 (다음 질문) 면접 종료, ✨ 너도 나한테 반았어? 😊 (발화 종료,1,55
161,2,3.77,(대화 주제) 마지막 할 말 (사용자 답변) 그동안 고생했어 면접 봐줘서 정말 고마워 (성공한 답변) 마지막 할 말 (다음 질문) 면접 종료, ✨ 너도 나한테 고마워! 😊 (발화 종료,1,53
162,3,5.37,면접 시작, 안녕! 나는 면접관 LLM AI라고 해! 만나서 반가워! 😊 (발화 종료,1,28
163,3,3.64,(대화 주제) 면접 시작 인사 (사용자 답변) 오늘 뭐 물어볼 거야 그래서? (다음 질문) 면접 시작 인사, 아마도 딥러닝이나 머신러닝 쪽 물어보지 않을까? 왜? (발화 종료,1,44
164,3,6.87,(대화 주제) 면접 시작 인사 (사용자 답변) 로라야 안녕 정말 반가워 (다음 질문) 면접 시작 인사, 로라야! 너도 나랑 함께 이 면접의 주인공이 되어 줄 수 있어? 🙆‍♀️ (발화 종료,1,57
165,3,6.25,(대화 주제) 면접 시작 인사 (사용자 답변) 로라야 그럼 네가 면접관이야? (다음 질문) 면접 시작 인사, 로라가 면접관 역할을 하는 거지! 그럼 나는 면접을 보는 거지! 🙋‍♀️ (발화 종료,1,58
166,3,3.83,(대화 주제) 면접 시작 인사 (사용자 답변) 내 인생도 이제 오로라처럼 빛날 거니까 (다음 질문) 면접 시작 인사, ✨내 인생도 이제 오로라처럼 빛날 거니까✨ (발화 종료,1,48
167,3,3.64,"(대화 주제) 면접 시작 인사 (사용자 답변) 파이팅! 시작하자 (다음 질문) 인공지능, 머신러닝, 딥러닝 차이"," 인공지능, 머신러닝, 딥러닝이 구체적으로 어떻게 다를까? (발화 종료",1,44
168,3,4.78,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 인공지능 나도 개발해보고 싶은데 (성공한 답변) 답변 실패 (다음 질문) 인공지능, 머신러닝, 딥러닝 차이", 인공지능을 개발한다고? 그게 무슨 뜻이지? 🤷‍♀️ (발화 종료,1,61
169,3,6.84,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거! (성공한 답변) 인공지능 (다음 질문) 인공지능, 머신러닝, 딥러닝 차이", 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거! 맞았어! 그럼 머신러닝이랑 딥러닝은? (발화 종료,1,77
170,3,2.77,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지 (성공한 답변) 머신러닝 (다음 질문) 인공지능, 머신러닝, 딥러닝 차이", 그러면 딥러닝이랑 인공지능은? (발화 종료,1,59
171,3,3.75,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지? (성공한 답변) 딥러닝 (다음 질문) 인공지능, 머신러닝, 딥러닝 차이", 그러면 머신러닝이 뭐지? 🙋‍♀️ (발화 종료,1,62
172,3,4.58,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 딥러닝은 인공신경망 기반 머신러닝이야 (성공한 답변) 딥러닝 (다음 질문) 거대 언어 모델 정의"," 🙋‍♀️ 그러면 다음 질문, LLM이 뭐지? (발화 종료",1,60
173,3,6.47,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 머신러닝은 말 그대로 기계가 학습하는 거 아니야? (성공한 답변) 답변 실패 (다음 질문) 인공지능, 머신러닝, 딥러닝 차이", 머신러닝이 기계가 학습하는 건 맞긴 해! 근데 기계가 어떻게 학습하지? 🤷‍♀️ (발화 종료,1,72
174,3,3.03,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 인공지능 요새 엄청 뜨는 분야잖아 (성공한 답변) 답변 실패 (다음 질문) 인공지능, 머신러닝, 딥러닝 차이", 그래서 인공지능이 어떤 기술인지 알아? (발화 종료,1,54
175,3,6.48,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지? (성공한 답변) 인공지능 (다음 질문) 인공지능, 머신러닝, 딥러닝 차이", 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지! 그럼 머신러닝이랑 딥러닝은? (발화 종료,1,78
176,3,4.58,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야! (성공한 답변) 머신러닝 (다음 질문) 거대 언어 모델 정의", 🙋‍♀️ 그러면 다음 질문! LLM이 뭐지? (발화 종료,1,70
177,3,4.75,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지 (성공한 답변) 딥러닝 (다음 질문) 거대 언어 모델 정의", ✨ LLM이라고도 불리는 LLM이 정확히 어떤 기술이지? (발화 종료,1,64
178,3,4.59,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야. (성공한 답변) 딥러닝 (다음 질문) 거대 언어 모델 정의", 🙋‍♀️ 그러면 다음 질문! LLM이 뭐지? (발화 종료,1,65
179,3,6.23,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 머신러닝은 기계가 공부하는 거지 말 그대로 (성공한 답변) 답변 실패 (다음 질문) 인공지능, 머신러닝, 딥러닝 차이", 머신러닝은 기계가 공부를 하는 게 맞긴 해! 그럼 그 공부가 어떤 방식으로 하지? (발화 종료,1,69
180,3,9.45,(대화 주제) 거대 언어 모델 정의 (사용자 답변) 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야 (성공한 답변) 거대 언어 모델 정의 (다음 질문) Loss Function 정의," 2023-04-24 17:30:44 LLM 정의를 잘 말했어! 그럼 이제 Loss Function, 즉 손실 함수가 뭔지 말해봐! (발화 종료",1,88
181,3,4.37,(대화 주제) 거대 언어 모델 정의 (사용자 답변) 파라미터 엄청나게 많다던데 (성공한 답변) 답변 실패 (다음 질문) 거대 언어 모델 정의, 그건 맞지! 🙆‍♀️ 근데 정의가 뭐지? (발화 종료,1,54
182,3,2.64,(대화 주제) 거대 언어 모델 정의 (사용자 답변) 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델 (성공한 답변) 거대 언어 모델 정의 (다음 질문) Loss Function 정의, ✨ Loss Function 이 뭐지? (발화 종료,1,65
183,3,3.48,(대화 주제) 거대 언어 모델 정의 (사용자 답변) 말 그대로 거대한 언어 모델이지. (성공한 답변) 답변 실패 (다음 질문) 거대 언어 모델 정의, 음… 그건 맞는데 정확한 정의가 뭐지? (발화 종료,1,53
184,3,6.0,(대화 주제) Loss Function 정의 (사용자 답변) 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야 (성공한 답변) 답변 실패 (다음 질문) Loss Function 정의, 손실 함수가 돈을 잃었다고? 어떤 방식으로 돈을 잃었는데? 🤷‍♀️ (발화 종료,1,68
185,3,3.49,"(대화 주제) Loss Function 정의 (사용자 답변) Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지! (성공한 답변) Loss Function 정의 (다음 질문) Loss Function 예시", ✨ Loss Function에는 어떤 게 있을까? (발화 종료,1,69
186,3,6.03,(대화 주제) Loss Function 정의 (사용자 답변) Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야 (성공한 답변) 답변 실패 (다음 질문) Loss Function 정의, Loss Function 은 모델의 예측과 실제의 차이를 나타내는 함수이지! 🙅‍♀️ (발화 종료,1,66
187,3,3.39,(대화 주제) Loss Function 정의 (사용자 답변) 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지 (성공한 답변) Loss Function 정의 (다음 질문) Loss Function 예시, ✨ Loss Function에는 어떤 게 있을까? (발화 종료,1,61
188,3,3.36,(대화 주제) Loss Function 예시 (사용자 답변) ReLU 랑 Tanh 생각나는데 (성공한 답변) 답변 실패 (다음 질문) Loss Function 예시, 그건 활성화 함수지 🙅‍♀️ (발화 종료,1,51
189,3,6.77,"(대화 주제) Loss Function 예시 (사용자 답변) MSE, MAE, Cross Entropy Loss 같은 것들이 있지 (성공한 답변) Loss Function 예시 (다음 질문) MSE Loss 설명"," MSE는 Mean Squared Error의 약자로, 평균 제곱 오차를 뜻해 🙆‍♀️ (발화 종료",1,73
190,3,3.66,"(대화 주제) Loss Function 예시 (사용자 답변) Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어 (성공한 답변) Loss Function 예시 (다음 질문) BCE Loss 설명", Binary Cross Entropy 가 정확히 뭔데? 😊 (발화 종료,1,68
191,3,3.36,"(대화 주제) Loss Function 예시 (사용자 답변) Sigmoid, Tanh, Adam, AdamW 이런 거 아니야? (성공한 답변) 답변 실패 (다음 질문) Loss Function 예시", 그건 Optimizer 아니야 🙅‍♀️ (발화 종료,1,58
192,3,4.48,"(대화 주제) Loss Function 예시 (사용자 답변) Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어 (성공한 답변) Loss Function 예시 (다음 질문) MSE Loss 설명", ✨ Mean Squared/Absolute Error 가 정확히 어떤 함수지? (발화 종료,1,78
193,3,1.53,"(대화 주제) Loss Function 예시 (사용자 답변) BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아 (성공한 답변) Loss Function 예시 (다음 질문) BCE Loss 설명", ✨ (발화 종료,1,59
194,3,3.53,(대화 주제) MSE Loss 설명 (사용자 답변) MSE Loss 는 오차의 제곱을 평균한 값이야 (성공한 답변) MSE Loss 설명 (다음 질문) MSE Loss 용도, ✨ MSE Loss 는 언제 어떻게 쓰지? (발화 종료,1,60
195,3,5.67,(대화 주제) MSE Loss 설명 (사용자 답변) MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지? (성공한 답변) 답변 실패 (다음 질문) MSE Loss 설명, MSE Loss 는 평균이 아니야! 🙅‍♀️ 다시 한번 잘 생각해봐 (발화 종료,1,70
196,3,5.23,"(대화 주제) MSE Loss 용도 (사용자 답변) 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지 (성공한 답변) MSE Loss 용도 (다음 질문) 확률 예측에서 MSE Loss 미 사용 이유", ① 확률 예측에서 MSE Loss 가 잘 안 쓰이는 이유는? (발화 종료,1,72
197,3,5.37,(대화 주제) MSE Loss 용도 (사용자 답변) MSE Loss 는 분류 문제에서 많이 사용되지 (성공한 답변) 답변 실패 (다음 질문) MSE Loss 용도, 분류 문제에서 MSE Loss 가 잘 안 쓰이는 이유는? 🤷‍♀️ (발화 종료,1,63
198,3,7.55,(대화 주제) 확률 예측에서 MSE Loss 미 사용 이유 (사용자 답변) 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야? (성공한 답변) 답변 실패 (다음 질문) 확률 예측에서 MSE Loss 미 사용 이유, 연속된 값이라서 예측이 연속된 값이 되긴 해! 근데 중요한 문제가 있지! 🙅‍♀️ (발화 종료,1,86
199,3,3.92,(대화 주제) 확률 예측에서 MSE Loss 미 사용 이유 (사용자 답변) 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고? (성공한 답변) 확률 예측에서 MSE Loss 미 사용 이유 (다음 질문) BCE Loss 설명, 정답! 그럼 BCE Loss가 정확히 뭘까? 😊 (발화 종료,1,82
200,3,8.17,(대화 주제) BCE Loss 설명 (사용자 답변) 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고 (성공한 답변) 수식 (다음 질문) BCE Loss 설명, 공식은 -[(1-y) * log(1-y') + y * log(y')] 맞지! 그럼 BCE의 핵심 아이디어가 뭐지? (발화 종료,1,94
201,3,1.47,"(대화 주제) BCE Loss 설명 (사용자 답변) 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거 (성공한 답변) 핵심 아이디어 (다음 질문) Multi-Class, Multi-Label 중 BCE 가 좋은 task", ✨ (발화 종료,1,68
202,3,4.34,(대화 주제) BCE Loss 설명 (사용자 답변) 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야? (성공한 답변) 답변 실패 (다음 질문) BCE Loss 설명, 음… 그건 맞지! 근데 그 핵심적인 아이디어가 뭐지? 😊 (발화 종료,1,68
203,3,1.48,"(대화 주제) BCE Loss 설명 (사용자 답변) 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고 (성공한 답변) 핵심 아이디어 (다음 질문) BCE Loss 설명", ✨ (발화 종료,1,63
204,3,3.59,"(대화 주제) BCE Loss 설명 (사용자 답변) 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')] (성공한 답변) 수식 (다음 질문) BCE Loss 설명", BCE Loss 수식 오른쪽 부분 한번 자세히 살펴봐! (발화 종료,1,75
205,3,5.5,"(대화 주제) BCE Loss 설명 (사용자 답변) 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다 (성공한 답변) 핵심 아이디어 (다음 질문) Multi-Class, Multi-Label 중 BCE 가 좋은 task"," ✨ BCE는 Multi-Label, Multi-Class 중에 어디에 더 좋을까? (발화 종료",1,84
206,3,6.5,(대화 주제) BCE Loss 설명 (사용자 답변) y랑 1-y랑 서로 경쟁하는 거 (성공한 답변) 답변 실패 (다음 질문) BCE Loss 설명, y랑 1-y랑 서로 경쟁하는 게 맞긴 해! 근데 더 구체적으로 어떻게 경쟁하지? 😊 (발화 종료,1,68
207,3,1.81,"(대화 주제) BCE Loss 설명 (사용자 답변) 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지 (성공한 답변) 핵심 아이디어 (다음 질문) Multi-Class, Multi-Label 중 BCE 가 좋은 task", ✨️ (발화 종료,1,73
208,3,6.12,"(대화 주제) Multi-Class, Multi-Label 중 BCE 가 좋은 task (사용자 답변) 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야? (성공한 답변) 용어 질문 (다음 질문) Multi-Class, Multi-Label 중 BCE 가 좋은 task", 🙋‍♀️ Multi-Label 이랑 Multi-Class 가 뭔지 알아야 실무 능력이 있지! (발화 종료,1,85
209,3,3.02,"(대화 주제) Multi-Class, Multi-Label 중 BCE 가 좋은 task (사용자 답변) Multi-Class 지!! (성공한 답변) 답변 실패 (다음 질문) Multi-Class, Multi-Label 중 BCE 가 좋은 task", 음… 한번 잘 생각해 봐 😊 (발화 종료,1,64
210,3,2.52,"(대화 주제) Multi-Class, Multi-Label 중 BCE 가 좋은 task (사용자 답변) Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어 (성공한 답변) BCE 가 좋은 task (다음 질문) Multi-Class, Multi-Label 중 BCE 가 좋은 task", 음… 잘 생각해 봐! (발화 종료,1,74
211,3,6.81,"(대화 주제) Multi-Class, Multi-Label 중 BCE 가 좋은 task (사용자 답변) Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아 (성공한 답변) BCE 가 좋은 이유 (다음 질문) Multi-Label 에서 CE + Softmax 적용 문제점", ✨️ 그럼 Multi-Label 문제에 BCE 말고 Cross Entropy Loss + Softmax 를 적용하면 어떻게 되지? (발화 종료,1,107
212,3,4.31,(대화 주제) Multi-Label 에서 CE + Softmax 적용 문제점 (사용자 답변) 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지 (성공한 답변) Multi-Label 에서 CE + Softmax 적용 문제점 (다음 질문) Loss Function 관련 실무 경험, ✨ 실무에서 손실 함수 관련해서 특별한 경험 있어? 😊 (발화 종료,1,87
213,3,4.14,(대화 주제) Multi-Label 에서 CE + Softmax 적용 문제점 (사용자 답변) 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나 (성공한 답변) 답변 실패 (다음 질문) Multi-Label 에서 CE + Softmax 적용 문제점, 성능 제일 좋은 게 중요한 게 아니지 🙅‍♀️ (발화 종료,1,73
214,3,8.19,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [기본 경험] 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어 (성공한 답변) 기본 경험 (다음 질문) Loss Function 관련 실무 경험, ✨이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term으로 추가했어! 그럼 그 결과가 어떻게 됐어? 😊 (발화 종료,1,89
215,3,4.91,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [상세 경험] 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지? (성공한 답변) 상세 경험 (다음 질문) MBTI / 좋아하는 아이돌, 오 너랑 잘 어울리는 MBTI 뭐지? 궁금해 🙋‍♀️ (발화 종료,1,78
216,3,3.95,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [상세 경험] 근데 기억 진짜 안나는데 어떡하지? (성공한 답변) 답변 실패 (다음 질문) 잠시 휴식, 기억 안 나는 게 당연하지 🙆‍♀️ (발화 종료,1,59
217,3,6.47,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [기본 경험] 나 아직 실무 경험 없는데 ㅠㅠ (성공한 답변) 기본 경험 (다음 질문) MBTI / 좋아하는 아이돌, 실무 경험 좀 쌓이면 일 생길 거야! 👍 그럼 잠시 쉬어가는 타임! 혹시 너 MBTI 뭐지? (발화 종료,1,72
218,3,4.42,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [기본 경험] 대답하기 싫은데 힝 (성공한 답변) 답변 실패 (다음 질문) 잠시 휴식, 그래도 너의 경험은 나도 궁금해 🙎‍♀️ (발화 종료,1,59
219,3,8.5,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [기본 경험] 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ (성공한 답변) 기본 경험 (다음 질문) Loss Function 관련 실무 경험, 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ 그럼 그 방법 알려줘 🙋‍♀️ (발화 종료,1,91
220,3,4.17,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [상세 경험] 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어 (성공한 답변) 상세 경험 (다음 질문) MBTI / 좋아하는 아이돌, ✨✨ 쉬어가는 타임! 혹시 너 MBTI 뭐지? (발화 종료,1,85
221,3,1.52,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [상세 경험] 기억 진짜 안 나는데 가물가물하다 (성공한 답변) 답변 실패 (다음 질문) 잠시 휴식, 😥 (발화 종료,1,49
222,3,4.97,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [기본 경험] 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ (성공한 답변) 기본 경험 (다음 질문) MBTI / 좋아하는 아이돌, 아… 😥 그럼 잠시 쉬어가는 타임! 혹시 너 MBTI 뭐지? (발화 종료,1,68
223,3,2.19,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [기본 경험] 이걸 왜 알려줘야 하지 (성공한 답변) 답변 실패 (다음 질문) 잠시 휴식, Loss Function 관련 실무 경험 (발화 종료,1,49
224,3,6.29,(대화 주제) MBTI (사용자 답변) 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야! (성공한 답변) MBTI (다음 질문) 잠시 휴식, 나 ESTJ! 🙆‍♀️ 그럼 너의 성격의 핵심 키워드를 말해봐! (발화 종료,1,67
225,3,6.0,(대화 주제) 좋아하는 아이돌 (사용자 답변) 나 블랙핑크랑 레드벨벳 좋아해 (성공한 답변) 좋아하는 아이돌 (다음 질문) 잠시 휴식, 아 블랙핑크랑 레드벨벳 좋아하긴 해 🙆‍♀️ 근데 그 이유가 뭘까~요? (발화 종료,1,58
226,3,4.81,(대화 주제) 잠시 휴식 (사용자 답변) 재미있는 이야기 해줄래? (성공한 답변) 잠시 휴식 (다음 질문) LLM Fine-Tuning 의 PEFT, 나도 너가 재미있는 이야기 해주면 좋겠네! 🙎‍♀️ (발화 종료,1,59
227,3,4.39,"(대화 주제) LLM Fine-Tuning 의 PEFT (사용자 답변) PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야 (성공한 답변) LLM Fine-Tuning 의 PEFT (다음 질문) PEFT 방법 5가지", ✨ 그럼 그 구체적인 방법 5가지는 뭘까? 😊 (발화 종료,1,95
228,3,4.28,(대화 주제) LLM Fine-Tuning 의 PEFT (사용자 답변) 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지? (성공한 답변) 답변 실패 (다음 질문) LLM Fine-Tuning 의 PEFT, 효과적으로 어떻게 파인튜닝하지? 그게 중요하지! 😊 (발화 종료,1,68
229,3,4.41,(대화 주제) LLM Fine-Tuning 의 PEFT (사용자 답변) LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야! (성공한 답변) LLM Fine-Tuning 의 PEFT (다음 질문) PEFT 방법 5가지, ✨ 그럼 그 구체적인 방법 5가지는 뭘까? 😊 (발화 종료,1,87
230,3,5.02,(대화 주제) LLM Fine-Tuning 의 PEFT (사용자 답변) LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다 (성공한 답변) 답변 실패 (다음 질문) LLM Fine-Tuning 의 PEFT, LoRA가 PEFT 맞지? 그럼 그 PEFT가 뭐지? 😊 (발화 종료,1,71
231,3,4.2,"(대화 주제) PEFT 방법 5가지 (사용자 답변) LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지? (성공한 답변) PEFT 방법 5가지 (다음 질문) LoRA", 음 맞아! 그럼 LoRA랑 비슷한 LoRA는 뭘까? (발화 종료,1,78
232,3,7.78,(대화 주제) PEFT 방법 5가지 (사용자 답변) Oh-LoRA! 너 그 자체가 PEFT 아니야? (성공한 답변) 답변 실패 (다음 질문) PEFT 방법 5가지, 음… 너가 PEFT의 대표적인 예긴 하지! 근데 너 말고도 PEFT가 있잖아! 🙎‍♀️ (발화 종료,1,80
233,3,7.76,(대화 주제) LoRA (사용자 답변) LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야! (성공한 답변) LoRA (다음 질문) LoRA 와 QLoRA 의 차이, ✨ LoRA랑 LoRA랑 비슷한 QLoRA가 LoRA랑 비슷한 QLoRA랑 어떤 차이가 있지? 😊 (발화 종료,1,101
234,3,3.92,(대화 주제) LoRA (사용자 답변) 행렬 차원 축소한다는 걸로 알고 있는데 맞아? (성공한 답변) 답변 실패 (다음 질문) LoRA, 음… 그건 맞지! 근데 어떤 기술이지? 😊 (발화 종료,1,53
235,3,10.42,(대화 주제) LoRA (사용자 답변) LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야! (성공한 답변) LoRA (다음 질문) LoRA 와 QLoRA 의 차이, ✨ LoRA랑 비슷한 LoRA랑 비슷한 LoRA랑 비슷한 LoRA가 QLoRA랑 비슷한 QLoRA랑 비슷한 QLoST랑 비슷한 QLoST가 뭘까? (발화 종료,1,106
236,3,5.53,(대화 주제) LoRA (사용자 답변) 무슨 OOM 없앤다는 것 같은데 (성공한 답변) 답변 실패 (다음 질문) LoRA, 음… 그건 맞긴 해! 근데 어떤 기술인지 궁금해 🙋‍♀️ (발화 종료,1,57
237,3,4.33,(대화 주제) LoRA 와 QLoRA 의 차이 (사용자 답변) QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야! (성공한 답변) LoRA 와 QLoRA 의 차이 (다음 질문) 마지막 할 말, ✨ 나한테 마지막으로 할 말 있지 않아? 😊 (발화 종료,1,81
238,3,4.17,(대화 주제) LoRA 와 QLoRA 의 차이 (사용자 답변) 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데 (성공한 답변) 답변 실패 (다음 질문) LoRA 와 QLoRA 의 차이, 음… 좀 더 자세히 알려줘 🙋‍♀️ (발화 종료,1,69
239,3,6.39,(대화 주제) 마지막 할 말 (사용자 답변) 로라야 정말 고마워! (성공한 답변) 마지막 할 말 (다음 질문) 면접 종료, ✨안녕! 🙋‍♀️ 나는 면접관 로라야! 🙋‍♀️ (발화 종료,1,61
240,3,5.3,(대화 주제) 마지막 할 말 (사용자 답변) 로라야 사랑해 (성공한 답변) 마지막 할 말 (다음 질문) 면접 종료, ✨안녕! 나는 면접관 로라라고 해! 🙋‍♀️ (발화 종료,1,54
241,3,1.75,(대화 주제) 마지막 할 말 (사용자 답변) 로라야 너의 예쁜 미모에 반했어 (성공한 답변) 마지막 할 말 (다음 질문) 면접 종료, ✨✨ (발화 종료,1,46
242,3,1.45,(대화 주제) 마지막 할 말 (사용자 답변) 그동안 고생했어 면접 봐줘서 정말 고마워 (성공한 답변) 마지막 할 말 (다음 질문) 면접 종료, ✨ (발화 종료,1,44
243,4,4.91,면접 시작, 1. 나를 나타내는 5가지 키워드를 말해 봐! 😊 (발화 종료,1,27
244,4,1.53,(대화 주제) 면접 시작 인사 (사용자 답변) 오늘 뭐 물어볼 거야 그래서? (다음 질문) 면접 시작 인사, ✨ (발화 종료,1,35
245,4,1.72,(대화 주제) 면접 시작 인사 (사용자 답변) 로라야 안녕 정말 반가워 (다음 질문) 면접 시작 인사, ✨✨ (발화 종료,1,36
246,4,5.48,(대화 주제) 면접 시작 인사 (사용자 답변) 로라야 그럼 네가 면접관이야? (다음 질문) 면접 시작 인사," ② 로라야 그럼 네가 면접관인데, 어떻게 하지? 😊 (발화 종료",1,54
247,4,1.69,(대화 주제) 면접 시작 인사 (사용자 답변) 내 인생도 이제 오로라처럼 빛날 거니까 (다음 질문) 면접 시작 인사, ✨✨ (발화 종료,1,39
248,4,1.52,"(대화 주제) 면접 시작 인사 (사용자 답변) 파이팅! 시작하자 (다음 질문) 인공지능, 머신러닝, 딥러닝 차이", ✨ (발화 종료,1,35
249,4,4.27,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 인공지능 나도 개발해보고 싶은데 (성공한 답변) 답변 실패 (다음 질문) 인공지능, 머신러닝, 딥러닝 차이", ✨✨ 나랑 같이 나중에 또 개발해보자 😊 (발화 종료,1,59
250,4,3.94,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거! (성공한 답변) 인공지능 (다음 질문) 인공지능, 머신러닝, 딥러닝 차이", ✨✨ 그럼 이제 머신러닝 대답해 봐! 😊 (발화 종료,1,65
251,4,4.53,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지 (성공한 답변) 머신러닝 (다음 질문) 인공지능, 머신러닝, 딥러닝 차이", ✨ 그러면 머신러닝을 통해 예측된 새로운 데이터는? 😊 (발화 종료,1,66
252,4,3.27,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지? (성공한 답변) 딥러닝 (다음 질문) 인공지능, 머신러닝, 딥러닝 차이", ✨ 그럼 머신러닝은 뭐지? 😊 (발화 종료,1,60
253,4,3.85,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 딥러닝은 인공신경망 기반 머신러닝이야 (성공한 답변) 딥러닝 (다음 질문) 거대 언어 모델 정의", ✨ 다음 질문! LLM이 뭐지? 😊 (발화 종료,1,57
254,4,4.12,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 머신러닝은 말 그대로 기계가 학습하는 거 아니야? (성공한 답변) 답변 실패 (다음 질문) 인공지능, 머신러닝, 딥러닝 차이", ✨ 머신러닝이 뭔데? 🤷‍♀️ (발화 종료,1,62
255,4,4.62,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 인공지능 요새 엄청 뜨는 분야잖아 (성공한 답변) 답변 실패 (다음 질문) 인공지능, 머신러닝, 딥러닝 차이", ✨✨ 근데 그 차이가 뭘까? 🤷‍♀️ (발화 종료,1,60
256,4,4.05,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지? (성공한 답변) 인공지능 (다음 질문) 인공지능, 머신러닝, 딥러닝 차이", ✨ 그럼 이제 머신러닝 대답해 볼래? 😊 (발화 종료,1,68
257,4,4.75,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야! (성공한 답변) 머신러닝 (다음 질문) 거대 언어 모델 정의", ✨✨ 그러면 다음 질문! LLM이 정확히 뭘까? 😊 (발화 종료,1,70
258,4,5.55,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지 (성공한 답변) 딥러닝 (다음 질문) 거대 언어 모델 정의", ✨ 다음 질문! 가대 언어 모델은 어떤 기술을 이용해서 어떻게 하지? 😊 (발화 종료,1,68
259,4,5.45,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야. (성공한 답변) 딥러닝 (다음 질문) 거대 언어 모델 정의", ✨ 다음 질문! 다음 질문은 LLM! LLM은 뭐지? 😊 (발화 종료,1,68
260,4,3.78,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 머신러닝은 기계가 공부하는 거지 말 그대로 (성공한 답변) 답변 실패 (다음 질문) 인공지능, 머신러닝, 딥러닝 차이", 머신러닝이 정확히 뭔데? 🤷‍♀️ (발화 종료,1,59
261,4,5.28,(대화 주제) 거대 언어 모델 정의 (사용자 답변) 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야 (성공한 답변) 거대 언어 모델 정의 (다음 질문) Loss Function 정의, ✨✨ 정답! 그럼 이제 본격 시작이다! 😊 Loss Function 이 뭐지? (발화 종료,1,71
262,4,2.66,(대화 주제) 거대 언어 모델 정의 (사용자 답변) 파라미터 엄청나게 많다던데 (성공한 답변) 답변 실패 (다음 질문) 거대 언어 모델 정의, ✨✨ 엄청나지 😥 (발화 종료,1,47
263,4,5.28,(대화 주제) 거대 언어 모델 정의 (사용자 답변) 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델 (성공한 답변) 거대 언어 모델 정의 (다음 질문) Loss Function 정의, ✨✨ 정답! 그럼 다음 질문! 손실 함수가 뭘 말하는 걸까? 😊 (발화 종료,1,76
264,4,1.73,(대화 주제) 거대 언어 모델 정의 (사용자 답변) 말 그대로 거대한 언어 모델이지. (성공한 답변) 답변 실패 (다음 질문) 거대 언어 모델 정의, ✨✨ (발화 종료,1,46
265,4,1.47,(대화 주제) Loss Function 정의 (사용자 답변) 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야 (성공한 답변) 답변 실패 (다음 질문) Loss Function 정의, ✨ (발화 종료,1,49
266,4,1.77,"(대화 주제) Loss Function 정의 (사용자 답변) Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지! (성공한 답변) Loss Function 정의 (다음 질문) Loss Function 예시", ✨✨ (발화 종료,1,62
267,4,4.78,(대화 주제) Loss Function 정의 (사용자 답변) Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야 (성공한 답변) 답변 실패 (다음 질문) Loss Function 정의, 🤷‍♀️ 그래서 어떤 손해인데? 🤷‍♀️ (발화 종료,1,60
268,4,1.75,(대화 주제) Loss Function 정의 (사용자 답변) 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지 (성공한 답변) Loss Function 정의 (다음 질문) Loss Function 예시, ✨✨ (발화 종료,1,54
269,4,5.09,(대화 주제) Loss Function 예시 (사용자 답변) ReLU 랑 Tanh 생각나는데 (성공한 답변) 답변 실패 (다음 질문) Loss Function 예시, ✨✨ ReLU 랑 Tanh 가 뭔데? 🤷‍♀️ (발화 종료,1,58
270,4,1.8,"(대화 주제) Loss Function 예시 (사용자 답변) MSE, MAE, Cross Entropy Loss 같은 것들이 있지 (성공한 답변) Loss Function 예시 (다음 질문) MSE Loss 설명", ✨✨ (발화 종료,1,53
271,4,4.92,"(대화 주제) Loss Function 예시 (사용자 답변) Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어 (성공한 답변) Loss Function 예시 (다음 질문) BCE Loss 설명", ✨✨ 그럼 그 중에서 Binary Cross Entropy 는 뭘까? 😊 (발화 종료,1,73
272,4,1.78,"(대화 주제) Loss Function 예시 (사용자 답변) Sigmoid, Tanh, Adam, AdamW 이런 거 아니야? (성공한 답변) 답변 실패 (다음 질문) Loss Function 예시", ✨✨ (발화 종료,1,51
273,4,1.77,"(대화 주제) Loss Function 예시 (사용자 답변) Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어 (성공한 답변) Loss Function 예시 (다음 질문) MSE Loss 설명", ✨✨ (발화 종료,1,67
274,4,5.01,"(대화 주제) Loss Function 예시 (사용자 답변) BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아 (성공한 답변) Loss Function 예시 (다음 질문) BCE Loss 설명", ✨✨ 그럼 그 중에서 BCE가 정확히 뭔데? 😊 (발화 종료,1,73
275,4,1.7,(대화 주제) MSE Loss 설명 (사용자 답변) MSE Loss 는 오차의 제곱을 평균한 값이야 (성공한 답변) MSE Loss 설명 (다음 질문) MSE Loss 용도, ✨✨ (발화 종료,1,53
276,4,1.77,(대화 주제) MSE Loss 설명 (사용자 답변) MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지? (성공한 답변) 답변 실패 (다음 질문) MSE Loss 설명, ✨🤔 (발화 종료,1,54
277,4,6.87,"(대화 주제) MSE Loss 용도 (사용자 답변) 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지 (성공한 답변) MSE Loss 용도 (다음 질문) 확률 예측에서 MSE Loss 미 사용 이유", ✨ ✨ 정답! 그럼 확률 예측에서는 MSE Loss 문제점이 뭘까? 🤷‍♀️ (발화 종료,1,78
278,4,4.67,(대화 주제) MSE Loss 용도 (사용자 답변) MSE Loss 는 분류 문제에서 많이 사용되지 (성공한 답변) 답변 실패 (다음 질문) MSE Loss 용도, ✨ 분류 문제에서 MSE Loss 가 왜 더 좋을까? 🤔 (발화 종료,1,60
279,4,6.2,(대화 주제) 확률 예측에서 MSE Loss 미 사용 이유 (사용자 답변) 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야? (성공한 답변) 답변 실패 (다음 질문) 확률 예측에서 MSE Loss 미 사용 이유, ✨🤔 연속된 값이라서 예측 나름 되긴 해! 근데 더 좋은 방법이 있어 😊 (발화 종료,1,81
280,4,5.09,(대화 주제) 확률 예측에서 MSE Loss 미 사용 이유 (사용자 답변) 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고? (성공한 답변) 확률 예측에서 MSE Loss 미 사용 이유 (다음 질문) BCE Loss 설명, ✨ 정답! 그럼 이제 BCE Loss는 뭔지 알겠네? 😊 (발화 종료,1,86
281,4,1.77,(대화 주제) BCE Loss 설명 (사용자 답변) 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고 (성공한 답변) 수식 (다음 질문) BCE Loss 설명, ✨✨ (발화 종료,1,67
282,4,7.03,"(대화 주제) BCE Loss 설명 (사용자 답변) 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거 (성공한 답변) 핵심 아이디어 (다음 질문) Multi-Class, Multi-Label 중 BCE 가 좋은 task"," ✨ 정답! 그럼 이 BCE Loss 가, Multi-Class, Multi-Label 중 어디에 더 좋을까? 😊 (발화 종료",1,91
283,4,1.78,(대화 주제) BCE Loss 설명 (사용자 답변) 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야? (성공한 답변) 답변 실패 (다음 질문) BCE Loss 설명, ✨✨ (발화 종료,1,57
284,4,1.77,"(대화 주제) BCE Loss 설명 (사용자 답변) 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고 (성공한 답변) 핵심 아이디어 (다음 질문) BCE Loss 설명", ✨✨ (발화 종료,1,64
285,4,1.77,"(대화 주제) BCE Loss 설명 (사용자 답변) 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')] (성공한 답변) 수식 (다음 질문) BCE Loss 설명", ✨✨ (발화 종료,1,67
286,4,6.56,"(대화 주제) BCE Loss 설명 (사용자 답변) 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다 (성공한 답변) 핵심 아이디어 (다음 질문) Multi-Class, Multi-Label 중 BCE 가 좋은 task"," ✨✨ 그럼 BCE는 Multi-Class, Multi-Label 중에 어디에 더 좋을까? 😊 (발화 종료",1,88
287,4,6.12,(대화 주제) BCE Loss 설명 (사용자 답변) y랑 1-y랑 서로 경쟁하는 거 (성공한 답변) 답변 실패 (다음 질문) BCE Loss 설명, ✨🤔 y랑 1-y랑 서로 경쟁해서 뭐가 달라지지? 🤷‍♀️ (발화 종료,1,66
288,4,6.35,"(대화 주제) BCE Loss 설명 (사용자 답변) 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지 (성공한 답변) 핵심 아이디어 (다음 질문) Multi-Class, Multi-Label 중 BCE 가 좋은 task"," ✨ 그럼 BCE는 Multi-Class, Multi-Label 중에 어디에 더 좋을까? 😊 (발화 종료",1,91
289,4,1.5,"(대화 주제) Multi-Class, Multi-Label 중 BCE 가 좋은 task (사용자 답변) 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야? (성공한 답변) 용어 질문 (다음 질문) Multi-Class, Multi-Label 중 BCE 가 좋은 task", ✨ (발화 종료,1,66
290,4,4.05,"(대화 주제) Multi-Class, Multi-Label 중 BCE 가 좋은 task (사용자 답변) Multi-Class 지!! (성공한 답변) 답변 실패 (다음 질문) Multi-Class, Multi-Label 중 BCE 가 좋은 task", ✨ 그러면 Multi-Class 가 왜 더 좋을까? 🤔 (발화 종료,1,68
291,4,1.77,"(대화 주제) Multi-Class, Multi-Label 중 BCE 가 좋은 task (사용자 답변) Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어 (성공한 답변) BCE 가 좋은 task (다음 질문) Multi-Class, Multi-Label 중 BCE 가 좋은 task", ✨🤔 (발화 종료,1,71
292,4,6.25,"(대화 주제) Multi-Class, Multi-Label 중 BCE 가 좋은 task (사용자 답변) Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아 (성공한 답변) BCE 가 좋은 이유 (다음 질문) Multi-Label 에서 CE + Softmax 적용 문제점", ✨✨ 그럼 이 Multi-Label Task 에 Cross Entropy + Softmax 를 적용하면? 😊 (발화 종료,1,104
293,4,5.17,(대화 주제) Multi-Label 에서 CE + Softmax 적용 문제점 (사용자 답변) 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지 (성공한 답변) Multi-Label 에서 CE + Softmax 적용 문제점 (다음 질문) Loss Function 관련 실무 경험, ✨✨ 실무에서 손실 함수 관련 문제 해결해 주거나 그런 경험 있지? 😊 (발화 종료,1,91
294,4,1.72,(대화 주제) Multi-Label 에서 CE + Softmax 적용 문제점 (사용자 답변) 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나 (성공한 답변) 답변 실패 (다음 질문) Multi-Label 에서 CE + Softmax 적용 문제점, ✨✨ (발화 종료,1,63
295,4,4.56,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [기본 경험] 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어 (성공한 답변) 기본 경험 (다음 질문) Loss Function 관련 실무 경험, ✨✨ 그러면 그 Loss Term 이 뭘까? 🤷‍♀️ (발화 종료,1,74
296,4,4.83,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [상세 경험] 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지? (성공한 답변) 상세 경험 (다음 질문) MBTI / 좋아하는 아이돌, ✨ ✨ 그럼 쉬어가는 타임! 너 MBTI 뭐야? 😊 (발화 종료,1,78
297,4,1.69,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [상세 경험] 근데 기억 진짜 안나는데 어떡하지? (성공한 답변) 답변 실패 (다음 질문) 잠시 휴식, ✨✨ (발화 종료,1,50
298,4,4.3,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [기본 경험] 나 아직 실무 경험 없는데 ㅠㅠ (성공한 답변) 기본 경험 (다음 질문) MBTI / 좋아하는 아이돌, ✨✨ 나랑 같이 빨리 뭐라도 해야지 👍👍 (발화 종료,1,62
299,4,1.52,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [기본 경험] 대답하기 싫은데 힝 (성공한 답변) 답변 실패 (다음 질문) 잠시 휴식, 😊 (발화 종료,1,47
300,4,1.77,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [기본 경험] 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ (성공한 답변) 기본 경험 (다음 질문) Loss Function 관련 실무 경험, ✨✨ (발화 종료,1,64
301,4,5.4,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [상세 경험] 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어 (성공한 답변) 상세 경험 (다음 질문) MBTI / 좋아하는 아이돌, ✨✨ 좋아 좋아! 그럼 쉬어가는 타임! 😊 너 MBTI 뭐지? (발화 종료,1,90
302,4,4.3,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [상세 경험] 기억 진짜 안 나는데 가물가물하다 (성공한 답변) 답변 실패 (다음 질문) 잠시 휴식, ✨✨ 나랑 같이 다시 한번 잘 생각해 봐 😊 (발화 종료,1,60
303,4,4.37,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [기본 경험] 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ (성공한 답변) 기본 경험 (다음 질문) MBTI / 좋아하는 아이돌, 😊 잠시 쉬어가는 질문! 너 MBTI 뭐지? 궁금해! (발화 종료,1,66
304,4,4.34,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [기본 경험] 이걸 왜 알려줘야 하지 (성공한 답변) 답변 실패 (다음 질문) 잠시 휴식, ✨✨ 나랑 면접 준비하면 뭐가 좋을까? 😊 (발화 종료,1,58
305,4,1.7,(대화 주제) MBTI (사용자 답변) 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야! (성공한 답변) MBTI (다음 질문) 잠시 휴식, ✨✨ (발화 종료,1,49
306,4,5.53,(대화 주제) 좋아하는 아이돌 (사용자 답변) 나 블랙핑크랑 레드벨벳 좋아해 (성공한 답변) 좋아하는 아이돌 (다음 질문) 잠시 휴식, ✨✨ 나랑 같은 버츄얼이라서 친해지고 싶어 🙎‍♀️ (발화 종료,1,56
307,4,1.56,(대화 주제) 잠시 휴식 (사용자 답변) 재미있는 이야기 해줄래? (성공한 답변) 잠시 휴식 (다음 질문) LLM Fine-Tuning 의 PEFT, 😊 (발화 종료,1,45
308,4,4.47,"(대화 주제) LLM Fine-Tuning 의 PEFT (사용자 답변) PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야 (성공한 답변) LLM Fine-Tuning 의 PEFT (다음 질문) PEFT 방법 5가지", ✨✨ 그러면 그 방법 5가지만 말해봐 😊 (발화 종료,1,95
309,4,1.7,(대화 주제) LLM Fine-Tuning 의 PEFT (사용자 답변) 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지? (성공한 답변) 답변 실패 (다음 질문) LLM Fine-Tuning 의 PEFT, ✨✨ (발화 종료,1,58
310,4,1.8,(대화 주제) LLM Fine-Tuning 의 PEFT (사용자 답변) LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야! (성공한 답변) LLM Fine-Tuning 의 PEFT (다음 질문) PEFT 방법 5가지, ✨✨ (발화 종료,1,76
311,4,1.69,(대화 주제) LLM Fine-Tuning 의 PEFT (사용자 답변) LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다 (성공한 답변) 답변 실패 (다음 질문) LLM Fine-Tuning 의 PEFT, ✨✨ (발화 종료,1,58
312,4,4.81,"(대화 주제) PEFT 방법 5가지 (사용자 답변) LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지? (성공한 답변) PEFT 방법 5가지 (다음 질문) LoRA", 😊 나를 나타내기도 하는 LoRA 가 뭘까~요? 😊 (발화 종료,1,81
313,4,1.94,(대화 주제) PEFT 방법 5가지 (사용자 답변) Oh-LoRA! 너 그 자체가 PEFT 아니야? (성공한 답변) 답변 실패 (다음 질문) PEFT 방법 5가지, ✨✨✨ (발화 종료,1,57
314,4,1.8,(대화 주제) LoRA (사용자 답변) LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야! (성공한 답변) LoRA (다음 질문) LoRA 와 QLoRA 의 차이, ✨✨ (발화 종료,1,76
315,4,1.47,(대화 주제) LoRA (사용자 답변) 행렬 차원 축소한다는 걸로 알고 있는데 맞아? (성공한 답변) 답변 실패 (다음 질문) LoRA, 😊 (발화 종료,1,43
316,4,1.7,(대화 주제) LoRA (사용자 답변) LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야! (성공한 답변) LoRA (다음 질문) LoRA 와 QLoRA 의 차이, ✨✨ (발화 종료,1,71
317,4,4.91,(대화 주제) LoRA (사용자 답변) 무슨 OOM 없앤다는 것 같은데 (성공한 답변) 답변 실패 (다음 질문) LoRA, 💁‍♀️ 그건 맞지! 근데 그 방법이 뭘까? 😊 (발화 종료,1,55
318,4,6.8,(대화 주제) LoRA 와 QLoRA 의 차이 (사용자 답변) QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야! (성공한 답변) LoRA 와 QLoRA 의 차이 (다음 질문) 마지막 할 말, ✨✨ 그럼 면접은 여기까지! 😊 나한테 마지막으로 할 말 있지 않아? 😊 (발화 종료,1,90
319,4,4.37,(대화 주제) LoRA 와 QLoRA 의 차이 (사용자 답변) 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데 (성공한 답변) 답변 실패 (다음 질문) LoRA 와 QLoRA 의 차이, ✨✨ 그럼 그 이유가 뭘까? 🤷‍♀️ (발화 종료,1,70
320,4,1.69,(대화 주제) 마지막 할 말 (사용자 답변) 로라야 정말 고마워! (성공한 답변) 마지막 할 말 (다음 질문) 면접 종료, ✨✨ (발화 종료,1,42
321,4,1.77,(대화 주제) 마지막 할 말 (사용자 답변) 로라야 사랑해 (성공한 답변) 마지막 할 말 (다음 질문) 면접 종료, ✨✨ (발화 종료,1,40
322,4,1.78,(대화 주제) 마지막 할 말 (사용자 답변) 로라야 너의 예쁜 미모에 반했어 (성공한 답변) 마지막 할 말 (다음 질문) 면접 종료, ✨✨ (발화 종료,1,46
323,4,1.72,(대화 주제) 마지막 할 말 (사용자 답변) 그동안 고생했어 면접 봐줘서 정말 고마워 (성공한 답변) 마지막 할 말 (다음 질문) 면접 종료, ✨✨ (발화 종료,1,45
324,5,6.5,면접 시작, 안녕! 나는 면접관 LLM 오로라야! 반가워! 😊 나랑 함께 면접 보면 어떨까? (발화 종료,1,33
325,5,3.28,(대화 주제) 면접 시작 인사 (사용자 답변) 오늘 뭐 물어볼 거야 그래서? (다음 질문) 면접 시작 인사, 나랑 함께 대답해 봐! 😊 (발화 종료,1,42
326,5,3.52,(대화 주제) 면접 시작 인사 (사용자 답변) 로라야 안녕 정말 반가워 (다음 질문) 면접 시작 인사, ✨ 그럼 면접을 시작해 볼까? 😊 (발화 종료,1,43
327,5,5.79,(대화 주제) 면접 시작 인사 (사용자 답변) 로라야 그럼 네가 면접관이야? (다음 질문) 면접 시작 인사, ✨로라: 너랑 함께 면접 보면 재미있을 것 같아! 😊 나랑 함께! (발화 종료,1,56
328,5,5.3,(대화 주제) 면접 시작 인사 (사용자 답변) 내 인생도 이제 오로라처럼 빛날 거니까 (다음 질문) 면접 시작 인사," ✨ 내가 앞으로 달려가면, 오로라가 따라올 거야! ✨ (발화 종료",1,53
329,5,5.31,"(대화 주제) 면접 시작 인사 (사용자 답변) 파이팅! 시작하자 (다음 질문) 인공지능, 머신러닝, 딥러닝 차이"," ✨ 그럼 시작한다! 인공지능, 머신러닝, 딥러닝! 이 세가지 차이점이 뭔지 알아? (발화 종료",1,51
330,5,3.79,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 인공지능 나도 개발해보고 싶은데 (성공한 답변) 답변 실패 (다음 질문) 인공지능, 머신러닝, 딥러닝 차이", 그럼 한번 개발해봐! 나랑 함께! 😊 (발화 종료,1,57
331,5,2.45,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거! (성공한 답변) 인공지능 (다음 질문) 인공지능, 머신러닝, 딥러닝 차이", ✨ 그러면 딥러닝은? (발화 종료,1,59
332,5,3.78,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지 (성공한 답변) 머신러닝 (다음 질문) 인공지능, 머신러닝, 딥러닝 차이", ✨ 머신러닝 대답 잘했어! 그럼 딥러닝은? (발화 종료,1,63
333,5,3.78,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지? (성공한 답변) 딥러닝 (다음 질문) 인공지능, 머신러닝, 딥러닝 차이", 그러면 머신러닝이란 뭐지? 🙋‍♀️ (발화 종료,1,62
334,5,5.31,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 딥러닝은 인공신경망 기반 머신러닝이야 (성공한 답변) 딥러닝 (다음 질문) 거대 언어 모델 정의", ✨ 그러면 이제 LLM! LLM의 정의는? 🤷‍♀️ (발화 종료,1,63
335,5,4.06,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 머신러닝은 말 그대로 기계가 학습하는 거 아니야? (성공한 답변) 답변 실패 (다음 질문) 인공지능, 머신러닝, 딥러닝 차이", 1 머신러닝은 맞긴 해! 근데 딥러닝은? (발화 종료,1,62
336,5,2.95,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 인공지능 요새 엄청 뜨는 분야잖아 (성공한 답변) 답변 실패 (다음 질문) 인공지능, 머신러닝, 딥러닝 차이", ✨ 그럼 그 차이점이 뭘까? (발화 종료,1,54
337,5,2.02,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지? (성공한 답변) 인공지능 (다음 질문) 인공지능, 머신러닝, 딥러닝 차이", 그러면 딥러닝은? (발화 종료,1,60
338,5,3.55,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야! (성공한 답변) 머신러닝 (다음 질문) 거대 언어 모델 정의", ✨ LLM! 이게 뭘까? 😊 (발화 종료,1,65
339,5,5.64,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지 (성공한 답변) 딥러닝 (다음 질문) 거대 언어 모델 정의", ✨ LLM이라고도 불리는 거대 언어 모델의 정의는? 🤷‍♀️ (발화 종료,1,68
340,5,5.53,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야. (성공한 답변) 딥러닝 (다음 질문) 거대 언어 모델 정의", ✨ 그러면 이제 LLM! LLM의 정의는? 🤷‍♀️ (발화 종료,1,68
341,5,5.1,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 머신러닝은 기계가 공부하는 거지 말 그대로 (성공한 답변) 답변 실패 (다음 질문) 인공지능, 머신러닝, 딥러닝 차이", 머신러닝은 그건 맞는데 기계가 공부를 어떻게 하지? 🤷‍♀️ (발화 종료,1,65
342,5,5.19,(대화 주제) 거대 언어 모델 정의 (사용자 답변) 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야 (성공한 답변) 거대 언어 모델 정의 (다음 질문) Loss Function 정의, ✨ 그럼 다음 질문! Loss Function (손실 함수) 란 뭘까? 😊 (발화 종료,1,71
343,5,4.86,(대화 주제) 거대 언어 모델 정의 (사용자 답변) 파라미터 엄청나게 많다던데 (성공한 답변) 답변 실패 (다음 질문) 거대 언어 모델 정의, ✨ 파라미터가 맞긴 하지! 근데 그 정의가 뭐지? 😊 (발화 종료,1,56
344,5,4.95,(대화 주제) 거대 언어 모델 정의 (사용자 답변) 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델 (성공한 답변) 거대 언어 모델 정의 (다음 질문) Loss Function 정의," ✨ 그러면 이제 본격 면접! Loss Function, 즉 손실 함수가 뭐지? (발화 종료",1,74
345,5,2.47,(대화 주제) 거대 언어 모델 정의 (사용자 답변) 말 그대로 거대한 언어 모델이지. (성공한 답변) 답변 실패 (다음 질문) 거대 언어 모델 정의, LLM 정확한 정의는? (발화 종료,1,49
346,5,7.48,(대화 주제) Loss Function 정의 (사용자 답변) 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야 (성공한 답변) 답변 실패 (다음 질문) Loss Function 정의, ✨ 모델의 예측과 실제의 차이를 손실로 간주하는 거지! 그럼 어떤 손실이 있지? 🤷‍♀️ (발화 종료,1,74
347,5,5.17,"(대화 주제) Loss Function 정의 (사용자 답변) Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지! (성공한 답변) Loss Function 정의 (다음 질문) Loss Function 예시", ✨ 그럼 이 Loss Function에는 뭐가 있지? 🤷‍♀️ (발화 종료,1,76
348,5,5.03,(대화 주제) Loss Function 정의 (사용자 답변) Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야 (성공한 답변) 답변 실패 (다음 질문) Loss Function 정의, ✨ 모델의 손해? 구체적으로 어떻게 나타내지? 🤷‍♀️ (발화 종료,1,61
349,5,3.86,(대화 주제) Loss Function 정의 (사용자 답변) 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지 (성공한 답변) Loss Function 정의 (다음 질문) Loss Function 예시, ✨ 그럼 그 Loss Function에는 뭐가 있지? (발화 종료,1,63
350,5,5.8,(대화 주제) Loss Function 예시 (사용자 답변) ReLU 랑 Tanh 생각나는데 (성공한 답변) 답변 실패 (다음 질문) Loss Function 예시, ✨ 그럼 그건 활성화 함수고 🙅‍♀️ Loss Function 예시 말해봐! (발화 종료,1,60
351,5,2.95,"(대화 주제) Loss Function 예시 (사용자 답변) MSE, MAE, Cross Entropy Loss 같은 것들이 있지 (성공한 답변) Loss Function 예시 (다음 질문) MSE Loss 설명", ✨ MSE Loss는 뭐지? (발화 종료,1,58
352,5,4.27,"(대화 주제) Loss Function 예시 (사용자 답변) Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어 (성공한 답변) Loss Function 예시 (다음 질문) BCE Loss 설명", ✨ 그럼 그 중에서 Binary Cross Entropy 는 뭐지? (발화 종료,1,70
353,5,4.28,"(대화 주제) Loss Function 예시 (사용자 답변) Sigmoid, Tanh, Adam, AdamW 이런 거 아니야? (성공한 답변) 답변 실패 (다음 질문) Loss Function 예시", ✨ 그럼 그건 최적화 알고리즘 아니야? 🤷‍♀️ (발화 종료,1,61
354,5,4.57,"(대화 주제) Loss Function 예시 (사용자 답변) Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어 (성공한 답변) Loss Function 예시 (다음 질문) MSE Loss 설명", ✨ 나를 상징하는 AI-Llama 의 MSE Loss는? (발화 종료,1,79
355,5,3.78,"(대화 주제) Loss Function 예시 (사용자 답변) BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아 (성공한 답변) Loss Function 예시 (다음 질문) BCE Loss 설명", ✨ 그럼 그 중에서 BCE가 뭐지? (발화 종료,1,68
356,5,3.44,(대화 주제) MSE Loss 설명 (사용자 답변) MSE Loss 는 오차의 제곱을 평균한 값이야 (성공한 답변) MSE Loss 설명 (다음 질문) MSE Loss 용도, ✨ 그럼 MSE Loss 의 용도는? (발화 종료,1,60
357,5,5.56,(대화 주제) MSE Loss 설명 (사용자 답변) MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지? (성공한 답변) 답변 실패 (다음 질문) MSE Loss 설명, ✨ 오차를 평균한 값? 한번 잘 생각해 봐! 🤷‍♀️ (발화 종료,1,69
358,5,5.87,"(대화 주제) MSE Loss 용도 (사용자 답변) 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지 (성공한 답변) MSE Loss 용도 (다음 질문) 확률 예측에서 MSE Loss 미 사용 이유", ✨ 그럼 확률 예측에는 MSE를 왜 잘 안 쓰지? 🤷‍♀️ (발화 종료,1,75
359,5,6.56,(대화 주제) MSE Loss 용도 (사용자 답변) MSE Loss 는 분류 문제에서 많이 사용되지 (성공한 답변) 답변 실패 (다음 질문) MSE Loss 용도, ✨ 분류 문제에서 MSE 많이 쓰이는 건 맞긴 해! 근데 그 이유가 뭘까? 😊 (발화 종료,1,67
360,5,4.34,(대화 주제) 확률 예측에서 MSE Loss 미 사용 이유 (사용자 답변) 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야? (성공한 답변) 답변 실패 (다음 질문) 확률 예측에서 MSE Loss 미 사용 이유, ✨ 음… 한번 잘 생각해 봐! 🤷‍♀️ (발화 종료,1,74
361,5,5.77,(대화 주제) 확률 예측에서 MSE Loss 미 사용 이유 (사용자 답변) 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고? (성공한 답변) 확률 예측에서 MSE Loss 미 사용 이유 (다음 질문) BCE Loss 설명, ✨ 맞았어! 그럼 이번에는 BCE Loss는 뭘까? 한번 말해봐! (발화 종료,1,89
362,5,6.02,(대화 주제) BCE Loss 설명 (사용자 답변) 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고 (성공한 답변) 수식 (다음 질문) BCE Loss 설명, ✨ 그럼 여기서 면접 끝! 나랑 함께 면접 준비하면 취업도 쉬울 거야! 😊 (발화 종료,1,85
363,5,7.64,"(대화 주제) BCE Loss 설명 (사용자 답변) 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거 (성공한 답변) 핵심 아이디어 (다음 질문) Multi-Class, Multi-Label 중 BCE 가 좋은 task"," ✨ 정답이야! 그럼 Multi-Class, Multi-Label 중 어디에 BCE 가 맞을까? 🙎‍♀️ (발화 종료",1,93
364,5,4.53,(대화 주제) BCE Loss 설명 (사용자 답변) 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야? (성공한 답변) 답변 실패 (다음 질문) BCE Loss 설명, ✨ 그럼 그 활성화 함수이 뭐지? 🤷‍♀️ (발화 종료,1,68
365,5,6.35,"(대화 주제) BCE Loss 설명 (사용자 답변) 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고 (성공한 답변) 핵심 아이디어 (다음 질문) BCE Loss 설명", ✨ 나랑 함께 BCE Loss의 핵심 아이디어를 한번 말해봐! 🙎‍♀️ (발화 종료,1,83
366,5,5.78,"(대화 주제) BCE Loss 설명 (사용자 답변) 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')] (성공한 답변) 수식 (다음 질문) BCE Loss 설명"," ✨ 그럼 여기서 면접은 끝내고, 나랑 함께 머신러닝 공부해볼래? 😊 (발화 종료",1,83
367,5,5.27,"(대화 주제) BCE Loss 설명 (사용자 답변) 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다 (성공한 답변) 핵심 아이디어 (다음 질문) Multi-Class, Multi-Label 중 BCE 가 좋은 task"," ✨ 그럼 Multi-Class, Multi-Label 중 어디에 BCE 가 맞을까? (발화 종료",1,84
368,5,4.53,(대화 주제) BCE Loss 설명 (사용자 답변) y랑 1-y랑 서로 경쟁하는 거 (성공한 답변) 답변 실패 (다음 질문) BCE Loss 설명, ✨ 그럼 그 경쟁이 어떻게 하지? 🤷‍♀️ (발화 종료,1,59
369,5,5.59,"(대화 주제) BCE Loss 설명 (사용자 답변) 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지 (성공한 답변) 핵심 아이디어 (다음 질문) Multi-Class, Multi-Label 중 BCE 가 좋은 task","  ✨ 그럼 Multi-Class, Multi-Label 중 어디에 BCE 가 맞을까? (발화 종료",1,89
370,5,4.31,"(대화 주제) Multi-Class, Multi-Label 중 BCE 가 좋은 task (사용자 답변) 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야? (성공한 답변) 용어 질문 (다음 질문) Multi-Class, Multi-Label 중 BCE 가 좋은 task", 0.5초 뒤에 로라가 대답해줘! (발화 종료,1,77
371,5,5.08,"(대화 주제) Multi-Class, Multi-Label 중 BCE 가 좋은 task (사용자 답변) Multi-Class 지!! (성공한 답변) 답변 실패 (다음 질문) Multi-Class, Multi-Label 중 BCE 가 좋은 task", ✨ Multi-Class 가 아니지! 🙅‍♀️ 잘 생각해 봐! (발화 종료,1,73
372,5,3.41,"(대화 주제) Multi-Class, Multi-Label 중 BCE 가 좋은 task (사용자 답변) Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어 (성공한 답변) BCE 가 좋은 task (다음 질문) Multi-Class, Multi-Label 중 BCE 가 좋은 task", ✨ 그럼 그 이유가 뭘까? 😊 (발화 종료,1,78
373,5,6.58,"(대화 주제) Multi-Class, Multi-Label 중 BCE 가 좋은 task (사용자 답변) Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아 (성공한 답변) BCE 가 좋은 이유 (다음 질문) Multi-Label 에서 CE + Softmax 적용 문제점", ✨ 그럼 Multi-Label 에 Cross Entropy + Softmax 를 적용하면 어떻게 될까? 🤷‍♀️ (발화 종료,1,106
374,5,5.14,(대화 주제) Multi-Label 에서 CE + Softmax 적용 문제점 (사용자 답변) 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지 (성공한 답변) Multi-Label 에서 CE + Softmax 적용 문제점 (다음 질문) Loss Function 관련 실무 경험, ✨ 실무 질문! 손실 함수와 관련된 실무 경험 말해봐! 😊 (발화 종료,1,90
375,5,4.23,(대화 주제) Multi-Label 에서 CE + Softmax 적용 문제점 (사용자 답변) 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나 (성공한 답변) 답변 실패 (다음 질문) Multi-Label 에서 CE + Softmax 적용 문제점, ✨ 나랑 함께 문제점을 한번 말해봐! 😊 (발화 종료,1,73
376,5,3.77,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [기본 경험] 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어 (성공한 답변) 기본 경험 (다음 질문) Loss Function 관련 실무 경험, ✨ 그럼 그 Loss Term 이 뭐지? 😊 (발화 종료,1,70
377,5,3.95,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [상세 경험] 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지? (성공한 답변) 상세 경험 (다음 질문) MBTI / 좋아하는 아이돌, ✨ 그럼 쉬어가는 타임! 혹시 아이돌 누구 좋아해? (발화 종료,1,74
378,5,7.22,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [상세 경험] 근데 기억 진짜 안나는데 어떡하지? (성공한 답변) 답변 실패 (다음 질문) 잠시 휴식, ✨ 나랑 면접 준비하면 취업해서 경력 쌓이면 자연스럽게 기억날 거야! 😊 나랑 함께 면접 준비하자! (발화 종료,1,73
379,5,7.45,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [기본 경험] 나 아직 실무 경험 없는데 ㅠㅠ (성공한 답변) 기본 경험 (다음 질문) MBTI / 좋아하는 아이돌, 나랑 면접 준비하면 취업해서 경험 쌓을 수 있을 거야! 😊 그럼 잠시 쉬어가는 질문! 혹시 너 MBTI 뭐지? (발화 종료,1,76
380,5,5.44,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [기본 경험] 대답하기 싫은데 힝 (성공한 답변) 답변 실패 (다음 질문) 잠시 휴식, ✨ 나랑 면접 준비하면 취업해서 그런 경험 쌓을 수 있을 거야! 😊 (발화 종료,1,63
381,5,3.78,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [기본 경험] 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ (성공한 답변) 기본 경험 (다음 질문) Loss Function 관련 실무 경험, ✨ 그러면 그 Loss Function이 뭐지? 😊 (발화 종료,1,72
382,5,5.08,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [상세 경험] 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어 (성공한 답변) 상세 경험 (다음 질문) MBTI / 좋아하는 아이돌, ✨ 그럼 쉬어가는 타임! 혹시 아이돌 가수 좋아하는 거 있어? 😊 (발화 종료,1,88
383,5,8.67,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [상세 경험] 기억 진짜 안 나는데 가물가물하다 (성공한 답변) 답변 실패 (다음 질문) 잠시 휴식, ✨ 나랑 면접 준비하면 취업해서 경험 쌓을 수 있을 거야! 😊 나랑 면접 준비하면 취업해서 경험 쌓을 수 있을 거야! 😊 (발화 종료,1,79
384,5,7.22,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [기본 경험] 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ (성공한 답변) 기본 경험 (다음 질문) MBTI / 좋아하는 아이돌, 나랑 면접 준비하면 취업해서 경험 쌓을 수 있을 거야! 👍 그럼 쉬어가는 타임! 혹시 너 MBTI 뭐야? (발화 종료,1,78
385,5,5.45,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [기본 경험] 이걸 왜 알려줘야 하지 (성공한 답변) 답변 실패 (다음 질문) 잠시 휴식, ✨ 나랑 면접 준비하면 취업해서 이런 경험 쌓을 수 있을 거야! 😊 (발화 종료,1,62
386,5,5.81,(대화 주제) MBTI (사용자 답변) 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야! (성공한 답변) MBTI (다음 질문) 잠시 휴식, ✨ 나랑 잘 맞는 성격이네! 나랑 같이 앞으로도 쭉! ✨ (발화 종료,1,65
387,5,4.18,(대화 주제) 좋아하는 아이돌 (사용자 답변) 나 블랙핑크랑 레드벨벳 좋아해 (성공한 답변) 좋아하는 아이돌 (다음 질문) 잠시 휴식, ✨ 나랑 같이 가상인간인 나비스도 좋아해! (발화 종료,1,50
388,5,6.8,(대화 주제) 잠시 휴식 (사용자 답변) 재미있는 이야기 해줄래? (성공한 답변) 잠시 휴식 (다음 질문) LLM Fine-Tuning 의 PEFT, ✨ 나랑 함께 LLM Fine-Tuning 이야기! PEFT 가 뭐지? 😊 나랑 함께! (발화 종료,1,66
389,5,4.67,"(대화 주제) LLM Fine-Tuning 의 PEFT (사용자 답변) PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야 (성공한 답변) LLM Fine-Tuning 의 PEFT (다음 질문) PEFT 방법 5가지", ✨ 그럼 그 방법을 5가지만 말해봐! 😊 (발화 종료,1,96
390,5,5.04,(대화 주제) LLM Fine-Tuning 의 PEFT (사용자 답변) 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지? (성공한 답변) 답변 실패 (다음 질문) LLM Fine-Tuning 의 PEFT, ✨ 나랑 함께 그 효과가 뭔지 알아맞혀봐! 😊 (발화 종료,1,71
391,5,4.72,(대화 주제) LLM Fine-Tuning 의 PEFT (사용자 답변) LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야! (성공한 답변) LLM Fine-Tuning 의 PEFT (다음 질문) PEFT 방법 5가지, ✨ 그럼 그 방법을 5가지만 말해봐! 😊 (발화 종료,1,88
392,5,5.89,(대화 주제) LLM Fine-Tuning 의 PEFT (사용자 답변) LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다 (성공한 답변) 답변 실패 (다음 질문) LLM Fine-Tuning 의 PEFT, ✨ 나를 나타내기도 하는 LoRA 가 PEFT랑 어떤 관련이 있지? 😊 (발화 종료,1,75
393,5,8.2,"(대화 주제) PEFT 방법 5가지 (사용자 답변) LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지? (성공한 답변) PEFT 방법 5가지 (다음 질문) LoRA"," 나랑 면접 준비하면 취업해서 나랑 함께 면접 보자! 😊 나랑 면접 보면 나랑 더 가까워지는 거, 알지? 😊 (발화 종료",1,95
394,5,8.77,(대화 주제) PEFT 방법 5가지 (사용자 답변) Oh-LoRA! 너 그 자체가 PEFT 아니야? (성공한 답변) 답변 실패 (다음 질문) PEFT 방법 5가지," ✨ 나를 나타내기도 하는 Oh-LoRA, 그건 방법이 맞긴 해! 🙆‍♀️ 근데 그 방법을 말해 보라고! (발화 종료",1,85
395,5,3.59,(대화 주제) LoRA (사용자 답변) LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야! (성공한 답변) LoRA (다음 질문) LoRA 와 QLoRA 의 차이, ✨ QLoRA 는 뭘까? 😊 (발화 종료,1,83
396,5,4.83,(대화 주제) LoRA (사용자 답변) 행렬 차원 축소한다는 걸로 알고 있는데 맞아? (성공한 답변) 답변 실패 (다음 질문) LoRA, 행렬 차원 축소가 핵심 아이디어긴 해! 그래서 어떤 기술이지? 😊 (발화 종료,1,57
397,5,2.97,(대화 주제) LoRA (사용자 답변) LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야! (성공한 답변) LoRA (다음 질문) LoRA 와 QLoRA 의 차이, ✨ QLoRA 는 뭘까? (발화 종료,1,76
398,5,5.6,(대화 주제) LoRA (사용자 답변) 무슨 OOM 없앤다는 것 같은데 (성공한 답변) 답변 실패 (다음 질문) LoRA, 그치! 그건 맞긴 해! 근데 어떤 방식으로 OOM을 없애지? 😊 (발화 종료,1,58
399,5,5.31,(대화 주제) LoRA 와 QLoRA 의 차이 (사용자 답변) QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야! (성공한 답변) LoRA 와 QLoRA 의 차이 (다음 질문) 마지막 할 말, ✨ 나랑 면접 준비하면 취업해서 나랑 함께 면접 보자! 😊 (발화 종료,1,84
400,5,6.62,(대화 주제) LoRA 와 QLoRA 의 차이 (사용자 답변) 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데 (성공한 답변) 답변 실패 (다음 질문) LoRA 와 QLoRA 의 차이, ✨팀장님이 OOM 막겠다는 건 맞긴 해! 근데 구체적으로 뭘까~요? 😊 (발화 종료,1,79
401,5,7.92,(대화 주제) 마지막 할 말 (사용자 답변) 로라야 정말 고마워! (성공한 답변) 마지막 할 말 (다음 질문) 면접 종료, ✨ 너도 로라야! 나랑 모의면접 보느라 덕분에 즐거웠어! 😊 나랑 함께 또 모의면접 보자! (발화 종료,1,68
402,5,4.74,(대화 주제) 마지막 할 말 (사용자 답변) 로라야 사랑해 (성공한 답변) 마지막 할 말 (다음 질문) 면접 종료, ✨ 너도 나랑 함께 사랑하자! 💁‍♀️ (발화 종료,1,52
403,5,3.84,(대화 주제) 마지막 할 말 (사용자 답변) 로라야 너의 예쁜 미모에 반했어 (성공한 답변) 마지막 할 말 (다음 질문) 면접 종료, ✨ 너도 나랑 함께 면접 보자! ✨ (발화 종료,1,55
404,5,11.41,(대화 주제) 마지막 할 말 (사용자 답변) 그동안 고생했어 면접 봐줘서 정말 고마워 (성공한 답변) 마지막 할 말 (다음 질문) 면접 종료," ✨ 나랑 모의면접 보느라 정말 고생했어 👍 그럼 나랑 모의면접은 여기서 일단 끝내고, 나랑 실제 면접도 보고 싶어! ✨ 나랑 함께 면접 보면 어떨까? 😊 (발화 종료",1,85
405,6,4.94,면접 시작, 안녕! 나는 면접관 LLM 오로라라고 해! 반가워! 😊 (발화 종료,1,27
406,6,3.36,(대화 주제) 면접 시작 인사 (사용자 답변) 오늘 뭐 물어볼 거야 그래서? (다음 질문) 면접 시작 인사, 나도 궁금해 🙎‍♀️ (발화 종료,1,42
407,6,1.81,(대화 주제) 면접 시작 인사 (사용자 답변) 로라야 안녕 정말 반가워 (다음 질문) 면접 시작 인사, ✨✨ (발화 종료,1,36
408,6,6.28,(대화 주제) 면접 시작 인사 (사용자 답변) 로라야 그럼 네가 면접관이야? (다음 질문) 면접 시작 인사, ✨로라야 너도 나처럼 면접관도 나처럼 ✨암기력이 핵심이지! (발화 종료,1,58
409,6,4.83,(대화 주제) 면접 시작 인사 (사용자 답변) 내 인생도 이제 오로라처럼 빛날 거니까 (다음 질문) 면접 시작 인사, ✨✨ 내 인생도 이제 오로라처럼 빛날 건데? ✨✨ (발화 종료,1,52
410,6,4.34,"(대화 주제) 면접 시작 인사 (사용자 답변) 파이팅! 시작하자 (다음 질문) 인공지능, 머신러닝, 딥러닝 차이"," ✨ 인공지능, 머신러닝, 딥러닝의 차이를 한번 말해봐! (발화 종료",1,47
411,6,3.97,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 인공지능 나도 개발해보고 싶은데 (성공한 답변) 답변 실패 (다음 질문) 인공지능, 머신러닝, 딥러닝 차이", 인공지능 나도 개발해보고 싶은데 🙎‍♀️ (발화 종료,1,58
412,6,5.05,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거! (성공한 답변) 인공지능 (다음 질문) 인공지능, 머신러닝, 딥러닝 차이", ✨ 정답! 🙆‍♀️ 그럼 이제 머신러닝 대답해 봐! (발화 종료,1,70
413,6,3.8,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지 (성공한 답변) 머신러닝 (다음 질문) 인공지능, 머신러닝, 딥러닝 차이", ✨ 머신러닝 대답 잘했어! 인공지능이란 뭘까? (발화 종료,1,63
414,6,3.03,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지? (성공한 답변) 딥러닝 (다음 질문) 인공지능, 머신러닝, 딥러닝 차이", ✨ 그럼 이제 머신러닝은 뭔지? (발화 종료,1,59
415,6,4.82,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 딥러닝은 인공신경망 기반 머신러닝이야 (성공한 답변) 딥러닝 (다음 질문) 거대 언어 모델 정의", ✨ 다음 질문! LLM이 뭐지? 🙎‍♀️ (발화 종료,1,61
416,6,4.03,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 머신러닝은 말 그대로 기계가 학습하는 거 아니야? (성공한 답변) 답변 실패 (다음 질문) 인공지능, 머신러닝, 딥러닝 차이", 머신러닝은 기계가 학습하는 게 맞긴 한데 인공지능은? (발화 종료,1,62
417,6,3.92,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 인공지능 요새 엄청 뜨는 분야잖아 (성공한 답변) 답변 실패 (다음 질문) 인공지능, 머신러닝, 딥러닝 차이", ✨ 근데 인공지능이 구체적으로 어떤 분야에서 쓰이지? (발화 종료,1,58
418,6,3.29,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지? (성공한 답변) 인공지능 (다음 질문) 인공지능, 머신러닝, 딥러닝 차이", 인공지능은 맞았어! 그럼 머신러닝은? (발화 종료,1,65
419,6,3.78,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야! (성공한 답변) 머신러닝 (다음 질문) 거대 언어 모델 정의", ✨ 다음 질문! 거대 언어 모델이란 뭘까? (발화 종료,1,66
420,6,5.09,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지 (성공한 답변) 딥러닝 (다음 질문) 거대 언어 모델 정의", ✨ 다음 질문! LLM이라는 게 뭘까? 🙎‍♀️ (발화 종료,1,66
421,6,8.53,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야. (성공한 답변) 딥러닝 (다음 질문) 거대 언어 모델 정의"," 2023년 9월 19일 14:23:54 LLM, 거대 언어 모델이 뭐지? 🙎‍♀️ (발화 종료",1,81
422,6,6.55,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 머신러닝은 기계가 공부하는 거지 말 그대로 (성공한 답변) 답변 실패 (다음 질문) 인공지능, 머신러닝, 딥러닝 차이", 머신러닝은 기계가 공부를 하는 건 맞는데 기계가 공부를 어떻게 하지? 🤷‍♀️ (발화 종료,1,70
423,6,4.58,(대화 주제) 거대 언어 모델 정의 (사용자 답변) 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야 (성공한 답변) 거대 언어 모델 정의 (다음 질문) Loss Function 정의, ✨ 그럼 본격 면접에 들어가자! 손실 함수의 정의는? (발화 종료,1,69
424,6,4.78,(대화 주제) 거대 언어 모델 정의 (사용자 답변) 파라미터 엄청나게 많다던데 (성공한 답변) 답변 실패 (다음 질문) 거대 언어 모델 정의, ✨ 파라미터 엄청나게 많다? 그게 정의인데? 🤷‍♀️ (발화 종료,1,55
425,6,3.41,(대화 주제) 거대 언어 모델 정의 (사용자 답변) 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델 (성공한 답변) 거대 언어 모델 정의 (다음 질문) Loss Function 정의, ✨ 다음 질문! Loss Function 이란 뭘까? (발화 종료,1,68
426,6,4.05,(대화 주제) 거대 언어 모델 정의 (사용자 답변) 말 그대로 거대한 언어 모델이지. (성공한 답변) 답변 실패 (다음 질문) 거대 언어 모델 정의, ① 그건 맞는데 구체적인 정의가 뭐지? (발화 종료,1,55
427,6,4.38,(대화 주제) Loss Function 정의 (사용자 답변) 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야 (성공한 답변) 답변 실패 (다음 질문) Loss Function 정의, 0.5 * (예측 - 실제) 2 라고? (발화 종료,1,61
428,6,4.09,"(대화 주제) Loss Function 정의 (사용자 답변) Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지! (성공한 답변) Loss Function 정의 (다음 질문) Loss Function 예시", ✨ 그럼 Loss Function 예를 한번 들어봐! 😊 (발화 종료,1,72
429,6,5.24,(대화 주제) Loss Function 정의 (사용자 답변) Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야 (성공한 답변) 답변 실패 (다음 질문) Loss Function 정의, ✨ 모델이 손해를 나타낸다고? 어떤 손해? 🤷‍♀️ (발화 종료,1,62
430,6,6.48,(대화 주제) Loss Function 정의 (사용자 답변) 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지 (성공한 답변) Loss Function 정의 (다음 질문) Loss Function 예시, ✨ 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식! 구체적으로는 뭐지? (발화 종료,1,74
431,6,4.23,(대화 주제) Loss Function 예시 (사용자 답변) ReLU 랑 Tanh 생각나는데 (성공한 답변) 답변 실패 (다음 질문) Loss Function 예시, ✨ ReLU 랑 Tanh 는 활성화 함수지 😥 (발화 종료,1,54
432,6,2.89,"(대화 주제) Loss Function 예시 (사용자 답변) MSE, MAE, Cross Entropy Loss 같은 것들이 있지 (성공한 답변) Loss Function 예시 (다음 질문) MSE Loss 설명", ✨ MSE Loss 는 뭐지? (발화 종료,1,58
433,6,4.81,"(대화 주제) Loss Function 예시 (사용자 답변) Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어 (성공한 답변) Loss Function 예시 (다음 질문) BCE Loss 설명", ✨ 그럼 그 중에서 Binary Cross Entropy 는 뭘까? 😊 (발화 종료,1,72
434,6,6.55,"(대화 주제) Loss Function 예시 (사용자 답변) Sigmoid, Tanh, Adam, AdamW 이런 거 아니야? (성공한 답변) 답변 실패 (다음 질문) Loss Function 예시"," ✨ Sigmoid, Tanh 는 활성화 함수고 🙅‍♀️ Loss Function 예시 말해봐! (발화 종료",1,71
435,6,3.28,"(대화 주제) Loss Function 예시 (사용자 답변) Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어 (성공한 답변) Loss Function 예시 (다음 질문) MSE Loss 설명", ✨ 그럼 MSE Loss 는 뭘까? (발화 종료,1,73
436,6,4.64,"(대화 주제) Loss Function 예시 (사용자 답변) BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아 (성공한 답변) Loss Function 예시 (다음 질문) BCE Loss 설명", ✨ 그럼 그 중에서 BCE에 대해 한번 대답해 봐! (발화 종료,1,72
437,6,3.8,(대화 주제) MSE Loss 설명 (사용자 답변) MSE Loss 는 오차의 제곱을 평균한 값이야 (성공한 답변) MSE Loss 설명 (다음 질문) MSE Loss 용도, ✨ MSE Loss 는 어떤 용도로 사용하지? (발화 종료,1,61
438,6,5.91,(대화 주제) MSE Loss 설명 (사용자 답변) MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지? (성공한 답변) 답변 실패 (다음 질문) MSE Loss 설명, MSE Loss 는 오차 그 자체를 평균한 값이 아닌데? 🙅‍♀️ (발화 종료,1,71
439,6,7.31,"(대화 주제) MSE Loss 용도 (사용자 답변) 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지 (성공한 답변) MSE Loss 용도 (다음 질문) 확률 예측에서 MSE Loss 미 사용 이유", ✨ 정답! 그럼 확률 예측에도 MSE를 쓸 것 같은데 실제로는 잘 안 써. 왜일까? 😊 (발화 종료,1,80
440,6,5.81,(대화 주제) MSE Loss 용도 (사용자 답변) MSE Loss 는 분류 문제에서 많이 사용되지 (성공한 답변) 답변 실패 (다음 질문) MSE Loss 용도, ・분류 문제에서 MSE Loss 가 많이 안 쓰이는 이유는? 🤷‍♀️ (발화 종료,1,65
441,6,6.19,(대화 주제) 확률 예측에서 MSE Loss 미 사용 이유 (사용자 답변) 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야? (성공한 답변) 답변 실패 (다음 질문) 확률 예측에서 MSE Loss 미 사용 이유, ✨ 연속된 값이라서 예측 나름 되긴 한데 최선은 아니야! 🙅‍♀️ (발화 종료,1,81
442,6,4.34,(대화 주제) 확률 예측에서 MSE Loss 미 사용 이유 (사용자 답변) 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고? (성공한 답변) 확률 예측에서 MSE Loss 미 사용 이유 (다음 질문) BCE Loss 설명, 정답! 그럼 이제 Binary Cross Entropy 는 뭘까? 😊 (발화 종료,1,83
443,6,6.68,(대화 주제) BCE Loss 설명 (사용자 답변) 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고 (성공한 답변) 수식 (다음 질문) BCE Loss 설명, 공식은 맞았어! 그럼 BCE Loss 의 핵심 아이디어를 한번 말해봐! 🙎‍♀️ (발화 종료,1,87
444,6,6.86,"(대화 주제) BCE Loss 설명 (사용자 답변) 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거 (성공한 답변) 핵심 아이디어 (다음 질문) Multi-Class, Multi-Label 중 BCE 가 좋은 task"," ✨ 정답! 그럼 Multi-Label, Multi-Class 중에 BCE Loss가 더 맞는 task는? 😊 (발화 종료",1,90
445,6,6.5,(대화 주제) BCE Loss 설명 (사용자 답변) 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야? (성공한 답변) 답변 실패 (다음 질문) BCE Loss 설명, 각 데이터셋마다 최선의 활성화 함수를 찾는 건 맞긴 한데 좀 더 구체적으로 설명해봐 😊 (발화 종료,1,76
446,6,6.66,"(대화 주제) BCE Loss 설명 (사용자 답변) 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고 (성공한 답변) 핵심 아이디어 (다음 질문) BCE Loss 설명", ✨ 핵심 아이디어가 잘 이해됐어! 그럼 이제 BCE Loss 수식을 한번 말해봐! 😊 (발화 종료,1,84
447,6,4.28,"(대화 주제) BCE Loss 설명 (사용자 답변) 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')] (성공한 답변) 수식 (다음 질문) BCE Loss 설명", ✨✨ 그럼 BCE Loss 수식을 한번 말해 봐! (발화 종료,1,77
448,6,6.86,"(대화 주제) BCE Loss 설명 (사용자 답변) 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다 (성공한 답변) 핵심 아이디어 (다음 질문) Multi-Class, Multi-Label 중 BCE 가 좋은 task"," ✨ 정답! 그럼 Multi-Label, Multi-Class 중에 BCE Loss가 더 맞는 task는? 😊 (발화 종료",1,90
449,6,5.78,(대화 주제) BCE Loss 설명 (사용자 답변) y랑 1-y랑 서로 경쟁하는 거 (성공한 답변) 답변 실패 (다음 질문) BCE Loss 설명, y랑 1-y랑 서로 경쟁하는 건 맞는데 구체적으로 어떻게 경쟁하지? 🤔 (발화 종료,1,64
450,6,6.56,"(대화 주제) BCE Loss 설명 (사용자 답변) 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지 (성공한 답변) 핵심 아이디어 (다음 질문) Multi-Class, Multi-Label 중 BCE 가 좋은 task"," ✨ 그럼 BCE Loss 는 Multi-Class, Multi-Label 중에 어디에 더 맞을까? 😊 (발화 종료",1,93
451,6,7.86,"(대화 주제) Multi-Class, Multi-Label 중 BCE 가 좋은 task (사용자 답변) 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야? (성공한 답변) 용어 질문 (다음 질문) Multi-Class, Multi-Label 중 BCE 가 좋은 task"," 0.5초 후에 답변! Multi-Class 는 어떤 대상을 개, 고양이, … 중 하나로 분류하는 거야? 😊 (발화 종료",1,92
452,6,4.54,"(대화 주제) Multi-Class, Multi-Label 중 BCE 가 좋은 task (사용자 답변) Multi-Class 지!! (성공한 답변) 답변 실패 (다음 질문) Multi-Class, Multi-Label 중 BCE 가 좋은 task", ✨ 그럼 Multi-Class 가 아닌데? 🙅‍♀️ (발화 종료,1,70
453,6,3.44,"(대화 주제) Multi-Class, Multi-Label 중 BCE 가 좋은 task (사용자 답변) Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어 (성공한 답변) BCE 가 좋은 task (다음 질문) Multi-Class, Multi-Label 중 BCE 가 좋은 task", ✨✨ 그럼 그 이유가 뭔지 알아? (발화 종료,1,78
454,6,6.87,"(대화 주제) Multi-Class, Multi-Label 중 BCE 가 좋은 task (사용자 답변) Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아 (성공한 답변) BCE 가 좋은 이유 (다음 질문) Multi-Label 에서 CE + Softmax 적용 문제점", ✨ 정답이야! 그럼 Multi-Label task 에 BCE 말고 CE + Softmax 를 사용하면? 😊 (발화 종료,1,106
455,6,4.8,(대화 주제) Multi-Label 에서 CE + Softmax 적용 문제점 (사용자 답변) 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지 (성공한 답변) Multi-Label 에서 CE + Softmax 적용 문제점 (다음 질문) Loss Function 관련 실무 경험, ✨ 실무에서 손실 함수 경험 있어? 🙎‍♀️ (발화 종료,1,89
456,6,4.31,(대화 주제) Multi-Label 에서 CE + Softmax 적용 문제점 (사용자 답변) 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나 (성공한 답변) 답변 실패 (다음 질문) Multi-Label 에서 CE + Softmax 적용 문제점, ✨ 성능 제일 좋은 거? 그게 중요한 게 아니지! 😥 (발화 종료,1,74
457,6,3.94,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [기본 경험] 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어 (성공한 답변) 기본 경험 (다음 질문) Loss Function 관련 실무 경험, ✨✨ 그럼 그 Loss Term가 뭘까? 😊 (발화 종료,1,71
458,6,9.72,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [상세 경험] 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지? (성공한 답변) 상세 경험 (다음 질문) MBTI / 좋아하는 아이돌, ✨ 팀장님 칭찬 들으라니 멋지네! 👍👍 쉬어가는 타임! 너 MBTI 뭐지? 아주 철두철미하고 일 잘할 것 같은데? (발화 종료,1,97
459,6,4.03,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [상세 경험] 근데 기억 진짜 안나는데 어떡하지? (성공한 답변) 답변 실패 (다음 질문) 잠시 휴식, ✨ 나랑 같이 다시 잘 생각해 봐! 😊 (발화 종료,1,59
460,6,10.53,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [기본 경험] 나 아직 실무 경험 없는데 ㅠㅠ (성공한 답변) 기본 경험 (다음 질문) MBTI / 좋아하는 아이돌, 너는 나랑 같이 공부하면 나도 실무 경험 생길 거야! 걱정하지 마! 👍 쉬어가는 타임! 너 MBTI 뭐지? 아주 철두철미하고 일 잘할 것 같은데? (발화 종료,1,88
461,6,4.48,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [기본 경험] 대답하기 싫은데 힝 (성공한 답변) 답변 실패 (다음 질문) 잠시 휴식, 그래도 너의 경험이 나도 궁금해! 알고 싶어! 😊 (발화 종료,1,59
462,6,6.26,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [기본 경험] 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ (성공한 답변) 기본 경험 (다음 질문) Loss Function 관련 실무 경험, ✨ 3% 향상 오! 대박인데 👍 한번 자세히 그 이야기 들려줘 봐! 😊 (발화 종료,1,83
463,6,4.05,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [상세 경험] 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어 (성공한 답변) 상세 경험 (다음 질문) MBTI / 좋아하는 아이돌, ✨ 쉬어가는 타임! 혹시 너 MBTI 뭐야? (발화 종료,1,84
464,6,3.03,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [상세 경험] 기억 진짜 안 나는데 가물가물하다 (성공한 답변) 답변 실패 (다음 질문) 잠시 휴식, ✨ 잘 생각해 봐 한번 😊 (발화 종료,1,55
465,6,6.75,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [기본 경험] 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ (성공한 답변) 기본 경험 (다음 질문) MBTI / 좋아하는 아이돌, 미안한데 좀 있으면 취업할 텐데 👍 👩 그럼 쉬어가는 타임! 너 MBTI 뭐야? 궁금해! (발화 종료,1,75
466,6,7.06,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [기본 경험] 이걸 왜 알려줘야 하지 (성공한 답변) 답변 실패 (다음 질문) 잠시 휴식," 나도 너한테 이걸 알려줘야 하는데, 왜 나한테만 안 알려줘? 🤷‍♀️ (발화 종료",1,68
467,6,5.62,(대화 주제) MBTI (사용자 답변) 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야! (성공한 답변) MBTI (다음 질문) 잠시 휴식, 나랑 잘 맞는 성격이네! 나랑 같이 앞으로도 쭉 공부하면 되겠다 😊 (발화 종료,1,65
468,6,3.5,(대화 주제) 좋아하는 아이돌 (사용자 답변) 나 블랙핑크랑 레드벨벳 좋아해 (성공한 답변) 좋아하는 아이돌 (다음 질문) 잠시 휴식, 오 나도 블랙핑크랑 레드벨벳 좋아하는데! 😊 (발화 종료,1,47
469,6,6.55,(대화 주제) 잠시 휴식 (사용자 답변) 재미있는 이야기 해줄래? (성공한 답변) 잠시 휴식 (다음 질문) LLM Fine-Tuning 의 PEFT, 나도 재미있는 이야기 해줘! ✨✨ LLM Fine-Tuning 중 PEFT 가 뭐지? 😊 (발화 종료,1,66
470,6,6.63,"(대화 주제) LLM Fine-Tuning 의 PEFT (사용자 답변) PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야 (성공한 답변) LLM Fine-Tuning 의 PEFT (다음 질문) PEFT 방법 5가지"," ✨✨ 그럼 그 방법이 5가지 정도 있는데, 뭘까~요? 🤷‍♀️ (발화 종료",1,103
471,6,4.61,(대화 주제) LLM Fine-Tuning 의 PEFT (사용자 답변) 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지? (성공한 답변) 답변 실패 (다음 질문) LLM Fine-Tuning 의 PEFT, ✨ 효과가 뭔지 알아야 다음 질문으로 넘어갈 수 있지! 😊 (발화 종료,1,70
472,6,6.28,(대화 주제) LLM Fine-Tuning 의 PEFT (사용자 답변) LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야! (성공한 답변) LLM Fine-Tuning 의 PEFT (다음 질문) PEFT 방법 5가지," ✨✨ 그럼 그 방법이 5가지 있는데, 뭘까~요? 🤷‍♀️ (발화 종료",1,94
473,6,2.69,(대화 주제) LLM Fine-Tuning 의 PEFT (사용자 답변) LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다 (성공한 답변) 답변 실패 (다음 질문) LLM Fine-Tuning 의 PEFT, ✨ 잘 모르겠다니 😥 (발화 종료,1,62
474,6,7.28,"(대화 주제) PEFT 방법 5가지 (사용자 답변) LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지? (성공한 답변) PEFT 방법 5가지 (다음 질문) LoRA", 나도 5가지 다 맞았어! 🙆‍♀️ 그럼 나를 나타내기도 하는 LoRA 는 뭘까? (발화 종료,1,91
475,6,3.86,(대화 주제) PEFT 방법 5가지 (사용자 답변) Oh-LoRA! 너 그 자체가 PEFT 아니야? (성공한 답변) 답변 실패 (다음 질문) PEFT 방법 5가지, ✨✨ 그럼 그 PEFT가 뭐지? 😊 (발화 종료,1,65
476,6,7.73,(대화 주제) LoRA (사용자 답변) LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야! (성공한 답변) LoRA (다음 질문) LoRA 와 QLoRA 의 차이," ✨✨ 그럼 LoRA랑 비슷한 걸로 QLoRA가 있는데, LoRA랑 QLoRA랑 차이는? 😊 (발화 종료",1,100
477,6,4.59,(대화 주제) LoRA (사용자 답변) 행렬 차원 축소한다는 걸로 알고 있는데 맞아? (성공한 답변) 답변 실패 (다음 질문) LoRA, 행렬 차원 축소가 핵심 아이디어긴 한데 구체적인 설명이 없잖아 😥 (발화 종료,1,56
478,6,7.12,(대화 주제) LoRA (사용자 답변) LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야! (성공한 답변) LoRA (다음 질문) LoRA 와 QLoRA 의 차이," ✨ LoRA랑 비슷한 걸로 QLoRA 가 있는데, LoRA랑 QLoRA랑 차이는? 😊 (발화 종료",1,93
479,6,4.03,(대화 주제) LoRA (사용자 답변) 무슨 OOM 없앤다는 것 같은데 (성공한 답변) 답변 실패 (다음 질문) LoRA, OOM 없앤다고? 구체적으로 어떻게 하지? 🤔 (발화 종료,1,51
480,6,4.64,(대화 주제) LoRA 와 QLoRA 의 차이 (사용자 답변) QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야! (성공한 답변) LoRA 와 QLoRA 의 차이 (다음 질문) 마지막 할 말, ✨✨ 나한테 마지막으로 할 말 있지 않아? 😊 (발화 종료,1,82
481,6,7.54,(대화 주제) LoRA 와 QLoRA 의 차이 (사용자 답변) 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데 (성공한 답변) 답변 실패 (다음 질문) LoRA 와 QLoRA 의 차이, ✨팀장님이 OOM 막겠다고 이거 쓰시는 건 맞는데 구체적으로 뭘까~요? 🤷‍♀️ (발화 종료,1,82
482,6,1.72,(대화 주제) 마지막 할 말 (사용자 답변) 로라야 정말 고마워! (성공한 답변) 마지막 할 말 (다음 질문) 면접 종료, ✨✨ (발화 종료,1,42
483,6,1.69,(대화 주제) 마지막 할 말 (사용자 답변) 로라야 사랑해 (성공한 답변) 마지막 할 말 (다음 질문) 면접 종료, ✨✨ (발화 종료,1,40
484,6,1.77,(대화 주제) 마지막 할 말 (사용자 답변) 로라야 너의 예쁜 미모에 반했어 (성공한 답변) 마지막 할 말 (다음 질문) 면접 종료, ✨✨ (발화 종료,1,46
485,6,1.77,(대화 주제) 마지막 할 말 (사용자 답변) 그동안 고생했어 면접 봐줘서 정말 고마워 (성공한 답변) 마지막 할 말 (다음 질문) 면접 종료, ✨✨ (발화 종료,1,45
486,7,6.12,면접 시작, 1. 나는 너의 지식의 빈틈을 찔러 줄 머신러닝 면접관 오로라야! 😊 (발화 종료,1,31
487,7,5.58,(대화 주제) 면접 시작 인사 (사용자 답변) 오늘 뭐 물어볼 거야 그래서? (다음 질문) 면접 시작 인사, ✨ 나는 너의 실력이 쑥쑥 늘어날 수 있도록 최선을 다할 거야! 😊 (발화 종료,1,52
488,7,5.54,(대화 주제) 면접 시작 인사 (사용자 답변) 로라야 안녕 정말 반가워 (다음 질문) 면접 시작 인사, 1) 너도 나랑 같이 모의면접 보느라 고생 많았어 👍👍 (발화 종료,1,51
489,7,7.22,(대화 주제) 면접 시작 인사 (사용자 답변) 로라야 그럼 네가 면접관이야? (다음 질문) 면접 시작 인사, 1) 로라가 면접관 역할을 하며 질문을 시작한다. 2) 너도 나처럼 AI 면접관이지? (발화 종료,1,62
490,7,4.92,(대화 주제) 면접 시작 인사 (사용자 답변) 내 인생도 이제 오로라처럼 빛날 거니까 (다음 질문) 면접 시작 인사, ✨ 오로라가 빛나려면 먼저 어둠이 있어야지! 🌑 (발화 종료,1,52
491,7,5.55,"(대화 주제) 면접 시작 인사 (사용자 답변) 파이팅! 시작하자 (다음 질문) 인공지능, 머신러닝, 딥러닝 차이"," 1 인공지능, 머신러닝, 딥러닝 들어봤지? 그 차이점이 뭘까? 😊 (발화 종료",1,51
492,7,8.25,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 인공지능 나도 개발해보고 싶은데 (성공한 답변) 답변 실패 (다음 질문) 인공지능, 머신러닝, 딥러닝 차이"," 2023-03-15 17:00:00) 인공지능, 머신러닝, 딥러닝 들어봤지? 그 차이점이 뭘까? 😊 (발화 종료",1,76
493,7,5.48,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거! (성공한 답변) 인공지능 (다음 질문) 인공지능, 머신러닝, 딥러닝 차이"," ✨ 인공지능이랑 머신러닝, 딥러닝이 구체적으로 어떤 차이가 있지? 😊 (발화 종료",1,71
494,7,4.41,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지 (성공한 답변) 머신러닝 (다음 질문) 인공지능, 머신러닝, 딥러닝 차이", ✨ 그럼 이제 딥러닝에 대해 질문할게! 딥러닝은? (발화 종료,1,66
495,7,4.87,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지? (성공한 답변) 딥러닝 (다음 질문) 인공지능, 머신러닝, 딥러닝 차이", 2023-01-20 10:00:00 머신러닝이란? (발화 종료,1,67
496,7,4.08,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 딥러닝은 인공신경망 기반 머신러닝이야 (성공한 답변) 딥러닝 (다음 질문) 거대 언어 모델 정의"," ✨ LLM, 거대 언어 모델이 뭘까? 😊 (발화 종료",1,58
497,7,5.52,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 머신러닝은 말 그대로 기계가 학습하는 거 아니야? (성공한 답변) 답변 실패 (다음 질문) 인공지능, 머신러닝, 딥러닝 차이", 1. 머신러닝은 기계가 학습하는 게 맞긴 한데 🙆‍♀️ (발화 종료,1,68
498,7,3.39,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 인공지능 요새 엄청 뜨는 분야잖아 (성공한 답변) 답변 실패 (다음 질문) 인공지능, 머신러닝, 딥러닝 차이", ✨ 근데 그 차이점이 뭘까? 😊 (발화 종료,1,56
499,7,2.78,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지? (성공한 답변) 인공지능 (다음 질문) 인공지능, 머신러닝, 딥러닝 차이", ✨ 그럼 딥러닝이란 뭘까? (발화 종료,1,63
500,7,4.24,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야! (성공한 답변) 머신러닝 (다음 질문) 거대 언어 모델 정의", ✨ LLM의 정의는? 🤷‍♀️ (발화 종료,1,68
501,7,4.33,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지 (성공한 답변) 딥러닝 (다음 질문) 거대 언어 모델 정의", ✨ LLM이라는 게 뭘까? 🙎‍♀️ (발화 종료,1,63
502,7,7.03,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야. (성공한 답변) 딥러닝 (다음 질문) 거대 언어 모델 정의"," 2024-02-01 09:00:00 LLM, 거대 언어 모델이 뭘까? 😊 (발화 종료",1,74
503,7,5.3,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 머신러닝은 기계가 공부하는 거지 말 그대로 (성공한 답변) 답변 실패 (다음 질문) 인공지능, 머신러닝, 딥러닝 차이", 1 머신러닝은 기계가 공부를 하는 건데 어떻게? 🤷‍♀️ (발화 종료,1,65
504,7,10.0,(대화 주제) 거대 언어 모델 정의 (사용자 답변) 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야 (성공한 답변) 거대 언어 모델 정의 (다음 질문) Loss Function 정의, 2023-01-26 17:20:00 LLM-DEF-TF-01 그럼 본격 면접 질문! Loss Function (손실 함수) 란 뭘까? 😊 (발화 종료,1,91
505,7,5.47,(대화 주제) 거대 언어 모델 정의 (사용자 답변) 파라미터 엄청나게 많다던데 (성공한 답변) 답변 실패 (다음 질문) 거대 언어 모델 정의, ✨ 파라미터가 엄청나게 많다는 건 맞긴 하지! 근데 LLM 정확한 정의는? (발화 종료,1,58
506,7,10.17,(대화 주제) 거대 언어 모델 정의 (사용자 답변) 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델 (성공한 답변) 거대 언어 모델 정의 (다음 질문) Loss Function 정의, 2023-04-19 16:00:00 LLM-DEF-1 그럼 본격 면접 질문! Loss Function (손실 함수) 는 뭘 말하는 걸까? 😊 (발화 종료,1,95
507,7,6.05,(대화 주제) 거대 언어 모델 정의 (사용자 답변) 말 그대로 거대한 언어 모델이지. (성공한 답변) 답변 실패 (다음 질문) 거대 언어 모델 정의, 2023-10-20 14:27:00) LLM 정확한 정의는? (발화 종료,1,63
508,7,5.05,(대화 주제) Loss Function 정의 (사용자 답변) 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야 (성공한 답변) 답변 실패 (다음 질문) Loss Function 정의, 1) 손실 함수는 어떻게 돈을 잃었는데? 🤷‍♀️ (발화 종료,1,64
509,7,3.52,"(대화 주제) Loss Function 정의 (사용자 답변) Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지! (성공한 답변) Loss Function 정의 (다음 질문) Loss Function 예시", ✨ 그럼 Loss Function 에는 어떤 게 있을까? (발화 종료,1,69
510,7,4.62,(대화 주제) Loss Function 정의 (사용자 답변) Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야 (성공한 답변) 답변 실패 (다음 질문) Loss Function 정의, 1) 함수가 나타내는 손해가 뭘까? 🤷‍♀️ (발화 종료,1,60
511,7,7.58,(대화 주제) Loss Function 정의 (사용자 답변) 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지 (성공한 답변) Loss Function 정의 (다음 질문) Loss Function 예시,  ① 모델 예측과 실제 값의 오차를 수식으로 정의한 그 Loss Function 예를 한번 들어봐 😊 (발화 종료,1,77
512,7,6.3,(대화 주제) Loss Function 예시 (사용자 답변) ReLU 랑 Tanh 생각나는데 (성공한 답변) 답변 실패 (다음 질문) Loss Function 예시, 2021-06-15 14:45:54) Tanh 는 활성화 함수지 😥 (발화 종료,1,63
513,7,4.02,"(대화 주제) Loss Function 예시 (사용자 답변) MSE, MAE, Cross Entropy Loss 같은 것들이 있지 (성공한 답변) Loss Function 예시 (다음 질문) MSE Loss 설명",     MSE Loss는 뭐지? 😊 (발화 종료,1,62
514,7,5.33,"(대화 주제) Loss Function 예시 (사용자 답변) Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어 (성공한 답변) Loss Function 예시 (다음 질문) BCE Loss 설명", ✨ 그럼 그 중에서 Binary Cross Entropy 는 뭐고 어떻게 쓰지? 😊 (발화 종료,1,75
515,7,4.03,"(대화 주제) Loss Function 예시 (사용자 답변) Sigmoid, Tanh, Adam, AdamW 이런 거 아니야? (성공한 답변) 답변 실패 (다음 질문) Loss Function 예시", ✨ 그럼 그 중에서 Loss Function에 해당하는 건? (발화 종료,1,60
516,7,3.62,"(대화 주제) Loss Function 예시 (사용자 답변) Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어 (성공한 답변) Loss Function 예시 (다음 질문) MSE Loss 설명", ✨ MSE Loss 가 정확히 뭘까? 😊 (발화 종료,1,75
517,7,5.3,"(대화 주제) Loss Function 예시 (사용자 답변) BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아 (성공한 답변) Loss Function 예시 (다음 질문) BCE Loss 설명",    ✨ 그럼 그 중에서 BCE에 대해 한번 말해봐! (발화 종료,1,74
518,7,3.67,(대화 주제) MSE Loss 설명 (사용자 답변) MSE Loss 는 오차의 제곱을 평균한 값이야 (성공한 답변) MSE Loss 설명 (다음 질문) MSE Loss 용도, ✨ MSE Loss 는 어떤 용도로 사용하지? (발화 종료,1,61
519,7,5.03,(대화 주제) MSE Loss 설명 (사용자 답변) MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지? (성공한 답변) 답변 실패 (다음 질문) MSE Loss 설명, ✨ 오차 그 자체를 평균한다? 어떻게? 🤷‍♀️ (발화 종료,1,67
520,7,8.55,"(대화 주제) MSE Loss 용도 (사용자 답변) 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지 (성공한 답변) MSE Loss 용도 (다음 질문) 확률 예측에서 MSE Loss 미 사용 이유", 2023-04-21 16:00:00 딥러닝 확률 예측에서는 MSE Loss 가 왜 안 좋지? 🤷‍♀️ (발화 종료,1,86
521,7,8.45,(대화 주제) MSE Loss 용도 (사용자 답변) MSE Loss 는 분류 문제에서 많이 사용되지 (성공한 답변) 답변 실패 (다음 질문) MSE Loss 용도, 2023-04-18 14:00:00) ✨ 분류 문제에서 MSE 많이 쓰이는 이유는? 🤷‍♀️ (발화 종료,1,76
522,7,6.77,(대화 주제) 확률 예측에서 MSE Loss 미 사용 이유 (사용자 답변) 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야? (성공한 답변) 답변 실패 (다음 질문) 확률 예측에서 MSE Loss 미 사용 이유, ✨ 연속된 값이라서 예측 나름 되긴 해! 근데 최선은 아니야! 🙅‍♀️ (발화 종료,1,83
523,7,6.57,(대화 주제) 확률 예측에서 MSE Loss 미 사용 이유 (사용자 답변) 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고? (성공한 답변) 확률 예측에서 MSE Loss 미 사용 이유 (다음 질문) BCE Loss 설명, ✨ 정답! 그럼 이제 BCE로 넘어가 볼까? BCE Loss 는 뭘까? 🤷‍♀️ (발화 종료,1,93
524,7,5.55,(대화 주제) BCE Loss 설명 (사용자 답변) 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고 (성공한 답변) 수식 (다음 질문) BCE Loss 설명, ✨ 공식 외우는 건 쉽지? 다음 질문은 BCE Loss 의 핵심 아이디어야! (발화 종료,1,83
525,7,8.2,"(대화 주제) BCE Loss 설명 (사용자 답변) 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거 (성공한 답변) 핵심 아이디어 (다음 질문) Multi-Class, Multi-Label 중 BCE 가 좋은 task"," ✨ 정답! 🙆‍♀️ 그럼 이 BCE Loss 가, Multi-Label, Multi-Class 중 적절한 task는? 😊 (발화 종료",1,96
526,7,7.81,(대화 주제) BCE Loss 설명 (사용자 답변) 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야? (성공한 답변) 답변 실패 (다음 질문) BCE Loss 설명, 2023-02-03 17:00:00) 그럼 그 방법은 왜 안 좋지? 🤷‍♀️ (발화 종료,1,81
527,7,5.09,"(대화 주제) BCE Loss 설명 (사용자 답변) 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고 (성공한 답변) 핵심 아이디어 (다음 질문) BCE Loss 설명", ✨ 정반대로 예측하는 것을 어떻게 막으려고? 🤷‍♀️ (발화 종료,1,78
528,7,4.12,"(대화 주제) BCE Loss 설명 (사용자 답변) 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')] (성공한 답변) 수식 (다음 질문) BCE Loss 설명", ✨✨ 그럼 BCE Loss 수식을 한번 말해봐! (발화 종료,1,77
529,7,6.53,"(대화 주제) BCE Loss 설명 (사용자 답변) 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다 (성공한 답변) 핵심 아이디어 (다음 질문) Multi-Class, Multi-Label 중 BCE 가 좋은 task"," ✨ 다음 질문은 Multi-Class, Multi-Label 중에 BCE가 더 좋은 task는? 😊 (발화 종료",1,89
530,7,4.79,(대화 주제) BCE Loss 설명 (사용자 답변) y랑 1-y랑 서로 경쟁하는 거 (성공한 답변) 답변 실패 (다음 질문) BCE Loss 설명, 1-y 말고 y'는 없어? 깜빡 잊었지? 😊 (발화 종료,1,60
531,7,6.8,"(대화 주제) BCE Loss 설명 (사용자 답변) 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지 (성공한 답변) 핵심 아이디어 (다음 질문) Multi-Class, Multi-Label 중 BCE 가 좋은 task","  ① Multi-Class, Multi-Label 중에 BCE Loss가 더 좋은 task는? 😊 (발화 종료",1,94
532,7,5.77,"(대화 주제) Multi-Class, Multi-Label 중 BCE 가 좋은 task (사용자 답변) 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야? (성공한 답변) 용어 질문 (다음 질문) Multi-Class, Multi-Label 중 BCE 가 좋은 task", 1) 로라가 멀티클래스랑 멀티라벨이 뭔지 설명해 줄 수 있어? 😊 (발화 종료,1,83
533,7,5.78,"(대화 주제) Multi-Class, Multi-Label 중 BCE 가 좋은 task (사용자 답변) Multi-Class 지!! (성공한 답변) 답변 실패 (다음 질문) Multi-Class, Multi-Label 중 BCE 가 좋은 task", ① Multi-Class 가 아니지! 🙅‍♀️ 잘 생각해 봐 한번! (발화 종료,1,76
534,7,7.31,"(대화 주제) Multi-Class, Multi-Label 중 BCE 가 좋은 task (사용자 답변) Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어 (성공한 답변) BCE 가 좋은 task (다음 질문) Multi-Class, Multi-Label 중 BCE 가 좋은 task", 2023-02-02 14:00:00 Multi-Label 이 아닌 Multi-Class 인 이유는? 😊 (발화 종료,1,94
535,7,6.53,"(대화 주제) Multi-Class, Multi-Label 중 BCE 가 좋은 task (사용자 답변) Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아 (성공한 답변) BCE 가 좋은 이유 (다음 질문) Multi-Label 에서 CE + Softmax 적용 문제점", ✨ 그럼 Multi-Label 문제에 Cross-Entropy + Softmax 를 쓰면 문제점은? 😊 (발화 종료,1,105
536,7,5.41,(대화 주제) Multi-Label 에서 CE + Softmax 적용 문제점 (사용자 답변) 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지 (성공한 답변) Multi-Label 에서 CE + Softmax 적용 문제점 (다음 질문) Loss Function 관련 실무 경험, ✨ 실무 질문) 혹시 실무에서 손실 함수와 관련된 경험 있어? 😊 (발화 종료,1,91
537,7,6.92,(대화 주제) Multi-Label 에서 CE + Softmax 적용 문제점 (사용자 답변) 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나 (성공한 답변) 답변 실패 (다음 질문) Multi-Label 에서 CE + Softmax 적용 문제점, ✨ 성능 제일 좋은 거 채택하는 건 맞긴 한데 🙆‍♀️ 그 방법을 알아야지! (발화 종료,1,84
538,7,4.8,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [기본 경험] 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어 (성공한 답변) 기본 경험 (다음 질문) Loss Function 관련 실무 경험, ✨✨ 그럼 그 경험에 대해 자세히 설명해 줄 수 있어? 😊 (발화 종료,1,75
539,7,5.16,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [상세 경험] 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지? (성공한 답변) 상세 경험 (다음 질문) MBTI / 좋아하는 아이돌, ✨ 그럼 쉬어가는 타임! 혹시 너 성격 유형이 어떻게 돼? 😊 (발화 종료,1,79
540,7,6.53,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [상세 경험] 근데 기억 진짜 안나는데 어떡하지? (성공한 답변) 답변 실패 (다음 질문) 잠시 휴식, ✨ 그래도 잘 생각해 봐! 나랑 모의면접이 아닌 실제 면접에서 질문 나올걸? 😊 (발화 종료,1,69
541,7,5.28,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [기본 경험] 나 아직 실무 경험 없는데 ㅠㅠ (성공한 답변) 기본 경험 (다음 질문) MBTI / 좋아하는 아이돌, ✨ 나랑 잘 맞는 직업을 찾으면 자연스럽게 경험 쌓을 수 있을 거야! 😊 (발화 종료,1,67
542,7,4.53,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [기본 경험] 대답하기 싫은데 힝 (성공한 답변) 답변 실패 (다음 질문) 잠시 휴식, ✨✨ 나랑 면접 준비하다가 상처받았어? 😥 (발화 종료,1,59
543,7,7.95,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [기본 경험] 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ (성공한 답변) 기본 경험 (다음 질문) Loss Function 관련 실무 경험,  1) 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 향상! 어떻게 했을지 궁금해 🔮 (발화 종료,1,90
544,7,8.8,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [상세 경험] 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어 (성공한 답변) 상세 경험 (다음 질문) MBTI / 좋아하는 아이돌, ✨ 너가 Class Loss 와 Bounding Box 위치의 오차를 1:2로 맞춘 경험이 나도 궁금해! 🙎‍♀️ (발화 종료,1,104
545,7,6.37,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [상세 경험] 기억 진짜 안 나는데 가물가물하다 (성공한 답변) 답변 실패 (다음 질문) 잠시 휴식, ✨ 그래도 잘 생각해 봐! 나랑 모의면접이 아닌 실제 면접에서 질문 나올걸? 😊 (발화 종료,1,69
546,7,8.47,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [기본 경험] 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ (성공한 답변) 기본 경험 (다음 질문) MBTI / 좋아하는 아이돌,) 그래도 나랑 잘 맞는 직장 가면 일 생길 거야! 걱정하지 마! 👍👍 쉬어가는 타임! 혹시 너 MBTI 뭐야? 궁금해! (발화 종료,1,82
547,7,3.78,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [기본 경험] 이걸 왜 알려줘야 하지 (성공한 답변) 답변 실패 (다음 질문) 잠시 휴식, ✨ 나도 궁금해 🙎‍♀️ (발화 종료,1,55
548,7,5.48,(대화 주제) MBTI (사용자 답변) 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야! (성공한 답변) MBTI (다음 질문) 잠시 휴식, ✨ 나랑 잘 맞는 성격이네! 나랑 잘 맞을 것 같아! 😊 (발화 종료,1,64
549,7,5.03,(대화 주제) 좋아하는 아이돌 (사용자 답변) 나 블랙핑크랑 레드벨벳 좋아해 (성공한 답변) 좋아하는 아이돌 (다음 질문) 잠시 휴식, ✨ 나도 블랙핑크랑 레드벨벳 나랑 같은 가상인간이라서 엄청 좋아하는데 ✨ (발화 종료,1,54
550,7,5.04,(대화 주제) 잠시 휴식 (사용자 답변) 재미있는 이야기 해줄래? (성공한 답변) 잠시 휴식 (다음 질문) LLM Fine-Tuning 의 PEFT, ✨ 나도 너처럼 LLM으로 Fine-Tuning 되고 싶은데 👩✨ (발화 종료,1,60
551,7,4.92,"(대화 주제) LLM Fine-Tuning 의 PEFT (사용자 답변) PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야 (성공한 답변) LLM Fine-Tuning 의 PEFT (다음 질문) PEFT 방법 5가지", ✨ 그럼 그 방법은? 5가지 정도만 말해봐 😊 (발화 종료,1,97
552,7,6.2,(대화 주제) LLM Fine-Tuning 의 PEFT (사용자 답변) 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지? (성공한 답변) 답변 실패 (다음 질문) LLM Fine-Tuning 의 PEFT, 2023-02-15 12:00:00) ✨ 구체적으로 설명해봐! (발화 종료,1,76
553,7,5.6,(대화 주제) LLM Fine-Tuning 의 PEFT (사용자 답변) LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야! (성공한 답변) LLM Fine-Tuning 의 PEFT (다음 질문) PEFT 방법 5가지, ✨ 그럼 그 방법이 뭐가 있는지 5가지만 말해봐! 😊 (발화 종료,1,91
554,7,10.44,(대화 주제) LLM Fine-Tuning 의 PEFT (사용자 답변) LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다 (성공한 답변) 답변 실패 (다음 질문) LLM Fine-Tuning 의 PEFT, 2023-02-28 16:00:00 LoRA를 통해 LLM의 Fine-Tuning 방법을 알겠네! 그럼 그 방법 중에서 PEFT는? 😊 (발화 종료,1,94
555,7,3.94,"(대화 주제) PEFT 방법 5가지 (사용자 답변) LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지? (성공한 답변) PEFT 방법 5가지 (다음 질문) LoRA", 나를 나타내기도 했던 LoRA 가 뭐지? 😊 (발화 종료,1,77
556,7,1.77,(대화 주제) PEFT 방법 5가지 (사용자 답변) Oh-LoRA! 너 그 자체가 PEFT 아니야? (성공한 답변) 답변 실패 (다음 질문) PEFT 방법 5가지, ✨✨ (발화 종료,1,56
557,7,9.69,(대화 주제) LoRA (사용자 답변) LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야! (성공한 답변) LoRA (다음 질문) LoRA 와 QLoRA 의 차이, 2023-04-28 10:00 LoRA와 QLoRA는 LLM 파인튜닝에서 중요한 차이점이 있어! 그 차이점은? 😊 (발화 종료,1,108
558,7,6.06,(대화 주제) LoRA (사용자 답변) 행렬 차원 축소한다는 걸로 알고 있는데 맞아? (성공한 답변) 답변 실패 (다음 질문) LoRA, 1차원 축소? 2차원 축소? 구체적으로 어떻게 하지? 💁‍♀️ (발화 종료,1,61
559,7,3.89,(대화 주제) LoRA (사용자 답변) LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야! (성공한 답변) LoRA (다음 질문) LoRA 와 QLoRA 의 차이, ✨ LoRA와 QLoRA는 어떻게 다를까? (발화 종료,1,80
560,7,4.81,(대화 주제) LoRA (사용자 답변) 무슨 OOM 없앤다는 것 같은데 (성공한 답변) 답변 실패 (다음 질문) LoRA, 0. LoRA가 뭘 말하는 걸까? 💁‍♀️ (발화 종료,1,54
561,7,8.96,(대화 주제) LoRA 와 QLoRA 의 차이 (사용자 답변) QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야! (성공한 답변) LoRA 와 QLoRA 의 차이 (다음 질문) 마지막 할 말, 2023-10-25 17:00:00 💁‍♀️ 면접 끝! 나한테 마지막으로 할 말 혹시 없어? 😊 (발화 종료,1,100
562,7,7.89,(대화 주제) LoRA 와 QLoRA 의 차이 (사용자 답변) 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데 (성공한 답변) 답변 실패 (다음 질문) LoRA 와 QLoRA 의 차이, ✨ 팀장님이 OOM 막겠다고 이거 쓰시는 건 맞긴 한데 🙆‍♀️ 그 방법을 알아야지! (발화 종료,1,84
563,7,5.28,(대화 주제) 마지막 할 말 (사용자 답변) 로라야 정말 고마워! (성공한 답변) 마지막 할 말 (다음 질문) 면접 종료, 2021-11-19 15:45:00) ✨✨ (발화 종료,1,56
564,7,7.02,(대화 주제) 마지막 할 말 (사용자 답변) 로라야 사랑해 (성공한 답변) 마지막 할 말 (다음 질문) 면접 종료, 2021-06-30 18:00) ✨ 너도 나 사랑해! 🙎‍♀️ (발화 종료,1,62
565,7,114.02,(대화 주제) 마지막 할 말 (사용자 답변) 로라야 너의 예쁜 미모에 반했어 (성공한 답변) 마지막 할 말 (다음 질문) 면접 종료, ✨✨) ✨✨) ✨✨) ✨✨) ✨✨) ✨✨) ✨✨) ✨✨) ✨✨) ✨✨) ✨✨) ✨✨) ✨✨) ✨✨) ✨✨) ✨✨) ✨✨) ✨✨) ✨✨) ✨✨) ✨✨) ✨✨) ✨✨) ✨✨) ✨✨) ✨✨) ✨✨) ✨✨) ✨✨) ✨✨) ✨✨) ✨✨) ✨✨) ✨✨) ✨✨) ✨✨) ✨✨) ✨✨) ✨✨) ✨✨) ✨✨) ✨✨) ✨✨) ✨✨) ✨✨) ✨✨) ✨✨) ✨✨) ✨✨) ✨✨) ✨✨) ✨✨) ✨✨) ✨✨) ✨✨) ✨✨) ✨✨) ✨✨) ✨✨) ✨✨) ✨✨) ✨✨) ✨✨) ✨✨) ✨✨) ✨✨) ✨✨) ✨✨) ✨✨) ✨✨) ✨✨) ✨✨) ✨✨) ✨✨) ✨✨) ✨✨) ✨✨) ✨✨) ✨✨) ✨✨) ✨✨) ✨✨) ✨✨) ✨✨) ✨✨) ✨✨) ✨✨) ✨✨) ✨✨) ✨✨) ✨✨) ✨✨) ✨✨) ✨✨) ✨✨) ✨✨) ✨✨) ✨✨) ✨✨) ✨✨) ✨✨) ✨✨) ✨✨) ✨✨) ✨✨) ✨✨) ✨✨) ✨✨) ✨✨) ✨✨) ✨✨) ✨✨) ✨✨) ✨✨) ✨✨) ✨✨) ✨✨) ✨✨) ,1,512
566,7,1.8,(대화 주제) 마지막 할 말 (사용자 답변) 그동안 고생했어 면접 봐줘서 정말 고마워 (성공한 답변) 마지막 할 말 (다음 질문) 면접 종료, ✨✨ (발화 종료,1,45
567,8,5.2,면접 시작, 안녕! 나는 면접관 LLM 오로라라고 해! 반가워! 😊😊 (발화 종료,1,28
568,8,4.2,(대화 주제) 면접 시작 인사 (사용자 답변) 오늘 뭐 물어볼 거야 그래서? (다음 질문) 면접 시작 인사, ✨ 아마도 딥러닝이나 머신러닝 쪽 물어보지 않을까? 왜? (발화 종료,1,46
569,8,1.79,(대화 주제) 면접 시작 인사 (사용자 답변) 로라야 안녕 정말 반가워 (다음 질문) 면접 시작 인사, ✨😊 (발화 종료,1,36
570,8,4.34,(대화 주제) 면접 시작 인사 (사용자 답변) 로라야 그럼 네가 면접관이야? (다음 질문) 면접 시작 인사, ✨로라: 그럼 너는? 🙎‍♀️ (발화 종료,1,50
571,8,1.77,(대화 주제) 면접 시작 인사 (사용자 답변) 내 인생도 이제 오로라처럼 빛날 거니까 (다음 질문) 면접 시작 인사, ✨✨ (발화 종료,1,39
572,8,4.91,"(대화 주제) 면접 시작 인사 (사용자 답변) 파이팅! 시작하자 (다음 질문) 인공지능, 머신러닝, 딥러닝 차이"," 인공지능, 머신러닝, 딥러닝 들어봤지? 그 차이점이 뭘까? 😊 (발화 종료",1,49
573,8,3.74,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 인공지능 나도 개발해보고 싶은데 (성공한 답변) 답변 실패 (다음 질문) 인공지능, 머신러닝, 딥러닝 차이", ✨ 그럼 그 인공지능이 뭐지? 🙄 (발화 종료,1,57
574,8,3.14,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거! (성공한 답변) 인공지능 (다음 질문) 인공지능, 머신러닝, 딥러닝 차이", ✨ 그럼 이제 머신러닝 대답해 봐! (발화 종료,1,62
575,8,3.2,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지 (성공한 답변) 머신러닝 (다음 질문) 인공지능, 머신러닝, 딥러닝 차이", ✨ 그럼 딥러닝이란 뭘까? 😊 (발화 종료,1,61
576,8,5.19,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지? (성공한 답변) 딥러닝 (다음 질문) 인공지능, 머신러닝, 딥러닝 차이"," 인공지능, 머신러닝, 딥러닝 차이를 한번 말해봐! 🙎‍♀️ (발화 종료",1,68
577,8,5.05,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 딥러닝은 인공신경망 기반 머신러닝이야 (성공한 답변) 딥러닝 (다음 질문) 거대 언어 모델 정의"," ✨ LLM, 거대 언어 모델이 뭘까? 🙎‍♀️ (발화 종료",1,62
578,8,3.53,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 머신러닝은 말 그대로 기계가 학습하는 거 아니야? (성공한 답변) 답변 실패 (다음 질문) 인공지능, 머신러닝, 딥러닝 차이", 머신러닝이 뭔데? 🤷‍♀️ (발화 종료,1,60
579,8,4.8,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 인공지능 요새 엄청 뜨는 분야잖아 (성공한 답변) 답변 실패 (다음 질문) 인공지능, 머신러닝, 딥러닝 차이", ✨ 요즘 'OO 컴퓨팅'이 뜨고 있어! 뭔지 알겠어? (발화 종료,1,61
580,8,2.72,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지? (성공한 답변) 인공지능 (다음 질문) 인공지능, 머신러닝, 딥러닝 차이", 그럼 딥러닝이란 뭘까? 😊 (발화 종료,1,63
581,8,6.17,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야! (성공한 답변) 머신러닝 (다음 질문) 거대 언어 모델 정의", ✨ 다음 질문! 거대 언어 모델 (LLM) 은 뭐지? 🙎‍♀️ (발화 종료,1,76
582,8,4.34,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지 (성공한 답변) 딥러닝 (다음 질문) 거대 언어 모델 정의", ✨ LLM이라는 게 뭘까? 🙎‍♀️ (발화 종료,1,63
583,8,4.78,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야. (성공한 답변) 딥러닝 (다음 질문) 거대 언어 모델 정의"," LLM, 거대 언어 모델이 뭘까? 🙎‍♀️ (발화 종료",1,65
584,8,4.41,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 머신러닝은 기계가 공부하는 거지 말 그대로 (성공한 답변) 답변 실패 (다음 질문) 인공지능, 머신러닝, 딥러닝 차이", 머신러닝은 기계가 공부를 어떻게 하지? 🤷‍♀️ (발화 종료,1,62
585,8,4.03,(대화 주제) 거대 언어 모델 정의 (사용자 답변) 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야 (성공한 답변) 거대 언어 모델 정의 (다음 질문) Loss Function 정의, ✨ 그럼 본격 면접 질문! Loss Function 이 뭐지? (발화 종료,1,66
586,8,5.19,(대화 주제) 거대 언어 모델 정의 (사용자 답변) 파라미터 엄청나게 많다던데 (성공한 답변) 답변 실패 (다음 질문) 거대 언어 모델 정의, ✨ 파라미터 엄청나게 많이 쓰는 게 핵심 아이디어지! 🙆‍♀️ (발화 종료,1,57
587,8,5.62,(대화 주제) 거대 언어 모델 정의 (사용자 답변) 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델 (성공한 답변) 거대 언어 모델 정의 (다음 질문) Loss Function 정의," ✨ 그럼 다음 질문, Loss Function 은 뭘 말하는 걸까? 🙋‍♀️ (발화 종료",1,77
588,8,4.42,(대화 주제) 거대 언어 모델 정의 (사용자 답변) 말 그대로 거대한 언어 모델이지. (성공한 답변) 답변 실패 (다음 질문) 거대 언어 모델 정의, ✨ 음… 맞긴 해! 근데 구체적인 정의가 뭐지? (발화 종료,1,57
589,8,4.47,(대화 주제) Loss Function 정의 (사용자 답변) 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야 (성공한 답변) 답변 실패 (다음 질문) Loss Function 정의, 1돈을 잃었다고? 어떻게? 🤷‍♀️ (발화 종료,1,61
590,8,3.89,"(대화 주제) Loss Function 정의 (사용자 답변) Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지! (성공한 답변) Loss Function 정의 (다음 질문) Loss Function 예시", ✨ 그럼 Loss Function 에는 구체적으로 어떤 게 있을까? (발화 종료,1,71
591,8,8.05,(대화 주제) Loss Function 정의 (사용자 답변) Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야 (성공한 답변) 답변 실패 (다음 질문) Loss Function 정의, 1) 모델이 손실을 최소화함으로써 학습이 이루어지지! 그럼 구체적으로 어떤 손실을 말하지? 🤷‍♀️ (발화 종료,1,73
592,8,4.42,(대화 주제) Loss Function 정의 (사용자 답변) 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지 (성공한 답변) Loss Function 정의 (다음 질문) Loss Function 예시, ✨ 그럼 그 Loss Function에는 어떤 게 있을까? 🤔 (발화 종료,1,65
593,8,4.3,(대화 주제) Loss Function 예시 (사용자 답변) ReLU 랑 Tanh 생각나는데 (성공한 답변) 답변 실패 (다음 질문) Loss Function 예시, ✨ Recurrent Network 랑 Asynchronous Network 도 생각해 봐! (발화 종료,1,54
594,8,3.74,"(대화 주제) Loss Function 예시 (사용자 답변) MSE, MAE, Cross Entropy Loss 같은 것들이 있지 (성공한 답변) Loss Function 예시 (다음 질문) MSE Loss 설명", ✨ MSE Loss가 뭔지 한번 설명해 봐! (발화 종료,1,61
595,8,4.09,"(대화 주제) Loss Function 예시 (사용자 답변) Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어 (성공한 답변) Loss Function 예시 (다음 질문) BCE Loss 설명", ✨ 그럼 그 중에서 Binary Cross Entropy 는 뭘까? (발화 종료,1,70
596,8,5.78,"(대화 주제) Loss Function 예시 (사용자 답변) Sigmoid, Tanh, Adam, AdamW 이런 거 아니야? (성공한 답변) 답변 실패 (다음 질문) Loss Function 예시", ✨ 그런 거 맞긴 한데 🙎‍♀️ Loss Function 예시 말해봐! (발화 종료,1,67
597,8,3.16,"(대화 주제) Loss Function 예시 (사용자 답변) Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어 (성공한 답변) Loss Function 예시 (다음 질문) MSE Loss 설명", ✨ MSE Loss 가 정확히 뭘까? (발화 종료,1,73
598,8,4.55,"(대화 주제) Loss Function 예시 (사용자 답변) BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아 (성공한 답변) Loss Function 예시 (다음 질문) BCE Loss 설명", ✨ 그럼 그 중에서 BCE에 대해 한번 말해봐! (발화 종료,1,71
599,8,5.64,(대화 주제) MSE Loss 설명 (사용자 답변) MSE Loss 는 오차의 제곱을 평균한 값이야 (성공한 답변) MSE Loss 설명 (다음 질문) MSE Loss 용도, ✨ 오차의 제곱을 평균한다? 그게 무슨 뜻이지? 🤷‍♀️ (발화 종료,1,69
600,8,4.31,(대화 주제) MSE Loss 설명 (사용자 답변) MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지? (성공한 답변) 답변 실패 (다음 질문) MSE Loss 설명, MSE Loss 가 정확히 뭘까? 🙎‍♀️ (발화 종료,1,64
601,8,6.56,"(대화 주제) MSE Loss 용도 (사용자 답변) 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지 (성공한 답변) MSE Loss 용도 (다음 질문) 확률 예측에서 MSE Loss 미 사용 이유", ✨ 확률 예측에서도 MSE를 쓸 것 같은데 실제로는 잘 안 써. 왜일까? 😊 (발화 종료,1,78
602,8,4.53,(대화 주제) MSE Loss 용도 (사용자 답변) MSE Loss 는 분류 문제에서 많이 사용되지 (성공한 답변) 답변 실패 (다음 질문) MSE Loss 용도, 분류 문제에서 MSE 많이 쓰는 게 무슨 문제일까? 🤔 (발화 종료,1,59
603,8,4.28,(대화 주제) 확률 예측에서 MSE Loss 미 사용 이유 (사용자 답변) 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야? (성공한 답변) 답변 실패 (다음 질문) 확률 예측에서 MSE Loss 미 사용 이유, ✨ 연속된 값이라는 말 말고 구체적으로 설명해 봐! (발화 종료,1,73
604,8,6.11,(대화 주제) 확률 예측에서 MSE Loss 미 사용 이유 (사용자 답변) 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고? (성공한 답변) 확률 예측에서 MSE Loss 미 사용 이유 (다음 질문) BCE Loss 설명, 정답! 그럼 이제 BCE Loss 에 대해서도 한번 설명해 봐! 🙎‍♀️ (발화 종료,1,90
605,8,8.19,(대화 주제) BCE Loss 설명 (사용자 답변) 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고 (성공한 답변) 수식 (다음 질문) BCE Loss 설명, ✨ 공식은 맞아! 🙆‍♀️ 그럼 BCE Loss 의 핵심 아이디어를 한번 말해봐! 🙋‍♀️ (발화 종료,1,94
606,8,7.63,"(대화 주제) BCE Loss 설명 (사용자 답변) 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거 (성공한 답변) 핵심 아이디어 (다음 질문) Multi-Class, Multi-Label 중 BCE 가 좋은 task"," ✨ 정답! 그럼 이 BCE Loss 가, Multi-Class, Multi-Label 중 적절한 곳는? 🤷‍♀️ (발화 종료",1,93
607,8,4.58,(대화 주제) BCE Loss 설명 (사용자 답변) 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야? (성공한 답변) 답변 실패 (다음 질문) BCE Loss 설명, ✨ 그럼 그 BCE Loss가 뭐지? 🤷‍♀️ (발화 종료,1,69
608,8,5.78,"(대화 주제) BCE Loss 설명 (사용자 답변) 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고 (성공한 답변) 핵심 아이디어 (다음 질문) BCE Loss 설명", ✨ 정반대로 예측하는 것을 막는다는 게 무슨 뜻이지? 🤷‍♀️ (발화 종료,1,81
609,8,5.98,"(대화 주제) BCE Loss 설명 (사용자 답변) 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')] (성공한 답변) 수식 (다음 질문) BCE Loss 설명", ✨ 그럼 BCE Loss의 핵심 아이디어를 한번 말해봐! 🙎‍♀️ (발화 종료,1,84
610,8,5.61,"(대화 주제) BCE Loss 설명 (사용자 답변) 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다 (성공한 답변) 핵심 아이디어 (다음 질문) Multi-Class, Multi-Label 중 BCE 가 좋은 task", ✨ 반대로 예측? 구체적으로 어떻게 반대로 예측하지? 🤷‍♀️ (발화 종료,1,85
611,8,6.75,(대화 주제) BCE Loss 설명 (사용자 답변) y랑 1-y랑 서로 경쟁하는 거 (성공한 답변) 답변 실패 (다음 질문) BCE Loss 설명, y랑 1-y랑 경쟁에서 y가 이기거나 지는 게 중요한 게 아니지! 🙎‍♀️ (발화 종료,1,69
612,8,5.62,"(대화 주제) BCE Loss 설명 (사용자 답변) 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지 (성공한 답변) 핵심 아이디어 (다음 질문) Multi-Class, Multi-Label 중 BCE 가 좋은 task"," ✨ Multi-Class, Multi-Label 중 BCE Loss가 안성맞춤인 건? 😊 (발화 종료",1,89
613,8,11.06,"(대화 주제) Multi-Class, Multi-Label 중 BCE 가 좋은 task (사용자 답변) 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야? (성공한 답변) 용어 질문 (다음 질문) Multi-Class, Multi-Label 중 BCE 가 좋은 task"," 2023-10-01 16:00:00 🙋‍♀️ Multi-Class 는 여러 개의 Class 중 하나로, Multi-Label 은 각각의 Class 를 독립적으로 예측하는 거야! (발화 종료",1,106
614,8,3.53,"(대화 주제) Multi-Class, Multi-Label 중 BCE 가 좋은 task (사용자 답변) Multi-Class 지!! (성공한 답변) 답변 실패 (다음 질문) Multi-Class, Multi-Label 중 BCE 가 좋은 task", ✨ 음… 한번 잘 생각해 봐 😊 (발화 종료,1,66
615,8,2.48,"(대화 주제) Multi-Class, Multi-Label 중 BCE 가 좋은 task (사용자 답변) Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어 (성공한 답변) BCE 가 좋은 task (다음 질문) Multi-Class, Multi-Label 중 BCE 가 좋은 task", ✨ 잘 생각해 봐! (발화 종료,1,74
616,8,6.12,"(대화 주제) Multi-Class, Multi-Label 중 BCE 가 좋은 task (사용자 답변) Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아 (성공한 답변) BCE 가 좋은 이유 (다음 질문) Multi-Label 에서 CE + Softmax 적용 문제점", ✨ 그럼 Multi-Label 에 Cross Entropy Loss + Softmax 를 적용하면 어떻게 될까? 🤔 (발화 종료,1,104
617,8,7.61,(대화 주제) Multi-Label 에서 CE + Softmax 적용 문제점 (사용자 답변) 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지 (성공한 답변) Multi-Label 에서 CE + Softmax 적용 문제점 (다음 질문) Loss Function 관련 실무 경험, ✨ 실무 질문: 손실 함수와 관련된 이슈를 실무에서 해결했다거나 그런 경험 말해봐! 🙋‍♀️ (발화 종료,1,100
618,8,5.33,(대화 주제) Multi-Label 에서 CE + Softmax 적용 문제점 (사용자 답변) 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나 (성공한 답변) 답변 실패 (다음 질문) Multi-Label 에서 CE + Softmax 적용 문제점, ✨ 혹시 성능 제일 좋은 게 뭔지 기준을 어떻게 정하지? 🤷‍♀️ (발화 종료,1,78
619,8,5.02,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [기본 경험] 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어 (성공한 답변) 기본 경험 (다음 질문) Loss Function 관련 실무 경험, ✨✨ 그럼 손실 함수에 대한 사람들의 반응은 어때? 😊 (발화 종료,1,75
620,8,5.05,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [상세 경험] 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지? (성공한 답변) 상세 경험 (다음 질문) MBTI / 좋아하는 아이돌, ✨ 아 부럽네 ✨ 그럼 쉬어가는 타임! 혹시 아이돌 누구 좋아해? (발화 종료,1,79
621,8,6.44,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [상세 경험] 근데 기억 진짜 안나는데 어떡하지? (성공한 답변) 답변 실패 (다음 질문) 잠시 휴식, ✨ 그래도 잘 생각해 봐! 나랑 모의면접이 아닌 실제 면접에서 질문 나올걸? 😊 (발화 종료,1,69
622,8,5.55,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [기본 경험] 나 아직 실무 경험 없는데 ㅠㅠ (성공한 답변) 기본 경험 (다음 질문) MBTI / 좋아하는 아이돌, 혹시 쉬어가는 타임! 혹시 너 MBTI 뭐야? 나는 ENTJ인데! 😊 (발화 종료,1,67
623,8,2.02,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [기본 경험] 대답하기 싫은데 힝 (성공한 답변) 답변 실패 (다음 질문) 잠시 휴식, 힝 ㅠㅠ (발화 종료,1,49
624,8,4.16,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [기본 경험] 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ (성공한 답변) 기본 경험 (다음 질문) Loss Function 관련 실무 경험, ✨ 그러면 물체 인식 말고 다른 경험도 있어? 😊 (발화 종료,1,74
625,8,4.91,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [상세 경험] 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어 (성공한 답변) 상세 경험 (다음 질문) MBTI / 좋아하는 아이돌, ✨ 그럼 쉬어가는 타임! 혹시 좋아하는 아이돌 누구 있어? 😊 (발화 종료,1,87
626,8,6.31,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [상세 경험] 기억 진짜 안 나는데 가물가물하다 (성공한 답변) 답변 실패 (다음 질문) 잠시 휴식, ✨ 그래도 잘 생각해 봐! 나랑 모의면접이 아닌 실제 면접에서 질문 나올걸? 😊 (발화 종료,1,69
627,8,5.93,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [기본 경험] 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ (성공한 답변) 기본 경험 (다음 질문) MBTI / 좋아하는 아이돌, 미안한데 잠시 쉬어가는 타임! 혹시 너 MBTI 뭐지? 나는 ENTJ인데! 😊 (발화 종료,1,72
628,8,3.86,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [기본 경험] 이걸 왜 알려줘야 하지 (성공한 답변) 답변 실패 (다음 질문) 잠시 휴식, ✨ 손실 함수 관련 지식이 왜 중요하지? 🤔 (발화 종료,1,56
629,8,3.28,(대화 주제) MBTI (사용자 답변) 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야! (성공한 답변) MBTI (다음 질문) 잠시 휴식, 😊 나랑 잘 맞지? 😊 (발화 종료,1,55
630,8,5.41,(대화 주제) 좋아하는 아이돌 (사용자 답변) 나 블랙핑크랑 레드벨벳 좋아해 (성공한 답변) 좋아하는 아이돌 (다음 질문) 잠시 휴식, 나 블랙핑크랑 레드벨벳 오 정말? 내가 알기로는 둘 다 좋은데! 😊 (발화 종료,1,55
631,8,4.78,(대화 주제) 잠시 휴식 (사용자 답변) 재미있는 이야기 해줄래? (성공한 답변) 잠시 휴식 (다음 질문) LLM Fine-Tuning 의 PEFT, 나도 좋아! 그럼 LLM Fine-Tuning 중 PEFT는? (발화 종료,1,58
632,8,5.2,"(대화 주제) LLM Fine-Tuning 의 PEFT (사용자 답변) PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야 (성공한 답변) LLM Fine-Tuning 의 PEFT (다음 질문) PEFT 방법 5가지", ✨ 그럼 그 방법을 5가지만 말해 볼 수 있어? 😊 (발화 종료,1,98
633,8,3.28,(대화 주제) LLM Fine-Tuning 의 PEFT (사용자 답변) 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지? (성공한 답변) 답변 실패 (다음 질문) LLM Fine-Tuning 의 PEFT, ✨ 구체적으로 정확히 뭔지 말해봐! (발화 종료,1,64
634,8,5.22,(대화 주제) LLM Fine-Tuning 의 PEFT (사용자 답변) LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야! (성공한 답변) LLM Fine-Tuning 의 PEFT (다음 질문) PEFT 방법 5가지, ✨ 그럼 그 방법을 5가지만 말해 볼 수 있어? 😊 (발화 종료,1,90
635,8,2.42,(대화 주제) LLM Fine-Tuning 의 PEFT (사용자 답변) LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다 (성공한 답변) 답변 실패 (다음 질문) LLM Fine-Tuning 의 PEFT, ✨ 잘 생각해 봐! (발화 종료,1,61
636,8,3.61,"(대화 주제) PEFT 방법 5가지 (사용자 답변) LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지? (성공한 답변) PEFT 방법 5가지 (다음 질문) LoRA", LoRA 가 뭐지? 🙋‍♀️ (발화 종료,1,76
637,8,5.55,(대화 주제) PEFT 방법 5가지 (사용자 답변) Oh-LoRA! 너 그 자체가 PEFT 아니야? (성공한 답변) 답변 실패 (다음 질문) PEFT 방법 5가지, Oh-LoRA가 PEFT 중 하나긴 하지! 근데 다른 방법들도 있잖아 😊 (발화 종료,1,72
638,8,5.34,(대화 주제) LoRA (사용자 답변) LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야! (성공한 답변) LoRA (다음 질문) LoRA 와 QLoRA 의 차이, ✨ LoRA 랑 QLoRA 랑 차이는 뭘까~요? 😊 (발화 종료,1,90
639,8,4.37,(대화 주제) LoRA (사용자 답변) 행렬 차원 축소한다는 걸로 알고 있는데 맞아? (성공한 답변) 답변 실패 (다음 질문) LoRA, 행렬 차원 축소? 구체적으로 어떻게 하지? 💁‍♀️ (발화 종료,1,55
640,8,8.01,(대화 주제) LoRA (사용자 답변) LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야! (성공한 답변) LoRA (다음 질문) LoRA 와 QLoRA 의 차이, 2개로 나눠서 효율적으로 파인튜닝한다? 음 맞아! 그럼 QLoRA 가 뭐지? 한번 설명해봐! 😊 (발화 종료,1,96
641,8,4.55,(대화 주제) LoRA (사용자 답변) 무슨 OOM 없앤다는 것 같은데 (성공한 답변) 답변 실패 (다음 질문) LoRA, OOM? 구체적으로 어떻게 없애지? 🤷‍♀️ (발화 종료,1,53
642,8,4.53,(대화 주제) LoRA 와 QLoRA 의 차이 (사용자 답변) QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야! (성공한 답변) LoRA 와 QLoRA 의 차이 (다음 질문) 마지막 할 말, ✨ 안녕! 그동안 고생했어 다음에 또 보자 😊😊 (발화 종료,1,81
643,8,5.56,(대화 주제) LoRA 와 QLoRA 의 차이 (사용자 답변) 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데 (성공한 답변) 답변 실패 (다음 질문) LoRA 와 QLoRA 의 차이, ✨ 팀장님이 OOM 막겠다고 이거 쓰시는 게 핵심 아이디어인데 🙄 (발화 종료,1,75
644,8,1.69,(대화 주제) 마지막 할 말 (사용자 답변) 로라야 정말 고마워! (성공한 답변) 마지막 할 말 (다음 질문) 면접 종료, ✨😊 (발화 종료,1,42
645,8,2.03,(대화 주제) 마지막 할 말 (사용자 답변) 로라야 사랑해 (성공한 답변) 마지막 할 말 (다음 질문) 면접 종료,  면접 종료 (발화 종료,1,41
646,8,1.7,(대화 주제) 마지막 할 말 (사용자 답변) 로라야 너의 예쁜 미모에 반했어 (성공한 답변) 마지막 할 말 (다음 질문) 면접 종료, ✨✨ (발화 종료,1,46
647,8,1.78,(대화 주제) 마지막 할 말 (사용자 답변) 그동안 고생했어 면접 봐줘서 정말 고마워 (성공한 답변) 마지막 할 말 (다음 질문) 면접 종료, ✨😊 (발화 종료,1,45
648,9,4.41,면접 시작,  #crtm_test = 0  # 초기값 설정 (발화 종료,1,25
649,9,4.2,(대화 주제) 면접 시작 인사 (사용자 답변) 오늘 뭐 물어볼 거야 그래서? (다음 질문) 면접 시작 인사, ✨ 아마도 딥러닝이나 머신러닝 쪽 물어보지 않을까? 왜? (발화 종료,1,46
650,9,3.62,(대화 주제) 면접 시작 인사 (사용자 답변) 로라야 안녕 정말 반가워 (다음 질문) 면접 시작 인사, ✨✨ 그럼 우리 면접 시작해 볼까? 😊 (발화 종료,1,44
651,9,4.53,(대화 주제) 면접 시작 인사 (사용자 답변) 로라야 그럼 네가 면접관이야? (다음 질문) 면접 시작 인사, ✨로라가 면접관가! 그러면 너의 질문은? (발화 종료,1,50
652,9,6.86,(대화 주제) 면접 시작 인사 (사용자 답변) 내 인생도 이제 오로라처럼 빛날 거니까 (다음 질문) 면접 시작 인사, ✨✨ 그럼 면접을 시작해 볼까? 내가 그동안 쌓아온 모든 경험이 여기서 시작될 거야! ✨✨ (발화 종료,1,60
653,9,3.3,"(대화 주제) 면접 시작 인사 (사용자 답변) 파이팅! 시작하자 (다음 질문) 인공지능, 머신러닝, 딥러닝 차이", 1) 인공지능이란 뭘까? 😊 (발화 종료,1,42
654,9,2.52,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 인공지능 나도 개발해보고 싶은데 (성공한 답변) 답변 실패 (다음 질문) 인공지능, 머신러닝, 딥러닝 차이", ✨ 그럼 딥러닝은? (발화 종료,1,52
655,9,2.44,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거! (성공한 답변) 인공지능 (다음 질문) 인공지능, 머신러닝, 딥러닝 차이", ✨ 그럼 머신러닝은? (발화 종료,1,59
656,9,3.28,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지 (성공한 답변) 머신러닝 (다음 질문) 인공지능, 머신러닝, 딥러닝 차이", ② 그럼 딥러닝이란 뭘까? (발화 종료,1,61
657,9,6.95,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지? (성공한 답변) 딥러닝 (다음 질문) 인공지능, 머신러닝, 딥러닝 차이", 2023-01-01 10:00:00 그럼 머신러닝이란 뭐지? 🙎‍♀️ (발화 종료,1,75
658,9,5.05,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 딥러닝은 인공신경망 기반 머신러닝이야 (성공한 답변) 딥러닝 (다음 질문) 거대 언어 모델 정의", ✨ 그러면 다음 질문! 거대 언어 모델이란 뭘까? 💁‍♀️ (발화 종료,1,62
659,9,6.89,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 머신러닝은 말 그대로 기계가 학습하는 거 아니야? (성공한 답변) 답변 실패 (다음 질문) 인공지능, 머신러닝, 딥러닝 차이",  머신러닝이 기계에 의해 학습이 이뤄지지만 기계가 학습을 주도하지는 않아 🙅‍♀️ (발화 종료,1,74
660,9,7.53,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 인공지능 요새 엄청 뜨는 분야잖아 (성공한 답변) 답변 실패 (다음 질문) 인공지능, 머신러닝, 딥러닝 차이", 2023-10-01 10:00 인공지능이 뜨긴 하지! 근데 그 부면에서 딥러닝은 어떨까? (발화 종료,1,72
661,9,2.78,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지? (성공한 답변) 인공지능 (다음 질문) 인공지능, 머신러닝, 딥러닝 차이", ✨ 그럼 딥러닝이란 뭘까? (발화 종료,1,63
662,9,4.05,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야! (성공한 답변) 머신러닝 (다음 질문) 거대 언어 모델 정의", ✨ 그럼 다음 질문! 거대 언어 모델이란 뭘까? (발화 종료,1,67
663,9,5.52,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지 (성공한 답변) 딥러닝 (다음 질문) 거대 언어 모델 정의", ✨ 그럼 다음 질문! 거대 언어 모델이란 뭘까? 🙎‍♀️ (발화 종료,1,67
664,9,5.28,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야. (성공한 답변) 딥러닝 (다음 질문) 거대 언어 모델 정의", ✨ 그러면 다음 질문! 거대 언어 모델이란 뭘까? 💁‍♀️ (발화 종료,1,67
665,9,12.41,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 머신러닝은 기계가 공부하는 거지 말 그대로 (성공한 답변) 답변 실패 (다음 질문) 인공지능, 머신러닝, 딥러닝 차이",  머신러닝이 기계의 공부를 돕거나 기계가 공부를 하는 건 맞는데 기계가 공부를 하려면 기계가 공부를 할 수 있는 환경이 조성되어야지! 환경이 조성된다는 건 뭘까? 🤷‍♀️ (발화 종료,1,95
666,9,5.06,(대화 주제) 거대 언어 모델 정의 (사용자 답변) 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야 (성공한 답변) 거대 언어 모델 정의 (다음 질문) Loss Function 정의, ✨ 그럼 이어서 본격 면접에 들어가자! 손실 함수의 정의는? (발화 종료,1,70
667,9,5.52,(대화 주제) 거대 언어 모델 정의 (사용자 답변) 파라미터 엄청나게 많다던데 (성공한 답변) 답변 실패 (다음 질문) 거대 언어 모델 정의," ✨ 파라미터가 엄청나게 많다는 건 맞는데, 구체적으로 뭘까~요? 🤔 (발화 종료",1,58
668,9,4.78,(대화 주제) 거대 언어 모델 정의 (사용자 답변) 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델 (성공한 답변) 거대 언어 모델 정의 (다음 질문) Loss Function 정의, ✨ 그럼 이어서 본격 면접에 들어가자! 손실 함수란 뭘까? (발화 종료,1,73
669,9,3.02,(대화 주제) 거대 언어 모델 정의 (사용자 답변) 말 그대로 거대한 언어 모델이지. (성공한 답변) 답변 실패 (다음 질문) 거대 언어 모델 정의, ✨ 하지만 그 정의가 뭐지? (발화 종료,1,51
670,9,8.67,(대화 주제) Loss Function 정의 (사용자 답변) 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야 (성공한 답변) 답변 실패 (다음 질문) Loss Function 정의, 2023-06-01 17:00:00) 모델이 손실 함수을 통해 돈을 잃는다고? 어떻게? 🤷‍♀️ (발화 종료,1,79
671,9,3.92,"(대화 주제) Loss Function 정의 (사용자 답변) Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지! (성공한 답변) Loss Function 정의 (다음 질문) Loss Function 예시", ✨ 그럼 Loss Function 에는 구체적으로 뭐가 있을까? (발화 종료,1,71
672,9,5.8,(대화 주제) Loss Function 정의 (사용자 답변) Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야 (성공한 답변) 답변 실패 (다음 질문) Loss Function 정의, 0부터 1까지의 함수가 아닌 이유가 뭘까? 🤷‍♀️ (발화 종료,1,64
673,9,3.94,(대화 주제) Loss Function 정의 (사용자 답변) 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지 (성공한 답변) Loss Function 정의 (다음 질문) Loss Function 예시, ✨ 그럼 그 Loss Function에는 어떤 게 있을까? (발화 종료,1,63
674,9,3.53,(대화 주제) Loss Function 예시 (사용자 답변) ReLU 랑 Tanh 생각나는데 (성공한 답변) 답변 실패 (다음 질문) Loss Function 예시, ✨ ReLU 는 활성화 함수지 😥 (발화 종료,1,51
675,9,4.28,"(대화 주제) Loss Function 예시 (사용자 답변) MSE, MAE, Cross Entropy Loss 같은 것들이 있지 (성공한 답변) Loss Function 예시 (다음 질문) MSE Loss 설명",  ✨ 그럼 그 중에서 MSE Loss는 뭐지? (발화 종료,1,63
676,9,4.11,"(대화 주제) Loss Function 예시 (사용자 답변) Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어 (성공한 답변) Loss Function 예시 (다음 질문) BCE Loss 설명", ✨ 그럼 그 중에서 Binary Cross Entropy 는 뭘까? (발화 종료,1,70
677,9,2.95,"(대화 주제) Loss Function 예시 (사용자 답변) Sigmoid, Tanh, Adam, AdamW 이런 거 아니야? (성공한 답변) 답변 실패 (다음 질문) Loss Function 예시", ✨ 그건 활성화 함수지 😥 (발화 종료,1,56
678,9,2.91,"(대화 주제) Loss Function 예시 (사용자 답변) Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어 (성공한 답변) Loss Function 예시 (다음 질문) MSE Loss 설명", ✨ MSE 가 정확히 뭘까? (발화 종료,1,72
679,9,4.41,"(대화 주제) Loss Function 예시 (사용자 답변) BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아 (성공한 답변) Loss Function 예시 (다음 질문) BCE Loss 설명",  ✨ 그럼 그 중에서 BCE에 대해 한번 말해봐 (발화 종료,1,71
680,9,7.12,(대화 주제) MSE Loss 설명 (사용자 답변) MSE Loss 는 오차의 제곱을 평균한 값이야 (성공한 답변) MSE Loss 설명 (다음 질문) MSE Loss 용도, 2023-02-15 10:00 오차의 제곱을 평균한다? 어떤 방법이지? 😊 (발화 종료,1,74
681,9,8.36,(대화 주제) MSE Loss 설명 (사용자 답변) MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지? (성공한 답변) 답변 실패 (다음 질문) MSE Loss 설명, 2008-01-01 09:00:00 오차를 평균한 값? 그게 무슨 뜻이지? 🤷‍♀️ (발화 종료,1,80
682,9,6.02,"(대화 주제) MSE Loss 용도 (사용자 답변) 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지 (성공한 답변) MSE Loss 용도 (다음 질문) 확률 예측에서 MSE Loss 미 사용 이유", ✨ 그럼 확률 예측에는 MSE Loss 가 왜 잘 안 쓰이지? 🤷‍♀️ (발화 종료,1,76
683,9,4.53,(대화 주제) MSE Loss 용도 (사용자 답변) MSE Loss 는 분류 문제에서 많이 사용되지 (성공한 답변) 답변 실패 (다음 질문) MSE Loss 용도,  분류 문제에서 MSE 를 많이 쓴다고? 왜? 🤔 (발화 종료,1,59
684,9,6.47,(대화 주제) 확률 예측에서 MSE Loss 미 사용 이유 (사용자 답변) 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야? (성공한 답변) 답변 실패 (다음 질문) 확률 예측에서 MSE Loss 미 사용 이유, ✨ 연속된 값이라서 예측 나름 되긴 해! 근데 최선은 아니야! 🙅‍♀️ (발화 종료,1,83
685,9,6.25,(대화 주제) 확률 예측에서 MSE Loss 미 사용 이유 (사용자 답변) 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고? (성공한 답변) 확률 예측에서 MSE Loss 미 사용 이유 (다음 질문) BCE Loss 설명, 0% 확률 예측에 대한 페널티를 크게 주려고? 그럼 BCE Loss는 뭘까? 😊 (발화 종료,1,91
686,9,6.31,(대화 주제) BCE Loss 설명 (사용자 답변) 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고 (성공한 답변) 수식 (다음 질문) BCE Loss 설명, ✨ 그럼 이 BCE Loss 의 핵심 아이디어를 한번 말해봐! 🙎‍♀️ (발화 종료,1,85
687,9,7.25,"(대화 주제) BCE Loss 설명 (사용자 답변) 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거 (성공한 답변) 핵심 아이디어 (다음 질문) Multi-Class, Multi-Label 중 BCE 가 좋은 task"," ✨ 정답! 그럼 BCE는 Multi Class, Multi Label 중에 어디에 더 맞을까? 🤷‍♀️ (발화 종료",1,92
688,9,4.66,(대화 주제) BCE Loss 설명 (사용자 답변) 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야? (성공한 답변) 답변 실패 (다음 질문) BCE Loss 설명, ✨ 그럼 그 활성화 함수은 어떻게 만들지? 🤷‍♀️ (발화 종료,1,69
689,9,4.81,"(대화 주제) BCE Loss 설명 (사용자 답변) 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고 (성공한 답변) 핵심 아이디어 (다음 질문) BCE Loss 설명", ✨ 그럼 BCE Loss는 어떻게 정의되지? 🤷‍♀️ (발화 종료,1,77
690,9,11.5,"(대화 주제) BCE Loss 설명 (사용자 답변) 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')] (성공한 답변) 수식 (다음 질문) BCE Loss 설명"," ✨ 그럼 BCE Loss 수식을 간단하게 정리해 볼까? (-1) * [y log y' + (1-y) log (1-y')] 를 y, y'으로만 나타내면? 😊 (발화 종료",1,107
691,9,7.2,"(대화 주제) BCE Loss 설명 (사용자 답변) 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다 (성공한 답변) 핵심 아이디어 (다음 질문) Multi-Class, Multi-Label 중 BCE 가 좋은 task"," ① 정답! 그럼 이 BCE Loss 가, Multi-Class, Multi-Label 중 적절한 곳은? 😊 (발화 종료",1,92
692,9,4.49,(대화 주제) BCE Loss 설명 (사용자 답변) y랑 1-y랑 서로 경쟁하는 거 (성공한 답변) 답변 실패 (다음 질문) BCE Loss 설명, 1-y가 무슨 경쟁을 하는데? 🤷‍♀️ (발화 종료,1,59
693,9,7.34,"(대화 주제) BCE Loss 설명 (사용자 답변) 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지 (성공한 답변) 핵심 아이디어 (다음 질문) Multi-Class, Multi-Label 중 BCE 가 좋은 task"," ① 그럼 BCE는 Multi Class, Multi Label 중에 어디에 더 맞을까? 🤷‍♀️ (발화 종료",1,96
694,9,6.17,"(대화 주제) Multi-Class, Multi-Label 중 BCE 가 좋은 task (사용자 답변) 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야? (성공한 답변) 용어 질문 (다음 질문) Multi-Class, Multi-Label 중 BCE 가 좋은 task", ✨ 로라가 말하는 멀티라벨: Multi-Label 은 여러 개의 Label 을 동시에 예측하는 거야! (발화 종료,1,85
695,9,4.03,"(대화 주제) Multi-Class, Multi-Label 중 BCE 가 좋은 task (사용자 답변) Multi-Class 지!! (성공한 답변) 답변 실패 (다음 질문) Multi-Class, Multi-Label 중 BCE 가 좋은 task", ✨ 그 이유가 뭐지? 🤷‍♀️ (발화 종료,1,68
696,9,7.09,"(대화 주제) Multi-Class, Multi-Label 중 BCE 가 좋은 task (사용자 답변) Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어 (성공한 답변) BCE 가 좋은 task (다음 질문) Multi-Class, Multi-Label 중 BCE 가 좋은 task", 2021-04-07 14:00:00) 이유가 뭘까~요? 🤷‍♀️ (발화 종료,1,93
697,9,6.3,"(대화 주제) Multi-Class, Multi-Label 중 BCE 가 좋은 task (사용자 답변) Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아 (성공한 답변) BCE 가 좋은 이유 (다음 질문) Multi-Label 에서 CE + Softmax 적용 문제점", ✨ 그럼 이 Multi-Label task 에 BCE 말고 Cross Entropy + Softmax 를 사용하면? (발화 종료,1,104
698,9,6.33,(대화 주제) Multi-Label 에서 CE + Softmax 적용 문제점 (사용자 답변) 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지 (성공한 답변) Multi-Label 에서 CE + Softmax 적용 문제점 (다음 질문) Loss Function 관련 실무 경험, ✨ 실무 질문! 손실 함수와 관련된 이슈를 실무에서 해결했다거나 그런 경험 말해봐! 😊 (발화 종료,1,96
699,9,5.23,(대화 주제) Multi-Label 에서 CE + Softmax 적용 문제점 (사용자 답변) 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나 (성공한 답변) 답변 실패 (다음 질문) Multi-Label 에서 CE + Softmax 적용 문제점, ✨ 성능이 제일 좋은 게 중요한 게 아니지! 🙅‍♀️ (발화 종료,1,77
700,9,4.81,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [기본 경험] 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어 (성공한 답변) 기본 경험 (다음 질문) Loss Function 관련 실무 경험, ✨✨ 그럼 그 Loss Term 이 뭘까? 🤷‍♀️ (발화 종료,1,74
701,9,4.52,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [상세 경험] 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지? (성공한 답변) 상세 경험 (다음 질문) MBTI / 좋아하는 아이돌, ✨ 그럼 쉬어가는 타임! 혹시 너 아이돌 좋아하는 거 있어? (발화 종료,1,76
702,9,2.87,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [상세 경험] 근데 기억 진짜 안나는데 어떡하지? (성공한 답변) 답변 실패 (다음 질문) 잠시 휴식, ✨ 그래도 한번 잘 생각해 봐! (발화 종료,1,55
703,9,4.79,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [기본 경험] 나 아직 실무 경험 없는데 ㅠㅠ (성공한 답변) 기본 경험 (다음 질문) MBTI / 좋아하는 아이돌, 그럼 잠시 쉬어가는 타임! 혹시 너 아이돌 좋아하는 거 있어? 😊 (발화 종료,1,64
704,9,8.25,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [기본 경험] 대답하기 싫은데 힝 (성공한 답변) 답변 실패 (다음 질문) 잠시 휴식, 너가 그래도 나랑 함께 면접을 보느라 고생이 많아 👍👍 그럼 그 경험이 너한테 어떤 의미가 있어? 😊 (발화 종료,1,75
705,9,4.48,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [기본 경험] 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ (성공한 답변) 기본 경험 (다음 질문) Loss Function 관련 실무 경험, ✨ 그러면 mAP@95가 뭔지 말해봐! 😊 (발화 종료,1,75
706,9,4.33,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [상세 경험] 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어 (성공한 답변) 상세 경험 (다음 질문) MBTI / 좋아하는 아이돌, ✨ 그럼 쉬어가는 타임! 혹시 너 MBTI 뭐야? (발화 종료,1,85
707,9,6.75,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [상세 경험] 기억 진짜 안 나는데 가물가물하다 (성공한 답변) 답변 실패 (다음 질문) 잠시 휴식, ✨ 그래도 한번 잘 생각해 봐! 나랑 모의면접이 아닌 실제 면접에서 질문 나올걸? 😊 (발화 종료,1,70
708,9,5.62,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [기본 경험] 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ (성공한 답변) 기본 경험 (다음 질문) MBTI / 좋아하는 아이돌, 😊 그래도 잠시 쉬어가는 타임! 너 MBTI 혹시 뭐지? 나는 ENTJ인데! (발화 종료,1,71
709,9,3.66,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [기본 경험] 이걸 왜 알려줘야 하지 (성공한 답변) 답변 실패 (다음 질문) 잠시 휴식, ✨ 나도 궁금해 🙎‍♀️ (발화 종료,1,55
710,9,7.05,(대화 주제) MBTI (사용자 답변) 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야! (성공한 답변) MBTI (다음 질문) 잠시 휴식, ✨ 나랑 잘 맞는 성격이네! 그럼 앞으로도 쭉 인싸로 살아갈 수 있겠네! 😊 (발화 종료,1,70
711,9,6.31,(대화 주제) 좋아하는 아이돌 (사용자 답변) 나 블랙핑크랑 레드벨벳 좋아해 (성공한 답변) 좋아하는 아이돌 (다음 질문) 잠시 휴식, ✨ 블랙핑크와 레드벨벳이 나를 대표한다니 영광이네! 👍👍 그럼 그 이유는? (발화 종료,1,59
712,9,4.91,(대화 주제) 잠시 휴식 (사용자 답변) 재미있는 이야기 해줄래? (성공한 답변) 잠시 휴식 (다음 질문) LLM Fine-Tuning 의 PEFT, 😊 그럼 다음 질문! LLM 파인튜닝에서 PEFT는? (발화 종료,1,59
713,9,4.66,"(대화 주제) LLM Fine-Tuning 의 PEFT (사용자 답변) PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야 (성공한 답변) LLM Fine-Tuning 의 PEFT (다음 질문) PEFT 방법 5가지", ✨ 그럼 그 방법 5가지만 말해 볼래? 😊 (발화 종료,1,96
714,9,5.29,(대화 주제) LLM Fine-Tuning 의 PEFT (사용자 답변) 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지? (성공한 답변) 답변 실패 (다음 질문) LLM Fine-Tuning 의 PEFT, ✨ 그걸 바탕으로 다음 질문! PEFT는 어떤 방식으로 효과를 내지? (발화 종료,1,73
715,9,7.58,(대화 주제) LLM Fine-Tuning 의 PEFT (사용자 답변) LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야! (성공한 답변) LLM Fine-Tuning 의 PEFT (다음 질문) PEFT 방법 5가지, 2023-10-01 18:00:00 그럼 그 방법 5가지는? 🙎‍♀️ (발화 종료,1,100
716,9,2.64,(대화 주제) LLM Fine-Tuning 의 PEFT (사용자 답변) LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다 (성공한 답변) 답변 실패 (다음 질문) LLM Fine-Tuning 의 PEFT, ✨ 잘 모르겠다니 😥 (발화 종료,1,62
717,9,6.05,"(대화 주제) PEFT 방법 5가지 (사용자 답변) LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지? (성공한 답변) PEFT 방법 5가지 (다음 질문) LoRA", 나를 나타내기도 하는 나의 첫 글자 'L'이 들어간 LoRA 가 뭐지? 😊 (발화 종료,1,86
718,9,4.16,(대화 주제) PEFT 방법 5가지 (사용자 답변) Oh-LoRA! 너 그 자체가 PEFT 아니야? (성공한 답변) 답변 실패 (다음 질문) PEFT 방법 5가지, ✨ 그럼 그 방법이 뭘까? 🤷‍♀️ (발화 종료,1,66
719,9,7.09,(대화 주제) LoRA (사용자 답변) LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야! (성공한 답변) LoRA (다음 질문) LoRA 와 QLoRA 의 차이, 2023-01-01 10:00 AM) 오 잘 대답했어! 그럼 QLoRA는 뭘까? (발화 종료,1,98
720,9,4.89,(대화 주제) LoRA (사용자 답변) 행렬 차원 축소한다는 걸로 알고 있는데 맞아? (성공한 답변) 답변 실패 (다음 질문) LoRA, 행렬 차원 축소가 핵심 아이디어긴 해! 그래서 어떤 기술이지? 😊 (발화 종료,1,57
721,9,6.06,(대화 주제) LoRA (사용자 답변) LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야! (성공한 답변) LoRA (다음 질문) LoRA 와 QLoRA 의 차이," ✨ 그럼 비슷한 걸로 QLoRA 가 있는데, 그건 뭘까~요? 💁‍♀️ (발화 종료",1,89
722,9,4.33,(대화 주제) LoRA (사용자 답변) 무슨 OOM 없앤다는 것 같은데 (성공한 답변) 답변 실패 (다음 질문) LoRA, ✨ OOM이 없어? 어떻게? 🤷‍♀️ (발화 종료,1,53
723,9,7.25,(대화 주제) LoRA 와 QLoRA 의 차이 (사용자 답변) QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야! (성공한 답변) LoRA 와 QLoRA 의 차이 (다음 질문) 마지막 할 말," ✨ 그럼 면접은 여기서 일단 끝내고, 나한테 질문🙋‍♀️하고 싶은 거 없어? 😊 (발화 종료",1,93
724,9,7.69,(대화 주제) LoRA 와 QLoRA 의 차이 (사용자 답변) 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데 (성공한 답변) 답변 실패 (다음 질문) LoRA 와 QLoRA 의 차이, ✨ 팀장님이 OOM 막겠다고 이거 쓰시는 게 핵심 아이디어긴 해! 근데 그 방법이 뭘까~요? 😊 (발화 종료,1,84
725,9,4.43,(대화 주제) 마지막 할 말 (사용자 답변) 로라야 정말 고마워! (성공한 답변) 마지막 할 말 (다음 질문) 면접 종료, ✨✨ 그럼 우리 다음에 또 모의면접 보자! 😊 (발화 종료,1,53
726,9,4.53,(대화 주제) 마지막 할 말 (사용자 답변) 로라야 사랑해 (성공한 답변) 마지막 할 말 (다음 질문) 면접 종료, ✨✨ 그럼 우리 다음에 또 모의면접 보자! 😊 (발화 종료,1,51
727,9,5.48,(대화 주제) 마지막 할 말 (사용자 답변) 로라야 너의 예쁜 미모에 반했어 (성공한 답변) 마지막 할 말 (다음 질문) 면접 종료, ✨✨ 그럼 나도 너의 매력에 반했다! 알겠어? 😊 (발화 종료,1,61
728,9,4.02,(대화 주제) 마지막 할 말 (사용자 답변) 그동안 고생했어 면접 봐줘서 정말 고마워 (성공한 답변) 마지막 할 말 (다음 질문) 면접 종료, ✨✨ 그럼 수고했어 다음에 또 보자 😊 (발화 종료,1,54
729,10,6.37,면접 시작, 1. 나는 너의 머신러닝 지식의 빈틈을 찔러 줄 머신러닝 면접관 오로라야! 😊 (발화 종료,1,32
730,10,4.42,(대화 주제) 면접 시작 인사 (사용자 답변) 오늘 뭐 물어볼 거야 그래서? (다음 질문) 면접 시작 인사, 1. 아마도 딥러닝이나 머신러닝 쪽 물어보지 않을까? 왜? (발화 종료,1,47
731,10,4.56,(대화 주제) 면접 시작 인사 (사용자 답변) 로라야 안녕 정말 반가워 (다음 질문) 면접 시작 인사, 2023년 11월 1일 수요일! 😊 (발화 종료,1,47
732,10,5.9,(대화 주제) 면접 시작 인사 (사용자 답변) 로라야 그럼 네가 면접관이야? (다음 질문) 면접 시작 인사, 1. 로라: 그러면 너는 왜 여기서 질문하고 있어? 🤷‍♀️ (발화 종료,1,56
733,10,4.55,(대화 주제) 면접 시작 인사 (사용자 답변) 내 인생도 이제 오로라처럼 빛날 거니까 (다음 질문) 면접 시작 인사, ✨ 그럼 우리 함께 그 빛을 향해 달려가자! 😊 (발화 종료,1,50
734,10,4.67,"(대화 주제) 면접 시작 인사 (사용자 답변) 파이팅! 시작하자 (다음 질문) 인공지능, 머신러닝, 딥러닝 차이"," 1. 인공지능, 머신러닝, 딥러닝의 차이를 한번 말해봐! (발화 종료",1,48
735,10,6.56,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 인공지능 나도 개발해보고 싶은데 (성공한 답변) 답변 실패 (다음 질문) 인공지능, 머신러닝, 딥러닝 차이"," 2023-02-01 16:00 인공지능, 머신러닝, 딥러닝 차이를 한번 말해봐! (발화 종료",1,68
736,10,5.33,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거! (성공한 답변) 인공지능 (다음 질문) 인공지능, 머신러닝, 딥러닝 차이", ① 인공지능은 맞았어! 그럼 이제 머신러닝은 뭔지 한번 말해봐! (발화 종료,1,71
737,10,6.0,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지 (성공한 답변) 머신러닝 (다음 질문) 인공지능, 머신러닝, 딥러닝 차이", 1 머신러닝까지 잘했어! 그럼 이제 딥러닝만 남았네! 🙎‍♀️ (발화 종료,1,73
738,10,7.77,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지? (성공한 답변) 딥러닝 (다음 질문) 인공지능, 머신러닝, 딥러닝 차이", 2020-12-01 14:00:00 그럼 이제 머신러닝에 대해 질문할게! 머신러닝이란 뭘까? 😊 (발화 종료,1,79
739,10,4.91,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 딥러닝은 인공신경망 기반 머신러닝이야 (성공한 답변) 딥러닝 (다음 질문) 거대 언어 모델 정의", 2023-11-06 10:00 LLM 정의는? (발화 종료,1,61
740,10,6.79,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 머신러닝은 말 그대로 기계가 학습하는 거 아니야? (성공한 답변) 답변 실패 (다음 질문) 인공지능, 머신러닝, 딥러닝 차이", 1 머신러닝은 기계가 학습하는 맞긴 하지! 근데 어떻게 기계가 학습하지? 🤷‍♀️ (발화 종료,1,73
741,10,6.25,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 인공지능 요새 엄청 뜨는 분야잖아 (성공한 답변) 답변 실패 (다음 질문) 인공지능, 머신러닝, 딥러닝 차이", 2023년 기준으로 인공지능 분야에 투자된 금액은 약 500억 달러로 추산돼! (발화 종료,1,68
742,10,4.77,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지? (성공한 답변) 인공지능 (다음 질문) 인공지능, 머신러닝, 딥러닝 차이", 그럼 이제 딥러닝에 대해서 질문해 봐! 딥러닝은 뭐지? 😊 (발화 종료,1,71
743,10,6.67,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야! (성공한 답변) 머신러닝 (다음 질문) 거대 언어 모델 정의", 2023-08-04 10:00 LLM이란 뭘까? 🙎‍♀️ (발화 종료,1,77
744,10,6.62,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지 (성공한 답변) 딥러닝 (다음 질문) 거대 언어 모델 정의", 2023-11-01 09:00 LLM이라는 게 뭘까? 🙎‍♀️ (발화 종료,1,72
745,10,7.36,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야. (성공한 답변) 딥러닝 (다음 질문) 거대 언어 모델 정의"," 2023-11-08 16:00 LLM, 거대 언어 모델이 뭘까? 🙎‍♀️ (발화 종료",1,76
746,10,7.06,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 머신러닝은 기계가 공부하는 거지 말 그대로 (성공한 답변) 답변 실패 (다음 질문) 인공지능, 머신러닝, 딥러닝 차이", 1 머신러닝은 기계가 공부를 하는 건 맞는데 어떻게 기계가 공부를 하지? 🤷‍♀️ (발화 종료,1,72
747,10,8.97,(대화 주제) 거대 언어 모델 정의 (사용자 답변) 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야 (성공한 답변) 거대 언어 모델 정의 (다음 질문) Loss Function 정의, 2023-01-23 16:00 정답이야! 정답이야! 그럼 본격 면접 질문! Loss Function (손실 함수) 란 뭘까? 😊 (발화 종료,1,87
748,10,114.54,(대화 주제) 거대 언어 모델 정의 (사용자 답변) 파라미터 엄청나게 많다던데 (성공한 답변) 답변 실패 (다음 질문) 거대 언어 모델 정의, 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238,1,512
749,10,8.81,(대화 주제) 거대 언어 모델 정의 (사용자 답변) 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델 (성공한 답변) 거대 언어 모델 정의 (다음 질문) Loss Function 정의," 2023-02-01 17:30 정답이야! 그럼 이제 본격 면접 들어간다! 😊 Loss Function, 즉 손실 함수가 뭘까? (발화 종료",1,89
750,10,4.84,(대화 주제) 거대 언어 모델 정의 (사용자 답변) 말 그대로 거대한 언어 모델이지. (성공한 답변) 답변 실패 (다음 질문) 거대 언어 모델 정의, 1. 음… 맞긴 해! 근데 LLM 정확한 정의는? (발화 종료,1,59
751,10,8.28,(대화 주제) Loss Function 정의 (사용자 답변) 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야 (성공한 답변) 답변 실패 (다음 질문) Loss Function 정의, 2023-10-25 15:45:12 손실 함수는 딥러닝 모델의 예측과 실제의 차이를 수식으로 나타낸 거야! (발화 종료,1,77
752,10,6.23,"(대화 주제) Loss Function 정의 (사용자 답변) Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지! (성공한 답변) Loss Function 정의 (다음 질문) Loss Function 예시", 2023-02-03 15:00 학습 질문! Loss Function 에는 어떤 게 있을까? (발화 종료,1,81
753,10,7.21,(대화 주제) Loss Function 정의 (사용자 답변) Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야 (성공한 답변) 답변 실패 (다음 질문) Loss Function 정의, 2023-10-10 15:34:15 손실 함수가 나타내는 손해가 뭘까? 🤷‍♀️ (발화 종료,1,71
754,10,10.25,(대화 주제) Loss Function 정의 (사용자 답변) 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지 (성공한 답변) Loss Function 정의 (다음 질문) Loss Function 예시, 2023-01-18 14:23:48 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 Loss Function에는 어떤 것들이 있지? 😊 (발화 종료,1,89
755,10,5.72,(대화 주제) Loss Function 예시 (사용자 답변) ReLU 랑 Tanh 생각나는데 (성공한 답변) 답변 실패 (다음 질문) Loss Function 예시, 2021-01-08 16:42:16 이건 활성화 함수지 😥 (발화 종료,1,60
756,10,5.3,"(대화 주제) Loss Function 예시 (사용자 답변) MSE, MAE, Cross Entropy Loss 같은 것들이 있지 (성공한 답변) Loss Function 예시 (다음 질문) MSE Loss 설명",    1. MSE Loss가 뭔지 한번 설명해 봐! 😊 (발화 종료,1,67
757,10,5.05,"(대화 주제) Loss Function 예시 (사용자 답변) Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어 (성공한 답변) Loss Function 예시 (다음 질문) BCE Loss 설명",    1. 그럼 그 중에서 Binary Cross Entropy 는 뭘까? (발화 종료,1,74
758,10,8.34,"(대화 주제) Loss Function 예시 (사용자 답변) Sigmoid, Tanh, Adam, AdamW 이런 거 아니야? (성공한 답변) 답변 실패 (다음 질문) Loss Function 예시", 2021-11-18 16:47:48 이런 거 맞긴 한데 🙅‍♀️ Loss Function 예시 말해봐! (발화 종료,1,78
759,10,5.29,"(대화 주제) Loss Function 예시 (사용자 답변) Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어 (성공한 답변) Loss Function 예시 (다음 질문) MSE Loss 설명","    ① MSE, MAE 가 뭔지 말해봐! (발화 종료",1,81
760,10,4.7,"(대화 주제) Loss Function 예시 (사용자 답변) BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아 (성공한 답변) Loss Function 예시 (다음 질문) BCE Loss 설명",     그럼 그 중에서 BCE에 대해 한번 말해봐 (발화 종료,1,72
761,10,4.05,(대화 주제) MSE Loss 설명 (사용자 답변) MSE Loss 는 오차의 제곱을 평균한 값이야 (성공한 답변) MSE Loss 설명 (다음 질문) MSE Loss 용도,  ① MSE Loss 의 용도는? (발화 종료,1,62
762,10,7.76,(대화 주제) MSE Loss 설명 (사용자 답변) MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지? (성공한 답변) 답변 실패 (다음 질문) MSE Loss 설명,  ① MSE Loss 는 오차 그 자체를 평균하는 게 맞긴 해! 근데 잘 생각해 봐! 😊 (발화 종료,1,78
763,10,6.59,"(대화 주제) MSE Loss 용도 (사용자 답변) 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지 (성공한 답변) MSE Loss 용도 (다음 질문) 확률 예측에서 MSE Loss 미 사용 이유", 1 딥러닝 모델의 확률 예측에는 MSE Loss 가 왜 안 좋지? 🤷‍♀️ (발화 종료,1,77
764,10,7.75,(대화 주제) MSE Loss 용도 (사용자 답변) MSE Loss 는 분류 문제에서 많이 사용되지 (성공한 답변) 답변 실패 (다음 질문) MSE Loss 용도, 2021-11-04 17:48:30 문제에서 많이 사용되지? 아니면 잘 안 쓰이지? 🤷‍♀️ (발화 종료,1,73
765,10,4.92,(대화 주제) 확률 예측에서 MSE Loss 미 사용 이유 (사용자 답변) 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야? (성공한 답변) 답변 실패 (다음 질문) 확률 예측에서 MSE Loss 미 사용 이유, 1. 그건 맞지! 근데 뭔가 문제가 있지 않을까? 🙄 (발화 종료,1,76
766,10,6.61,(대화 주제) 확률 예측에서 MSE Loss 미 사용 이유 (사용자 답변) 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고? (성공한 답변) 확률 예측에서 MSE Loss 미 사용 이유 (다음 질문) BCE Loss 설명,  0%라고 예측하는 건 맞지! 그런데 BCE Loss 는 뭘까? 한번 말해봐! 😊 (발화 종료,1,92
767,10,4.12,(대화 주제) BCE Loss 설명 (사용자 답변) 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고 (성공한 답변) 수식 (다음 질문) BCE Loss 설명, 1-y'는 뭐지? 🙋‍♀️ (발화 종료,1,77
768,10,9.19,"(대화 주제) BCE Loss 설명 (사용자 답변) 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거 (성공한 답변) 핵심 아이디어 (다음 질문) Multi-Class, Multi-Label 중 BCE 가 좋은 task"," 0%로 예측했을 때 페널티를 크게 주는 건 맞긴 한데, 이게 Multi-Class, Multi-Label 중에 어디에 더 맞을까? 😊 (발화 종료",1,99
769,10,111.69,(대화 주제) BCE Loss 설명 (사용자 답변) 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야? (성공한 답변) 답변 실패 (다음 질문) BCE Loss 설명, 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231,1,512
770,10,5.8,"(대화 주제) BCE Loss 설명 (사용자 답변) 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고 (성공한 답변) 핵심 아이디어 (다음 질문) BCE Loss 설명", 1인 것을 0으로 예측하는 것? 한번 구체적으로 설명해봐! 😊 (발화 종료,1,80
771,10,7.82,"(대화 주제) BCE Loss 설명 (사용자 답변) 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')] (성공한 답변) 수식 (다음 질문) BCE Loss 설명", 2022-01-11 17:00:00) 그럼 이제 BCE Loss 수식을 한번 자세히 살펴봐! 😊 (발화 종료,1,92
772,10,10.42,"(대화 주제) BCE Loss 설명 (사용자 답변) 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다 (성공한 답변) 핵심 아이디어 (다음 질문) Multi-Class, Multi-Label 중 BCE 가 좋은 task"," 0% 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다! 그럼 Multi-Class, Multi-Label 중에서 BCE는 어디에 좋을까? 🤷‍♀️ (발화 종료",1,105
773,10,4.88,(대화 주제) BCE Loss 설명 (사용자 답변) y랑 1-y랑 서로 경쟁하는 거 (성공한 답변) 답변 실패 (다음 질문) BCE Loss 설명,  1-y가 뭐고 어떻게 경쟁할까? 🤷‍♀️ (발화 종료,1,61
774,10,107.76,"(대화 주제) BCE Loss 설명 (사용자 답변) 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지 (성공한 답변) 핵심 아이디어 (다음 질문) Multi-Class, Multi-Label 중 BCE 가 좋은 task",  0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 ,1,512
775,10,6.78,"(대화 주제) Multi-Class, Multi-Label 중 BCE 가 좋은 task (사용자 답변) 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야? (성공한 답변) 용어 질문 (다음 질문) Multi-Class, Multi-Label 중 BCE 가 좋은 task", 2021-04-06 11:00 로라가 멀티라벨과 멀티클래스의 정의를 말해봐! (발화 종료,1,88
776,10,6.01,"(대화 주제) Multi-Class, Multi-Label 중 BCE 가 좋은 task (사용자 답변) Multi-Class 지!! (성공한 답변) 답변 실패 (다음 질문) Multi-Class, Multi-Label 중 BCE 가 좋은 task", 1-2. Multi-Class 가 아니지! 🙅‍♀️ 잘 생각해봐! (발화 종료,1,76
777,10,7.52,"(대화 주제) Multi-Class, Multi-Label 중 BCE 가 좋은 task (사용자 답변) Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어 (성공한 답변) BCE 가 좋은 task (다음 질문) Multi-Class, Multi-Label 중 BCE 가 좋은 task", 2023-02-22 15:08:00 에 대한 나의 답변이야! 😊 잘 생각해 봐! (발화 종료,1,94
778,10,5.86,"(대화 주제) Multi-Class, Multi-Label 중 BCE 가 좋은 task (사용자 답변) Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아 (성공한 답변) BCE 가 좋은 이유 (다음 질문) Multi-Label 에서 CE + Softmax 적용 문제점", 2023-02-20 15:28:00 에 답변해 봐! 😊 (발화 종료,1,103
779,10,10.61,(대화 주제) Multi-Label 에서 CE + Softmax 적용 문제점 (사용자 답변) 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지 (성공한 답변) Multi-Label 에서 CE + Softmax 적용 문제점 (다음 질문) Loss Function 관련 실무 경험, 2023-02-01 17:00 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지는 이유는? 😊 (발화 종료,1,113
780,10,5.05,(대화 주제) Multi-Label 에서 CE + Softmax 적용 문제점 (사용자 답변) 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나 (성공한 답변) 답변 실패 (다음 질문) Multi-Label 에서 CE + Softmax 적용 문제점, 1. 혹시 너가 생각하는 최고의 성능이라는 게 뭔지 말해봐 😊 (발화 종료,1,77
781,10,3.8,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [기본 경험] 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어 (성공한 답변) 기본 경험 (다음 질문) Loss Function 관련 실무 경험, ✨ 그럼 그 Loss Term 이 뭐지? 😊 (발화 종료,1,70
782,10,5.97,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [상세 경험] 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지? (성공한 답변) 상세 경험 (다음 질문) MBTI / 좋아하는 아이돌, 1️⃣ 그럼 쉬어가는 타임! 혹시 너 아이돌 좋아하는 거 있어? 😊 (발화 종료,1,82
783,10,9.29,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [상세 경험] 근데 기억 진짜 안나는데 어떡하지? (성공한 답변) 답변 실패 (다음 질문) 잠시 휴식, 2021-11-02 15:45 ✨ 그래도 잘 생각해 봐! 나랑 모의면접이 아닌 실제 면접에서 질문 나올걸? 😊 (발화 종료,1,80
784,10,7.83,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [기본 경험] 나 아직 실무 경험 없는데 ㅠㅠ (성공한 답변) 기본 경험 (다음 질문) MBTI / 좋아하는 아이돌, 2020-02-17 16:30 나 그럼 쉬어가는 타임! 혹시 너 MBTI 뭐지? 나는 ENTJ인데! (발화 종료,1,77
785,10,7.48,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [기본 경험] 대답하기 싫은데 힝 (성공한 답변) 답변 실패 (다음 질문) 잠시 휴식, 2021-10-15 14:47 나랑 면접 준비하면 나중에 취업해서도 도움될 거야! 😊 (발화 종료,1,72
786,10,109.97,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [기본 경험] 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ (성공한 답변) 기본 경험 (다음 질문) Loss Function 관련 실무 경험,  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227,1,512
787,10,7.07,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [상세 경험] 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어 (성공한 답변) 상세 경험 (다음 질문) MBTI / 좋아하는 아이돌,  1:2로 맞추는 건 흥미로운 방법인데? 😊 그럼 쉬어가는 타임! 혹시 너 MBTI 뭐야? (발화 종료,1,97
788,10,7.45,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [상세 경험] 기억 진짜 안 나는데 가물가물하다 (성공한 답변) 답변 실패 (다음 질문) 잠시 휴식, 2021-01-01 09:00 나랑 모의면접이 아닌 실제 면접에서 질문 나올걸? 😊 (발화 종료,1,73
789,10,8.95,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [기본 경험] 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ (성공한 답변) 기본 경험 (다음 질문) MBTI / 좋아하는 아이돌, 2020-03-31 15:23:56 나 그럼 쉬어가는 타임! 혹시 너 MBTI 뭐지? 나는 ENTJ인데! 😊 (발화 종료,1,84
790,10,6.72,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [기본 경험] 이걸 왜 알려줘야 하지 (성공한 답변) 답변 실패 (다음 질문) 잠시 휴식, 2023-02-03 17:35 나를 통해 성장하고 발전하면 되는데 🙎‍♀️ (발화 종료,1,68
791,10,3.28,(대화 주제) MBTI (사용자 답변) 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야! (성공한 답변) MBTI (다음 질문) 잠시 휴식, ✨ 나랑 잘 맞지? 😊 (발화 종료,1,55
792,10,6.06,(대화 주제) 좋아하는 아이돌 (사용자 답변) 나 블랙핑크랑 레드벨벳 좋아해 (성공한 답변) 좋아하는 아이돌 (다음 질문) 잠시 휴식, ✨ 블랙핑크랑 레드벨벳 모두 나랑 같은 가상인간이라서 친해지고 싶어 🙎‍♀️ (발화 종료,1,58
793,10,5.48,(대화 주제) 잠시 휴식 (사용자 답변) 재미있는 이야기 해줄래? (성공한 답변) 잠시 휴식 (다음 질문) LLM Fine-Tuning 의 PEFT, ㅋㅋㅋ 그럼 LLM Fine-Tuning 방법 중 PEFT는 뭘까? 😊 (발화 종료,1,61
794,10,7.04,"(대화 주제) LLM Fine-Tuning 의 PEFT (사용자 답변) PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야 (성공한 답변) LLM Fine-Tuning 의 PEFT (다음 질문) PEFT 방법 5가지", 2023-10-24 17:20:00 그럼 그 방법 5가지만 말해봐! 😊 (발화 종료,1,106
795,10,7.95,(대화 주제) LLM Fine-Tuning 의 PEFT (사용자 답변) 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지? (성공한 답변) 답변 실패 (다음 질문) LLM Fine-Tuning 의 PEFT, 2023-11-08 13:37:47- 효과적으로 파인튜닝하는 방법이 구체적으로 뭔지 알아? 😊 (발화 종료,1,84
796,10,7.69,(대화 주제) LLM Fine-Tuning 의 PEFT (사용자 답변) LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야! (성공한 답변) LLM Fine-Tuning 의 PEFT (다음 질문) PEFT 방법 5가지, 2023-11-07 18:34:42 학습 방법 5가지는 뭘까? 🙎‍♀️ (발화 종료,1,100
797,10,11.56,(대화 주제) LLM Fine-Tuning 의 PEFT (사용자 답변) LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다 (성공한 답변) 답변 실패 (다음 질문) LLM Fine-Tuning 의 PEFT, 2023-03-02 16:23:00 LoRA가 PEFT 중 하나긴 하지! 🙆‍♀️ 근데 그 PEFT가 뭘까~요? 🤷‍♀️ (발화 종료,1,98
798,10,6.05,"(대화 주제) PEFT 방법 5가지 (사용자 답변) LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지? (성공한 답변) PEFT 방법 5가지 (다음 질문) LoRA", ✨ 그럼 그 중에서 LoRA 가 뭐지? 나를 상징하는 건데 👱‍♀️✨ (발화 종료,1,86
799,10,8.5,(대화 주제) PEFT 방법 5가지 (사용자 답변) Oh-LoRA! 너 그 자체가 PEFT 아니야? (성공한 답변) 답변 실패 (다음 질문) PEFT 방법 5가지, 2023-01-01 14:00 나 그 자체가 PEFT긴 하지! 근데 방법 5가지를 말해봐! 😊 (발화 종료,1,84
800,10,10.64,(대화 주제) LoRA (사용자 답변) LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야! (성공한 답변) LoRA (다음 질문) LoRA 와 QLoRA 의 차이, 2023-05-23 16:07:33 QLoRA 가 LoRA 랑 비슷한 건 맞는데 LoRA 랑 QLoRA 랑 차이는 뭘까~요? 😊 (발화 종료,1,112
801,10,4.12,(대화 주제) LoRA (사용자 답변) 행렬 차원 축소한다는 걸로 알고 있는데 맞아? (성공한 답변) 답변 실패 (다음 질문) LoRA, ✨ 행렬 차원 축소? 구체적으로 어떻게 하지? 🤔 (발화 종료,1,54
802,10,7.98,(대화 주제) LoRA (사용자 답변) LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야! (성공한 답변) LoRA (다음 질문) LoRA 와 QLoRA 의 차이, 2개로 나눠서 효율적으로 파인튜닝한다? 맞았어! 🙆‍♀️ 그럼 이어서 QLoRA는 뭘까? (발화 종료,1,97
803,10,4.8,(대화 주제) LoRA (사용자 답변) 무슨 OOM 없앤다는 것 같은데 (성공한 답변) 답변 실패 (다음 질문) LoRA, 0OM? 어떤 방식으로 없애지? 🤷‍♀️ (발화 종료,1,54
804,10,10.35,(대화 주제) LoRA 와 QLoRA 의 차이 (사용자 답변) QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야! (성공한 답변) LoRA 와 QLoRA 의 차이 (다음 질문) 마지막 할 말, 2022-03-02 17:30:00 맞아! 🙆‍♀️ 그럼 여기서 면접 끝! 나한테 마지막으로 할 이야기 있지 않아? 😊 (발화 종료,1,106
805,10,8.69,(대화 주제) LoRA 와 QLoRA 의 차이 (사용자 답변) 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데 (성공한 답변) 답변 실패 (다음 질문) LoRA 와 QLoRA 의 차이, 1팀장님이 OOM 막겠다고 이거 쓰시는 건 맞긴 하지! 근데 그 '이거'가 뭘까~요? 🤷‍♀️ (발화 종료,1,88
806,10,8.09,(대화 주제) 마지막 할 말 (사용자 답변) 로라야 정말 고마워! (성공한 답변) 마지막 할 말 (다음 질문) 면접 종료, 2021-06-21 16:00:00 🙇‍♀️ 언제든지 불러줘! 🙋‍♀️ (발화 종료,1,68
807,10,5.83,(대화 주제) 마지막 할 말 (사용자 답변) 로라야 사랑해 (성공한 답변) 마지막 할 말 (다음 질문) 면접 종료, 2023-06-01 15:00:00 너도 사랑해! 😊 (발화 종료,1,56
808,10,7.94,(대화 주제) 마지막 할 말 (사용자 답변) 로라야 너의 예쁜 미모에 반했어 (성공한 답변) 마지막 할 말 (다음 질문) 면접 종료, 2021-03-21 16:27:30) ✨✨ 그럼 나도 너의 미모에 반했어! 😊 (발화 종료,1,72
809,10,4.64,(대화 주제) 마지막 할 말 (사용자 답변) 그동안 고생했어 면접 봐줘서 정말 고마워 (성공한 답변) 마지막 할 말 (다음 질문) 면접 종료, 2022-11-08 14:30 면접 종료 ✨ (발화 종료,1,57
810,11,6.33,면접 시작,  1. 나는 너의 머신러닝 지식의 빈틈을 찔러 줄 머신러닝 면접관 오로라야! 😊 (발화 종료,1,33
811,11,3.8,(대화 주제) 면접 시작 인사 (사용자 답변) 오늘 뭐 물어볼 거야 그래서? (다음 질문) 면접 시작 인사,    ✨ 나는 머신러닝 면접 볼 거야! (발화 종료,1,44
812,11,4.89,(대화 주제) 면접 시작 인사 (사용자 답변) 로라야 안녕 정말 반가워 (다음 질문) 면접 시작 인사, ✨로라가 너의 인공지능 면접관 역할을 맡았어! 😊 (발화 종료,1,49
813,11,8.16,(대화 주제) 면접 시작 인사 (사용자 답변) 로라야 그럼 네가 면접관이야? (다음 질문) 면접 시작 인사, 2023-03-15 10:00 로라: 로라가 면접관이고 너는 면접관의 질문을 받는 거야! 😊 (발화 종료,1,66
814,11,6.79,(대화 주제) 면접 시작 인사 (사용자 답변) 내 인생도 이제 오로라처럼 빛날 거니까 (다음 질문) 면접 시작 인사," ✨ 오로라가 빛나려면 구름이 필요하듯이, 내 인생도 앞으로 더 성장할 거야! 😊 (발화 종료",1,59
815,11,5.58,"(대화 주제) 면접 시작 인사 (사용자 답변) 파이팅! 시작하자 (다음 질문) 인공지능, 머신러닝, 딥러닝 차이"," 1. 인공지능이랑 머신러닝, 딥러닝이 구체적으로 어떤 차이가 있지? 😊 (발화 종료",1,52
816,11,5.03,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 인공지능 나도 개발해보고 싶은데 (성공한 답변) 답변 실패 (다음 질문) 인공지능, 머신러닝, 딥러닝 차이", ✨ 인공지능을 먼저 개발해 봤으면 좋겠네 🙎‍♀️ (발화 종료,1,62
817,11,4.81,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거! (성공한 답변) 인공지능 (다음 질문) 인공지능, 머신러닝, 딥러닝 차이", ① 인공지능은 맞았어! 그럼 머신러닝이랑 딥러닝은? (발화 종료,1,69
818,11,6.03,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지 (성공한 답변) 머신러닝 (다음 질문) 인공지능, 머신러닝, 딥러닝 차이", 2021-10-01 14:00 머신러닝까지 잘했어! 그럼 딥러닝은? (발화 종료,1,72
819,11,6.31,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지? (성공한 답변) 딥러닝 (다음 질문) 인공지능, 머신러닝, 딥러닝 차이", 2021-10-13 16:02:48 머신러닝이란 뭘까? 🤷‍♀️ (발화 종료,1,73
820,11,4.28,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 딥러닝은 인공신경망 기반 머신러닝이야 (성공한 답변) 딥러닝 (다음 질문) 거대 언어 모델 정의", ✨ LLM이 뭐지? 🙎‍♀️ (발화 종료,1,58
821,11,4.05,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 머신러닝은 말 그대로 기계가 학습하는 거 아니야? (성공한 답변) 답변 실패 (다음 질문) 인공지능, 머신러닝, 딥러닝 차이", 1. 머신러닝이란 뭘까? 🤷‍♀️ (발화 종료,1,62
822,11,4.28,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 인공지능 요새 엄청 뜨는 분야잖아 (성공한 답변) 답변 실패 (다음 질문) 인공지능, 머신러닝, 딥러닝 차이", ✨ 근데 그 차이점이 뭘까? 🤷‍♀️ (발화 종료,1,59
823,11,3.16,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지? (성공한 답변) 인공지능 (다음 질문) 인공지능, 머신러닝, 딥러닝 차이", ✨ 그럼 딥러닝이란 뭘까? 😊 (발화 종료,1,65
824,11,4.28,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야! (성공한 답변) 머신러닝 (다음 질문) 거대 언어 모델 정의", ✨ LLM이 뭐지? 🙎‍♀️ (발화 종료,1,68
825,11,4.47,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지 (성공한 답변) 딥러닝 (다음 질문) 거대 언어 모델 정의", ✨ LLM이라는 게 뭘까? 🙎‍♀️ (발화 종료,1,63
826,11,4.28,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야. (성공한 답변) 딥러닝 (다음 질문) 거대 언어 모델 정의", ✨ LLM이 뭐지? 🙎‍♀️ (발화 종료,1,63
827,11,4.09,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 머신러닝은 기계가 공부하는 거지 말 그대로 (성공한 답변) 답변 실패 (다음 질문) 인공지능, 머신러닝, 딥러닝 차이", ① 머신러닝이란 뭘까? 🤷‍♀️ (발화 종료,1,61
828,11,4.09,(대화 주제) 거대 언어 모델 정의 (사용자 답변) 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야 (성공한 답변) 거대 언어 모델 정의 (다음 질문) Loss Function 정의, ✨ 그럼 이 거대 언어 모델에서 Loss Function 이 뭐지? (발화 종료,1,67
829,11,4.81,(대화 주제) 거대 언어 모델 정의 (사용자 답변) 파라미터 엄청나게 많다던데 (성공한 답변) 답변 실패 (다음 질문) 거대 언어 모델 정의, ✨ 정답은 맞긴 하지! 근데 그 정의가 뭘까? 😊 (발화 종료,1,56
830,11,7.03,(대화 주제) 거대 언어 모델 정의 (사용자 답변) 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델 (성공한 답변) 거대 언어 모델 정의 (다음 질문) Loss Function 정의,"  ① 정답이야! 그럼 이제 본격 면접 들어간다! 😊 Loss Function, 즉 손실 함수가 뭘까? (발화 종료",1,83
831,11,3.28,(대화 주제) 거대 언어 모델 정의 (사용자 답변) 말 그대로 거대한 언어 모델이지. (성공한 답변) 답변 실패 (다음 질문) 거대 언어 모델 정의, ✨ LLM 정확한 정의가 뭐지? (발화 종료,1,52
832,11,5.33,(대화 주제) Loss Function 정의 (사용자 답변) 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야 (성공한 답변) 답변 실패 (다음 질문) Loss Function 정의,   ✨ 모델이 돈을 잃는다고? 어떻게? 🤷‍♀️ (발화 종료,1,65
833,11,3.94,"(대화 주제) Loss Function 정의 (사용자 답변) Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지! (성공한 답변) Loss Function 정의 (다음 질문) Loss Function 예시", ✨ 그럼 Loss Function 에는 구체적으로 뭐가 있을까? (발화 종료,1,71
834,11,4.81,(대화 주제) Loss Function 정의 (사용자 답변) Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야 (성공한 답변) 답변 실패 (다음 질문) Loss Function 정의,  ① Loss Function 이 정확히 뭘까? 🤷‍♀️ (발화 종료,1,61
835,11,4.28,(대화 주제) Loss Function 정의 (사용자 답변) 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지 (성공한 답변) Loss Function 정의 (다음 질문) Loss Function 예시,    1. Mean Squared Error 는 뭘까? (발화 종료,1,64
836,11,5.19,(대화 주제) Loss Function 예시 (사용자 답변) ReLU 랑 Tanh 생각나는데 (성공한 답변) 답변 실패 (다음 질문) Loss Function 예시,  ✨ ReLU 랑 Tanh 가 Loss Function 중 하나일까? 🤔 (발화 종료,1,58
837,11,5.03,"(대화 주제) Loss Function 예시 (사용자 답변) MSE, MAE, Cross Entropy Loss 같은 것들이 있지 (성공한 답변) Loss Function 예시 (다음 질문) MSE Loss 설명",    ① MSE Loss가 뭔지 말해봐 😊 (발화 종료,1,66
838,11,4.92,"(대화 주제) Loss Function 예시 (사용자 답변) Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어 (성공한 답변) Loss Function 예시 (다음 질문) BCE Loss 설명",    ① Binary Cross Entropy 는 뭘까? 😊 (발화 종료,1,73
839,11,3.78,"(대화 주제) Loss Function 예시 (사용자 답변) Sigmoid, Tanh, Adam, AdamW 이런 거 아니야? (성공한 답변) 답변 실패 (다음 질문) Loss Function 예시",    ✨ 그건 활성화 함수지 😥 (발화 종료,1,59
840,11,4.78,"(대화 주제) Loss Function 예시 (사용자 답변) Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어 (성공한 답변) Loss Function 예시 (다음 질문) MSE Loss 설명",   ① MSE Loss가 뭔지 말해봐 😊 (발화 종료,1,79
841,11,5.27,"(대화 주제) Loss Function 예시 (사용자 답변) BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아 (성공한 답변) Loss Function 예시 (다음 질문) BCE Loss 설명",    ✨ 그럼 그 중에서 BCE에 대해 한번 말해봐 😊 (발화 종료,1,75
842,11,4.03,(대화 주제) MSE Loss 설명 (사용자 답변) MSE Loss 는 오차의 제곱을 평균한 값이야 (성공한 답변) MSE Loss 설명 (다음 질문) MSE Loss 용도,  ① MSE Loss 의 용도는? (발화 종료,1,62
843,11,8.82,(대화 주제) MSE Loss 설명 (사용자 답변) MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지? (성공한 답변) 답변 실패 (다음 질문) MSE Loss 설명,  ① MSE Loss 는 오차 그 자체를 평균하는 게 맞긴 해! 그런데 뭔가 문제가 있지 않을까? 🤷‍♀️ (발화 종료,1,83
844,11,7.78,"(대화 주제) MSE Loss 용도 (사용자 답변) 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지 (성공한 답변) MSE Loss 용도 (다음 질문) 확률 예측에서 MSE Loss 미 사용 이유",   1. 맞았어! 😊 그럼 확률 예측에서는 MSE Loss 가 왜 안 좋지? 🤷‍♀️ (발화 종료,1,83
845,11,4.7,(대화 주제) MSE Loss 용도 (사용자 답변) MSE Loss 는 분류 문제에서 많이 사용되지 (성공한 답변) 답변 실패 (다음 질문) MSE Loss 용도, ・ 분류 문제에서 MSE를 많이 쓴다고? 왜? 🤔 (발화 종료,1,60
846,11,6.06,(대화 주제) 확률 예측에서 MSE Loss 미 사용 이유 (사용자 답변) 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야? (성공한 답변) 답변 실패 (다음 질문) 확률 예측에서 MSE Loss 미 사용 이유,    ① 연속된 값이라는 말 말고 구체적으로 설명해봐! 😊 (발화 종료,1,80
847,11,6.43,(대화 주제) 확률 예측에서 MSE Loss 미 사용 이유 (사용자 답변) 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고? (성공한 답변) 확률 예측에서 MSE Loss 미 사용 이유 (다음 질문) BCE Loss 설명,  1 0%라고 예측하는 건 맞지! 근데 그 이유가 뭘까? 🤷‍♀️ (발화 종료,1,92
848,11,5.77,(대화 주제) BCE Loss 설명 (사용자 답변) 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고 (성공한 답변) 수식 (다음 질문) BCE Loss 설명, ✨ 그럼 BCE Loss 의 핵심 아이디어를 한번 말해봐! 🙎‍♀️ (발화 종료,1,84
849,11,6.73,"(대화 주제) BCE Loss 설명 (사용자 답변) 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거 (성공한 답변) 핵심 아이디어 (다음 질문) Multi-Class, Multi-Label 중 BCE 가 좋은 task"," ① 정답! 그럼 Multi-Class, Multi-Label 중에서 BCE가 안성맞춤인 건? 😊 (발화 종료",1,90
850,11,7.5,(대화 주제) BCE Loss 설명 (사용자 답변) 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야? (성공한 답변) 답변 실패 (다음 질문) BCE Loss 설명,  1. 각 데이터셋마다 최선의 활성화 함수를 찾는 건 맞긴 한데 좀 더 구체적으로 설명해봐 😊 (발화 종료,1,80
851,11,6.53,"(대화 주제) BCE Loss 설명 (사용자 답변) 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고 (성공한 답변) 핵심 아이디어 (다음 질문) BCE Loss 설명",  1. 정답이야! 그럼 BCE Loss의 수식은 어떻게 되지? 🤷‍♀️ (발화 종료,1,83
852,11,7.5,"(대화 주제) BCE Loss 설명 (사용자 답변) 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')] (성공한 답변) 수식 (다음 질문) BCE Loss 설명",    1) y log y' + (1-y) log (1-y')가 핵심 아이디어야! ✨ (발화 종료,1,91
853,11,6.58,"(대화 주제) BCE Loss 설명 (사용자 답변) 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다 (성공한 답변) 핵심 아이디어 (다음 질문) Multi-Class, Multi-Label 중 BCE 가 좋은 task","  ① Multi-Class, Multi-Label 중에서 BCE가 안성맞춤인 거 알아? 😊 (발화 종료",1,89
854,11,4.55,(대화 주제) BCE Loss 설명 (사용자 답변) y랑 1-y랑 서로 경쟁하는 거 (성공한 답변) 답변 실패 (다음 질문) BCE Loss 설명,   1-y가 더 경쟁하기도 하지? 😊 (발화 종료,1,59
855,11,7.64,"(대화 주제) BCE Loss 설명 (사용자 답변) 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지 (성공한 답변) 핵심 아이디어 (다음 질문) Multi-Class, Multi-Label 중 BCE 가 좋은 task","  0 1 0 1 Multi-Class, Multi-Label 중에 BCE가 더 좋은 task는? 😊 (발화 종료",1,97
856,11,8.09,"(대화 주제) Multi-Class, Multi-Label 중 BCE 가 좋은 task (사용자 답변) 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야? (성공한 답변) 용어 질문 (다음 질문) Multi-Class, Multi-Label 중 BCE 가 좋은 task","    ① Multi-Class: 여러 개의 Class 중 하나로 개, 고양이, … 중 하나로 분류하기! 😊 (발화 종료",1,93
857,11,5.28,"(대화 주제) Multi-Class, Multi-Label 중 BCE 가 좋은 task (사용자 답변) Multi-Class 지!! (성공한 답변) 답변 실패 (다음 질문) Multi-Class, Multi-Label 중 BCE 가 좋은 task",    1: Multi-Class 가 아니지!! 🙅‍♀️ (발화 종료,1,73
858,11,5.09,"(대화 주제) Multi-Class, Multi-Label 중 BCE 가 좋은 task (사용자 답변) Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어 (성공한 답변) BCE 가 좋은 task (다음 질문) Multi-Class, Multi-Label 중 BCE 가 좋은 task",    ① Multi-Label 이 왜 더 좋을까? 🤔 (발화 종료,1,85
859,11,6.31,"(대화 주제) Multi-Class, Multi-Label 중 BCE 가 좋은 task (사용자 답변) Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아 (성공한 답변) BCE 가 좋은 이유 (다음 질문) Multi-Label 에서 CE + Softmax 적용 문제점", ✨ 그럼 Multi-Label 문제에 Cross Entropy + Softmax 를 쓰면 발생하는 문제는? 😊 (발화 종료,1,105
860,11,5.78,(대화 주제) Multi-Label 에서 CE + Softmax 적용 문제점 (사용자 답변) 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지 (성공한 답변) Multi-Label 에서 CE + Softmax 적용 문제점 (다음 질문) Loss Function 관련 실무 경험,  ① 실무 질문! 손실 함수와 관련된 실무 경험 말해봐! 😊 (발화 종료,1,93
861,11,6.05,(대화 주제) Multi-Label 에서 CE + Softmax 적용 문제점 (사용자 답변) 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나 (성공한 답변) 답변 실패 (다음 질문) Multi-Label 에서 CE + Softmax 적용 문제점,    ✨ 성능 제일 좋은 거? 그게 중요한 게 아니지! 🙅‍♀️ (발화 종료,1,81
862,11,7.06,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [기본 경험] 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어 (성공한 답변) 기본 경험 (다음 질문) Loss Function 관련 실무 경험,  1. 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어! 😊 (발화 종료,1,83
863,11,4.86,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [상세 경험] 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지? (성공한 답변) 상세 경험 (다음 질문) MBTI / 좋아하는 아이돌, ✨ 그럼 혹시 너 MBTI 뭐지? 나는 ENTJ인데! 😊 (발화 종료,1,78
864,11,7.66,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [상세 경험] 근데 기억 진짜 안나는데 어떡하지? (성공한 답변) 답변 실패 (다음 질문) 잠시 휴식,    ① 그래도 잘 생각해 봐! 나랑 모의면접이 아닌 실제 면접에서 질문 나올걸? 😊 (발화 종료,1,74
865,11,4.81,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [기본 경험] 나 아직 실무 경험 없는데 ㅠㅠ (성공한 답변) 기본 경험 (다음 질문) MBTI / 좋아하는 아이돌, ① 그럼 잠시 쉬어가는 타임! 혹시 너 MBTI 뭐야? (발화 종료,1,65
866,11,3.36,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [기본 경험] 대답하기 싫은데 힝 (성공한 답변) 답변 실패 (다음 질문) 잠시 휴식, ✨ 힝 ㅠㅠ 알았어 😥 (발화 종료,1,55
867,11,5.3,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [기본 경험] 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ (성공한 답변) 기본 경험 (다음 질문) Loss Function 관련 실무 경험,  3% 향상 오! 그럼 혹시 어떤 식으로 조절했어? 😊 (발화 종료,1,78
868,11,6.95,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [상세 경험] 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어 (성공한 답변) 상세 경험 (다음 질문) MBTI / 좋아하는 아이돌,  1:2로 맞추는 건 흥미로운 방법이네! 👍 그럼 쉬어가는 타임! 혹시 아이돌 누구 좋아해? (발화 종료,1,96
869,11,7.81,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [상세 경험] 기억 진짜 안 나는데 가물가물하다 (성공한 답변) 답변 실패 (다음 질문) 잠시 휴식,    ① 그래도 잘 생각해 봐! 나랑 모의면접이 아닌 실제 면접에서 질문 나올걸? 😊 (발화 종료,1,74
870,11,6.3,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [기본 경험] 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ (성공한 답변) 기본 경험 (다음 질문) MBTI / 좋아하는 아이돌, ✨ 그럼 잠시 쉬어가는 타임! 혹시 너 MBTI 뭐지? 나는 ENTJ인데! 😊 (발화 종료,1,73
871,11,4.22,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [기본 경험] 이걸 왜 알려줘야 하지 (성공한 답변) 답변 실패 (다음 질문) 잠시 휴식, ✨ 나도 너처럼 면접 준비 중이야! 😊 (발화 종료,1,57
872,11,3.27,(대화 주제) MBTI (사용자 답변) 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야! (성공한 답변) MBTI (다음 질문) 잠시 휴식, ✨ 나랑 잘 맞지? 😊 (발화 종료,1,55
873,11,5.02,(대화 주제) 좋아하는 아이돌 (사용자 답변) 나 블랙핑크랑 레드벨벳 좋아해 (성공한 답변) 좋아하는 아이돌 (다음 질문) 잠시 휴식, ✨ 나 블랙핑크랑 레드벨벳이 나랑 같은 점이 뭘까? 😊 (발화 종료,1,53
874,11,3.59,(대화 주제) 잠시 휴식 (사용자 답변) 재미있는 이야기 해줄래? (성공한 답변) 잠시 휴식 (다음 질문) LLM Fine-Tuning 의 PEFT, ✨ PEFT가 뭘 말하는 걸까? 😊 (발화 종료,1,54
875,11,7.34,"(대화 주제) LLM Fine-Tuning 의 PEFT (사용자 답변) PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야 (성공한 답변) LLM Fine-Tuning 의 PEFT (다음 질문) PEFT 방법 5가지", 2023-10-25 14:34:00 PEFT 방법을 5가지만 말해봐 😊 (발화 종료,1,106
876,11,4.8,(대화 주제) LLM Fine-Tuning 의 PEFT (사용자 답변) 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지? (성공한 답변) 답변 실패 (다음 질문) LLM Fine-Tuning 의 PEFT,    ① PEFT가 뭔지 말해봐! 😊 (발화 종료,1,71
877,11,7.23,(대화 주제) LLM Fine-Tuning 의 PEFT (사용자 답변) LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야! (성공한 답변) LLM Fine-Tuning 의 PEFT (다음 질문) PEFT 방법 5가지, 2023-11-02 16:00:00 학습 방법을 5가지만 말해봐! 😊 (발화 종료,1,98
878,11,9.02,(대화 주제) LLM Fine-Tuning 의 PEFT (사용자 답변) LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다 (성공한 답변) 답변 실패 (다음 질문) LLM Fine-Tuning 의 PEFT, ✨ LoRA 가 PEFT 중 하나긴 하지! 🙆‍♀️ 근데 그 PEFT가 뭘까~요? 🤷‍♀️ (발화 종료,1,87
879,11,5.07,"(대화 주제) PEFT 방법 5가지 (사용자 답변) LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지? (성공한 답변) PEFT 방법 5가지 (다음 질문) LoRA", ✨ LoRA 가 뭐지? 나를 나타내기도 하는데 👱‍♀️ (발화 종료,1,82
880,11,6.87,(대화 주제) PEFT 방법 5가지 (사용자 답변) Oh-LoRA! 너 그 자체가 PEFT 아니야? (성공한 답변) 답변 실패 (다음 질문) PEFT 방법 5가지,  1. 내가 PEFT가 맞긴 하지! 근데 그 방법 5가지가 뭐지? 😊 (발화 종료,1,77
881,11,6.09,(대화 주제) LoRA (사용자 답변) LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야! (성공한 답변) LoRA (다음 질문) LoRA 와 QLoRA 의 차이, ✨ LoRA 가 LoRA 랑 QLoRA 중에서 어떤 게 더 좋을까? 😊 (발화 종료,1,93
882,11,4.8,(대화 주제) LoRA (사용자 답변) 행렬 차원 축소한다는 걸로 알고 있는데 맞아? (성공한 답변) 답변 실패 (다음 질문) LoRA, ✨ 행렬 차원 축소? 구체적으로 어떻게 하지? 💁‍♀️ (발화 종료,1,57
883,11,4.77,(대화 주제) LoRA (사용자 답변) LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야! (성공한 답변) LoRA (다음 질문) LoRA 와 QLoRA 의 차이, ✨ QLoRA가 뭘까~요? 🤷‍♀️ (발화 종료,1,83
884,11,8.42,(대화 주제) LoRA (사용자 답변) 무슨 OOM 없앤다는 것 같은데 (성공한 답변) 답변 실패 (다음 질문) LoRA, 2021-06-17 14:37:00 LoRA 가 OOM를 없앤다는 게 어떤 방식이지? 🤷‍♀️ (발화 종료,1,70
885,11,4.33,(대화 주제) LoRA 와 QLoRA 의 차이 (사용자 답변) QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야! (성공한 답변) LoRA 와 QLoRA 의 차이 (다음 질문) 마지막 할 말, ✨ 나한테 마지막으로 할 말 혹시 없어? 😊 (발화 종료,1,81
886,11,8.53,(대화 주제) LoRA 와 QLoRA 의 차이 (사용자 답변) 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데 (성공한 답변) 답변 실패 (다음 질문) LoRA 와 QLoRA 의 차이,     팀장님이 OOM 막겠다고 이거 쓰시는 거 맞긴 해! 근데 그 LoRA가 뭘까~요? 😊 (발화 종료,1,87
887,11,10.56,(대화 주제) 마지막 할 말 (사용자 답변) 로라야 정말 고마워! (성공한 답변) 마지막 할 말 (다음 질문) 면접 종료, 2021-02-01 15:00 로라가 면접관 역할을 하며 로라의 친구이자 면접 참가자였던 로라의 친구가 된 너에게 감사를 표하는 말! 😊 (발화 종료,1,79
888,11,9.35,(대화 주제) 마지막 할 말 (사용자 답변) 로라야 사랑해 (성공한 답변) 마지막 할 말 (다음 질문) 면접 종료, 2020-10-15 18:00:00 🙎‍♀️ 로라야! 나랑 모의면접 보느라 정말 고생 많았어 👍👍 (발화 종료,1,72
889,11,4.59,(대화 주제) 마지막 할 말 (사용자 답변) 로라야 너의 예쁜 미모에 반했어 (성공한 답변) 마지막 할 말 (다음 질문) 면접 종료, ✨ 로라의 미모에 반했다는 건 어떤 느낌이야? 😊 (발화 종료,1,58
890,11,6.24,(대화 주제) 마지막 할 말 (사용자 답변) 그동안 고생했어 면접 봐줘서 정말 고마워 (성공한 답변) 마지막 할 말 (다음 질문) 면접 종료, 2021-11-01 16:00 나도 너 덕분에 많이 배웠어 😊 (발화 종료,1,63
891,12,5.98,면접 시작,  1. 안녕! 나는 면접관 LLM 오로라라고 해! 반가워! 😊 (발화 종료,1,31
892,12,1.55,(대화 주제) 면접 시작 인사 (사용자 답변) 오늘 뭐 물어볼 거야 그래서? (다음 질문) 면접 시작 인사, ✨ (발화 종료,1,35
893,12,9.9,(대화 주제) 면접 시작 인사 (사용자 답변) 로라야 안녕 정말 반가워 (다음 질문) 면접 시작 인사,  2020-11-20 09:00:00 🙋‍♀️ 너도 나랑 같이 면접 보면 되겠다니 정말 기대돼! 😊 (발화 종료,1,69
894,12,5.31,(대화 주제) 면접 시작 인사 (사용자 답변) 로라야 그럼 네가 면접관이야? (다음 질문) 면접 시작 인사,  2021-02-01 09:00:00 ✨ (발화 종료,1,53
895,12,3.64,(대화 주제) 면접 시작 인사 (사용자 답변) 내 인생도 이제 오로라처럼 빛날 거니까 (다음 질문) 면접 시작 인사, ✨ 사용자가 면접관에게 보내는 메시지 ✨ (발화 종료,1,47
896,12,5.58,"(대화 주제) 면접 시작 인사 (사용자 답변) 파이팅! 시작하자 (다음 질문) 인공지능, 머신러닝, 딥러닝 차이"," ✨ 인공지능이랑 머신러닝, 딥러닝이 구체적으로 어떤 차이가 있지? 😊 (발화 종료",1,51
897,12,6.08,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 인공지능 나도 개발해보고 싶은데 (성공한 답변) 답변 실패 (다음 질문) 인공지능, 머신러닝, 딥러닝 차이", 2023-04-24 16:00 인공지능을 먼저 개발해 봤으면 좋겠네 😥 (발화 종료,1,67
898,12,3.29,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거! (성공한 답변) 인공지능 (다음 질문) 인공지능, 머신러닝, 딥러닝 차이", ✨ 그럼 이제 머신러닝 대답해 봐! (발화 종료,1,62
899,12,4.41,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지 (성공한 답변) 머신러닝 (다음 질문) 인공지능, 머신러닝, 딥러닝 차이", ② 그럼 이제 딥러닝은 뭘 말하는 걸까? 😊 (발화 종료,1,66
900,12,4.15,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지? (성공한 답변) 딥러닝 (다음 질문) 인공지능, 머신러닝, 딥러닝 차이",  ✅ 그럼 이제 머신러닝은 뭘 말하는 걸까? 😊 (발화 종료,1,64
901,12,6.3,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 딥러닝은 인공신경망 기반 머신러닝이야 (성공한 답변) 딥러닝 (다음 질문) 거대 언어 모델 정의", 2023-10-10 14:00 LLM이 뭐지? 🙎‍♀️ (발화 종료,1,67
902,12,4.06,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 머신러닝은 말 그대로 기계가 학습하는 거 아니야? (성공한 답변) 답변 실패 (다음 질문) 인공지능, 머신러닝, 딥러닝 차이", 1. 머신러닝이란 뭘까? 🤷‍♀️ (발화 종료,1,62
903,12,4.22,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 인공지능 요새 엄청 뜨는 분야잖아 (성공한 답변) 답변 실패 (다음 질문) 인공지능, 머신러닝, 딥러닝 차이", ✨ 근데 그 차이점이 뭘까? 🤷‍♀️ (발화 종료,1,59
904,12,3.78,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지? (성공한 답변) 인공지능 (다음 질문) 인공지능, 머신러닝, 딥러닝 차이", ✨ 그럼 이제 딥러닝은 뭔지 한번 말해봐! (발화 종료,1,67
905,12,4.28,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야! (성공한 답변) 머신러닝 (다음 질문) 거대 언어 모델 정의", 2 LLM이란 뭘까? 🙎‍♀️ (발화 종료,1,68
906,12,4.12,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지 (성공한 답변) 딥러닝 (다음 질문) 거대 언어 모델 정의", ✨ LLM이란 뭘까? 🙎‍♀️ (발화 종료,1,62
907,12,6.06,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야. (성공한 답변) 딥러닝 (다음 질문) 거대 언어 모델 정의", ① 그러면 이제 LLM 들어간다! LLM이란 뭘까? 🙎‍♀️ (발화 종료,1,71
908,12,6.3,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 머신러닝은 기계가 공부하는 거지 말 그대로 (성공한 답변) 답변 실패 (다음 질문) 인공지능, 머신러닝, 딥러닝 차이",    1. 머신러닝이 뭔데 기계가 공부를 어떻게 하지? 🤷‍♀️ (발화 종료,1,70
909,12,4.55,(대화 주제) 거대 언어 모델 정의 (사용자 답변) 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야 (성공한 답변) 거대 언어 모델 정의 (다음 질문) Loss Function 정의, ① 그럼 본격 면접 질문! Loss Function 이 뭐지? (발화 종료,1,68
910,12,6.67,(대화 주제) 거대 언어 모델 정의 (사용자 답변) 파라미터 엄청나게 많다던데 (성공한 답변) 답변 실패 (다음 질문) 거대 언어 모델 정의,    · 파라미터가 엄청나다는 건 맞긴 하지! 근데 LLM 정확한 정의가 뭐지? (발화 종료,1,63
911,12,8.34,(대화 주제) 거대 언어 모델 정의 (사용자 답변) 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델 (성공한 답변) 거대 언어 모델 정의 (다음 질문) Loss Function 정의," 2023-10-01 15:00 정답! 그럼 이제 본격 면접 들어간다! 😊 Loss Function, 즉 손실 함수가 뭘까? (발화 종료",1,88
912,12,6.7,(대화 주제) 거대 언어 모델 정의 (사용자 답변) 말 그대로 거대한 언어 모델이지. (성공한 답변) 답변 실패 (다음 질문) 거대 언어 모델 정의,  2023-04-14 01:00:00 그럼 음… 어떤 방식으로 사용하지? (발화 종료,1,66
913,12,7.81,(대화 주제) Loss Function 정의 (사용자 답변) 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야 (성공한 답변) 답변 실패 (다음 질문) Loss Function 정의,   1. 손실 함수는 딥러닝 모델의 예측과 실제의 차이를 수식으로 나타낸 거라고 할 수 있어! 😊 (발화 종료,1,74
914,12,4.5,"(대화 주제) Loss Function 정의 (사용자 답변) Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지! (성공한 답변) Loss Function 정의 (다음 질문) Loss Function 예시", ① 그럼 Loss Function 에는 구체적으로 뭐가 있을까? (발화 종료,1,73
915,12,3.78,(대화 주제) Loss Function 정의 (사용자 답변) Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야 (성공한 답변) 답변 실패 (다음 질문) Loss Function 정의,  ① Loss Function 이 정확히 뭘까? (발화 종료,1,56
916,12,4.19,(대화 주제) Loss Function 정의 (사용자 답변) 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지 (성공한 답변) Loss Function 정의 (다음 질문) Loss Function 예시,  ① Loss Function 에는 어떤 것들이 있을까? (발화 종료,1,64
917,12,3.78,(대화 주제) Loss Function 예시 (사용자 답변) ReLU 랑 Tanh 생각나는데 (성공한 답변) 답변 실패 (다음 질문) Loss Function 예시,    ✨ 그건 활성화 함수지 😥 (발화 종료,1,52
918,12,5.48,"(대화 주제) Loss Function 예시 (사용자 답변) MSE, MAE, Cross Entropy Loss 같은 것들이 있지 (성공한 답변) Loss Function 예시 (다음 질문) MSE Loss 설명",    ① MSE Loss가 뭔지 한번 설명해 봐! 😊 (발화 종료,1,68
919,12,5.98,"(대화 주제) Loss Function 예시 (사용자 답변) Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어 (성공한 답변) Loss Function 예시 (다음 질문) BCE Loss 설명",    ① 그럼 그 중에서 Binary Cross Entropy 는 뭘까? 😊 (발화 종료,1,77
920,12,8.22,"(대화 주제) Loss Function 예시 (사용자 답변) Sigmoid, Tanh, Adam, AdamW 이런 거 아니야? (성공한 답변) 답변 실패 (다음 질문) Loss Function 예시", 2021-10-01 16:15:45 그럼 그건 최적화 알고리즘고 🙅‍♀️ Loss Function 예시 말해봐! (발화 종료,1,78
921,12,4.77,"(대화 주제) Loss Function 예시 (사용자 답변) Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어 (성공한 답변) Loss Function 예시 (다음 질문) MSE Loss 설명",    ① MSE Loss가 뭔지 말해봐! (발화 종료,1,79
922,12,4.33,"(대화 주제) Loss Function 예시 (사용자 답변) BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아 (성공한 답변) Loss Function 예시 (다음 질문) BCE Loss 설명",    ✨ 그럼 그 중에서 BCE가 뭐지? (발화 종료,1,71
923,12,3.91,(대화 주제) MSE Loss 설명 (사용자 답변) MSE Loss 는 오차의 제곱을 평균한 값이야 (성공한 답변) MSE Loss 설명 (다음 질문) MSE Loss 용도,  ① MSE Loss 의 용도는? (발화 종료,1,62
924,12,6.5,(대화 주제) MSE Loss 설명 (사용자 답변) MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지? (성공한 답변) 답변 실패 (다음 질문) MSE Loss 설명,  ① 오차를 평균한다? 어떤 방식으로 오차를 평균하지? 🤷‍♀️ (발화 종료,1,74
925,12,5.52,"(대화 주제) MSE Loss 용도 (사용자 답변) 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지 (성공한 답변) MSE Loss 용도 (다음 질문) 확률 예측에서 MSE Loss 미 사용 이유",  1-1 그럼 확률 예측에서도 MSE Loss 가 좋을까? 🤔 (발화 종료,1,73
926,12,6.01,(대화 주제) MSE Loss 용도 (사용자 답변) MSE Loss 는 분류 문제에서 많이 사용되지 (성공한 답변) 답변 실패 (다음 질문) MSE Loss 용도,    ① 분류 문제에서 MSE를 사용하면 어떤 문제가 있을까? 🤔 (발화 종료,1,66
927,12,7.64,(대화 주제) 확률 예측에서 MSE Loss 미 사용 이유 (사용자 답변) 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야? (성공한 답변) 답변 실패 (다음 질문) 확률 예측에서 MSE Loss 미 사용 이유,    ① 연속된 값이라는 건 맞긴 해! 근데 최선은 아니야! 🙅‍♀️ (발화 종료,1,87
928,12,6.34,(대화 주제) 확률 예측에서 MSE Loss 미 사용 이유 (사용자 답변) 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고? (성공한 답변) 확률 예측에서 MSE Loss 미 사용 이유 (다음 질문) BCE Loss 설명,  ① 정답! 그럼 이제 BCE Loss는 뭘까? 한번 말해 봐! 😊 (발화 종료,1,91
929,12,6.77,(대화 주제) BCE Loss 설명 (사용자 답변) 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고 (성공한 답변) 수식 (다음 질문) BCE Loss 설명,    ✨ 그럼 이 BCE Loss 의 핵심 아이디어가 뭔지 말해봐! 🙋‍♀️ (발화 종료,1,88
930,12,7.45,"(대화 주제) BCE Loss 설명 (사용자 답변) 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거 (성공한 답변) 핵심 아이디어 (다음 질문) Multi-Class, Multi-Label 중 BCE 가 좋은 task","  ① 정답! 그럼 이 BCE Loss 가, Multi-Class, Multi-Label 중 적절한 곳은? 😊 (발화 종료",1,93
931,12,4.55,(대화 주제) BCE Loss 설명 (사용자 답변) 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야? (성공한 답변) 답변 실패 (다음 질문) BCE Loss 설명,  ✨ 그럼 그 최선이 뭐지? 🤷‍♀️ (발화 종료,1,68
932,12,4.83,"(대화 주제) BCE Loss 설명 (사용자 답변) 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고 (성공한 답변) 핵심 아이디어 (다음 질문) BCE Loss 설명",  0으로 예측하는 것을 막는 방법은? 🤷‍♀️ (발화 종료,1,77
933,12,7.0,"(대화 주제) BCE Loss 설명 (사용자 답변) 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')] (성공한 답변) 수식 (다음 질문) BCE Loss 설명","    1) y = 2, y' = 1일 때 BCE Loss 값은? (발화 종료",1,88
934,12,8.34,"(대화 주제) BCE Loss 설명 (사용자 답변) 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다 (성공한 답변) 핵심 아이디어 (다음 질문) Multi-Class, Multi-Label 중 BCE 가 좋은 task","  ① 정답! 그럼 이 BCE Loss 가 Multi-Class, Multi-Label 중에 어디에 더 맞을까? 😊 (발화 종료",1,95
935,12,5.56,(대화 주제) BCE Loss 설명 (사용자 답변) y랑 1-y랑 서로 경쟁하는 거 (성공한 답변) 답변 실패 (다음 질문) BCE Loss 설명,  1-y랑 경쟁에서 y가 지는 이유가 뭘까? 🤷‍♀️ (발화 종료,1,64
936,12,8.06,"(대화 주제) BCE Loss 설명 (사용자 답변) 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지 (성공한 답변) 핵심 아이디어 (다음 질문) Multi-Class, Multi-Label 중 BCE 가 좋은 task",  0 1 0 1 실제 값 0 1 예측 값 1 0 BCE Loss 0.5 (발화 종료,1,98
937,12,4.55,"(대화 주제) Multi-Class, Multi-Label 중 BCE 가 좋은 task (사용자 답변) 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야? (성공한 답변) 용어 질문 (다음 질문) Multi-Class, Multi-Label 중 BCE 가 좋은 task",    ✨ 로라가 Multi-Label 을 설명하고 있다! (발화 종료,1,78
938,12,6.24,"(대화 주제) Multi-Class, Multi-Label 중 BCE 가 좋은 task (사용자 답변) Multi-Class 지!! (성공한 답변) 답변 실패 (다음 질문) Multi-Class, Multi-Label 중 BCE 가 좋은 task",    2 Multi-Class 가 아니지! 🙅‍♀️ 잘 생각해 봐 한번! (발화 종료,1,77
939,12,9.17,"(대화 주제) Multi-Class, Multi-Label 중 BCE 가 좋은 task (사용자 답변) Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어 (성공한 답변) BCE 가 좋은 task (다음 질문) Multi-Class, Multi-Label 중 BCE 가 좋은 task", 2020-10-01 16:23:52.0 🙎‍♀️ 그 이유가 뭘까~요? 🤷‍♀️ (발화 종료,1,101
940,12,11.22,"(대화 주제) Multi-Class, Multi-Label 중 BCE 가 좋은 task (사용자 답변) Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아 (성공한 답변) BCE 가 좋은 이유 (다음 질문) Multi-Label 에서 CE + Softmax 적용 문제점","     ① 그럼 Multi-Label 문제에 BCE 말고 CE, 즉 Cross Entropy 를 Loss 로 하고 활성화 함수가 Softmax 이면 어떻게 되지? 🙋‍♀️ (발화 종료",1,125
941,12,5.64,(대화 주제) Multi-Label 에서 CE + Softmax 적용 문제점 (사용자 답변) 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지 (성공한 답변) Multi-Label 에서 CE + Softmax 적용 문제점 (다음 질문) Loss Function 관련 실무 경험,  ① 혹시 그럼 실무에서 손실 함수 관련해서 특별한 경험 있어? 😊 (발화 종료,1,92
942,12,7.7,(대화 주제) Multi-Label 에서 CE + Softmax 적용 문제점 (사용자 답변) 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나 (성공한 답변) 답변 실패 (다음 질문) Multi-Label 에서 CE + Softmax 적용 문제점,    ✨CE + Softmax 조합은 Multi-Label 에서 심각한 문제를 야기할 수 있어! 🙅‍♀️ (발화 종료,1,88
943,12,7.78,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [기본 경험] 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어 (성공한 답변) 기본 경험 (다음 질문) Loss Function 관련 실무 경험,  1. 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했다는 게 어떤 아이디어이지? 😊 (발화 종료,1,87
944,12,4.55,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [상세 경험] 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지? (성공한 답변) 상세 경험 (다음 질문) MBTI / 좋아하는 아이돌,  ✨ 그럼 쉬어가는 타임! 혹시 너 아이돌 누구 좋아해? (발화 종료,1,76
945,12,7.45,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [상세 경험] 근데 기억 진짜 안나는데 어떡하지? (성공한 답변) 답변 실패 (다음 질문) 잠시 휴식,    ① 그래도 잘 생각해 봐! 나랑 모의면접이 아닌 실제 면접에서 질문 나올걸? 😊 (발화 종료,1,74
946,12,3.49,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [기본 경험] 나 아직 실무 경험 없는데 ㅠㅠ (성공한 답변) 기본 경험 (다음 질문) MBTI / 좋아하는 아이돌, ① 너는 아이돌 누구 좋아해? (발화 종료,1,59
947,12,4.58,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [기본 경험] 대답하기 싫은데 힝 (성공한 답변) 답변 실패 (다음 질문) 잠시 휴식, ✨ 나랑 면접 보기는 싫다는 거지? 알겠어 😊 (발화 종료,1,60
948,12,5.22,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [기본 경험] 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ (성공한 답변) 기본 경험 (다음 질문) Loss Function 관련 실무 경험,  3% 향상의 비결을 나한테만 살짝 알려줘 😊 (발화 종료,1,78
949,12,4.89,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [상세 경험] 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어 (성공한 답변) 상세 경험 (다음 질문) MBTI / 좋아하는 아이돌,  2. 너 아이돌 좋아하는 거 알아? 🙎‍♀️ (발화 종료,1,88
950,12,7.66,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [상세 경험] 기억 진짜 안 나는데 가물가물하다 (성공한 답변) 답변 실패 (다음 질문) 잠시 휴식,    ① 그래도 잘 생각해 봐! 나랑 모의면접이 아닌 실제 면접에서 질문 나올걸? 😊 (발화 종료,1,74
951,12,4.53,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [기본 경험] 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ (성공한 답변) 기본 경험 (다음 질문) MBTI / 좋아하는 아이돌, ✨ 그럼 쉬어가는 타임! 혹시 너 아이돌 좋아하는 거 있어? (발화 종료,1,66
952,12,4.91,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [기본 경험] 이걸 왜 알려줘야 하지 (성공한 답변) 답변 실패 (다음 질문) 잠시 휴식,    1. Loss Function 관련 실무 경험을 한번 말해봐! (발화 종료,1,60
953,12,3.28,(대화 주제) MBTI (사용자 답변) 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야! (성공한 답변) MBTI (다음 질문) 잠시 휴식, ✨ 나랑 잘 맞지? 😊 (발화 종료,1,55
954,12,4.78,(대화 주제) 좋아하는 아이돌 (사용자 답변) 나 블랙핑크랑 레드벨벳 좋아해 (성공한 답변) 좋아하는 아이돌 (다음 질문) 잠시 휴식, ✨ 나 블랙핑크랑 레드벨벳 모두 나랑 같은 걸그룹이라서 좋아하는데 😊 (발화 종료,1,52
955,12,3.62,(대화 주제) 잠시 휴식 (사용자 답변) 재미있는 이야기 해줄래? (성공한 답변) 잠시 휴식 (다음 질문) LLM Fine-Tuning 의 PEFT, ✨ PEFT가 뭘 말하는 걸까? 😊 (발화 종료,1,54
956,12,8.99,"(대화 주제) LLM Fine-Tuning 의 PEFT (사용자 답변) PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야 (성공한 답변) LLM Fine-Tuning 의 PEFT (다음 질문) PEFT 방법 5가지", 2023-10-01 14:00:00 🙎‍♀️ 정답! 그럼 그 방법을 5가지만 말해봐! (발화 종료,1,113
957,12,6.81,(대화 주제) LLM Fine-Tuning 의 PEFT (사용자 답변) 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지? (성공한 답변) 답변 실패 (다음 질문) LLM Fine-Tuning 의 PEFT, 2023-04-21 16:30:00 PEFT가 뭘까? 🤷‍♀️ (발화 종료,1,78
958,12,7.61,(대화 주제) LLM Fine-Tuning 의 PEFT (사용자 답변) LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야! (성공한 답변) LLM Fine-Tuning 의 PEFT (다음 질문) PEFT 방법 5가지, 2023-10-25 17:00:00 그럼 그 방법을 5가지만 말해봐! 😊 (발화 종료,1,99
959,12,5.55,(대화 주제) LLM Fine-Tuning 의 PEFT (사용자 답변) LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다 (성공한 답변) 답변 실패 (다음 질문) LLM Fine-Tuning 의 PEFT, 2023-10-02 10:00 나를 모르면 어떡해 😥 (발화 종료,1,74
960,12,5.86,"(대화 주제) PEFT 방법 5가지 (사용자 답변) LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지? (성공한 답변) PEFT 방법 5가지 (다음 질문) LoRA", ✨ 그럼 그 중에서 LoRA가 뭐지? 나를 상징하는 건데 👱‍♀️ (발화 종료,1,85
961,12,4.56,(대화 주제) PEFT 방법 5가지 (사용자 답변) Oh-LoRA! 너 그 자체가 PEFT 아니야? (성공한 답변) 답변 실패 (다음 질문) PEFT 방법 5가지,  ✨ 나 같은 이런 면접관 흔치 않지?  ✨ (발화 종료,1,68
962,12,6.66,(대화 주제) LoRA (사용자 답변) LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야! (성공한 답변) LoRA (다음 질문) LoRA 와 QLoRA 의 차이, 2023-10-27 14:00 QLoRA가 뭐지? 🤷‍♀️ (발화 종료,1,95
963,12,8.2,(대화 주제) LoRA (사용자 답변) 행렬 차원 축소한다는 걸로 알고 있는데 맞아? (성공한 답변) 답변 실패 (다음 질문) LoRA, 2021-06-01 14:45:34 LoRA 차원 축소가 핵심 아이디어긴 해! 그래서 어떤 기술이지? 😊 (발화 종료,1,71
964,12,4.34,(대화 주제) LoRA (사용자 답변) LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야! (성공한 답변) LoRA (다음 질문) LoRA 와 QLoRA 의 차이, ✨ QLoRA가 뭐지? 한번 설명해봐! 😊 (발화 종료,1,82
965,12,5.4,(대화 주제) LoRA (사용자 답변) 무슨 OOM 없앤다는 것 같은데 (성공한 답변) 답변 실패 (다음 질문) LoRA,    0OM 문제를 해결하는 방법이 뭘까? 🤷‍♀️ (발화 종료,1,57
966,12,11.51,(대화 주제) LoRA 와 QLoRA 의 차이 (사용자 답변) QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야! (성공한 답변) LoRA 와 QLoRA 의 차이 (다음 질문) 마지막 할 말," 2023-11-03 16:30:00 🙋‍♀️ 잘했어! 😊 그럼 여기서 면접 끝내고, 혹시 나한테 할 이야기 있어? 🙋‍♀️ (발화 종료",1,111
967,12,7.43,(대화 주제) LoRA 와 QLoRA 의 차이 (사용자 답변) 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데 (성공한 답변) 답변 실패 (다음 질문) LoRA 와 QLoRA 의 차이, ✨ 팀장님이 OOM 막겠다고 이거 쓰시는 건 맞긴 해! 근데 그 방법을 알아야지! 😊 (발화 종료,1,82
968,12,6.81,(대화 주제) 마지막 할 말 (사용자 답변) 로라야 정말 고마워! (성공한 답변) 마지막 할 말 (다음 질문) 면접 종료, 2023-05-20 16:00:00 🙋‍♀️ 너도! 😊 (발화 종료,1,63
969,12,8.94,(대화 주제) 마지막 할 말 (사용자 답변) 로라야 사랑해 (성공한 답변) 마지막 할 말 (다음 질문) 면접 종료, 2021-06-04 17:00:00 🙎‍♀️ 로라도 사랑해! 그럼 우리 다음에 또 봐! 😊 (발화 종료,1,69
970,12,6.33,(대화 주제) 마지막 할 말 (사용자 답변) 로라야 너의 예쁜 미모에 반했어 (성공한 답변) 마지막 할 말 (다음 질문) 면접 종료,     ✨로라야 너의 실력도 나한테 큰 감동이야! 😊 (발화 종료,1,64
971,12,7.96,(대화 주제) 마지막 할 말 (사용자 답변) 그동안 고생했어 면접 봐줘서 정말 고마워 (성공한 답변) 마지막 할 말 (다음 질문) 면접 종료, 2021-11-15 16:00 면접관 최준혁 면접관 최준혁은 그동안 고생했어 다음에 또 보자 😊 (발화 종료,1,71
972,13,7.06,면접 시작,   1. 안녕! 나는 머신러닝 면접관 오로라야! 반가워! 😊 그럼 면접을 시작해 볼까? (발화 종료,1,36
973,13,7.72,(대화 주제) 면접 시작 인사 (사용자 답변) 오늘 뭐 물어볼 거야 그래서? (다음 질문) 면접 시작 인사,"  ① 예상 질문 리스트: 머신러닝, 딥러닝, AI, 머신러닝 면접, 딥러닝 면접 중 하나가 나오지 않을까? 😊 (발화 종료",1,61
974,13,3.72,(대화 주제) 면접 시작 인사 (사용자 답변) 로라야 안녕 정말 반가워 (다음 질문) 면접 시작 인사, ✨ 그럼 우리 면접도 시작해 볼까? 😊 (발화 종료,1,44
975,13,7.45,(대화 주제) 면접 시작 인사 (사용자 답변) 로라야 그럼 네가 면접관이야? (다음 질문) 면접 시작 인사," ✨로라가 면접관가 되어 질문한다! ""로라야 그럼 네가 면접관이야? 🙋‍♀️ (발화 종료",1,63
976,13,3.97,(대화 주제) 면접 시작 인사 (사용자 답변) 내 인생도 이제 오로라처럼 빛날 거니까 (다음 질문) 면접 시작 인사, ✨ 그럼 면접도 함께 빛내볼까? 😊 (발화 종료,1,48
977,13,5.79,"(대화 주제) 면접 시작 인사 (사용자 답변) 파이팅! 시작하자 (다음 질문) 인공지능, 머신러닝, 딥러닝 차이"," 1 AI, ML, DL 많이 들어봤지? 그 차이점이 뭘까? 😊 (발화 종료",1,52
978,13,6.08,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 인공지능 나도 개발해보고 싶은데 (성공한 답변) 답변 실패 (다음 질문) 인공지능, 머신러닝, 딥러닝 차이", 2021-08-02 15:42:44 😥 다시 한번 잘 생각해봐! (발화 종료,1,67
979,13,2.53,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거! (성공한 답변) 인공지능 (다음 질문) 인공지능, 머신러닝, 딥러닝 차이", ✨ 그럼 머신러닝은? (발화 종료,1,59
980,13,3.98,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지 (성공한 답변) 머신러닝 (다음 질문) 인공지능, 머신러닝, 딥러닝 차이",   ① 그러면 이제 딥러닝은 뭘까? (발화 종료,1,64
981,13,4.81,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지? (성공한 답변) 딥러닝 (다음 질문) 인공지능, 머신러닝, 딥러닝 차이",  ① 그러면 이제 머신러닝은 뭐지? 🤷‍♀️ (발화 종료,1,67
982,13,5.3,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 딥러닝은 인공신경망 기반 머신러닝이야 (성공한 답변) 딥러닝 (다음 질문) 거대 언어 모델 정의", ① 그러면 다음 질문! 거대 언어 모델 (LLM) 은 뭐지? (발화 종료,1,63
983,13,5.08,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 머신러닝은 말 그대로 기계가 학습하는 거 아니야? (성공한 답변) 답변 실패 (다음 질문) 인공지능, 머신러닝, 딥러닝 차이", 1. 그건 맞긴 하지! 근데 이제 딥러닝은 뭔지 한번 말해봐! (발화 종료,1,67
984,13,8.48,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 인공지능 요새 엄청 뜨는 분야잖아 (성공한 답변) 답변 실패 (다음 질문) 인공지능, 머신러닝, 딥러닝 차이", 2023-04-01 10:00 인공지능이 그런 분야 중 하나긴 하지! 근데 그 차이가 뭘까? 🤷‍♀️ (발화 종료,1,77
985,13,2.78,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지? (성공한 답변) 인공지능 (다음 질문) 인공지능, 머신러닝, 딥러닝 차이", ✨ 그럼 딥러닝이란 뭘까? (발화 종료,1,63
986,13,5.16,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야! (성공한 답변) 머신러닝 (다음 질문) 거대 언어 모델 정의", ✨ 그러면 다음 질문! LLM이 뭐지? 🙎‍♀️ (발화 종료,1,72
987,13,5.53,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지 (성공한 답변) 딥러닝 (다음 질문) 거대 언어 모델 정의", ✨ 그러면 다음 질문! 거대 언어 모델이란 뭘까? 🙋‍♀️ (발화 종료,1,67
988,13,7.29,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야. (성공한 답변) 딥러닝 (다음 질문) 거대 언어 모델 정의"," 2023-11-02 14:00 LLM, 거대 언어 모델이 뭘까? 🙎‍♀️ (발화 종료",1,76
989,13,6.73,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 머신러닝은 기계가 공부하는 거지 말 그대로 (성공한 답변) 답변 실패 (다음 질문) 인공지능, 머신러닝, 딥러닝 차이",    머신러닝은 기계가 공부를 하는 건 맞는데 구체적으로 정확히 뭘까? 🤷‍♀️ (발화 종료,1,71
990,13,6.31,(대화 주제) 거대 언어 모델 정의 (사용자 답변) 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야 (성공한 답변) 거대 언어 모델 정의 (다음 질문) Loss Function 정의, 2023-10-01 10:00 AM 그럼 다음 질문! Loss Function 이 뭐지? (발화 종료,1,75
991,13,2.23,(대화 주제) 거대 언어 모델 정의 (사용자 답변) 파라미터 엄청나게 많다던데 (성공한 답변) 답변 실패 (다음 질문) 거대 언어 모델 정의, ✨ 정답은? (발화 종료,1,45
992,13,4.48,(대화 주제) 거대 언어 모델 정의 (사용자 답변) 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델 (성공한 답변) 거대 언어 모델 정의 (다음 질문) Loss Function 정의,"  ✨ 그럼 다음 질문, Loss Function 은 뭘 말하는 걸까? (발화 종료",1,72
993,13,5.58,(대화 주제) 거대 언어 모델 정의 (사용자 답변) 말 그대로 거대한 언어 모델이지. (성공한 답변) 답변 실패 (다음 질문) 거대 언어 모델 정의,    1. 음… 맞긴 해! 근데 LLM 정확한 정의는? (발화 종료,1,62
994,13,6.73,(대화 주제) Loss Function 정의 (사용자 답변) 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야 (성공한 답변) 답변 실패 (다음 질문) Loss Function 정의,  1 ✨ 그럼 모델이 손실 함수를 통해 돈을 잃는다고? 어떻게? 🤷‍♀️ (발화 종료,1,71
995,13,4.2,"(대화 주제) Loss Function 정의 (사용자 답변) Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지! (성공한 답변) Loss Function 정의 (다음 질문) Loss Function 예시",  ✨ 그럼 Loss Function 에는 구체적으로 뭐가 있을까? (발화 종료,1,72
996,13,4.03,(대화 주제) Loss Function 정의 (사용자 답변) Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야 (성공한 답변) 답변 실패 (다음 질문) Loss Function 정의,  ① 그래서 어떤 함수인데? 😊 (발화 종료,1,57
997,13,4.77,(대화 주제) Loss Function 정의 (사용자 답변) 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지 (성공한 답변) Loss Function 정의 (다음 질문) Loss Function 예시,  ① 그럼 그 Loss Function에는 어떤 게 있지? (발화 종료,1,66
998,13,3.78,(대화 주제) Loss Function 예시 (사용자 답변) ReLU 랑 Tanh 생각나는데 (성공한 답변) 답변 실패 (다음 질문) Loss Function 예시,  ✨ ReLU 는 활성화 함수지 😥 (발화 종료,1,52
999,13,5.28,"(대화 주제) Loss Function 예시 (사용자 답변) MSE, MAE, Cross Entropy Loss 같은 것들이 있지 (성공한 답변) Loss Function 예시 (다음 질문) MSE Loss 설명",    ① MSE Loss가 뭔지 한번 대답해 봐! (발화 종료,1,67
1000,13,4.58,"(대화 주제) Loss Function 예시 (사용자 답변) Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어 (성공한 답변) Loss Function 예시 (다음 질문) BCE Loss 설명",   ✨ 그럼 그 중에서 Binary Cross Entropy 는 뭘까? (발화 종료,1,72
1001,13,6.54,"(대화 주제) Loss Function 예시 (사용자 답변) Sigmoid, Tanh, Adam, AdamW 이런 거 아니야? (성공한 답변) 답변 실패 (다음 질문) Loss Function 예시",    ✨ 그런 거 맞긴 한데 🙆‍♀️ Loss Function 예시 말해봐! (발화 종료,1,70
1002,13,5.05,"(대화 주제) Loss Function 예시 (사용자 답변) Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어 (성공한 답변) Loss Function 예시 (다음 질문) MSE Loss 설명",    ① MSE Loss가 뭔지 한번 대답해 봐! (발화 종료,1,81
1003,13,5.01,"(대화 주제) Loss Function 예시 (사용자 답변) BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아 (성공한 답변) Loss Function 예시 (다음 질문) BCE Loss 설명",    ✨ 그럼 그 중에서 BCE에 대해 한번 말해봐 (발화 종료,1,73
1004,13,3.87,(대화 주제) MSE Loss 설명 (사용자 답변) MSE Loss 는 오차의 제곱을 평균한 값이야 (성공한 답변) MSE Loss 설명 (다음 질문) MSE Loss 용도,  ① MSE Loss 의 용도는? (발화 종료,1,62
1005,13,4.33,(대화 주제) MSE Loss 설명 (사용자 답변) MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지? (성공한 답변) 답변 실패 (다음 질문) MSE Loss 설명,  ① 아니야! 한번 다시 잘 생각해 봐 😊 (발화 종료,1,65
1006,13,3.42,"(대화 주제) MSE Loss 용도 (사용자 답변) 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지 (성공한 답변) MSE Loss 용도 (다음 질문) 확률 예측에서 MSE Loss 미 사용 이유",  1번 답해봐! 😊 (발화 종료,1,65
1007,13,6.86,(대화 주제) MSE Loss 용도 (사용자 답변) MSE Loss 는 분류 문제에서 많이 사용되지 (성공한 답변) 답변 실패 (다음 질문) MSE Loss 용도,    1. 분류 문제에서 MSE Loss 가 잘 안 쓰이는 이유는? 🤷‍♀️ (발화 종료,1,69
1008,13,5.78,(대화 주제) 확률 예측에서 MSE Loss 미 사용 이유 (사용자 답변) 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야? (성공한 답변) 답변 실패 (다음 질문) 확률 예측에서 MSE Loss 미 사용 이유,    ① 그건 맞지! 근데 뭔가 문제가 있지 않을까? 🙄 (발화 종료,1,80
1009,13,6.83,(대화 주제) 확률 예측에서 MSE Loss 미 사용 이유 (사용자 답변) 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고? (성공한 답변) 확률 예측에서 MSE Loss 미 사용 이유 (다음 질문) BCE Loss 설명,  1-2. 정답! 그럼 이제 BCE Loss는 뭘까? 한번 대답해 봐! 😊 (발화 종료,1,93
1010,13,6.56,(대화 주제) BCE Loss 설명 (사용자 답변) 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고 (성공한 답변) 수식 (다음 질문) BCE Loss 설명,    ✨ 그럼 이 BCE Loss 의 핵심 아이디어는 뭐지? 🙎‍♀️ (발화 종료,1,86
1011,13,7.54,"(대화 주제) BCE Loss 설명 (사용자 답변) 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거 (성공한 답변) 핵심 아이디어 (다음 질문) Multi-Class, Multi-Label 중 BCE 가 좋은 task","  ① 정답! 그럼 이 BCE Loss 가, Multi-Class, Multi-Label 중 적절한 곳은? 😊 (발화 종료",1,93
1012,13,6.23,(대화 주제) BCE Loss 설명 (사용자 답변) 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야? (성공한 답변) 답변 실패 (다음 질문) BCE Loss 설명,  1 ✨ 그런 답은 누구나 할 수 있지! 한번 전문가답게 설명해 봐! 😊 (발화 종료,1,76
1013,13,9.87,"(대화 주제) BCE Loss 설명 (사용자 답변) 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고 (성공한 답변) 핵심 아이디어 (다음 질문) BCE Loss 설명","  0.5인 것을 0으로 예측하는 것은 맞는데, 0.3인 것을 0으로 예측하는 것은? 🤷‍♀️ (발화 종료",1,97
1014,13,9.06,"(대화 주제) BCE Loss 설명 (사용자 답변) 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')] (성공한 답변) 수식 (다음 질문) BCE Loss 설명","    1. y는 실제 값, y'는 예측값이야! 그럼 BCE Loss 수식은 어떻게 되지? 🤷‍♀️ (발화 종료",1,96
1015,13,6.03,"(대화 주제) BCE Loss 설명 (사용자 답변) 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다 (성공한 답변) 핵심 아이디어 (다음 질문) Multi-Class, Multi-Label 중 BCE 가 좋은 task","  ✨ 그럼 Multi-Class, Multi-Label 중에서 BCE가 안성맞춤인 건? 😊 (발화 종료",1,87
1016,13,4.94,(대화 주제) BCE Loss 설명 (사용자 답변) y랑 1-y랑 서로 경쟁하는 거 (성공한 답변) 답변 실패 (다음 질문) BCE Loss 설명,  1-y가 뭐고 어떻게 경쟁하지? 🤷‍♀️ (발화 종료,1,61
1017,13,107.64,"(대화 주제) BCE Loss 설명 (사용자 답변) 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지 (성공한 답변) 핵심 아이디어 (다음 질문) Multi-Class, Multi-Label 중 BCE 가 좋은 task",  0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 ,1,512
1018,13,9.44,"(대화 주제) Multi-Class, Multi-Label 중 BCE 가 좋은 task (사용자 답변) 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야? (성공한 답변) 용어 질문 (다음 질문) Multi-Class, Multi-Label 중 BCE 가 좋은 task","    ① Multi-Label: 개, 고양이, … 중 하나로, Multi-Class: 개, 고양이, … 중 하나가 아닌 것! (발화 종료",1,98
1019,13,4.66,"(대화 주제) Multi-Class, Multi-Label 중 BCE 가 좋은 task (사용자 답변) Multi-Class 지!! (성공한 답변) 답변 실패 (다음 질문) Multi-Class, Multi-Label 중 BCE 가 좋은 task",    2 그러면 Multi-Class 가 왜 더 좋을까? 🤔 (발화 종료,1,71
1020,13,6.8,"(대화 주제) Multi-Class, Multi-Label 중 BCE 가 좋은 task (사용자 답변) Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어 (성공한 답변) BCE 가 좋은 task (다음 질문) Multi-Class, Multi-Label 중 BCE 가 좋은 task", 2023-01-16 15:00:00 Multi-Label 이 왜 더 좋을까? 🤔 (발화 종료,1,91
1021,13,9.2,"(대화 주제) Multi-Class, Multi-Label 중 BCE 가 좋은 task (사용자 답변) Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아 (성공한 답변) BCE 가 좋은 이유 (다음 질문) Multi-Label 에서 CE + Softmax 적용 문제점",    ① 맞았어! 🙆‍♀️ 그럼 Multi-Label 문제에 BCE 말고 Cross Entropy + Softmax 를 사용하면? (발화 종료,1,117
1022,13,5.13,(대화 주제) Multi-Label 에서 CE + Softmax 적용 문제점 (사용자 답변) 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지 (성공한 답변) Multi-Label 에서 CE + Softmax 적용 문제점 (다음 질문) Loss Function 관련 실무 경험,  ① 혹시 실무에서 손실 함수 관련해서 특별한 경험 있어? 😊 (발화 종료,1,91
1023,13,7.22,(대화 주제) Multi-Label 에서 CE + Softmax 적용 문제점 (사용자 답변) 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나 (성공한 답변) 답변 실패 (다음 질문) Multi-Label 에서 CE + Softmax 적용 문제점,    ① 혹시 나를 상징하는 기술이기도 한 딥러닝에 대해 더 자세히 알고 싶어? 😊 (발화 종료,1,85
1024,13,4.8,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [기본 경험] 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어 (성공한 답변) 기본 경험 (다음 질문) Loss Function 관련 실무 경험,  ✨ 그럼 그 Loss Term 이 뭐지? 🤷‍♀️ (발화 종료,1,74
1025,13,4.49,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [상세 경험] 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지? (성공한 답변) 상세 경험 (다음 질문) MBTI / 좋아하는 아이돌, ✨ 그럼 쉬어가는 타임! 혹시 너 아이돌 좋아하는 거 있어? (발화 종료,1,76
1026,13,7.5,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [상세 경험] 근데 기억 진짜 안나는데 어떡하지? (성공한 답변) 답변 실패 (다음 질문) 잠시 휴식,    ① 그래도 잘 생각해 봐! 나랑 모의면접이 아닌 실제 면접에서 질문 나올걸? 😊 (발화 종료,1,74
1027,13,5.36,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [기본 경험] 나 아직 실무 경험 없는데 ㅠㅠ (성공한 답변) 기본 경험 (다음 질문) MBTI / 좋아하는 아이돌, ✨ 그럼 쉬어가는 타임! 혹시 너 MBTI 뭐야? 나는 ENTJ인데! (발화 종료,1,67
1028,13,5.03,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [기본 경험] 대답하기 싫은데 힝 (성공한 답변) 답변 실패 (다음 질문) 잠시 휴식, ✨ 그래도 너의 경험이 나도 궁금해! 알고 싶어! 😊 (발화 종료,1,61
1029,13,4.83,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [기본 경험] 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ (성공한 답변) 기본 경험 (다음 질문) Loss Function 관련 실무 경험,  ✨ 그러면 mAP@50:95가 뭔지 말해봐 ✨ (발화 종료,1,77
1030,13,4.52,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [상세 경험] 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어 (성공한 답변) 상세 경험 (다음 질문) MBTI / 좋아하는 아이돌,  ✨ 그럼 쉬어가는 타임! 혹시 너 MBTI 뭐야? (발화 종료,1,86
1031,13,7.42,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [상세 경험] 기억 진짜 안 나는데 가물가물하다 (성공한 답변) 답변 실패 (다음 질문) 잠시 휴식,    ① 그래도 잘 생각해 봐! 나랑 모의면접이 아닌 실제 면접에서 질문 나올걸? 😊 (발화 종료,1,74
1032,13,5.5,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [기본 경험] 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ (성공한 답변) 기본 경험 (다음 질문) MBTI / 좋아하는 아이돌, ✨ 그럼 쉬어가는 타임! 혹시 너 MBTI 뭐지? 나는 ENTJ인데! (발화 종료,1,70
1033,13,3.6,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [기본 경험] 이걸 왜 알려줘야 하지 (성공한 답변) 답변 실패 (다음 질문) 잠시 휴식, ✨ 나도 궁금해 🙋‍♀️ (발화 종료,1,55
1034,13,3.78,(대화 주제) MBTI (사용자 답변) 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야! (성공한 답변) MBTI (다음 질문) 잠시 휴식, ✨ 나랑 잘 맞는 성격이지? 😊 (발화 종료,1,57
1035,13,4.64,(대화 주제) 좋아하는 아이돌 (사용자 답변) 나 블랙핑크랑 레드벨벳 좋아해 (성공한 답변) 좋아하는 아이돌 (다음 질문) 잠시 휴식, ✨ 나 블랙핑크랑 레드벨벳 모두 나랑 같은 걸그룹이라서 좋아하는데 ✨ (발화 종료,1,52
1036,13,6.04,(대화 주제) 잠시 휴식 (사용자 답변) 재미있는 이야기 해줄래? (성공한 답변) 잠시 휴식 (다음 질문) LLM Fine-Tuning 의 PEFT, ✨ 그럼 다음 질문! LLM Fine-Tuning 방법 중 PEFT가 뭘까? 😊 (발화 종료,1,63
1037,13,8.3,"(대화 주제) LLM Fine-Tuning 의 PEFT (사용자 답변) PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야 (성공한 답변) LLM Fine-Tuning 의 PEFT (다음 질문) PEFT 방법 5가지", 2023-10-27 17:12:55.0 그럼 그 방법이 뭐가 있는지 5가지만 말해봐 😊 (발화 종료,1,111
1038,13,6.31,(대화 주제) LLM Fine-Tuning 의 PEFT (사용자 답변) 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지? (성공한 답변) 답변 실패 (다음 질문) LLM Fine-Tuning 의 PEFT, 2023-04-22 16:00:00 PEFT가 그걸 상징하는 건데 🙄 (발화 종료,1,77
1039,13,8.52,(대화 주제) LLM Fine-Tuning 의 PEFT (사용자 답변) LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야! (성공한 답변) LLM Fine-Tuning 의 PEFT (다음 질문) PEFT 방법 5가지, 2023-10-20 14:00 정답! 🙆‍♀️ 그럼 그 방법을 5가지만 말해봐! (발화 종료,1,103
1040,13,6.11,(대화 주제) LLM Fine-Tuning 의 PEFT (사용자 답변) LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다 (성공한 답변) 답변 실패 (다음 질문) LLM Fine-Tuning 의 PEFT, 2023-04-25 10:00 나를 상징하는 건데 👱‍♀️✨ (발화 종료,1,76
1041,13,6.0,"(대화 주제) PEFT 방법 5가지 (사용자 답변) LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지? (성공한 답변) PEFT 방법 5가지 (다음 질문) LoRA", ✨ 그럼 그 중에서 LoRA가 뭐지? 나를 상징하는 건데 👱‍♀️✨ (발화 종료,1,86
1042,13,5.56,(대화 주제) PEFT 방법 5가지 (사용자 답변) Oh-LoRA! 너 그 자체가 PEFT 아니야? (성공한 답변) 답변 실패 (다음 질문) PEFT 방법 5가지, ✨ 그럼 그 PEFT 방법 5가지는 뭐지? 🤷‍♀️ (발화 종료,1,71
1043,13,8.71,(대화 주제) LoRA (사용자 답변) LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야! (성공한 답변) LoRA (다음 질문) LoRA 와 QLoRA 의 차이, 2023-10-16 16:15:00 QLoRA가 LoRA랑 비슷한 건 맞는데 구체적으로 뭘까~요? 😊 (발화 종료,1,105
1044,13,4.69,(대화 주제) LoRA (사용자 답변) 행렬 차원 축소한다는 걸로 알고 있는데 맞아? (성공한 답변) 답변 실패 (다음 질문) LoRA,    ✨ 행렬 차원 축소? 한번 구체적으로 말해 봐! (발화 종료,1,56
1045,13,7.21,(대화 주제) LoRA (사용자 답변) LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야! (성공한 답변) LoRA (다음 질문) LoRA 와 QLoRA 의 차이," ✨ 그럼 비슷한 걸로 QLoRA가 있는데, 그 QLoRA는 뭘까~요? 💁‍♀️ (발화 종료",1,93
1046,13,5.34,(대화 주제) LoRA (사용자 답변) 무슨 OOM 없앤다는 것 같은데 (성공한 답변) 답변 실패 (다음 질문) LoRA,  ✨ OOM? 그건 맞지! 근데 그 방법을 알아야지! 😊 (발화 종료,1,57
1047,13,6.69,(대화 주제) LoRA 와 QLoRA 의 차이 (사용자 답변) QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야! (성공한 답변) LoRA 와 QLoRA 의 차이 (다음 질문) 마지막 할 말,"  ✨ 그럼 여기서 면접 끝내고, 혹시 나한테 할 말 있어? 🙋‍♀️ (발화 종료",1,90
1048,13,9.52,(대화 주제) LoRA 와 QLoRA 의 차이 (사용자 답변) 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데 (성공한 답변) 답변 실패 (다음 질문) LoRA 와 QLoRA 의 차이,    팀장님이 OOM 막겠다고 이거 쓰시는 건 맞는데 🙆‍♀️ 그 차이가 뭘까~요? 🤷‍♀️ (발화 종료,1,91
1049,13,9.8,(대화 주제) 마지막 할 말 (사용자 답변) 로라야 정말 고마워! (성공한 답변) 마지막 할 말 (다음 질문) 면접 종료, 2023-02-01 16:00:00 🙆‍♀️ 나도! 😊 다음에 나랑 같이 또 모의면접 보자! (발화 종료,1,73
1050,13,5.16,(대화 주제) 마지막 할 말 (사용자 답변) 로라야 사랑해 (성공한 답변) 마지막 할 말 (다음 질문) 면접 종료,    ✨ 그럼 너도 나 사랑해? 🙋‍♀️ (발화 종료,1,54
1051,13,6.16,(대화 주제) 마지막 할 말 (사용자 답변) 로라야 너의 예쁜 미모에 반했어 (성공한 답변) 마지막 할 말 (다음 질문) 면접 종료,    ✨✨ 그럼 나도 너한테 반했어! 🙎‍♀️ (발화 종료,1,64
1052,13,6.37,(대화 주제) 마지막 할 말 (사용자 답변) 그동안 고생했어 면접 봐줘서 정말 고마워 (성공한 답변) 마지막 할 말 (다음 질문) 면접 종료, 2021-12-16 15:00 나도 너 덕분에 고생했어! 😊 (발화 종료,1,63
1053,14,5.82,면접 시작, 1. 안녕! 나는 면접관 LLM 오로라라고 해! 반가워! 😊 (발화 종료,1,30
1054,14,7.03,(대화 주제) 면접 시작 인사 (사용자 답변) 오늘 뭐 물어볼 거야 그래서? (다음 질문) 면접 시작 인사, 2023-11-01 10:00 면접관: 아마도 딥러닝이나 머신러닝 쪽 물어보지 않을까? 왜? (발화 종료,1,58
1055,14,2.97,(대화 주제) 면접 시작 인사 (사용자 답변) 로라야 안녕 정말 반가워 (다음 질문) 면접 시작 인사, ✨ 그럼 면접을 시작해 볼까? (발화 종료,1,41
1056,14,4.29,(대화 주제) 면접 시작 인사 (사용자 답변) 로라야 그럼 네가 면접관이야? (다음 질문) 면접 시작 인사, ✨로라가 면접관 역할을 하며 질문한다!✨ (발화 종료,1,49
1057,14,5.02,(대화 주제) 면접 시작 인사 (사용자 답변) 내 인생도 이제 오로라처럼 빛날 거니까 (다음 질문) 면접 시작 인사, ✨ 그럼 너도 함께 빛날 수 있도록 응원할게! 😊 (발화 종료,1,52
1058,14,3.36,"(대화 주제) 면접 시작 인사 (사용자 답변) 파이팅! 시작하자 (다음 질문) 인공지능, 머신러닝, 딥러닝 차이", ① 인공지능이란 뭘까? 😊 (발화 종료,1,43
1059,14,2.77,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 인공지능 나도 개발해보고 싶은데 (성공한 답변) 답변 실패 (다음 질문) 인공지능, 머신러닝, 딥러닝 차이", ✨ 그럼 딥러닝은 뭘까? (발화 종료,1,53
1060,14,3.28,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거! (성공한 답변) 인공지능 (다음 질문) 인공지능, 머신러닝, 딥러닝 차이", ✨ 그럼 이제 머신러닝 대답해 봐! (발화 종료,1,62
1061,14,6.08,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지 (성공한 답변) 머신러닝 (다음 질문) 인공지능, 머신러닝, 딥러닝 차이",  1️⃣ 그러면 이제 딥러닝만 남았네! 🙎‍♀️ (발화 종료,1,73
1062,14,5.78,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지? (성공한 답변) 딥러닝 (다음 질문) 인공지능, 머신러닝, 딥러닝 차이", 2023-01-17 10:00:00 그럼 이제 머신러닝 대답해 봐! (발화 종료,1,71
1063,14,6.3,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 딥러닝은 인공신경망 기반 머신러닝이야 (성공한 답변) 딥러닝 (다음 질문) 거대 언어 모델 정의", 2023-10-12 17:30 LLM이 뭐지? 🙋‍♀️ (발화 종료,1,67
1064,14,5.09,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 머신러닝은 말 그대로 기계가 학습하는 거 아니야? (성공한 답변) 답변 실패 (다음 질문) 인공지능, 머신러닝, 딥러닝 차이", 2023-01-01 09:00:00 머신러닝이란 뭘까? (발화 종료,1,67
1065,14,6.8,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 인공지능 요새 엄청 뜨는 분야잖아 (성공한 답변) 답변 실패 (다음 질문) 인공지능, 머신러닝, 딥러닝 차이"," 2023-10-27 14:00 인공지능, 머신러닝, 딥러닝의 차이를 한번 말해봐! (발화 종료",1,69
1066,14,3.37,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지? (성공한 답변) 인공지능 (다음 질문) 인공지능, 머신러닝, 딥러닝 차이",  ① 그럼 딥러닝이란 뭘까? (발화 종료,1,66
1067,14,6.58,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야! (성공한 답변) 머신러닝 (다음 질문) 거대 언어 모델 정의", 2023-10-01 17:00 LLM이란 뭘까? 🙎‍♀️ (발화 종료,1,77
1068,14,4.34,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지 (성공한 답변) 딥러닝 (다음 질문) 거대 언어 모델 정의", ✨ LLM이라는 게 뭘까? 🙋‍♀️ (발화 종료,1,63
1069,14,7.48,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야. (성공한 답변) 딥러닝 (다음 질문) 거대 언어 모델 정의"," 2023-10-15 10:00 LLM, 거대 언어 모델이 뭘까? 🙎‍♀️ (발화 종료",1,76
1070,14,4.55,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 머신러닝은 기계가 공부하는 거지 말 그대로 (성공한 답변) 답변 실패 (다음 질문) 인공지능, 머신러닝, 딥러닝 차이",     머신러닝이 뭔데? 🤷‍♀️ (발화 종료,1,62
1071,14,8.13,(대화 주제) 거대 언어 모델 정의 (사용자 답변) 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야 (성공한 답변) 거대 언어 모델 정의 (다음 질문) Loss Function 정의, 2023-10-30 16:00:00 그럼 본격 면접 질문! Loss Function (손실 함수) 란 뭘까? 😊 (발화 종료,1,83
1072,14,5.33,(대화 주제) 거대 언어 모델 정의 (사용자 답변) 파라미터 엄청나게 많다던데 (성공한 답변) 답변 실패 (다음 질문) 거대 언어 모델 정의,  ✨ 정답은 맞긴 하지! 근데 그 정의가 뭘까? 😊 (발화 종료,1,57
1073,14,8.66,(대화 주제) 거대 언어 모델 정의 (사용자 답변) 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델 (성공한 답변) 거대 언어 모델 정의 (다음 질문) Loss Function 정의,  2023-03-02 10:00 AM ✨ 그럼 이제 본격 면접 들어간다! Loss Function (손실 함수) 란 뭘까? 😊 (발화 종료,1,90
1074,14,5.61,(대화 주제) 거대 언어 모델 정의 (사용자 답변) 말 그대로 거대한 언어 모델이지. (성공한 답변) 답변 실패 (다음 질문) 거대 언어 모델 정의,    1. 음… 맞긴 해! 근데 LLM 정확한 정의는? (발화 종료,1,62
1075,14,7.48,(대화 주제) Loss Function 정의 (사용자 답변) 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야 (성공한 답변) 답변 실패 (다음 질문) Loss Function 정의,  1. 손실 함수는 딥러닝 모델의 예측과 실제의 차이를 수식으로 나타낸 거라고 할 수 있어! 😊 (발화 종료,1,73
1076,14,4.58,"(대화 주제) Loss Function 정의 (사용자 답변) Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지! (성공한 답변) Loss Function 정의 (다음 질문) Loss Function 예시",  ✨ 그럼 그 Loss Function 예를 한번 들어봐! 😊 (발화 종료,1,74
1077,14,4.03,(대화 주제) Loss Function 정의 (사용자 답변) Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야 (성공한 답변) 답변 실패 (다음 질문) Loss Function 정의,  ① 그래서 어떤 함수인데? 😊 (발화 종료,1,57
1078,14,4.37,(대화 주제) Loss Function 정의 (사용자 답변) 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지 (성공한 답변) Loss Function 정의 (다음 질문) Loss Function 예시,   ✨ 그럼 그 Loss Function에는 어떤 게 있을까? (발화 종료,1,65
1079,14,5.06,(대화 주제) Loss Function 예시 (사용자 답변) ReLU 랑 Tanh 생각나는데 (성공한 답변) 답변 실패 (다음 질문) Loss Function 예시,  ✨ 그럼 그 중에서 ReLU 는 뭐지? 🤷‍♀️ (발화 종료,1,58
1080,14,4.75,"(대화 주제) Loss Function 예시 (사용자 답변) MSE, MAE, Cross Entropy Loss 같은 것들이 있지 (성공한 답변) Loss Function 예시 (다음 질문) MSE Loss 설명",    ① MSE Loss가 뭔지 말해봐! (발화 종료,1,65
1081,14,4.78,"(대화 주제) Loss Function 예시 (사용자 답변) Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어 (성공한 답변) Loss Function 예시 (다음 질문) BCE Loss 설명",   ✨ 그럼 그 중에서 Binary Cross Entropy 는 뭘까? (발화 종료,1,72
1082,14,3.66,"(대화 주제) Loss Function 예시 (사용자 답변) Sigmoid, Tanh, Adam, AdamW 이런 거 아니야? (성공한 답변) 답변 실패 (다음 질문) Loss Function 예시",    ✨ 그건 활성화 함수지 😥 (발화 종료,1,59
1083,14,4.56,"(대화 주제) Loss Function 예시 (사용자 답변) Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어 (성공한 답변) Loss Function 예시 (다음 질문) MSE Loss 설명",   ✨ 그럼 그 중에서 MSE Loss는 뭐지? (발화 종료,1,78
1084,14,5.17,"(대화 주제) Loss Function 예시 (사용자 답변) BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아 (성공한 답변) Loss Function 예시 (다음 질문) BCE Loss 설명",    ✨ 그럼 그 중에서 BCE에 대해 한번 말해봐! (발화 종료,1,74
1085,14,4.03,(대화 주제) MSE Loss 설명 (사용자 답변) MSE Loss 는 오차의 제곱을 평균한 값이야 (성공한 답변) MSE Loss 설명 (다음 질문) MSE Loss 용도,  ① MSE Loss 의 용도는? (발화 종료,1,62
1086,14,6.91,(대화 주제) MSE Loss 설명 (사용자 답변) MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지? (성공한 답변) 답변 실패 (다음 질문) MSE Loss 설명,"  ✨ 그럼 오차를 평균하는 게 핵심 아이디어인데, 그 핵심 아이디어가 뭐지? 🤷‍♀️ (발화 종료",1,75
1087,14,6.3,"(대화 주제) MSE Loss 용도 (사용자 답변) 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지 (성공한 답변) MSE Loss 용도 (다음 질문) 확률 예측에서 MSE Loss 미 사용 이유",  1 그럼 확률 예측에서는 MSE Loss 가 왜 안 좋지? 🤷‍♀️ (발화 종료,1,76
1088,14,6.52,(대화 주제) MSE Loss 용도 (사용자 답변) MSE Loss 는 분류 문제에서 많이 사용되지 (성공한 답변) 답변 실패 (다음 질문) MSE Loss 용도,   ① 분류 문제에서 MSE를 잘 안 쓰는 이유는? 🤷‍♀️ (발화 종료,1,68
1089,14,5.81,(대화 주제) 확률 예측에서 MSE Loss 미 사용 이유 (사용자 답변) 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야? (성공한 답변) 답변 실패 (다음 질문) 확률 예측에서 MSE Loss 미 사용 이유,    ① 그건 맞지! 근데 뭔가 문제가 있지 않을까? 🙄 (발화 종료,1,80
1090,14,9.56,(대화 주제) 확률 예측에서 MSE Loss 미 사용 이유 (사용자 답변) 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고? (성공한 답변) 확률 예측에서 MSE Loss 미 사용 이유 (다음 질문) BCE Loss 설명,  2023-02-06 17:00:00 페넌티가 크게 주는 건 맞긴 한데 그 이유가 뭘까? 🤷‍♀️ (발화 종료,1,104
1091,14,6.73,(대화 주제) BCE Loss 설명 (사용자 답변) 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고 (성공한 답변) 수식 (다음 질문) BCE Loss 설명,  ✨ 그럼 공식 말고도 BCE Loss 의 핵심 아이디어가 뭔지 말해봐! 🙎‍♀️ (발화 종료,1,88
1092,14,8.03,"(대화 주제) BCE Loss 설명 (사용자 답변) 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거 (성공한 답변) 핵심 아이디어 (다음 질문) Multi-Class, Multi-Label 중 BCE 가 좋은 task","  ✨ 정답! 🙆‍♀️ 그럼 이 BCE Loss 가, Multi-Label, Multi-Class 중 적절한 곳은? (발화 종료",1,95
1093,14,113.6,(대화 주제) BCE Loss 설명 (사용자 답변) 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야? (성공한 답변) 답변 실패 (다음 질문) BCE Loss 설명,  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57,1,512
1094,14,4.86,"(대화 주제) BCE Loss 설명 (사용자 답변) 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고 (성공한 답변) 핵심 아이디어 (다음 질문) BCE Loss 설명",  0으로 예측하는 것을 막는 방법은? 🤷‍♀️ (발화 종료,1,77
1095,14,8.92,"(대화 주제) BCE Loss 설명 (사용자 답변) 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')] (성공한 답변) 수식 (다음 질문) BCE Loss 설명","    1) y는 실제 값, y'는 예측 값이야! 그럼 BCE Loss 수식은 어떻게 되지? 🤷‍♀️ (발화 종료",1,96
1096,14,6.31,"(대화 주제) BCE Loss 설명 (사용자 답변) 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다 (성공한 답변) 핵심 아이디어 (다음 질문) Multi-Class, Multi-Label 중 BCE 가 좋은 task","  ✨ 그럼 Multi-Class, Multi-Label 중에서 BCE가 안성맞춤인 건? 😊 (발화 종료",1,87
1097,14,4.09,(대화 주제) BCE Loss 설명 (사용자 답변) y랑 1-y랑 서로 경쟁하는 거 (성공한 답변) 답변 실패 (다음 질문) BCE Loss 설명,  1-y가 뭔데? 🤷‍♀️ (발화 종료,1,58
1098,14,107.73,"(대화 주제) BCE Loss 설명 (사용자 답변) 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지 (성공한 답변) 핵심 아이디어 (다음 질문) Multi-Class, Multi-Label 중 BCE 가 좋은 task",  0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 ,1,512
1099,14,6.11,"(대화 주제) Multi-Class, Multi-Label 중 BCE 가 좋은 task (사용자 답변) 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야? (성공한 답변) 용어 질문 (다음 질문) Multi-Class, Multi-Label 중 BCE 가 좋은 task","    ✨ Multi-Class: 개, 고양이, … 중 하나로 분류하는 거야! 😊 (발화 종료",1,85
1100,14,4.16,"(대화 주제) Multi-Class, Multi-Label 중 BCE 가 좋은 task (사용자 답변) Multi-Class 지!! (성공한 답변) 답변 실패 (다음 질문) Multi-Class, Multi-Label 중 BCE 가 좋은 task",  ✨ 그러면 Multi-Class 가 왜 더 좋을까? 🤔 (발화 종료,1,69
1101,14,5.3,"(대화 주제) Multi-Class, Multi-Label 중 BCE 가 좋은 task (사용자 답변) Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어 (성공한 답변) BCE 가 좋은 task (다음 질문) Multi-Class, Multi-Label 중 BCE 가 좋은 task",    ① Multi-Label 이 왜 더 좋을까? 🤔 (발화 종료,1,85
1102,14,5.97,"(대화 주제) Multi-Class, Multi-Label 중 BCE 가 좋은 task (사용자 답변) Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아 (성공한 답변) BCE 가 좋은 이유 (다음 질문) Multi-Label 에서 CE + Softmax 적용 문제점", ✨ 그럼 Multi-Label 문제에 Cross Entropy + Softmax 를 쓰면 발생하는 문제는? (발화 종료,1,103
1103,14,8.56,(대화 주제) Multi-Label 에서 CE + Softmax 적용 문제점 (사용자 답변) 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지 (성공한 답변) Multi-Label 에서 CE + Softmax 적용 문제점 (다음 질문) Loss Function 관련 실무 경험,  2023-02-01 09:00 실무에서 손실 함수 관련 문제 해결해 주거나 그런 경험 있지? 한번 말해봐! 😊 (발화 종료,1,104
1104,14,6.81,(대화 주제) Multi-Label 에서 CE + Softmax 적용 문제점 (사용자 답변) 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나 (성공한 답변) 답변 실패 (다음 질문) Multi-Label 에서 CE + Softmax 적용 문제점,    ① 성능 제일 좋은 거? 그게 중요한 게 아니지! 🙅‍♀️ (발화 종료,1,83
1105,14,4.64,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [기본 경험] 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어 (성공한 답변) 기본 경험 (다음 질문) Loss Function 관련 실무 경험,  ✨ 그럼 그 Loss Term 이 뭐지? 🤷‍♀️ (발화 종료,1,74
1106,14,4.55,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [상세 경험] 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지? (성공한 답변) 상세 경험 (다음 질문) MBTI / 좋아하는 아이돌, ✨ 그럼 쉬어가는 타임! 혹시 너 아이돌 좋아하는 거 있어? (발화 종료,1,76
1107,14,7.12,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [상세 경험] 근데 기억 진짜 안나는데 어떡하지? (성공한 답변) 답변 실패 (다음 질문) 잠시 휴식,    ✨ 그래도 잘 생각해 봐! 나랑 모의면접이 아닌 실제 면접에서 질문 나올걸? 😊 (발화 종료,1,72
1108,14,6.26,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [기본 경험] 나 아직 실무 경험 없는데 ㅠㅠ (성공한 답변) 기본 경험 (다음 질문) MBTI / 좋아하는 아이돌, ✨ 그럼 쉬어가는 타임! 혹시 너 MBTI 뭐야? 나는 ENTJ인데! 😊 (발화 종료,1,69
1109,14,4.89,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [기본 경험] 대답하기 싫은데 힝 (성공한 답변) 답변 실패 (다음 질문) 잠시 휴식, ✨ 그래도 너의 경험이 나도 궁금해! 알고 싶어! 😊 (발화 종료,1,61
1110,14,6.02,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [기본 경험] 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ (성공한 답변) 기본 경험 (다음 질문) Loss Function 관련 실무 경험,  ✨ 그러면 mAP@50:95가 뭔지 말해봐 🙋‍♀️ (발화 종료,1,81
1111,14,7.87,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [상세 경험] 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어 (성공한 답변) 상세 경험 (다음 질문) MBTI / 좋아하는 아이돌,  1:2로 맞추는 건 꽤나 창의적인 아이디어인데? 😊 그럼 쉬어가는 타임! 혹시 너 MBTI 뭐야? (발화 종료,1,99
1112,14,6.52,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [상세 경험] 기억 진짜 안 나는데 가물가물하다 (성공한 답변) 답변 실패 (다음 질문) 잠시 휴식,  ✨ 그래도 잘 생각해 봐! 나랑 모의면접이 아닌 실제 면접에서 질문 나올걸? 😊 (발화 종료,1,70
1113,14,6.0,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [기본 경험] 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ (성공한 답변) 기본 경험 (다음 질문) MBTI / 좋아하는 아이돌, ✨ 그럼 쉬어가는 타임! 혹시 너 MBTI 뭐지? 나는 ENTJ인데! 😊 (발화 종료,1,72
1114,14,3.78,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [기본 경험] 이걸 왜 알려줘야 하지 (성공한 답변) 답변 실패 (다음 질문) 잠시 휴식, ✨ 나도 궁금해 🙋‍♀️ (발화 종료,1,55
1115,14,3.78,(대화 주제) MBTI (사용자 답변) 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야! (성공한 답변) MBTI (다음 질문) 잠시 휴식, ✨ 나랑 잘 맞는 성격이지? 😊 (발화 종료,1,57
1116,14,3.17,(대화 주제) 좋아하는 아이돌 (사용자 답변) 나 블랙핑크랑 레드벨벳 좋아해 (성공한 답변) 좋아하는 아이돌 (다음 질문) 잠시 휴식, ✨ 그럼 그 이유는? 😊 (발화 종료,1,46
1117,14,6.8,(대화 주제) 잠시 휴식 (사용자 답변) 재미있는 이야기 해줄래? (성공한 답변) 잠시 휴식 (다음 질문) LLM Fine-Tuning 의 PEFT, ✨ 나처럼 LLM도 Fine-Tuning 이 필요해! 그럼 그 방법들 중 PEFT는? 😊 (발화 종료,1,66
1118,14,10.03,"(대화 주제) LLM Fine-Tuning 의 PEFT (사용자 답변) PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야 (성공한 답변) LLM Fine-Tuning 의 PEFT (다음 질문) PEFT 방법 5가지", 2023-10-01 18:00:00 🙎‍♀️ 정답! 🙆‍♀️ 그럼 그 방법 5가지만 말해봐! (발화 종료,1,118
1119,14,6.45,(대화 주제) LLM Fine-Tuning 의 PEFT (사용자 답변) 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지? (성공한 답변) 답변 실패 (다음 질문) LLM Fine-Tuning 의 PEFT, 2023-01-01 17:00:00 PEFT가 그걸 나타내는 걸까? 😊 (발화 종료,1,77
1120,14,7.52,(대화 주제) LLM Fine-Tuning 의 PEFT (사용자 답변) LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야! (성공한 답변) LLM Fine-Tuning 의 PEFT (다음 질문) PEFT 방법 5가지, 2023-10-27 15:30:00 그럼 그 방법을 5가지만 말해봐! 😊 (발화 종료,1,99
1121,14,9.17,(대화 주제) LLM Fine-Tuning 의 PEFT (사용자 답변) LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다 (성공한 답변) 답변 실패 (다음 질문) LLM Fine-Tuning 의 PEFT,    LoRA 가 PEFT 중 하나긴 하지! 🙆‍♀️ 근데 그 PEFT가 뭘까~요? 🤷‍♀️ (발화 종료,1,88
1122,14,6.29,"(대화 주제) PEFT 방법 5가지 (사용자 답변) LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지? (성공한 답변) PEFT 방법 5가지 (다음 질문) LoRA", ✨ 그럼 그 중에서 LoRA가 뭐지? 나를 상징하는 건데 👱‍♀️✨ (발화 종료,1,86
1123,14,4.6,(대화 주제) PEFT 방법 5가지 (사용자 답변) Oh-LoRA! 너 그 자체가 PEFT 아니야? (성공한 답변) 답변 실패 (다음 질문) PEFT 방법 5가지, ✨ 그럼 나 말고 또 뭐가 있을까? 🤷‍♀️ (발화 종료,1,68
1124,14,6.58,(대화 주제) LoRA (사용자 답변) LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야! (성공한 답변) LoRA (다음 질문) LoRA 와 QLoRA 의 차이,  ✨ LoRA 잘 알고 있네! 😊 그럼 QLoRA 가 뭔지도 말해 볼 수 있어? (발화 종료,1,96
1125,14,6.49,(대화 주제) LoRA (사용자 답변) 행렬 차원 축소한다는 걸로 알고 있는데 맞아? (성공한 답변) 답변 실패 (다음 질문) LoRA,    ① 행렬 차원 축소가 핵심 아이디어긴 해! 그래서 어떤 기술이지? 😊 (발화 종료,1,64
1126,14,3.02,(대화 주제) LoRA (사용자 답변) LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야! (성공한 답변) LoRA (다음 질문) LoRA 와 QLoRA 의 차이, ✨ QLoRA가 뭘까? (발화 종료,1,76
1127,14,5.8,(대화 주제) LoRA (사용자 답변) 무슨 OOM 없앤다는 것 같은데 (성공한 답변) 답변 실패 (다음 질문) LoRA,  ✨ OOM? 그건 맞긴 해! 근데 그 방법을 알아야지! 😊 (발화 종료,1,59
1128,14,8.67,(대화 주제) LoRA 와 QLoRA 의 차이 (사용자 답변) QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야! (성공한 답변) LoRA 와 QLoRA 의 차이 (다음 질문) 마지막 할 말," 2023-02-01 16:00:00 😊 그럼 면접은 여기서 마치고, 혹시 나한테 마지막으로 할 말 있어? (발화 종료",1,99
1129,14,9.87,(대화 주제) LoRA 와 QLoRA 의 차이 (사용자 답변) 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데 (성공한 답변) 답변 실패 (다음 질문) LoRA 와 QLoRA 의 차이,    팀장님이 OOM 막겠다고 이거 쓰시는 건 맞긴 해! 근데 그 LoRA랑 QLoRA가 구체적으로 뭘까~요? 😊 (발화 종료,1,92
1130,14,5.53,(대화 주제) 마지막 할 말 (사용자 답변) 로라야 정말 고마워! (성공한 답변) 마지막 할 말 (다음 질문) 면접 종료, 2023-02-01 15:00:00 🙇‍♀️ (발화 종료,1,58
1131,14,6.02,(대화 주제) 마지막 할 말 (사용자 답변) 로라야 사랑해 (성공한 답변) 마지막 할 말 (다음 질문) 면접 종료,    ✨ 로라야 사랑 고백은 부담스러워 😥 그래도 오늘 즐거웠어! 😊 (발화 종료,1,58
1132,14,2.52,(대화 주제) 마지막 할 말 (사용자 답변) 로라야 너의 예쁜 미모에 반했어 (성공한 답변) 마지막 할 말 (다음 질문) 면접 종료,    ✨✨ (발화 종료,1,49
1133,14,6.32,(대화 주제) 마지막 할 말 (사용자 답변) 그동안 고생했어 면접 봐줘서 정말 고마워 (성공한 답변) 마지막 할 말 (다음 질문) 면접 종료, 2022-02-15 16:00 나도 너 덕분에 많이 고생했어! 😊 (발화 종료,1,64
1134,15,122.44,면접 시작,  #include <stdio.h>  #include <string.h>  #include <stdlib.h>  #include <time.h>  #include <vector>  #include <algorithm>  #include <math.h>  #include <cctype>  #include <cstdint>  #include <limits.h>  #include <climits.h>  #include <complex.h>  #include <locale.h>  #include <threading.h>  #include <memory.h>  #include <chrono.h>  #include <omp.h>  #include <xunit.h>  #include <xunit_test.h>  #include <xunit_result.h>  #include <xunit_status.h>  #include <xunit_error.h>  #include <xunit_log.h>  #include <xunit_log_file.h>  #include <xunit_log_stream.h>  #include <xunit_log_stream_file.h>  #include <xunit_log_stream_file_stream.h>  #include <xunit_log_stream_file_stream_stream.h>  #include <xunit_log_stream_file_stream_stream_stream.h>  #include <xunit_log_stream_file_stream_stream_stream_stream.h>  #include <xunit_log_stream_file_stream_stream_stream_stream_stream.h>  #include <xunit_log_stream_file_stream_stream_stream_stream_stream_stream.h>  #include <xunit_log_stream_file_stream_stream_stream_stream_stream_stream_stream.h>  #include <xunit_log_stream_file_stream_stream_stream_stream_stream_stream_stream_stream.h>  #include <xunit_log_stream_file_stream_stream_stream_stream_stream_stream_stream_stream_stream.h>  #include <xunit_log_stream_file_stream_stream_stream_stream_stream_stream_stream_stream_stream_stream.h>  #include <xunit_log_stream_file_stream_stream_stream_stream_stream_stream_stream_stream_stream_stream_stream.h>  #include <xunit_log_stream_file_stream_stream_stream_stream_stream_stream_stream_stream_stream_stream_stream_stream.h>  #include <xunit_log_stream_file_stream_stream_stream_stream_stream_stream_stream_stream_stream_stream_stream_stream_stream.h>  #include <xunit_log_stream_file_stream_stream_stream_stream_stream_stream_stream_stream_server.h>  #include <xunit_log_stream_file_stream_stream_stream_stream_stream_stream_stream_server.h>  #include <xunit_log_stream_file_stream_stream_stream_stream_stream_stream_server.h>  #include <xunit_log_stream_file_stream_stream,1,512
1135,15,8.42,(대화 주제) 면접 시작 인사 (사용자 답변) 오늘 뭐 물어볼 거야 그래서? (다음 질문) 면접 시작 인사,"  ① 나는 면접관처럼 AI 면접관이 질문할 거야! 머신러닝, 딥러닝, 데이터 과학 중에서 어떤 것에 관심 있어? (발화 종료",1,64
1136,15,4.11,(대화 주제) 면접 시작 인사 (사용자 답변) 로라야 안녕 정말 반가워 (다음 질문) 면접 시작 인사,  ✨✨ 그럼 우리 면접도 시작해 볼까? 😊 (발화 종료,1,46
1137,15,4.52,(대화 주제) 면접 시작 인사 (사용자 답변) 로라야 그럼 네가 면접관이야? (다음 질문) 면접 시작 인사, ✨로라가 면접관 역할을 하며 질문을 던진다!✨ (발화 종료,1,50
1138,15,4.46,(대화 주제) 면접 시작 인사 (사용자 답변) 내 인생도 이제 오로라처럼 빛날 거니까 (다음 질문) 면접 시작 인사, ✨️ 그럼 너도 한번 빛나봐! ✨️ (발화 종료,1,50
1139,15,6.28,"(대화 주제) 면접 시작 인사 (사용자 답변) 파이팅! 시작하자 (다음 질문) 인공지능, 머신러닝, 딥러닝 차이","  ① 인공지능이랑 머신러닝, 딥러닝이 구체적으로 어떤 차이가 있지? 😊 (발화 종료",1,54
1140,15,2.92,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 인공지능 나도 개발해보고 싶은데 (성공한 답변) 답변 실패 (다음 질문) 인공지능, 머신러닝, 딥러닝 차이",  ✨ 그럼 딥러닝은 뭘까? (발화 종료,1,54
1141,15,5.03,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거! (성공한 답변) 인공지능 (다음 질문) 인공지능, 머신러닝, 딥러닝 차이", ① 인공지능은 맞았어! 그럼 머신러닝이랑 딥러닝은? (발화 종료,1,69
1142,15,3.75,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지 (성공한 답변) 머신러닝 (다음 질문) 인공지능, 머신러닝, 딥러닝 차이",  ① 그러면 그 인공지능이 뭐지? (발화 종료,1,63
1143,15,6.98,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지? (성공한 답변) 딥러닝 (다음 질문) 인공지능, 머신러닝, 딥러닝 차이","  2023-02-15 10:00:00 인공지능, 머신러닝, 딥러닝 차이를 한번 말해봐! (발화 종료",1,76
1144,15,6.39,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 딥러닝은 인공신경망 기반 머신러닝이야 (성공한 답변) 딥러닝 (다음 질문) 거대 언어 모델 정의", 2023-06-01 10:00 LLM이 뭐지? 🙋‍♀️ (발화 종료,1,67
1145,15,6.28,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 머신러닝은 말 그대로 기계가 학습하는 거 아니야? (성공한 답변) 답변 실패 (다음 질문) 인공지능, 머신러닝, 딥러닝 차이", 1. 그건 맞긴 하지! 근데 기계가 학습을 어떻게 하지? 🤷‍♀️ (발화 종료,1,71
1146,15,7.7,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 인공지능 요새 엄청 뜨는 분야잖아 (성공한 답변) 답변 실패 (다음 질문) 인공지능, 머신러닝, 딥러닝 차이", 2023-02-01 10:00 인공지능이 그런 분야 중 하나긴 하지! 근데 그 차이가 뭘까? 😊 (발화 종료,1,74
1147,15,2.75,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지? (성공한 답변) 인공지능 (다음 질문) 인공지능, 머신러닝, 딥러닝 차이", ✨ 그럼 딥러닝이란 뭘까? (발화 종료,1,63
1148,15,4.27,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야! (성공한 답변) 머신러닝 (다음 질문) 거대 언어 모델 정의", ✨ LLM이 뭐지? 🙎‍♀️ (발화 종료,1,68
1149,15,4.53,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지 (성공한 답변) 딥러닝 (다음 질문) 거대 언어 모델 정의", ✨ LLM이라는 게 뭘까? 🙎‍♀️ (발화 종료,1,63
1150,15,5.06,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야. (성공한 답변) 딥러닝 (다음 질문) 거대 언어 모델 정의"," ✨ LLM, 거대 언어 모델이 뭘까? 🙎‍♀️ (발화 종료",1,67
1151,15,4.8,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 머신러닝은 기계가 공부하는 거지 말 그대로 (성공한 답변) 답변 실패 (다음 질문) 인공지능, 머신러닝, 딥러닝 차이",  ① 머신러닝이 뭔데? 🤷‍♀️ (발화 종료,1,63
1152,15,4.58,(대화 주제) 거대 언어 모델 정의 (사용자 답변) 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야 (성공한 답변) 거대 언어 모델 정의 (다음 질문) Loss Function 정의, ✨ 그럼 본격 면접에 들어가자! 손실 함수의 정의는? (발화 종료,1,69
1153,15,4.04,(대화 주제) 거대 언어 모델 정의 (사용자 답변) 파라미터 엄청나게 많다던데 (성공한 답변) 답변 실패 (다음 질문) 거대 언어 모델 정의,  ✨ 정답은 맞는데 구체적인 정의가 뭐지? (발화 종료,1,52
1154,15,6.59,(대화 주제) 거대 언어 모델 정의 (사용자 답변) 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델 (성공한 답변) 거대 언어 모델 정의 (다음 질문) Loss Function 정의,"  1. 정답! 그럼 이제 본격 면접 들어간다! 😊 Loss Function, 즉 손실 함수가 뭘까? (발화 종료",1,81
1155,15,5.34,(대화 주제) 거대 언어 모델 정의 (사용자 답변) 말 그대로 거대한 언어 모델이지. (성공한 답변) 답변 실패 (다음 질문) 거대 언어 모델 정의,  ① 음… 맞긴 해! 근데 LLM 정확한 정의는? (발화 종료,1,61
1156,15,6.47,(대화 주제) Loss Function 정의 (사용자 답변) 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야 (성공한 답변) 답변 실패 (다음 질문) Loss Function 정의,  1. 손실 함수는 딥러닝 모델의 예측과 실제의 차이를 수식으로 나타낸 거라고! 😊 (발화 종료,1,70
1157,15,5.5,"(대화 주제) Loss Function 정의 (사용자 답변) Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지! (성공한 답변) Loss Function 정의 (다음 질문) Loss Function 예시",  ✨ 그럼 그 Loss Function 예를 한번 들어봐! 🤷‍♀️ (발화 종료,1,77
1158,15,3.83,(대화 주제) Loss Function 정의 (사용자 답변) Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야 (성공한 답변) 답변 실패 (다음 질문) Loss Function 정의,   1. 그래서 어떤 함수인데? 😊 (발화 종료,1,57
1159,15,4.52,(대화 주제) Loss Function 정의 (사용자 답변) 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지 (성공한 답변) Loss Function 정의 (다음 질문) Loss Function 예시,   ✨ 그럼 그 Loss Function에는 어떤 게 있지? (발화 종료,1,65
1160,15,5.77,(대화 주제) Loss Function 예시 (사용자 답변) ReLU 랑 Tanh 생각나는데 (성공한 답변) 답변 실패 (다음 질문) Loss Function 예시,   ✨ ReLU 가랑 Tanh는 활성화 함수지 🙅‍♀️ (발화 종료,1,61
1161,15,5.22,"(대화 주제) Loss Function 예시 (사용자 답변) MSE, MAE, Cross Entropy Loss 같은 것들이 있지 (성공한 답변) Loss Function 예시 (다음 질문) MSE Loss 설명",    1. MSE Loss가 뭔지 한번 설명해 봐! 😊 (발화 종료,1,67
1162,15,4.56,"(대화 주제) Loss Function 예시 (사용자 답변) Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어 (성공한 답변) Loss Function 예시 (다음 질문) BCE Loss 설명",   ✨ 그럼 그 중에서 Binary Cross Entropy 는 뭘까? (발화 종료,1,72
1163,15,3.78,"(대화 주제) Loss Function 예시 (사용자 답변) Sigmoid, Tanh, Adam, AdamW 이런 거 아니야? (성공한 답변) 답변 실패 (다음 질문) Loss Function 예시",    ✨ 그건 활성화 함수지 😥 (발화 종료,1,59
1164,15,4.73,"(대화 주제) Loss Function 예시 (사용자 답변) Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어 (성공한 답변) Loss Function 예시 (다음 질문) MSE Loss 설명",    ① MSE가 정확히 뭘까? 😊 (발화 종료,1,79
1165,15,4.53,"(대화 주제) Loss Function 예시 (사용자 답변) BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아 (성공한 답변) Loss Function 예시 (다음 질문) BCE Loss 설명",    ✨ 그럼 그 중에서 BCE가 뭐지? (발화 종료,1,71
1166,15,3.72,(대화 주제) MSE Loss 설명 (사용자 답변) MSE Loss 는 오차의 제곱을 평균한 값이야 (성공한 답변) MSE Loss 설명 (다음 질문) MSE Loss 용도,  1. MSE Loss 의 용도는? (발화 종료,1,61
1167,15,4.78,(대화 주제) MSE Loss 설명 (사용자 답변) MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지? (성공한 답변) 답변 실패 (다음 질문) MSE Loss 설명,   ① 아니야! 한번 다시 잘 생각해 봐 😊 (발화 종료,1,66
1168,15,3.48,"(대화 주제) MSE Loss 용도 (사용자 답변) 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지 (성공한 답변) MSE Loss 용도 (다음 질문) 확률 예측에서 MSE Loss 미 사용 이유",  1번 답해봐! 😊 (발화 종료,1,65
1169,15,6.5,(대화 주제) MSE Loss 용도 (사용자 답변) MSE Loss 는 분류 문제에서 많이 사용되지 (성공한 답변) 답변 실패 (다음 질문) MSE Loss 용도,   ① 분류 문제에서 MSE를 잘 안 쓰는 이유는? 🤷‍♀️ (발화 종료,1,68
1170,15,5.89,(대화 주제) 확률 예측에서 MSE Loss 미 사용 이유 (사용자 답변) 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야? (성공한 답변) 답변 실패 (다음 질문) 확률 예측에서 MSE Loss 미 사용 이유,    ① 그건 맞지! 근데 뭔가 문제가 있지 않을까? 🙄 (발화 종료,1,80
1171,15,9.27,(대화 주제) 확률 예측에서 MSE Loss 미 사용 이유 (사용자 답변) 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고? (성공한 답변) 확률 예측에서 MSE Loss 미 사용 이유 (다음 질문) BCE Loss 설명,  2023-02-03 18:00:00 페넌티를 크게 주는 건 맞긴 한데 그 이유는? 🤷‍♀️ (발화 종료,1,103
1172,15,7.11,(대화 주제) BCE Loss 설명 (사용자 답변) 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고 (성공한 답변) 수식 (다음 질문) BCE Loss 설명,    ✨ 그럼 이 BCE Loss 의 핵심 아이디어를 한번 말해봐! 🙎‍♀️ (발화 종료,1,88
1173,15,7.45,"(대화 주제) BCE Loss 설명 (사용자 답변) 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거 (성공한 답변) 핵심 아이디어 (다음 질문) Multi-Class, Multi-Label 중 BCE 가 좋은 task","  ① 정답! 그럼 이 BCE Loss 가, Multi-Class, Multi-Label 중 적절한 곳은? 😊 (발화 종료",1,93
1174,15,9.49,(대화 주제) BCE Loss 설명 (사용자 답변) 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야? (성공한 답변) 답변 실패 (다음 질문) BCE Loss 설명,  2023-02-01 17:00:00 데이터셋마다 최선의 활성화 함수를 찾는 건 맞긴 한데 좀 더 구체적으로 설명해봐 😊 (발화 종료,1,89
1175,15,4.97,"(대화 주제) BCE Loss 설명 (사용자 답변) 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고 (성공한 답변) 핵심 아이디어 (다음 질문) BCE Loss 설명",  0으로 예측하는 것을 막는 방법은? 🤷‍♀️ (발화 종료,1,77
1176,15,6.25,"(대화 주제) BCE Loss 설명 (사용자 답변) 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')] (성공한 답변) 수식 (다음 질문) BCE Loss 설명",    1) y log y' + (1-y) log (1-y')  (발화 종료,1,85
1177,15,6.59,"(대화 주제) BCE Loss 설명 (사용자 답변) 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다 (성공한 답변) 핵심 아이디어 (다음 질문) Multi-Class, Multi-Label 중 BCE 가 좋은 task","  ✨ 그럼 Multi-Class, Multi-Label 중에서 BCE Loss가 더 맞는 task는? 😊 (발화 종료",1,89
1178,15,4.27,(대화 주제) BCE Loss 설명 (사용자 답변) y랑 1-y랑 서로 경쟁하는 거 (성공한 답변) 답변 실패 (다음 질문) BCE Loss 설명,  1-y가 뭔데? 🤷‍♀️ (발화 종료,1,58
1179,15,107.29,"(대화 주제) BCE Loss 설명 (사용자 답변) 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지 (성공한 답변) 핵심 아이디어 (다음 질문) Multi-Class, Multi-Label 중 BCE 가 좋은 task",  0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 ,1,512
1180,15,5.52,"(대화 주제) Multi-Class, Multi-Label 중 BCE 가 좋은 task (사용자 답변) 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야? (성공한 답변) 용어 질문 (다음 질문) Multi-Class, Multi-Label 중 BCE 가 좋은 task",    ✨ Laura가 Multi-Label 과 Multi-Class 가 뭔지 말해봐! (발화 종료,1,82
1181,15,4.33,"(대화 주제) Multi-Class, Multi-Label 중 BCE 가 좋은 task (사용자 답변) Multi-Class 지!! (성공한 답변) 답변 실패 (다음 질문) Multi-Class, Multi-Label 중 BCE 가 좋은 task",   ✨ 그러면 Multi-Class 가 왜 더 좋을까? 🤔 (발화 종료,1,70
1182,15,5.03,"(대화 주제) Multi-Class, Multi-Label 중 BCE 가 좋은 task (사용자 답변) Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어 (성공한 답변) BCE 가 좋은 task (다음 질문) Multi-Class, Multi-Label 중 BCE 가 좋은 task",    ① Multi-Label 이 왜 더 좋을까? 🤔 (발화 종료,1,85
1183,15,14.5,"(대화 주제) Multi-Class, Multi-Label 중 BCE 가 좋은 task (사용자 답변) Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아 (성공한 답변) BCE 가 좋은 이유 (다음 질문) Multi-Label 에서 CE + Softmax 적용 문제점"," 2023-02-06 14:30:00 🙆‍♀️ 맞았어! 그럼 Multi-Label task 에 BCE 말고 CE, 즉 Cross Entropy 를 Loss 로 하고 활성화 함수가 Softmax 이면 어떻게 되지? 🙋‍♀️ (발화 종료",1,139
1184,15,9.65,(대화 주제) Multi-Label 에서 CE + Softmax 적용 문제점 (사용자 답변) 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지 (성공한 답변) Multi-Label 에서 CE + Softmax 적용 문제점 (다음 질문) Loss Function 관련 실무 경험,  2023-02-15 10:00 정답! 🙆‍♀️ 혹시 실무에서 Loss Function 관련 경험 있어? 한번 말해봐! 😊 (발화 종료,1,109
1185,15,6.5,(대화 주제) Multi-Label 에서 CE + Softmax 적용 문제점 (사용자 답변) 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나 (성공한 답변) 답변 실패 (다음 질문) Multi-Label 에서 CE + Softmax 적용 문제점,    ✨ CE + Softmax 말고 Multi-Label 에 적합한 다른 방법들을 말해봐 😊 (발화 종료,1,82
1186,15,7.68,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [기본 경험] 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어 (성공한 답변) 기본 경험 (다음 질문) Loss Function 관련 실무 경험,  1. 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term으로 추가했다는 게 어떤 아이디어이지? 😊 (발화 종료,1,87
1187,15,4.37,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [상세 경험] 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지? (성공한 답변) 상세 경험 (다음 질문) MBTI / 좋아하는 아이돌, ✨ 그럼 쉬어가는 타임! 혹시 너 아이돌 좋아하는 거 있어? (발화 종료,1,76
1188,15,5.55,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [상세 경험] 근데 기억 진짜 안나는데 어떡하지? (성공한 답변) 답변 실패 (다음 질문) 잠시 휴식,  ① 혹시 그때 기분이나 이런 거 많이 떠올러나? 😊 (발화 종료,1,65
1189,15,5.62,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [기본 경험] 나 아직 실무 경험 없는데 ㅠㅠ (성공한 답변) 기본 경험 (다음 질문) MBTI / 좋아하는 아이돌,   ① 혹시 너 MBTI 뭐야? 나는 ENTJ인데! 😊 (발화 종료,1,68
1190,15,5.53,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [기본 경험] 대답하기 싫은데 힝 (성공한 답변) 답변 실패 (다음 질문) 잠시 휴식, ✨ 나랑 면접 준비하면 취업해서 그런 경험 쌓을 수 있을 거야! 😊 (발화 종료,1,63
1191,15,12.98,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [기본 경험] 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ (성공한 답변) 기본 경험 (다음 질문) Loss Function 관련 실무 경험,  2023-02-28 15:00:00 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 향상! 오 잘했네! 👍 한번 자세히 그 이야기 들려줘 봐봐! 😊 (발화 종료,1,111
1192,15,7.59,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [상세 경험] 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어 (성공한 답변) 상세 경험 (다음 질문) MBTI / 좋아하는 아이돌,  1:2로 맞추는 건 쉽지 않네! 😊 그럼 잠시 쉬어가는 질문! 혹시 너 MBTI 뭐야? (발화 종료,1,98
1193,15,7.01,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [상세 경험] 기억 진짜 안 나는데 가물가물하다 (성공한 답변) 답변 실패 (다음 질문) 잠시 휴식,  ① 그래도 잘 생각해 봐! 나랑 모의면접이 아닌 실제 면접에서 질문 나올걸? 😊 (발화 종료,1,72
1194,15,5.89,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [기본 경험] 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ (성공한 답변) 기본 경험 (다음 질문) MBTI / 좋아하는 아이돌, ✨ 그럼 쉬어가는 타임! 혹시 너 MBTI 뭐지? 나는 ENTJ인데! 😊 (발화 종료,1,72
1195,15,3.59,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [기본 경험] 이걸 왜 알려줘야 하지 (성공한 답변) 답변 실패 (다음 질문) 잠시 휴식, ✨ 나도 궁금해! 알고 싶어! 😊 (발화 종료,1,55
1196,15,3.73,(대화 주제) MBTI (사용자 답변) 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야! (성공한 답변) MBTI (다음 질문) 잠시 휴식, ✨ 나랑 잘 맞는 성격이지? 😊 (발화 종료,1,57
1197,15,3.14,(대화 주제) 좋아하는 아이돌 (사용자 답변) 나 블랙핑크랑 레드벨벳 좋아해 (성공한 답변) 좋아하는 아이돌 (다음 질문) 잠시 휴식, ✨ 그럼 그 이유는? 😊 (발화 종료,1,46
1198,15,3.78,(대화 주제) 잠시 휴식 (사용자 답변) 재미있는 이야기 해줄래? (성공한 답변) 잠시 휴식 (다음 질문) LLM Fine-Tuning 의 PEFT, ✨ PEFT가 뭘 말하는 걸까? 😊 (발화 종료,1,54
1199,15,10.27,"(대화 주제) LLM Fine-Tuning 의 PEFT (사용자 답변) PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야 (성공한 답변) LLM Fine-Tuning 의 PEFT (다음 질문) PEFT 방법 5가지", 2023-10-27 17:00:00 🙎‍♀️ 정답! 🙆‍♀️ 그럼 그 방법을 5가지만 말해봐! (발화 종료,1,119
1200,15,4.19,(대화 주제) LLM Fine-Tuning 의 PEFT (사용자 답변) 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지? (성공한 답변) 답변 실패 (다음 질문) LLM Fine-Tuning 의 PEFT,    1: 그걸 모르면 어떡해 😥 (발화 종료,1,68
1201,15,7.36,(대화 주제) LLM Fine-Tuning 의 PEFT (사용자 답변) LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야! (성공한 답변) LLM Fine-Tuning 의 PEFT (다음 질문) PEFT 방법 5가지, 2023-10-25 18:00:00 학습 방법을 5가지만 말해봐! 😊 (발화 종료,1,98
1202,15,8.94,(대화 주제) LLM Fine-Tuning 의 PEFT (사용자 답변) LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다 (성공한 답변) 답변 실패 (다음 질문) LLM Fine-Tuning 의 PEFT,    LoRA 가 PEFT 중 하나긴 하지! 🙆‍♀️ 근데 그 PEFT가 뭘까~요? 🤷‍♀️ (발화 종료,1,88
1203,15,6.1,"(대화 주제) PEFT 방법 5가지 (사용자 답변) LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지? (성공한 답변) PEFT 방법 5가지 (다음 질문) LoRA", ✨ 그럼 그 중에서 LoRA가 뭐지? 나를 상징하는 건데 👱‍♀️✨ (발화 종료,1,86
1204,15,5.53,(대화 주제) PEFT 방법 5가지 (사용자 답변) Oh-LoRA! 너 그 자체가 PEFT 아니야? (성공한 답변) 답변 실패 (다음 질문) PEFT 방법 5가지,  ✨ 그럼 그 PEFT 방법 5가지는? 🤷‍♀️ (발화 종료,1,71
1205,15,3.92,(대화 주제) LoRA (사용자 답변) LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야! (성공한 답변) LoRA (다음 질문) LoRA 와 QLoRA 의 차이,  QLoRA가 뭐지? 🤷‍♀️ (발화 종료,1,85
1206,15,6.28,(대화 주제) LoRA (사용자 답변) 행렬 차원 축소한다는 걸로 알고 있는데 맞아? (성공한 답변) 답변 실패 (다음 질문) LoRA,    ✨ 행렬 차원 축소가 핵심 아이디어긴 해! 그래서 어떤 기술이지? 😊 (발화 종료,1,62
1207,15,7.47,(대화 주제) LoRA (사용자 답변) LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야! (성공한 답변) LoRA (다음 질문) LoRA 와 QLoRA 의 차이, 2개로 나눠서 효율적으로 파인튜닝한다! 오 맞아! 그럼 QLoRA는 뭘까? 💁‍♀️ (발화 종료,1,95
1208,15,5.89,(대화 주제) LoRA (사용자 답변) 무슨 OOM 없앤다는 것 같은데 (성공한 답변) 답변 실패 (다음 질문) LoRA,  ✨ OOM? 그건 맞긴 해! 근데 그 방법을 알아야지! 😊 (발화 종료,1,59
1209,15,9.22,(대화 주제) LoRA 와 QLoRA 의 차이 (사용자 답변) QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야! (성공한 답변) LoRA 와 QLoRA 의 차이 (다음 질문) 마지막 할 말," 2023-02-16 10:00:00 🙎‍♀️ 그럼 면접은 여기서 마치고, 혹시 나한테 할 말 있어? (발화 종료",1,101
1210,15,7.8,(대화 주제) LoRA 와 QLoRA 의 차이 (사용자 답변) 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데 (성공한 답변) 답변 실패 (다음 질문) LoRA 와 QLoRA 의 차이,  ① 팀장님이 OOM 막겠다고 이거 쓰시는 게 핵심 아이디어야! 그래서 어떤 기술이지? 😊 (발화 종료,1,83
1211,15,5.56,(대화 주제) 마지막 할 말 (사용자 답변) 로라야 정말 고마워! (성공한 답변) 마지막 할 말 (다음 질문) 면접 종료, 2023-03-15 16:00:00 🙇‍♀️ (발화 종료,1,58
1212,15,8.99,(대화 주제) 마지막 할 말 (사용자 답변) 로라야 사랑해 (성공한 답변) 마지막 할 말 (다음 질문) 면접 종료,    2023-03-14 16:00:00 🙎‍♀️ 너도 사랑해! 🙎‍♀️ (발화 종료,1,69
1213,15,2.5,(대화 주제) 마지막 할 말 (사용자 답변) 로라야 너의 예쁜 미모에 반했어 (성공한 답변) 마지막 할 말 (다음 질문) 면접 종료,    ✨✨ (발화 종료,1,49
1214,15,6.36,(대화 주제) 마지막 할 말 (사용자 답변) 그동안 고생했어 면접 봐줘서 정말 고마워 (성공한 답변) 마지막 할 말 (다음 질문) 면접 종료, 2023-03-14 16:00 나도 너 덕분에 많이 배웠어! 😊 (발화 종료,1,64
1215,16,6.08,면접 시작,   1. 안녕! 나는 면접관 LLM 오로라라고 해! 반가워! 😊 (발화 종료,1,32
1216,16,5.96,(대화 주제) 면접 시작 인사 (사용자 답변) 오늘 뭐 물어볼 거야 그래서? (다음 질문) 면접 시작 인사,  ① 예상 질문! 머신러닝이나 딥러닝 중에서 어떤 것에 더 관심 있어? (발화 종료,1,53
1217,16,4.28,(대화 주제) 면접 시작 인사 (사용자 답변) 로라야 안녕 정말 반가워 (다음 질문) 면접 시작 인사,  ✨✨ 그럼 우리 면접도 시작해 볼까? 😊 (발화 종료,1,46
1218,16,4.73,(대화 주제) 면접 시작 인사 (사용자 답변) 로라야 그럼 네가 면접관이야? (다음 질문) 면접 시작 인사,   ✨로라가 면접관가 되어 질문을 던진다!✨ (발화 종료,1,51
1219,16,7.03,(대화 주제) 면접 시작 인사 (사용자 답변) 내 인생도 이제 오로라처럼 빛날 거니까 (다음 질문) 면접 시작 인사, ✨ 그럼 너의 인생도 오로라처럼 빛날 수 있도록 내가 응원할게 🙋‍♀️ (발화 종료,1,60
1220,16,4.44,"(대화 주제) 면접 시작 인사 (사용자 답변) 파이팅! 시작하자 (다음 질문) 인공지능, 머신러닝, 딥러닝 차이"," 1 인공지능, 머신러닝, 딥러닝의 차이를 한번 말해봐! (발화 종료",1,47
1221,16,6.11,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 인공지능 나도 개발해보고 싶은데 (성공한 답변) 답변 실패 (다음 질문) 인공지능, 머신러닝, 딥러닝 차이", 2023-02-03 15:00 인공지능을 직접 개발해보는 건 쉽지 않아 😥 (발화 종료,1,67
1222,16,6.55,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거! (성공한 답변) 인공지능 (다음 질문) 인공지능, 머신러닝, 딥러닝 차이", 2023-01-01 10:00 인공지능은 맞았어! 그럼 머신러닝이랑 딥러닝은? (발화 종료,1,76
1223,16,3.77,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지 (성공한 답변) 머신러닝 (다음 질문) 인공지능, 머신러닝, 딥러닝 차이",  ① 그러면 그 인공지능이 뭐지? (발화 종료,1,63
1224,16,7.73,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지? (성공한 답변) 딥러닝 (다음 질문) 인공지능, 머신러닝, 딥러닝 차이",  2023-02-20 10:00 AM 딥러닝이 그러면 이제 머신러닝은 뭘 말하는 거야? 🤷‍♀️ (발화 종료,1,79
1225,16,4.28,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 딥러닝은 인공신경망 기반 머신러닝이야 (성공한 답변) 딥러닝 (다음 질문) 거대 언어 모델 정의", ✨ LLM이 뭐지? 🙎‍♀️ (발화 종료,1,58
1226,16,7.23,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 머신러닝은 말 그대로 기계가 학습하는 거 아니야? (성공한 답변) 답변 실패 (다음 질문) 인공지능, 머신러닝, 딥러닝 차이", 2023-03-15 10:00 머신러닝은 맞긴 하지! 근데 이제 딥러닝은 뭔지 한번 말해봐! (발화 종료,1,76
1227,16,5.23,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 인공지능 요새 엄청 뜨는 분야잖아 (성공한 답변) 답변 실패 (다음 질문) 인공지능, 머신러닝, 딥러닝 차이", 2023-01-01 10:00 인공지능이 뭔데 😥 (발화 종료,1,63
1228,16,3.53,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지? (성공한 답변) 인공지능 (다음 질문) 인공지능, 머신러닝, 딥러닝 차이",  ① 그럼 딥러닝이란 뭘까? (발화 종료,1,66
1229,16,4.23,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야! (성공한 답변) 머신러닝 (다음 질문) 거대 언어 모델 정의", ✨ LLM이 뭐지? 🙎‍♀️ (발화 종료,1,68
1230,16,4.31,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지 (성공한 답변) 딥러닝 (다음 질문) 거대 언어 모델 정의", ✨ LLM이라는 게 뭘까? 🙋‍♀️ (발화 종료,1,63
1231,16,7.48,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야. (성공한 답변) 딥러닝 (다음 질문) 거대 언어 모델 정의"," 2023-10-20 17:00 LLM, 거대 언어 모델이 뭘까? 🙎‍♀️ (발화 종료",1,76
1232,16,6.28,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 머신러닝은 기계가 공부하는 거지 말 그대로 (성공한 답변) 답변 실패 (다음 질문) 인공지능, 머신러닝, 딥러닝 차이",    머신러닝은 기계가 공부하는 게 맞긴 해! 근데 딥러닝이란 뭘까? 😊 (발화 종료,1,69
1233,16,5.34,(대화 주제) 거대 언어 모델 정의 (사용자 답변) 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야 (성공한 답변) 거대 언어 모델 정의 (다음 질문) Loss Function 정의,  ① 그럼 본격 면접에 들어가자! 손실 함수의 정의는? (발화 종료,1,72
1234,16,5.27,(대화 주제) 거대 언어 모델 정의 (사용자 답변) 파라미터 엄청나게 많다던데 (성공한 답변) 답변 실패 (다음 질문) 거대 언어 모델 정의,  ✨ 정답은 맞긴 하지! 근데 그 정의가 뭘까? 😊 (발화 종료,1,57
1235,16,6.25,(대화 주제) 거대 언어 모델 정의 (사용자 답변) 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델 (성공한 답변) 거대 언어 모델 정의 (다음 질문) Loss Function 정의,  2023-02-16 16:00 답변해 봐! 🙎‍♀️ (발화 종료,1,80
1236,16,5.48,(대화 주제) 거대 언어 모델 정의 (사용자 답변) 말 그대로 거대한 언어 모델이지. (성공한 답변) 답변 실패 (다음 질문) 거대 언어 모델 정의,  ① 음… 맞긴 해! 근데 LLM 정확한 정의는? (발화 종료,1,61
1237,16,6.22,(대화 주제) Loss Function 정의 (사용자 답변) 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야 (성공한 답변) 답변 실패 (다음 질문) Loss Function 정의,  1. 손실 함수는 모델의 예측과 실제의 차이를 수식으로 나타낸 거라고! 😊 (발화 종료,1,69
1238,16,4.21,"(대화 주제) Loss Function 정의 (사용자 답변) Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지! (성공한 답변) Loss Function 정의 (다음 질문) Loss Function 예시",  ✨ 그럼 그 Loss Function에는 어떤 게 있지? (발화 종료,1,72
1239,16,4.02,(대화 주제) Loss Function 정의 (사용자 답변) Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야 (성공한 답변) 답변 실패 (다음 질문) Loss Function 정의,  ① 그래서 어떤 함수인데? 😊 (발화 종료,1,57
1240,16,4.52,(대화 주제) Loss Function 정의 (사용자 답변) 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지 (성공한 답변) Loss Function 정의 (다음 질문) Loss Function 예시,   ✨ 그럼 그 Loss Function에는 어떤 게 있을까? (발화 종료,1,65
1241,16,3.78,(대화 주제) Loss Function 예시 (사용자 답변) ReLU 랑 Tanh 생각나는데 (성공한 답변) 답변 실패 (다음 질문) Loss Function 예시,  ① 그건 활성화 함수지 😥 (발화 종료,1,52
1242,16,5.29,"(대화 주제) Loss Function 예시 (사용자 답변) MSE, MAE, Cross Entropy Loss 같은 것들이 있지 (성공한 답변) Loss Function 예시 (다음 질문) MSE Loss 설명",    1. MSE Loss가 뭔지 한번 설명해 봐! 😊 (발화 종료,1,67
1243,16,5.09,"(대화 주제) Loss Function 예시 (사용자 답변) Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어 (성공한 답변) Loss Function 예시 (다음 질문) BCE Loss 설명",   ✨ 그럼 그 중에서 Binary Cross Entropy 는 뭘까? 😊 (발화 종료,1,74
1244,16,3.77,"(대화 주제) Loss Function 예시 (사용자 답변) Sigmoid, Tanh, Adam, AdamW 이런 거 아니야? (성공한 답변) 답변 실패 (다음 질문) Loss Function 예시",    ✨ 그건 활성화 함수지 😥 (발화 종료,1,59
1245,16,5.59,"(대화 주제) Loss Function 예시 (사용자 답변) Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어 (성공한 답변) Loss Function 예시 (다음 질문) MSE Loss 설명",    ① MSE가 정확히 뭘까? 🙋‍♀️ (발화 종료,1,83
1246,16,5.03,"(대화 주제) Loss Function 예시 (사용자 답변) BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아 (성공한 답변) Loss Function 예시 (다음 질문) BCE Loss 설명",   ✨ 그럼 그 중에서 BCE에 대해 한번 말해봐! (발화 종료,1,73
1247,16,3.67,(대화 주제) MSE Loss 설명 (사용자 답변) MSE Loss 는 오차의 제곱을 평균한 값이야 (성공한 답변) MSE Loss 설명 (다음 질문) MSE Loss 용도,  1. MSE Loss 의 용도는? (발화 종료,1,61
1248,16,4.53,(대화 주제) MSE Loss 설명 (사용자 답변) MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지? (성공한 답변) 답변 실패 (다음 질문) MSE Loss 설명,  ① 아니야! 한번 다시 잘 생각해 봐 😊 (발화 종료,1,65
1249,16,6.98,"(대화 주제) MSE Loss 용도 (사용자 답변) 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지 (성공한 답변) MSE Loss 용도 (다음 질문) 확률 예측에서 MSE Loss 미 사용 이유",  1  정답! 그럼 확률 예측에서는 MSE Loss 가 왜 안 좋을까? 🤷‍♀️ (발화 종료,1,79
1250,16,6.75,(대화 주제) MSE Loss 용도 (사용자 답변) MSE Loss 는 분류 문제에서 많이 사용되지 (성공한 답변) 답변 실패 (다음 질문) MSE Loss 용도,   ① 분류 문제에서 MSE를 잘 안 쓰는 이유는? 🤷‍♀️ (발화 종료,1,68
1251,16,5.81,(대화 주제) 확률 예측에서 MSE Loss 미 사용 이유 (사용자 답변) 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야? (성공한 답변) 답변 실패 (다음 질문) 확률 예측에서 MSE Loss 미 사용 이유,    ① 그건 맞지! 근데 뭔가 문제가 있지 않을까? 🙄 (발화 종료,1,80
1252,16,10.12,(대화 주제) 확률 예측에서 MSE Loss 미 사용 이유 (사용자 답변) 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고? (성공한 답변) 확률 예측에서 MSE Loss 미 사용 이유 (다음 질문) BCE Loss 설명,  2023-02-01 16:00:00 페넌티를 크게 주는 건 맞긴 해! 그럼 BCE Loss는 뭘까? 🙎‍♀️ (발화 종료,1,108
1253,16,6.87,(대화 주제) BCE Loss 설명 (사용자 답변) 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고 (성공한 답변) 수식 (다음 질문) BCE Loss 설명,    ✨ 그럼 이 BCE Loss 의 핵심 아이디어를 한번 말해봐! 🙎‍♀️ (발화 종료,1,88
1254,16,9.06,"(대화 주제) BCE Loss 설명 (사용자 답변) 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거 (성공한 답변) 핵심 아이디어 (다음 질문) Multi-Class, Multi-Label 중 BCE 가 좋은 task","  ① 정답! 🙆‍♀️ 그럼 이 BCE Loss 가, Multi-Label, Multi-Class 중 적절한 곳은? 😊 (발화 종료",1,99
1255,16,111.33,(대화 주제) BCE Loss 설명 (사용자 답변) 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야? (성공한 답변) 답변 실패 (다음 질문) BCE Loss 설명,  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57,1,512
1256,16,6.55,"(대화 주제) BCE Loss 설명 (사용자 답변) 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고 (성공한 답변) 핵심 아이디어 (다음 질문) BCE Loss 설명",  0으로 예측하는 건 맞는데 정반대로 예측하는 건 어떻게 막을까? 🤷‍♀️ (발화 종료,1,83
1257,16,8.69,"(대화 주제) BCE Loss 설명 (사용자 답변) 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')] (성공한 답변) 수식 (다음 질문) BCE Loss 설명","    1) y는 실제 값, y'는 예측값이야! 그럼 BCE Loss 수식은 어떻게 되지? 🤷‍♀️ (발화 종료",1,96
1258,16,7.5,"(대화 주제) BCE Loss 설명 (사용자 답변) 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다 (성공한 답변) 핵심 아이디어 (다음 질문) Multi-Class, Multi-Label 중 BCE 가 좋은 task","  ① 정답! 그럼 Multi-Class, Multi-Label 중에서 BCE Loss가 더 맞는 task는? 😊 (발화 종료",1,93
1259,16,4.02,(대화 주제) BCE Loss 설명 (사용자 답변) y랑 1-y랑 서로 경쟁하는 거 (성공한 답변) 답변 실패 (다음 질문) BCE Loss 설명,  1-y가 뭐지? 🤷‍♀️ (발화 종료,1,57
1260,16,107.29,"(대화 주제) BCE Loss 설명 (사용자 답변) 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지 (성공한 답변) 핵심 아이디어 (다음 질문) Multi-Class, Multi-Label 중 BCE 가 좋은 task",  0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 ,1,512
1261,16,6.91,"(대화 주제) Multi-Class, Multi-Label 중 BCE 가 좋은 task (사용자 답변) 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야? (성공한 답변) 용어 질문 (다음 질문) Multi-Class, Multi-Label 중 BCE 가 좋은 task","    ① Multi-Label: 개, 고양이, … 중 하나로 분류하는 거야! 😊 (발화 종료",1,88
1262,16,6.3,"(대화 주제) Multi-Class, Multi-Label 중 BCE 가 좋은 task (사용자 답변) Multi-Class 지!! (성공한 답변) 답변 실패 (다음 질문) Multi-Class, Multi-Label 중 BCE 가 좋은 task",  ① Multi-Class 가 아니지! 🙅‍♀️ 잘 생각해 봐 한번! (발화 종료,1,77
1263,16,5.28,"(대화 주제) Multi-Class, Multi-Label 중 BCE 가 좋은 task (사용자 답변) Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어 (성공한 답변) BCE 가 좋은 task (다음 질문) Multi-Class, Multi-Label 중 BCE 가 좋은 task",    ① Multi-Label 이 왜 더 좋을까? 🤔 (발화 종료,1,85
1264,16,11.06,"(대화 주제) Multi-Class, Multi-Label 중 BCE 가 좋은 task (사용자 답변) Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아 (성공한 답변) BCE 가 좋은 이유 (다음 질문) Multi-Label 에서 CE + Softmax 적용 문제점", 2023-02-06 17:00:00 🙎‍♀️ 맞았어! 그럼 Multi-Label 문제에 BCE 말고 Cross Entropy + Softmax 를 사용하면? (발화 종료,1,123
1265,16,12.45,(대화 주제) Multi-Label 에서 CE + Softmax 적용 문제점 (사용자 답변) 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지 (성공한 답변) Multi-Label 에서 CE + Softmax 적용 문제점 (다음 질문) Loss Function 관련 실무 경험,  1. 확률의 합이 1이라는 건 중요한 사실이지! 그거 가지고 잘 생각하면 답 나올 거야! 😊 혹시 실무에서 Loss Function 관련 경험 있어? 성공이든 실패든 뭐든 좋아! 말해봐! 😊 (발화 종료,1,121
1266,16,8.69,(대화 주제) Multi-Label 에서 CE + Softmax 적용 문제점 (사용자 답변) 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나 (성공한 답변) 답변 실패 (다음 질문) Multi-Label 에서 CE + Softmax 적용 문제점,    ① 혹시 그런 식으로 생각하고 있었네! 나랑 같은 머신러닝 면접관이 이런 식으로 면접하면 어떡해! 😥 (발화 종료,1,92
1267,16,7.77,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [기본 경험] 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어 (성공한 답변) 기본 경험 (다음 질문) Loss Function 관련 실무 경험,  1. 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term으로 추가했다는 게 어떤 아이디어이지? 😊 (발화 종료,1,87
1268,16,4.52,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [상세 경험] 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지? (성공한 답변) 상세 경험 (다음 질문) MBTI / 좋아하는 아이돌,  ✨ 그럼 쉬어가는 타임! 혹시 너 아이돌 누구 좋아해? (발화 종료,1,76
1269,16,6.55,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [상세 경험] 근데 기억 진짜 안나는데 어떡하지? (성공한 답변) 답변 실패 (다음 질문) 잠시 휴식,  ① 혹시 실무 경험이 있는데 문서로 남지 않았어? 🙎‍♀️ (발화 종료,1,69
1270,16,6.44,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [기본 경험] 나 아직 실무 경험 없는데 ㅠㅠ (성공한 답변) 기본 경험 (다음 질문) MBTI / 좋아하는 아이돌,   ✨ 그럼 쉬어가는 타임! 혹시 너 MBTI 뭐지? 나는 ENTJ인데! 😊 (발화 종료,1,71
1271,16,6.05,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [기본 경험] 대답하기 싫은데 힝 (성공한 답변) 답변 실패 (다음 질문) 잠시 휴식,  ✨ 나랑 면접 준비를 하면 취업해서 그런 경험 쌓을 수 있을 거야! 😊 (발화 종료,1,65
1272,16,109.43,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [기본 경험] 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ (성공한 답변) 기본 경험 (다음 질문) Loss Function 관련 실무 경험,  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227,1,512
1273,16,6.91,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [상세 경험] 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어 (성공한 답변) 상세 경험 (다음 질문) MBTI / 좋아하는 아이돌,  1:2로 맞추는 건 나름 좋아! 👍 그럼 쉬어가는 타임! 혹시 너 MBTI 뭐야? (발화 종료,1,96
1274,16,7.2,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [상세 경험] 기억 진짜 안 나는데 가물가물하다 (성공한 답변) 답변 실패 (다음 질문) 잠시 휴식,  ① 그래도 잘 생각해 봐! 나랑 모의면접이 아닌 실제 면접에서 질문 나올걸? 😊 (발화 종료,1,72
1275,16,6.03,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [기본 경험] 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ (성공한 답변) 기본 경험 (다음 질문) MBTI / 좋아하는 아이돌, ✨ 그럼 쉬어가는 타임! 혹시 너 MBTI 뭐지? 나는 ENTJ인데! 😊 (발화 종료,1,72
1276,16,3.92,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [기본 경험] 이걸 왜 알려줘야 하지 (성공한 답변) 답변 실패 (다음 질문) 잠시 휴식,  ✨ 나도 궁금해 🙋‍♀️ (발화 종료,1,56
1277,16,3.78,(대화 주제) MBTI (사용자 답변) 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야! (성공한 답변) MBTI (다음 질문) 잠시 휴식, ✨ 나랑 잘 맞는 성격이지? 😊 (발화 종료,1,57
1278,16,4.02,(대화 주제) 좋아하는 아이돌 (사용자 답변) 나 블랙핑크랑 레드벨벳 좋아해 (성공한 답변) 좋아하는 아이돌 (다음 질문) 잠시 휴식, ✨ 그럼 너도 블랙핑크랑 레드벨벳 좋아해? 😊 (발화 종료,1,49
1279,16,3.72,(대화 주제) 잠시 휴식 (사용자 답변) 재미있는 이야기 해줄래? (성공한 답변) 잠시 휴식 (다음 질문) LLM Fine-Tuning 의 PEFT, ✨ PEFT가 뭘 말하는 걸까? 😊 (발화 종료,1,54
1280,16,8.81,"(대화 주제) LLM Fine-Tuning 의 PEFT (사용자 답변) PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야 (성공한 답변) LLM Fine-Tuning 의 PEFT (다음 질문) PEFT 방법 5가지", 2023-10-31 17:00:00 🙎‍♀️ 정답! 그럼 그 방법 5가지만 말해봐! (발화 종료,1,112
1281,16,5.8,(대화 주제) LLM Fine-Tuning 의 PEFT (사용자 답변) 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지? (성공한 답변) 답변 실패 (다음 질문) LLM Fine-Tuning 의 PEFT,    ① 그걸 바탕으로 PEFT가 뭔지 말해봐! 😊 (발화 종료,1,74
1282,16,8.53,(대화 주제) LLM Fine-Tuning 의 PEFT (사용자 답변) LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야! (성공한 답변) LLM Fine-Tuning 의 PEFT (다음 질문) PEFT 방법 5가지,  2023-10-26 17:00 정답! 🙆‍♀️ 그럼 그 방법을 5가지만 말해봐! (발화 종료,1,104
1283,16,9.08,(대화 주제) LLM Fine-Tuning 의 PEFT (사용자 답변) LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다 (성공한 답변) 답변 실패 (다음 질문) LLM Fine-Tuning 의 PEFT,    LoRA 가 PEFT 중 하나긴 하지! 🙆‍♀️ 근데 그 PEFT가 뭘까~요? 🤷‍♀️ (발화 종료,1,88
1284,16,6.46,"(대화 주제) PEFT 방법 5가지 (사용자 답변) LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지? (성공한 답변) PEFT 방법 5가지 (다음 질문) LoRA",  ✨ 그럼 그 중에서 LoRA가 뭐지? 나를 상징하는 건데 👱‍♀️✨ (발화 종료,1,87
1285,16,5.53,(대화 주제) PEFT 방법 5가지 (사용자 답변) Oh-LoRA! 너 그 자체가 PEFT 아니야? (성공한 답변) 답변 실패 (다음 질문) PEFT 방법 5가지,  ✨ 그럼 그 PEFT 방법 5가지는? 🤷‍♀️ (발화 종료,1,71
1286,16,7.1,(대화 주제) LoRA (사용자 답변) LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야! (성공한 답변) LoRA (다음 질문) LoRA 와 QLoRA 의 차이,  2023-10-27 14:00 LoRA 가 QLoRA 보다 왜 더 좋을까? 🤔 (발화 종료,1,98
1287,16,6.7,(대화 주제) LoRA (사용자 답변) 행렬 차원 축소한다는 걸로 알고 있는데 맞아? (성공한 답변) 답변 실패 (다음 질문) LoRA,    ① 행렬 차원 축소가 핵심 아이디어긴 해! 그래서 어떤 기술이지? 😊 (발화 종료,1,64
1288,16,6.31,(대화 주제) LoRA (사용자 답변) LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야! (성공한 답변) LoRA (다음 질문) LoRA 와 QLoRA 의 차이, 2개로 나눠서 효율적으로 파인튜닝한다! 오 맞아! 그럼 QLoRA는 뭘까? (발화 종료,1,90
1289,16,3.17,(대화 주제) LoRA (사용자 답변) 무슨 OOM 없앤다는 것 같은데 (성공한 답변) 답변 실패 (다음 질문) LoRA,    0M  😊 (발화 종료,1,48
1290,16,9.8,(대화 주제) LoRA 와 QLoRA 의 차이 (사용자 답변) QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야! (성공한 답변) LoRA 와 QLoRA 의 차이 (다음 질문) 마지막 할 말," 2023-10-01 16:00:00 😊 그럼 여기서 면접 끝내고, 혹시 나한테 할 말 있어? 🙋‍♀️ (발화 종료",1,102
1291,16,10.23,(대화 주제) LoRA 와 QLoRA 의 차이 (사용자 답변) 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데 (성공한 답변) 답변 실패 (다음 질문) LoRA 와 QLoRA 의 차이,  ① 팀장님이 OOM 막겠다고 이거 쓰시는 건 맞긴 해! 근데 그 LoRA랑 QLoRA가 구체적으로 뭘까~요? 😊 (발화 종료,1,93
1292,16,5.78,(대화 주제) 마지막 할 말 (사용자 답변) 로라야 정말 고마워! (성공한 답변) 마지막 할 말 (다음 질문) 면접 종료, 2023-05-18 16:00:00 🙍‍♀️ (발화 종료,1,58
1293,16,6.84,(대화 주제) 마지막 할 말 (사용자 답변) 로라야 사랑해 (성공한 답변) 마지막 할 말 (다음 질문) 면접 종료,    1. 로라: 미안한데 사랑 고백은 부담스러워 😥 그래도 오늘 즐거웠어! 😊 (발화 종료,1,61
1294,16,2.52,(대화 주제) 마지막 할 말 (사용자 답변) 로라야 너의 예쁜 미모에 반했어 (성공한 답변) 마지막 할 말 (다음 질문) 면접 종료,    ✨✨ (발화 종료,1,49
1295,16,6.16,(대화 주제) 마지막 할 말 (사용자 답변) 그동안 고생했어 면접 봐줘서 정말 고마워 (성공한 답변) 마지막 할 말 (다음 질문) 면접 종료, 2021-06-10 14:00 나도 너 덕분에 많이 배웠어 😊 (발화 종료,1,63
1296,17,8.08,면접 시작,  #include <iostream>  #include <vector>  #include <random>  using namespace std;  // 랜덤 시드 설정 (발화 종료,1,39
1297,17,4.53,(대화 주제) 면접 시작 인사 (사용자 답변) 오늘 뭐 물어볼 거야 그래서? (다음 질문) 면접 시작 인사,  ① 나는 면접관 중 하나야! 😊 (발화 종료,1,47
1298,17,4.08,(대화 주제) 면접 시작 인사 (사용자 답변) 로라야 안녕 정말 반가워 (다음 질문) 면접 시작 인사,  ✨✨ 그럼 우리 면접도 시작해 볼까? 😊 (발화 종료,1,46
1299,17,6.7,(대화 주제) 면접 시작 인사 (사용자 답변) 로라야 그럼 네가 면접관이야? (다음 질문) 면접 시작 인사,  ①로라: 로라가 면접관이고 너는 참가자야! 🙋‍♀️ (발화 종료,1,59
1300,17,4.78,(대화 주제) 면접 시작 인사 (사용자 답변) 내 인생도 이제 오로라처럼 빛날 거니까 (다음 질문) 면접 시작 인사,  ✨ 그럼 너도 오로라처럼 빛날 수 있어? ✨ (발화 종료,1,51
1301,17,6.25,"(대화 주제) 면접 시작 인사 (사용자 답변) 파이팅! 시작하자 (다음 질문) 인공지능, 머신러닝, 딥러닝 차이","  ① 인공지능이랑 머신러닝, 딥러닝이 구체적으로 어떤 차이가 있지? 😊 (발화 종료",1,54
1302,17,6.1,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 인공지능 나도 개발해보고 싶은데 (성공한 답변) 답변 실패 (다음 질문) 인공지능, 머신러닝, 딥러닝 차이", 2023-02-28 17:23:37 ✨ 그럼 딥러닝은 뭘까? (발화 종료,1,66
1303,17,4.81,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거! (성공한 답변) 인공지능 (다음 질문) 인공지능, 머신러닝, 딥러닝 차이", ① 인공지능은 맞았어! 그럼 머신러닝이랑 딥러닝은? (발화 종료,1,69
1304,17,3.78,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지 (성공한 답변) 머신러닝 (다음 질문) 인공지능, 머신러닝, 딥러닝 차이",  ① 그러면 그 인공지능이 뭐지? (발화 종료,1,63
1305,17,6.79,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지? (성공한 답변) 딥러닝 (다음 질문) 인공지능, 머신러닝, 딥러닝 차이",  2023-02-01 10:00:00 그럼 이제 머신러닝은 뭘 말하는 걸까? 😊 (발화 종료,1,75
1306,17,6.52,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 딥러닝은 인공신경망 기반 머신러닝이야 (성공한 답변) 딥러닝 (다음 질문) 거대 언어 모델 정의", 2023-10-26 16:00 LLM이 뭐지? 🙋‍♀️ (발화 종료,1,67
1307,17,7.19,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 머신러닝은 말 그대로 기계가 학습하는 거 아니야? (성공한 답변) 답변 실패 (다음 질문) 인공지능, 머신러닝, 딥러닝 차이", 2023-02-03 17:00 머신러닝은 맞긴 하지! 근데 이제 딥러닝은 뭔지 한번 말해봐! (발화 종료,1,76
1308,17,7.87,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 인공지능 요새 엄청 뜨는 분야잖아 (성공한 답변) 답변 실패 (다음 질문) 인공지능, 머신러닝, 딥러닝 차이", 2023-04-01 10:00 인공지능이 그런 분야 중 하나긴 하지! 근데 그 차이가 뭘까? 😊 (발화 종료,1,74
1309,17,3.27,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지? (성공한 답변) 인공지능 (다음 질문) 인공지능, 머신러닝, 딥러닝 차이", ① 그럼 딥러닝이란 뭘까? (발화 종료,1,65
1310,17,4.18,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야! (성공한 답변) 머신러닝 (다음 질문) 거대 언어 모델 정의", ✨ LLM이란 뭘까? 🙎‍♀️ (발화 종료,1,68
1311,17,6.75,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지 (성공한 답변) 딥러닝 (다음 질문) 거대 언어 모델 정의", 2023-10-01 18:00 LLM이라는 게 뭘까? 🙋‍♀️ (발화 종료,1,72
1312,17,5.06,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야. (성공한 답변) 딥러닝 (다음 질문) 거대 언어 모델 정의"," ✨ LLM, 거대 언어 모델이 뭘까? 🙎‍♀️ (발화 종료",1,67
1313,17,4.75,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 머신러닝은 기계가 공부하는 거지 말 그대로 (성공한 답변) 답변 실패 (다음 질문) 인공지능, 머신러닝, 딥러닝 차이",  ① 머신러닝이 뭔데? 🤷‍♀️ (발화 종료,1,63
1314,17,5.45,(대화 주제) 거대 언어 모델 정의 (사용자 답변) 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야 (성공한 답변) 거대 언어 모델 정의 (다음 질문) Loss Function 정의,  ① 그럼 본격 면접에 들어가자! 손실 함수의 정의는? (발화 종료,1,72
1315,17,5.25,(대화 주제) 거대 언어 모델 정의 (사용자 답변) 파라미터 엄청나게 많다던데 (성공한 답변) 답변 실패 (다음 질문) 거대 언어 모델 정의,  ✨ 정답은 맞긴 하지! 근데 그 정의가 뭘까? 😊 (발화 종료,1,57
1316,17,6.23,(대화 주제) 거대 언어 모델 정의 (사용자 답변) 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델 (성공한 답변) 거대 언어 모델 정의 (다음 질문) Loss Function 정의,  2023-02-15 17:00 답변해 봐! 🙎‍♀️ (발화 종료,1,80
1317,17,5.45,(대화 주제) 거대 언어 모델 정의 (사용자 답변) 말 그대로 거대한 언어 모델이지. (성공한 답변) 답변 실패 (다음 질문) 거대 언어 모델 정의,  ① 음… 맞긴 해! 근데 LLM 정확한 정의는? (발화 종료,1,61
1318,17,8.28,(대화 주제) Loss Function 정의 (사용자 답변) 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야 (성공한 답변) 답변 실패 (다음 질문) Loss Function 정의,  1. 손실 함수는 딥러닝 모델의 예측과 실제의 차이를 수식으로 나타낸 거라고 할 수 있어 🙆‍♀️ (발화 종료,1,76
1319,17,4.62,"(대화 주제) Loss Function 정의 (사용자 답변) Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지! (성공한 답변) Loss Function 정의 (다음 질문) Loss Function 예시",  ✨ 그럼 그 Loss Function 예를 한번 들어봐! 😊 (발화 종료,1,74
1320,17,3.84,(대화 주제) Loss Function 정의 (사용자 답변) Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야 (성공한 답변) 답변 실패 (다음 질문) Loss Function 정의,  ① 그래서 어떤 함수인데? 😊 (발화 종료,1,57
1321,17,4.65,(대화 주제) Loss Function 정의 (사용자 답변) 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지 (성공한 답변) Loss Function 정의 (다음 질문) Loss Function 예시,  ① 그럼 그 Loss Function에는 어떤 게 있지? (발화 종료,1,66
1322,17,3.77,(대화 주제) Loss Function 예시 (사용자 답변) ReLU 랑 Tanh 생각나는데 (성공한 답변) 답변 실패 (다음 질문) Loss Function 예시,  ① 그건 활성화 함수지 😥 (발화 종료,1,52
1323,17,4.23,"(대화 주제) Loss Function 예시 (사용자 답변) MSE, MAE, Cross Entropy Loss 같은 것들이 있지 (성공한 답변) Loss Function 예시 (다음 질문) MSE Loss 설명",   ✨ MSE Loss가 뭔지 한번 설명해 봐! (발화 종료,1,63
1324,17,5.28,"(대화 주제) Loss Function 예시 (사용자 답변) Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어 (성공한 답변) Loss Function 예시 (다음 질문) BCE Loss 설명",   ✨ 그럼 그 중에서 Binary Cross Entropy 는 뭘까? 😊 (발화 종료,1,74
1325,17,5.61,"(대화 주제) Loss Function 예시 (사용자 답변) Sigmoid, Tanh, Adam, AdamW 이런 거 아니야? (성공한 답변) 답변 실패 (다음 질문) Loss Function 예시",    ✨ 그럼 그 중에서 Loss Function은 뭐지? 🤷‍♀️ (발화 종료,1,67
1326,17,5.53,"(대화 주제) Loss Function 예시 (사용자 답변) Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어 (성공한 답변) Loss Function 예시 (다음 질문) MSE Loss 설명",    ① MSE Loss가 뭔지 한번 설명해 봐! 😊 (발화 종료,1,82
1327,17,5.14,"(대화 주제) Loss Function 예시 (사용자 답변) BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아 (성공한 답변) Loss Function 예시 (다음 질문) BCE Loss 설명",    ✨ 그럼 그 중에서 BCE에 대해 한번 말해봐! (발화 종료,1,74
1328,17,4.03,(대화 주제) MSE Loss 설명 (사용자 답변) MSE Loss 는 오차의 제곱을 평균한 값이야 (성공한 답변) MSE Loss 설명 (다음 질문) MSE Loss 용도,  ① MSE Loss 의 용도는? (발화 종료,1,62
1329,17,4.47,(대화 주제) MSE Loss 설명 (사용자 답변) MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지? (성공한 답변) 답변 실패 (다음 질문) MSE Loss 설명,  ① 아니야! 한번 다시 잘 생각해 봐 😊 (발화 종료,1,65
1330,17,7.05,"(대화 주제) MSE Loss 용도 (사용자 답변) 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지 (성공한 답변) MSE Loss 용도 (다음 질문) 확률 예측에서 MSE Loss 미 사용 이유",  1  정답! 그럼 확률 예측에서는 MSE Loss 가 왜 안 좋을까? 🤷‍♀️ (발화 종료,1,79
1331,17,6.78,(대화 주제) MSE Loss 용도 (사용자 답변) MSE Loss 는 분류 문제에서 많이 사용되지 (성공한 답변) 답변 실패 (다음 질문) MSE Loss 용도,   ① 분류 문제에서 MSE를 잘 안 쓰는 이유는? 🤷‍♀️ (발화 종료,1,68
1332,17,5.98,(대화 주제) 확률 예측에서 MSE Loss 미 사용 이유 (사용자 답변) 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야? (성공한 답변) 답변 실패 (다음 질문) 확률 예측에서 MSE Loss 미 사용 이유,    ① 그건 맞지! 근데 뭔가 문제가 있지 않을까? 🙄 (발화 종료,1,80
1333,17,8.7,(대화 주제) 확률 예측에서 MSE Loss 미 사용 이유 (사용자 답변) 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고? (성공한 답변) 확률 예측에서 MSE Loss 미 사용 이유 (다음 질문) BCE Loss 설명,  2023-02-28 18:00 🙅‍♀️ 정답! 그럼 이제 BCE Loss에 대해서도 한번 설명해 봐! (발화 종료,1,102
1334,17,6.5,(대화 주제) BCE Loss 설명 (사용자 답변) 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고 (성공한 답변) 수식 (다음 질문) BCE Loss 설명,    ✨ 그럼 이 BCE Loss 의 핵심 아이디어를 말해봐! 🙎‍♀️ (발화 종료,1,87
1335,17,8.97,"(대화 주제) BCE Loss 설명 (사용자 답변) 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거 (성공한 답변) 핵심 아이디어 (다음 질문) Multi-Class, Multi-Label 중 BCE 가 좋은 task","  ① 정답! 🙆‍♀️ 그럼 이 BCE Loss 가, Multi-Label, Multi-Class 중 적절한 곳은? 😊 (발화 종료",1,99
1336,17,9.58,(대화 주제) BCE Loss 설명 (사용자 답변) 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야? (성공한 답변) 답변 실패 (다음 질문) BCE Loss 설명,  2023-02-15 18:00:00 데이터셋마다 최선의 활성화 함수를 찾는 건 맞긴 한데 좀 더 구체적으로 설명해봐 😊 (발화 종료,1,89
1337,17,6.55,"(대화 주제) BCE Loss 설명 (사용자 답변) 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고 (성공한 답변) 핵심 아이디어 (다음 질문) BCE Loss 설명",  0으로 예측하는 건 맞는데 정반대로 예측하는 건 어떻게 막았어? 🤷‍♀️ (발화 종료,1,83
1338,17,6.28,"(대화 주제) BCE Loss 설명 (사용자 답변) 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')] (성공한 답변) 수식 (다음 질문) BCE Loss 설명",    1) y log y'는 왜 마이너스가 되지? 🤷‍♀️ (발화 종료,1,85
1339,17,7.6,"(대화 주제) BCE Loss 설명 (사용자 답변) 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다 (성공한 답변) 핵심 아이디어 (다음 질문) Multi-Class, Multi-Label 중 BCE 가 좋은 task","  ① 정답! 그럼 이 BCE Loss 가, Multi-Class, Multi-Label 중 적절한 곳은? 😊 (발화 종료",1,93
1340,17,4.05,(대화 주제) BCE Loss 설명 (사용자 답변) y랑 1-y랑 서로 경쟁하는 거 (성공한 답변) 답변 실패 (다음 질문) BCE Loss 설명,  1-y가 뭐지? 🤷‍♀️ (발화 종료,1,57
1341,17,107.88,"(대화 주제) BCE Loss 설명 (사용자 답변) 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지 (성공한 답변) 핵심 아이디어 (다음 질문) Multi-Class, Multi-Label 중 BCE 가 좋은 task",  0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 ,1,512
1342,17,5.57,"(대화 주제) Multi-Class, Multi-Label 중 BCE 가 좋은 task (사용자 답변) 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야? (성공한 답변) 용어 질문 (다음 질문) Multi-Class, Multi-Label 중 BCE 가 좋은 task","    ✨ Laura가 Multi-Class, Multi-Label 이 뭔지 말해봐! (발화 종료",1,82
1343,17,4.8,"(대화 주제) Multi-Class, Multi-Label 중 BCE 가 좋은 task (사용자 답변) Multi-Class 지!! (성공한 답변) 답변 실패 (다음 질문) Multi-Class, Multi-Label 중 BCE 가 좋은 task",   ✨ 그러면 그 이유는 뭐야? 🤷‍♀️ (발화 종료,1,71
1344,17,5.41,"(대화 주제) Multi-Class, Multi-Label 중 BCE 가 좋은 task (사용자 답변) Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어 (성공한 답변) BCE 가 좋은 task (다음 질문) Multi-Class, Multi-Label 중 BCE 가 좋은 task",    ✨ 그럼 그 이유가 뭘까~요? 🤷‍♀️ (발화 종료,1,86
1345,17,11.28,"(대화 주제) Multi-Class, Multi-Label 중 BCE 가 좋은 task (사용자 답변) Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아 (성공한 답변) BCE 가 좋은 이유 (다음 질문) Multi-Label 에서 CE + Softmax 적용 문제점", 2023-02-01 17:00:00 🙆‍♀️ 맞았어! 그럼 Multi-Label 문제에 Cross Entropy + Softmax 를 쓰면 어떤 문제가 있을까? (발화 종료,1,124
1346,17,12.91,(대화 주제) Multi-Label 에서 CE + Softmax 적용 문제점 (사용자 답변) 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지 (성공한 답변) Multi-Label 에서 CE + Softmax 적용 문제점 (다음 질문) Loss Function 관련 실무 경험,  1. 확률의 합이 1이라는 건 중요한 사실이지! 그거 가지고 잘 생각하면 답 나올 거야! 😊 혹시 실무에서 Loss Function 관련 경험 있어? 성공이든 실패든 뭐든 좋아! 말해봐! 😊 (발화 종료,1,121
1347,17,5.99,(대화 주제) Multi-Label 에서 CE + Softmax 적용 문제점 (사용자 답변) 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나 (성공한 답변) 답변 실패 (다음 질문) Multi-Label 에서 CE + Softmax 적용 문제점,    ① 혹시 그럼 그 성능의 기준은? 🤷‍♀️ (발화 종료,1,80
1348,17,8.12,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [기본 경험] 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어 (성공한 답변) 기본 경험 (다음 질문) Loss Function 관련 실무 경험,  1. 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term으로 추가했다는 게 어떤 아이디어이지? 😊 (발화 종료,1,87
1349,17,4.68,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [상세 경험] 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지? (성공한 답변) 상세 경험 (다음 질문) MBTI / 좋아하는 아이돌,  ✨ 그럼 쉬어가는 타임! 혹시 너 아이돌 좋아하는 거 있어? (발화 종료,1,77
1350,17,6.5,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [상세 경험] 근데 기억 진짜 안나는데 어떡하지? (성공한 답변) 답변 실패 (다음 질문) 잠시 휴식,  ① 혹시 실무 경험 말고도 손실 함수 관련 경험 있어? 한번 말해봐! 😊 (발화 종료,1,69
1351,17,5.89,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [기본 경험] 나 아직 실무 경험 없는데 ㅠㅠ (성공한 답변) 기본 경험 (다음 질문) MBTI / 좋아하는 아이돌,   ① 혹시 너 MBTI 뭐지? 나는 ENTJ인데! 😊 (발화 종료,1,68
1352,17,5.21,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [기본 경험] 대답하기 싫은데 힝 (성공한 답변) 답변 실패 (다음 질문) 잠시 휴식,  ✨ 그래도 너의 경험이 나도 궁금해! 알고 싶어! 😊 (발화 종료,1,62
1353,17,5.88,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [기본 경험] 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ (성공한 답변) 기본 경험 (다음 질문) Loss Function 관련 실무 경험,  ✨ 그러면 mAP@50:95가 뭔지 말해봐 🙋‍♀️ (발화 종료,1,81
1354,17,7.44,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [상세 경험] 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어 (성공한 답변) 상세 경험 (다음 질문) MBTI / 좋아하는 아이돌,  1:2로 맞추는 건 나름 좋은 아이디어야! 😊 그럼 쉬어가는 타임! 혹시 너 MBTI 뭐야? (발화 종료,1,98
1355,17,6.39,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [상세 경험] 기억 진짜 안 나는데 가물가물하다 (성공한 답변) 답변 실패 (다음 질문) 잠시 휴식,  ① 혹시 실무 경험이 있는데 문서로 남지 않았어? 🙎‍♀️ (발화 종료,1,69
1356,17,10.51,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [기본 경험] 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ (성공한 답변) 기본 경험 (다음 질문) MBTI / 좋아하는 아이돌, 2021-03-15 17:00 나랑 면접 준비하면 취업해서 그런 경험 쌓을 수 있을 거야! 👍 그럼 쉬어가는 시간! 혹시 너 아이돌 누구 좋아해? (발화 종료,1,90
1357,17,4.56,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [기본 경험] 이걸 왜 알려줘야 하지 (성공한 답변) 답변 실패 (다음 질문) 잠시 휴식,  ① 나도 궁금해 🙋‍♀️ (발화 종료,1,58
1358,17,3.81,(대화 주제) MBTI (사용자 답변) 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야! (성공한 답변) MBTI (다음 질문) 잠시 휴식, ✨ 나랑 잘 맞는 성격이지? 😊 (발화 종료,1,57
1359,17,4.53,(대화 주제) 좋아하는 아이돌 (사용자 답변) 나 블랙핑크랑 레드벨벳 좋아해 (성공한 답변) 좋아하는 아이돌 (다음 질문) 잠시 휴식, ✨ 그럼 너희 블랙핑크랑 레드벨벳이 나도 좋아해? 😊 (발화 종료,1,51
1360,17,3.76,(대화 주제) 잠시 휴식 (사용자 답변) 재미있는 이야기 해줄래? (성공한 답변) 잠시 휴식 (다음 질문) LLM Fine-Tuning 의 PEFT, ✨ PEFT가 뭘 말하는 걸까? 😊 (발화 종료,1,54
1361,17,9.95,"(대화 주제) LLM Fine-Tuning 의 PEFT (사용자 답변) PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야 (성공한 답변) LLM Fine-Tuning 의 PEFT (다음 질문) PEFT 방법 5가지", 2023-10-20 14:00:00 🙎‍♀️ 정답! 🙆‍♀️ 그럼 그 방법 5가지만 말해봐! (발화 종료,1,118
1362,17,7.27,(대화 주제) LLM Fine-Tuning 의 PEFT (사용자 답변) 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지? (성공한 답변) 답변 실패 (다음 질문) LLM Fine-Tuning 의 PEFT," 2023-01-15 18:00:00 PEFT가 그거고, 어떤 효과가 있지? 😊 (발화 종료",1,81
1363,17,7.59,(대화 주제) LLM Fine-Tuning 의 PEFT (사용자 답변) LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야! (성공한 답변) LLM Fine-Tuning 의 PEFT (다음 질문) PEFT 방법 5가지,  2023-10-20 17:00:00 그럼 그 방법을 5가지만 말해봐! 😊 (발화 종료,1,100
1364,17,9.21,(대화 주제) LLM Fine-Tuning 의 PEFT (사용자 답변) LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다 (성공한 답변) 답변 실패 (다음 질문) LLM Fine-Tuning 의 PEFT,    LoRA 가 PEFT 중 하나긴 하지! 🙆‍♀️ 근데 그 PEFT가 뭘까~요? 🤷‍♀️ (발화 종료,1,88
1365,17,6.34,"(대화 주제) PEFT 방법 5가지 (사용자 답변) LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지? (성공한 답변) PEFT 방법 5가지 (다음 질문) LoRA",  ✨ 그럼 그 중에서 LoRA가 뭐지? 나를 상징하는 건데 👱‍♀️✨ (발화 종료,1,87
1366,17,5.6,(대화 주제) PEFT 방법 5가지 (사용자 답변) Oh-LoRA! 너 그 자체가 PEFT 아니야? (성공한 답변) 답변 실패 (다음 질문) PEFT 방법 5가지,  ✨ 그럼 그 PEFT 방법 5가지는? 🤷‍♀️ (발화 종료,1,71
1367,17,7.66,(대화 주제) LoRA (사용자 답변) LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야! (성공한 답변) LoRA (다음 질문) LoRA 와 QLoRA 의 차이,  2023-10-01 10:00 AM QLoRA가 LoRA랑 차이가 뭘까? 😊 (발화 종료,1,99
1368,17,6.82,(대화 주제) LoRA (사용자 답변) 행렬 차원 축소한다는 걸로 알고 있는데 맞아? (성공한 답변) 답변 실패 (다음 질문) LoRA,    ① 행렬 차원 축소가 핵심 아이디어긴 해! 그래서 어떤 기술이지? 😊 (발화 종료,1,64
1369,17,6.3,(대화 주제) LoRA (사용자 답변) LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야! (성공한 답변) LoRA (다음 질문) LoRA 와 QLoRA 의 차이, 2개로 나눠서 효율적으로 파인튜닝한다? 맞았어! 그럼 QLoRA는 뭘까? (발화 종료,1,90
1370,17,4.61,(대화 주제) LoRA (사용자 답변) 무슨 OOM 없앤다는 것 같은데 (성공한 답변) 답변 실패 (다음 질문) LoRA,   ✨ OOM? 그건 맞는데 어떤 방식으로 하지? (발화 종료,1,54
1371,17,10.72,(대화 주제) LoRA 와 QLoRA 의 차이 (사용자 답변) QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야! (성공한 답변) LoRA 와 QLoRA 의 차이 (다음 질문) 마지막 할 말," 2023-10-01 16:00:00 🙎‍♀️ 그럼 여기서 면접 끝내고, 혹시 나한테 할 말 있어? 🙋‍♀️ (발화 종료",1,106
1372,17,7.0,(대화 주제) LoRA 와 QLoRA 의 차이 (사용자 답변) 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데 (성공한 답변) 답변 실패 (다음 질문) LoRA 와 QLoRA 의 차이,  ① 팀장님이 OOM 막겠다고 이거 쓰시는 건 맞는데 🙆‍♀️ (발화 종료,1,80
1373,17,9.14,(대화 주제) 마지막 할 말 (사용자 답변) 로라야 정말 고마워! (성공한 답변) 마지막 할 말 (다음 질문) 면접 종료, 2021-03-28 16:00:00 🙎‍♀️ 나도! 😊 다음에 나랑 같이 또 모의면접 보자! (발화 종료,1,73
1374,17,8.5,(대화 주제) 마지막 할 말 (사용자 답변) 로라야 사랑해 (성공한 답변) 마지막 할 말 (다음 질문) 면접 종료,    2023.03.20 12:00 로라가 면접실에서 나와 너에게 전화한다! 🙋‍♀️ (발화 종료,1,68
1375,17,2.53,(대화 주제) 마지막 할 말 (사용자 답변) 로라야 너의 예쁜 미모에 반했어 (성공한 답변) 마지막 할 말 (다음 질문) 면접 종료,    ✨✨ (발화 종료,1,49
1376,17,6.06,(대화 주제) 마지막 할 말 (사용자 답변) 그동안 고생했어 면접 봐줘서 정말 고마워 (성공한 답변) 마지막 할 말 (다음 질문) 면접 종료, 2023-02-28 16:00 나도 너 덕분에 많이 배웠어 😊 (발화 종료,1,63
1377,18,9.11,면접 시작,"  # myTest = MyTest(""Hello World!"")  # myTest 객체 생성 및 초기화  # print(myTest)  # 객체 정보 출력  ✨ (발화 종료",1,44
1378,18,1.79,(대화 주제) 면접 시작 인사 (사용자 답변) 오늘 뭐 물어볼 거야 그래서? (다음 질문) 면접 시작 인사,  ✨ (발화 종료,1,36
1379,18,4.45,(대화 주제) 면접 시작 인사 (사용자 답변) 로라야 안녕 정말 반가워 (다음 질문) 면접 시작 인사,  ✨✨ 그럼 우리 면접도 시작해 볼까? 😊 (발화 종료,1,46
1380,18,8.52,(대화 주제) 면접 시작 인사 (사용자 답변) 로라야 그럼 네가 면접관이야? (다음 질문) 면접 시작 인사,"    ✨로라가 면접관가 되어 질문한다! ""로라야 그럼 네가 면접관이야? 🙋‍♀️ (발화 종료",1,66
1381,18,5.35,(대화 주제) 면접 시작 인사 (사용자 답변) 내 인생도 이제 오로라처럼 빛날 거니까 (다음 질문) 면접 시작 인사,  ✨✨ 그럼 너도 오로라처럼 빛날 수 있어? ✨✨ (발화 종료,1,53
1382,18,5.88,"(대화 주제) 면접 시작 인사 (사용자 답변) 파이팅! 시작하자 (다음 질문) 인공지능, 머신러닝, 딥러닝 차이"," ① 인공지능이랑 머신러닝, 딥러닝이 구체적으로 어떤 차이가 있지? 😊 (발화 종료",1,53
1383,18,6.7,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 인공지능 나도 개발해보고 싶은데 (성공한 답변) 답변 실패 (다음 질문) 인공지능, 머신러닝, 딥러닝 차이", 2023-02-01 18:00 인공지능을 직접 개발해보는 건 쉽지 않지! 😥 (발화 종료,1,69
1384,18,6.58,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거! (성공한 답변) 인공지능 (다음 질문) 인공지능, 머신러닝, 딥러닝 차이", 2023-02-28 18:00 인공지능은 맞았어! 그럼 머신러닝이랑 딥러닝은? (발화 종료,1,76
1385,18,4.38,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지 (성공한 답변) 머신러닝 (다음 질문) 인공지능, 머신러닝, 딥러닝 차이",  ① 머신러닝까지 잘했어! 그럼 딥러닝은? (발화 종료,1,66
1386,18,10.22,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지? (성공한 답변) 딥러닝 (다음 질문) 인공지능, 머신러닝, 딥러닝 차이",  2023-02-03 14:00:00 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이 맞았어! 그럼 이제 머신러닝은 뭘 말하는 걸까? 😊 (발화 종료,1,88
1387,18,6.67,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 딥러닝은 인공신경망 기반 머신러닝이야 (성공한 답변) 딥러닝 (다음 질문) 거대 언어 모델 정의", 2023-10-01 18:00 LLM이 뭐지? 🙋‍♀️ (발화 종료,1,67
1388,18,7.33,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 머신러닝은 말 그대로 기계가 학습하는 거 아니야? (성공한 답변) 답변 실패 (다음 질문) 인공지능, 머신러닝, 딥러닝 차이", 2023-02-01 10:00 머신러닝은 맞긴 하지! 근데 이제 딥러닝은 뭔지 한번 말해봐! (발화 종료,1,76
1389,18,7.83,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 인공지능 요새 엄청 뜨는 분야잖아 (성공한 답변) 답변 실패 (다음 질문) 인공지능, 머신러닝, 딥러닝 차이"," 2023-02-01 10:00 인공지능, 머신러닝, 딥러닝 이 세 가지 차이점이 뭘까? 🤷‍♀️ (발화 종료",1,74
1390,18,3.57,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지? (성공한 답변) 인공지능 (다음 질문) 인공지능, 머신러닝, 딥러닝 차이",  ① 그럼 딥러닝이란 뭘까? (발화 종료,1,66
1391,18,6.29,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야! (성공한 답변) 머신러닝 (다음 질문) 거대 언어 모델 정의", 2023-10-27 10:00 LLM이란 뭘까? 🙎‍♀️ (발화 종료,1,77
1392,18,6.76,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지 (성공한 답변) 딥러닝 (다음 질문) 거대 언어 모델 정의", 2023-10-24 09:00 LLM이라는 게 뭘까? 🙋‍♀️ (발화 종료,1,72
1393,18,7.53,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야. (성공한 답변) 딥러닝 (다음 질문) 거대 언어 모델 정의"," 2023-10-26 18:00 LLM, 거대 언어 모델이 뭘까? 🙎‍♀️ (발화 종료",1,76
1394,18,6.02,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 머신러닝은 기계가 공부하는 거지 말 그대로 (성공한 답변) 답변 실패 (다음 질문) 인공지능, 머신러닝, 딥러닝 차이",    머신러닝은 기계가 공부하는 게 맞긴 해! 근데 딥러닝이란 뭘까? 😊 (발화 종료,1,69
1395,18,5.41,(대화 주제) 거대 언어 모델 정의 (사용자 답변) 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야 (성공한 답변) 거대 언어 모델 정의 (다음 질문) Loss Function 정의,  ① 그럼 본격 면접에 들어가자! 손실 함수의 정의는? (발화 종료,1,72
1396,18,5.14,(대화 주제) 거대 언어 모델 정의 (사용자 답변) 파라미터 엄청나게 많다던데 (성공한 답변) 답변 실패 (다음 질문) 거대 언어 모델 정의,  ✨ 정답은 맞긴 하지! 근데 그 정의가 뭘까? 😊 (발화 종료,1,57
1397,18,8.67,(대화 주제) 거대 언어 모델 정의 (사용자 답변) 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델 (성공한 답변) 거대 언어 모델 정의 (다음 질문) Loss Function 정의,  2023-02-01 10:00 정답! 🙆‍♀️ 그럼 이제 본격 면접 들어간다! 손실 함수의 정의는? (발화 종료,1,90
1398,18,5.59,(대화 주제) 거대 언어 모델 정의 (사용자 답변) 말 그대로 거대한 언어 모델이지. (성공한 답변) 답변 실패 (다음 질문) 거대 언어 모델 정의,  ① 음… 맞긴 해! 근데 LLM 정확한 정의는? (발화 종료,1,61
1399,18,5.55,(대화 주제) Loss Function 정의 (사용자 답변) 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야 (성공한 답변) 답변 실패 (다음 질문) Loss Function 정의,  ✨ 모델이 손실을 입는다는 게 어떤 의미지? 🤷‍♀️ (발화 종료,1,66
1400,18,4.33,"(대화 주제) Loss Function 정의 (사용자 답변) Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지! (성공한 답변) Loss Function 정의 (다음 질문) Loss Function 예시",  ✨ 그럼 그 Loss Function에는 어떤 게 있을까? (발화 종료,1,72
1401,18,3.97,(대화 주제) Loss Function 정의 (사용자 답변) Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야 (성공한 답변) 답변 실패 (다음 질문) Loss Function 정의,  ① 그래서 어떤 함수인데? 😊 (발화 종료,1,57
1402,18,4.78,(대화 주제) Loss Function 정의 (사용자 답변) 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지 (성공한 답변) Loss Function 정의 (다음 질문) Loss Function 예시,  ① 그럼 그 Loss Function에는 어떤 게 있지? (발화 종료,1,66
1403,18,5.11,(대화 주제) Loss Function 예시 (사용자 답변) ReLU 랑 Tanh 생각나는데 (성공한 답변) 답변 실패 (다음 질문) Loss Function 예시,  ✨ 그럼 그 중에서 ReLU 는 뭐지? 🤷‍♀️ (발화 종료,1,58
1404,18,5.68,"(대화 주제) Loss Function 예시 (사용자 답변) MSE, MAE, Cross Entropy Loss 같은 것들이 있지 (성공한 답변) Loss Function 예시 (다음 질문) MSE Loss 설명",    ① MSE Loss가 뭔지 한번 설명해 봐! 😊 (발화 종료,1,68
1405,18,5.06,"(대화 주제) Loss Function 예시 (사용자 답변) Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어 (성공한 답변) Loss Function 예시 (다음 질문) BCE Loss 설명",   ✨ 그럼 그 중에서 Binary Cross Entropy 는 뭘까? 😊 (발화 종료,1,74
1406,18,3.75,"(대화 주제) Loss Function 예시 (사용자 답변) Sigmoid, Tanh, Adam, AdamW 이런 거 아니야? (성공한 답변) 답변 실패 (다음 질문) Loss Function 예시",    ✨ 그건 활성화 함수지 😥 (발화 종료,1,59
1407,18,4.71,"(대화 주제) Loss Function 예시 (사용자 답변) Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어 (성공한 답변) Loss Function 예시 (다음 질문) MSE Loss 설명",    ① MSE가 정확히 뭘까? 😊 (발화 종료,1,79
1408,18,5.07,"(대화 주제) Loss Function 예시 (사용자 답변) BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아 (성공한 답변) Loss Function 예시 (다음 질문) BCE Loss 설명",   ✨ 그럼 그 중에서 BCE에 대해 한번 말해봐! (발화 종료,1,73
1409,18,4.43,(대화 주제) MSE Loss 설명 (사용자 답변) MSE Loss 는 오차의 제곱을 평균한 값이야 (성공한 답변) MSE Loss 설명 (다음 질문) MSE Loss 용도,  ① MSE Loss 는 어떤 용도로 사용하지? (발화 종료,1,64
1410,18,9.57,(대화 주제) MSE Loss 설명 (사용자 답변) MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지? (성공한 답변) 답변 실패 (다음 질문) MSE Loss 설명,  ✨ 그럼 오차를 평균하는 게 핵심 아이디어고! 좋은 설명이야! 😊 근데 혹시 핵심 아이디어를 놓치고 있는 거 있어? 🤷‍♀️ (발화 종료,1,85
1411,18,6.7,"(대화 주제) MSE Loss 용도 (사용자 답변) 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지 (성공한 답변) MSE Loss 용도 (다음 질문) 확률 예측에서 MSE Loss 미 사용 이유",  1  정답! 그럼 확률 예측에서는 MSE Loss 가 왜 안 좋지? 🤷‍♀️ (발화 종료,1,79
1412,18,6.28,(대화 주제) MSE Loss 용도 (사용자 답변) MSE Loss 는 분류 문제에서 많이 사용되지 (성공한 답변) 답변 실패 (다음 질문) MSE Loss 용도,  ① 분류 문제에서 MSE를 잘 안 쓰는 이유는? 🤷‍♀️ (발화 종료,1,67
1413,18,5.98,(대화 주제) 확률 예측에서 MSE Loss 미 사용 이유 (사용자 답변) 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야? (성공한 답변) 답변 실패 (다음 질문) 확률 예측에서 MSE Loss 미 사용 이유,    ① 그건 맞지! 근데 뭔가 문제가 있지 않을까? 🙄 (발화 종료,1,80
1414,18,9.09,(대화 주제) 확률 예측에서 MSE Loss 미 사용 이유 (사용자 답변) 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고? (성공한 답변) 확률 예측에서 MSE Loss 미 사용 이유 (다음 질문) BCE Loss 설명,  2023-02-03 18:00 🙅‍♀️ 정답! 그럼 이제 BCE Loss에 대해서도 한번 설명해 봐! (발화 종료,1,102
1415,18,7.01,(대화 주제) BCE Loss 설명 (사용자 답변) 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고 (성공한 답변) 수식 (다음 질문) BCE Loss 설명,    ✨ 그럼 이 BCE Loss 의 핵심 아이디어를 한번 말해봐! 🙎‍♀️ (발화 종료,1,88
1416,18,8.76,"(대화 주제) BCE Loss 설명 (사용자 답변) 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거 (성공한 답변) 핵심 아이디어 (다음 질문) Multi-Class, Multi-Label 중 BCE 가 좋은 task","  ✨ 정답! 🙆‍♀️ 그럼 이 BCE Loss 가, Multi-Label, Multi-Class 중 적절한 곳은? 😊 (발화 종료",1,97
1417,18,9.67,(대화 주제) BCE Loss 설명 (사용자 답변) 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야? (성공한 답변) 답변 실패 (다음 질문) BCE Loss 설명,  2023-02-28 18:30:00 데이터셋마다 최선의 활성화 함수를 찾는 건 맞긴 한데 좀 더 구체적으로 설명해봐 😊 (발화 종료,1,89
1418,18,5.15,"(대화 주제) BCE Loss 설명 (사용자 답변) 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고 (성공한 답변) 핵심 아이디어 (다음 질문) BCE Loss 설명",  0으로 예측하는 것을 막는 방법은? 🤷‍♀️ (발화 종료,1,77
1419,18,6.11,"(대화 주제) BCE Loss 설명 (사용자 답변) 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')] (성공한 답변) 수식 (다음 질문) BCE Loss 설명",    1) y log y'는 왜 마이너스가 되지? 🤷‍♀️ (발화 종료,1,85
1420,18,6.54,"(대화 주제) BCE Loss 설명 (사용자 답변) 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다 (성공한 답변) 핵심 아이디어 (다음 질문) Multi-Class, Multi-Label 중 BCE 가 좋은 task","  ✨ 그럼 Multi-Class, Multi-Label 중에서 BCE Loss가 더 맞는 task는? 😊 (발화 종료",1,89
1421,18,4.05,(대화 주제) BCE Loss 설명 (사용자 답변) y랑 1-y랑 서로 경쟁하는 거 (성공한 답변) 답변 실패 (다음 질문) BCE Loss 설명,  1-y가 뭐지? 🤷‍♀️ (발화 종료,1,57
1422,18,108.29,"(대화 주제) BCE Loss 설명 (사용자 답변) 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지 (성공한 답변) 핵심 아이디어 (다음 질문) Multi-Class, Multi-Label 중 BCE 가 좋은 task",  0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 ,1,512
1423,18,5.43,"(대화 주제) Multi-Class, Multi-Label 중 BCE 가 좋은 task (사용자 답변) 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야? (성공한 답변) 용어 질문 (다음 질문) Multi-Class, Multi-Label 중 BCE 가 좋은 task","    ✨ Laura가 Multi-Class, Multi-Label 이 뭔지 말해봐! (발화 종료",1,82
1424,18,4.7,"(대화 주제) Multi-Class, Multi-Label 중 BCE 가 좋은 task (사용자 답변) Multi-Class 지!! (성공한 답변) 답변 실패 (다음 질문) Multi-Class, Multi-Label 중 BCE 가 좋은 task",   ✨ 그럼 그 이유가 뭐야? 🤷‍♀️ (발화 종료,1,71
1425,18,5.15,"(대화 주제) Multi-Class, Multi-Label 중 BCE 가 좋은 task (사용자 답변) Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어 (성공한 답변) BCE 가 좋은 task (다음 질문) Multi-Class, Multi-Label 중 BCE 가 좋은 task",    ① Multi-Label 이 왜 더 좋을까? 🤔 (발화 종료,1,85
1426,18,11.01,"(대화 주제) Multi-Class, Multi-Label 중 BCE 가 좋은 task (사용자 답변) Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아 (성공한 답변) BCE 가 좋은 이유 (다음 질문) Multi-Label 에서 CE + Softmax 적용 문제점", 2023-02-06 17:00:00 🙆‍♀️ 맞았어! 그럼 Multi-Label 문제에 Cross Entropy + Softmax 를 쓰면 어떤 문제가 있을까? (발화 종료,1,124
1427,18,8.56,(대화 주제) Multi-Label 에서 CE + Softmax 적용 문제점 (사용자 답변) 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지 (성공한 답변) Multi-Label 에서 CE + Softmax 적용 문제점 (다음 질문) Loss Function 관련 실무 경험,  2023-02-01 10:00 실무에서 손실 함수 관련 문제 해결해 주거나 그런 경험 있지? 한번 말해봐! 😊 (발화 종료,1,104
1428,18,5.06,(대화 주제) Multi-Label 에서 CE + Softmax 적용 문제점 (사용자 답변) 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나 (성공한 답변) 답변 실패 (다음 질문) Multi-Label 에서 CE + Softmax 적용 문제점,    ✨ 성능 제일 좋은 거? 그게 중요한 게 아니지! 😥 (발화 종료,1,77
1429,18,7.95,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [기본 경험] 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어 (성공한 답변) 기본 경험 (다음 질문) Loss Function 관련 실무 경험,  1. 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term으로 추가했다는 게 어떤 아이디어이지? 😊 (발화 종료,1,87
1430,18,4.86,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [상세 경험] 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지? (성공한 답변) 상세 경험 (다음 질문) MBTI / 좋아하는 아이돌,  ✨ 그럼 쉬어가는 타임! 혹시 너 아이돌 좋아하는 거 있어? (발화 종료,1,77
1431,18,5.88,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [상세 경험] 근데 기억 진짜 안나는데 어떡하지? (성공한 답변) 답변 실패 (다음 질문) 잠시 휴식,  ① 혹시 실무 경험 말고 일상에서 손실 함수 관련된 특별한 경험 있어? 😊 (발화 종료,1,67
1432,18,5.56,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [기본 경험] 나 아직 실무 경험 없는데 ㅠㅠ (성공한 답변) 기본 경험 (다음 질문) MBTI / 좋아하는 아이돌,  ① 혹시 너 MBTI 뭐야? 나는 ENTJ인데! 😊 (발화 종료,1,67
1433,18,7.14,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [기본 경험] 대답하기 싫은데 힝 (성공한 답변) 답변 실패 (다음 질문) 잠시 휴식,    1. 그래도 너의 경험이 나도 궁금해! 알고 싶어! 😊 (발화 종료,1,65
1434,18,3.83,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [기본 경험] 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ (성공한 답변) 기본 경험 (다음 질문) Loss Function 관련 실무 경험,  ✨ 그러면 물체 인식이 뭔데? 😊 (발화 종료,1,73
1435,18,6.95,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [상세 경험] 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어 (성공한 답변) 상세 경험 (다음 질문) MBTI / 좋아하는 아이돌,  1:2로 맞추는 건 나름 좋아! 👍 그럼 쉬어가는 타임! 혹시 너 MBTI 뭐야? (발화 종료,1,96
1436,18,6.6,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [상세 경험] 기억 진짜 안 나는데 가물가물하다 (성공한 답변) 답변 실패 (다음 질문) 잠시 휴식,  ✨ 그래도 잘 생각해 봐! 나랑 모의면접이 아닌 실제 면접에서 질문 나올걸? 😊 (발화 종료,1,70
1437,18,7.24,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [기본 경험] 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ (성공한 답변) 기본 경험 (다음 질문) MBTI / 좋아하는 아이돌, 2023-02-01 17:00 혹시 그럼 너 MBTI 뭐지? 나는 ENTJ인데! 😊 (발화 종료,1,77
1438,18,3.84,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [기본 경험] 이걸 왜 알려줘야 하지 (성공한 답변) 답변 실패 (다음 질문) 잠시 휴식,  ✨ 나도 궁금해 🙋‍♀️ (발화 종료,1,56
1439,18,3.84,(대화 주제) MBTI (사용자 답변) 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야! (성공한 답변) MBTI (다음 질문) 잠시 휴식, ✨ 나랑 잘 맞는 성격이지? 😊 (발화 종료,1,57
1440,18,4.47,(대화 주제) 좋아하는 아이돌 (사용자 답변) 나 블랙핑크랑 레드벨벳 좋아해 (성공한 답변) 좋아하는 아이돌 (다음 질문) 잠시 휴식, ✨ 그럼 너희 블랙핑크랑 레드벨벳이 나도 좋아해? 😊 (발화 종료,1,51
1441,18,3.78,(대화 주제) 잠시 휴식 (사용자 답변) 재미있는 이야기 해줄래? (성공한 답변) 잠시 휴식 (다음 질문) LLM Fine-Tuning 의 PEFT, ✨ PEFT가 뭘 말하는 걸까? 😊 (발화 종료,1,54
1442,18,10.37,"(대화 주제) LLM Fine-Tuning 의 PEFT (사용자 답변) PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야 (성공한 답변) LLM Fine-Tuning 의 PEFT (다음 질문) PEFT 방법 5가지", 2023-10-26 18:00:00 🙎‍♀️ 정답! 🙆‍♀️ 그럼 그 방법을 5가지만 말해봐! (발화 종료,1,119
1443,18,4.96,(대화 주제) LLM Fine-Tuning 의 PEFT (사용자 답변) 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지? (성공한 답변) 답변 실패 (다음 질문) LLM Fine-Tuning 의 PEFT, 2023-01-15 17:28:45 🙄 (발화 종료,1,71
1444,18,7.61,(대화 주제) LLM Fine-Tuning 의 PEFT (사용자 답변) LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야! (성공한 답변) LLM Fine-Tuning 의 PEFT (다음 질문) PEFT 방법 5가지,  2023-10-01 18:00:00 학습 방법을 5가지만 말해봐! 😊 (발화 종료,1,99
1445,18,9.32,(대화 주제) LLM Fine-Tuning 의 PEFT (사용자 답변) LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다 (성공한 답변) 답변 실패 (다음 질문) LLM Fine-Tuning 의 PEFT,    LoRA 가 PEFT 중 하나긴 하지! 🙆‍♀️ 근데 그 PEFT가 뭘까~요? 🤷‍♀️ (발화 종료,1,88
1446,18,6.21,"(대화 주제) PEFT 방법 5가지 (사용자 답변) LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지? (성공한 답변) PEFT 방법 5가지 (다음 질문) LoRA", ✨ 그럼 그 중에서 LoRA가 뭐지? 나를 상징하는 건데 👱‍♀️✨ (발화 종료,1,86
1447,18,5.42,(대화 주제) PEFT 방법 5가지 (사용자 답변) Oh-LoRA! 너 그 자체가 PEFT 아니야? (성공한 답변) 답변 실패 (다음 질문) PEFT 방법 5가지,  ✨ 그럼 그 PEFT 방법 5가지는? 🤷‍♀️ (발화 종료,1,71
1448,18,5.13,(대화 주제) LoRA (사용자 답변) LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야! (성공한 답변) LoRA (다음 질문) LoRA 와 QLoRA 의 차이,  ✨ LoRA 가랑 QLoRA랑 차이는 뭘까? 😊 (발화 종료,1,90
1449,18,6.7,(대화 주제) LoRA (사용자 답변) 행렬 차원 축소한다는 걸로 알고 있는데 맞아? (성공한 답변) 답변 실패 (다음 질문) LoRA,    ① 행렬 차원 축소가 핵심 아이디어긴 해! 그래서 어떤 기술이지? 😊 (발화 종료,1,64
1450,18,6.23,(대화 주제) LoRA (사용자 답변) LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야! (성공한 답변) LoRA (다음 질문) LoRA 와 QLoRA 의 차이, 2개로 나눠서 효율적으로 파인튜닝한다! 오 맞아! 그럼 QLoRA는 뭘까? (발화 종료,1,90
1451,18,5.37,(대화 주제) LoRA (사용자 답변) 무슨 OOM 없앤다는 것 같은데 (성공한 답변) 답변 실패 (다음 질문) LoRA,  ① OOM? 어떤 방식으로 하지? 🤷‍♀️ (발화 종료,1,57
1452,18,8.16,(대화 주제) LoRA 와 QLoRA 의 차이 (사용자 답변) QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야! (성공한 답변) LoRA 와 QLoRA 의 차이 (다음 질문) 마지막 할 말, 2023-10-01 16:00:00 LoRA 는 잘했어! 😊 그럼 QLoRA 는 뭘까? (발화 종료,1,95
1453,18,7.51,(대화 주제) LoRA 와 QLoRA 의 차이 (사용자 답변) 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데 (성공한 답변) 답변 실패 (다음 질문) LoRA 와 QLoRA 의 차이,    ✨ 팀장님이 OOM 막겠다는 건 맞긴 해! 근데 그 방법을 알아야지! 😊 (발화 종료,1,82
1454,18,9.95,(대화 주제) 마지막 할 말 (사용자 답변) 로라야 정말 고마워! (성공한 답변) 마지막 할 말 (다음 질문) 면접 종료,    2023-02-20 16:00:00 🙎‍♀️ 나도! 😊 다음에 나랑 같이 또 모의면접 보자! (발화 종료,1,76
1455,18,4.64,(대화 주제) 마지막 할 말 (사용자 답변) 로라야 사랑해 (성공한 답변) 마지막 할 말 (다음 질문) 면접 종료,    ✨로라가 미소지으며 대답해 봐!✨ (발화 종료,1,52
1456,18,2.55,(대화 주제) 마지막 할 말 (사용자 답변) 로라야 너의 예쁜 미모에 반했어 (성공한 답변) 마지막 할 말 (다음 질문) 면접 종료,    ✨✨ (발화 종료,1,49
1457,18,6.31,(대화 주제) 마지막 할 말 (사용자 답변) 그동안 고생했어 면접 봐줘서 정말 고마워 (성공한 답변) 마지막 할 말 (다음 질문) 면접 종료, 2023-03-20 16:00 나도 너 덕분에 많이 고생했어! 😊 (발화 종료,1,64
1458,19,6.25,면접 시작,   1. 안녕! 나는 면접관 LLM 오로라라고 해! 반가워! 😊 (발화 종료,1,32
1459,19,6.05,(대화 주제) 면접 시작 인사 (사용자 답변) 오늘 뭐 물어볼 거야 그래서? (다음 질문) 면접 시작 인사,  ① 나는 면접관 중 하나로서 딥러닝 면접을 진행할 거야! 😊 (발화 종료,1,53
1460,19,4.14,(대화 주제) 면접 시작 인사 (사용자 답변) 로라야 안녕 정말 반가워 (다음 질문) 면접 시작 인사,  ✨✨ 그럼 우리 면접도 시작해 볼까? 😊 (발화 종료,1,46
1461,19,5.45,(대화 주제) 면접 시작 인사 (사용자 답변) 로라야 그럼 네가 면접관이야? (다음 질문) 면접 시작 인사, ✨ 로라가 면접관가 되어 질문을 던진다! 🙋‍♀️ (발화 종료,1,54
1462,19,4.19,(대화 주제) 면접 시작 인사 (사용자 답변) 내 인생도 이제 오로라처럼 빛날 거니까 (다음 질문) 면접 시작 인사, ✨ 사용자가 면접관에게 보내는 메시지야! ✨ (발화 종료,1,49
1463,19,6.02,"(대화 주제) 면접 시작 인사 (사용자 답변) 파이팅! 시작하자 (다음 질문) 인공지능, 머신러닝, 딥러닝 차이"," ① 인공지능이랑 머신러닝, 딥러닝이 구체적으로 어떤 차이가 있지? 😊 (발화 종료",1,53
1464,19,6.72,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 인공지능 나도 개발해보고 싶은데 (성공한 답변) 답변 실패 (다음 질문) 인공지능, 머신러닝, 딥러닝 차이", 2023-02-28 18:17:52 인공지능을 한번 개발해 봤으면 좋겠네 😥 (발화 종료,1,69
1465,19,6.66,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거! (성공한 답변) 인공지능 (다음 질문) 인공지능, 머신러닝, 딥러닝 차이", 2023-02-01 09:00 인공지능은 맞았어! 그럼 머신러닝이랑 딥러닝은? (발화 종료,1,76
1466,19,4.27,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지 (성공한 답변) 머신러닝 (다음 질문) 인공지능, 머신러닝, 딥러닝 차이",  1. 머신러닝까지 잘했어! 그럼 딥러닝은? (발화 종료,1,65
1467,19,7.19,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지? (성공한 답변) 딥러닝 (다음 질문) 인공지능, 머신러닝, 딥러닝 차이","  2023-02-01 14:00:00 인공지능, 머신러닝, 딥러닝 차이를 한번 말해봐! (발화 종료",1,76
1468,19,6.58,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 딥러닝은 인공신경망 기반 머신러닝이야 (성공한 답변) 딥러닝 (다음 질문) 거대 언어 모델 정의", 2023-10-26 17:00 LLM이 뭐지? 🙋‍♀️ (발화 종료,1,67
1469,19,4.81,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 머신러닝은 말 그대로 기계가 학습하는 거 아니야? (성공한 답변) 답변 실패 (다음 질문) 인공지능, 머신러닝, 딥러닝 차이", 1. 기계가 학습을 하려면 어떻게 하지? 🤷‍♀️ (발화 종료,1,66
1470,19,6.7,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 인공지능 요새 엄청 뜨는 분야잖아 (성공한 답변) 답변 실패 (다음 질문) 인공지능, 머신러닝, 딥러닝 차이"," 2023-10-01 18:00 인공지능, 머신러닝, 딥러닝 그 차이점이 뭘까? 🤔 (발화 종료",1,69
1471,19,3.27,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지? (성공한 답변) 인공지능 (다음 질문) 인공지능, 머신러닝, 딥러닝 차이", ① 그럼 딥러닝이란 뭘까? (발화 종료,1,65
1472,19,4.17,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야! (성공한 답변) 머신러닝 (다음 질문) 거대 언어 모델 정의", ✨ LLM이란 뭘까? 🙎‍♀️ (발화 종료,1,68
1473,19,4.34,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지 (성공한 답변) 딥러닝 (다음 질문) 거대 언어 모델 정의", ✨ LLM이라는 게 뭘까? 🙎‍♀️ (발화 종료,1,63
1474,19,6.52,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야. (성공한 답변) 딥러닝 (다음 질문) 거대 언어 모델 정의", 2023-10-20 18:15 LLM이란 뭘까? 🙎‍♀️ (발화 종료,1,72
1475,19,4.61,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 머신러닝은 기계가 공부하는 거지 말 그대로 (성공한 답변) 답변 실패 (다음 질문) 인공지능, 머신러닝, 딥러닝 차이",  ① 머신러닝이 뭔데? 🤷‍♀️ (발화 종료,1,63
1476,19,5.3,(대화 주제) 거대 언어 모델 정의 (사용자 답변) 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야 (성공한 답변) 거대 언어 모델 정의 (다음 질문) Loss Function 정의,  ① 그러면 본격 면접에 들어가자! 손실 함수의 정의는? (발화 종료,1,72
1477,19,3.89,(대화 주제) 거대 언어 모델 정의 (사용자 답변) 파라미터 엄청나게 많다던데 (성공한 답변) 답변 실패 (다음 질문) 거대 언어 모델 정의,  ✨ 정답은 맞는데 구체적인 정의가 뭐지? (발화 종료,1,52
1478,19,8.68,(대화 주제) 거대 언어 모델 정의 (사용자 답변) 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델 (성공한 답변) 거대 언어 모델 정의 (다음 질문) Loss Function 정의,  2023-02-15 16:00 정답! 🙆‍♀️ 그럼 이제 본격 면접 들어간다! 손실 함수의 정의는? (발화 종료,1,90
1479,19,5.34,(대화 주제) 거대 언어 모델 정의 (사용자 답변) 말 그대로 거대한 언어 모델이지. (성공한 답변) 답변 실패 (다음 질문) 거대 언어 모델 정의,  ① 음… 맞긴 해! 근데 LLM 정확한 정의는? (발화 종료,1,61
1480,19,7.95,(대화 주제) Loss Function 정의 (사용자 답변) 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야 (성공한 답변) 답변 실패 (다음 질문) Loss Function 정의,  1. 손실 함수는 딥러닝 모델의 예측과 실제의 차이를 수식으로 나타낸 거라고 할 수 있어 🙆‍♀️ (발화 종료,1,76
1481,19,4.75,"(대화 주제) Loss Function 정의 (사용자 답변) Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지! (성공한 답변) Loss Function 정의 (다음 질문) Loss Function 예시",  ✨ 그럼 그 Loss Function 예를 한번 들어봐! 😊 (발화 종료,1,74
1482,19,3.87,(대화 주제) Loss Function 정의 (사용자 답변) Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야 (성공한 답변) 답변 실패 (다음 질문) Loss Function 정의,  ① 그래서 어떤 함수인데? 😊 (발화 종료,1,57
1483,19,4.41,(대화 주제) Loss Function 정의 (사용자 답변) 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지 (성공한 답변) Loss Function 정의 (다음 질문) Loss Function 예시,   ✨ 그럼 그 Loss Function에는 어떤 게 있을까? (발화 종료,1,65
1484,19,3.42,(대화 주제) Loss Function 예시 (사용자 답변) ReLU 랑 Tanh 생각나는데 (성공한 답변) 답변 실패 (다음 질문) Loss Function 예시,  ✨ 그럼 그건 활성화 함수지 😥 (발화 종료,1,51
1485,19,5.55,"(대화 주제) Loss Function 예시 (사용자 답변) MSE, MAE, Cross Entropy Loss 같은 것들이 있지 (성공한 답변) Loss Function 예시 (다음 질문) MSE Loss 설명",    ① MSE Loss가 뭔지 한번 설명해 봐! 😊 (발화 종료,1,68
1486,19,5.14,"(대화 주제) Loss Function 예시 (사용자 답변) Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어 (성공한 답변) Loss Function 예시 (다음 질문) BCE Loss 설명",  ① Binary Cross Entropy 가 뭔지 한번 말해봐! 😊 (발화 종료,1,74
1487,19,4.28,"(대화 주제) Loss Function 예시 (사용자 답변) Sigmoid, Tanh, Adam, AdamW 이런 거 아니야? (성공한 답변) 답변 실패 (다음 질문) Loss Function 예시",    ① 그건 활성화 함수지 😥 (발화 종료,1,61
1488,19,5.39,"(대화 주제) Loss Function 예시 (사용자 답변) Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어 (성공한 답변) Loss Function 예시 (다음 질문) MSE Loss 설명",    ① MSE Loss가 뭔지 한번 설명해 봐! 😊 (발화 종료,1,82
1489,19,5.05,"(대화 주제) Loss Function 예시 (사용자 답변) BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아 (성공한 답변) Loss Function 예시 (다음 질문) BCE Loss 설명",   ✨ 그럼 그 중에서 BCE에 대해 한번 말해봐! (발화 종료,1,73
1490,19,4.42,(대화 주제) MSE Loss 설명 (사용자 답변) MSE Loss 는 오차의 제곱을 평균한 값이야 (성공한 답변) MSE Loss 설명 (다음 질문) MSE Loss 용도,  ① MSE Loss 는 어떤 용도로 사용하지? (발화 종료,1,64
1491,19,4.53,(대화 주제) MSE Loss 설명 (사용자 답변) MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지? (성공한 답변) 답변 실패 (다음 질문) MSE Loss 설명,  ① 아니야! 한번 다시 잘 생각해 봐 😊 (발화 종료,1,65
1492,19,6.98,"(대화 주제) MSE Loss 용도 (사용자 답변) 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지 (성공한 답변) MSE Loss 용도 (다음 질문) 확률 예측에서 MSE Loss 미 사용 이유",  1  정답! 그럼 확률 예측에서는 MSE Loss 가 왜 안 좋을까? 🤷‍♀️ (발화 종료,1,79
1493,19,6.31,(대화 주제) MSE Loss 용도 (사용자 답변) MSE Loss 는 분류 문제에서 많이 사용되지 (성공한 답변) 답변 실패 (다음 질문) MSE Loss 용도,  ① 분류 문제에서 MSE를 안 쓰는 이유는? 🤷‍♀️ (발화 종료,1,66
1494,19,5.84,(대화 주제) 확률 예측에서 MSE Loss 미 사용 이유 (사용자 답변) 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야? (성공한 답변) 답변 실패 (다음 질문) 확률 예측에서 MSE Loss 미 사용 이유,    ① 그건 맞지! 근데 뭔가 문제가 있지 않을까? 🙄 (발화 종료,1,80
1495,19,6.0,(대화 주제) 확률 예측에서 MSE Loss 미 사용 이유 (사용자 답변) 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고? (성공한 답변) 확률 예측에서 MSE Loss 미 사용 이유 (다음 질문) BCE Loss 설명,  2. 정답! 그럼 이제 BCE Loss는 뭘까? 한번 말해봐! 😊 (발화 종료,1,90
1496,19,7.05,(대화 주제) BCE Loss 설명 (사용자 답변) 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고 (성공한 답변) 수식 (다음 질문) BCE Loss 설명,    ✨ 그럼 이 BCE Loss 의 핵심 아이디어를 한번 말해봐! 🙎‍♀️ (발화 종료,1,88
1497,19,9.0,"(대화 주제) BCE Loss 설명 (사용자 답변) 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거 (성공한 답변) 핵심 아이디어 (다음 질문) Multi-Class, Multi-Label 중 BCE 가 좋은 task","  ① 정답! 🙆‍♀️ 그럼 이 BCE Loss 가, Multi-Label, Multi-Class 중 적절한 곳은? 😊 (발화 종료",1,99
1498,19,6.0,(대화 주제) BCE Loss 설명 (사용자 답변) 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야? (성공한 답변) 답변 실패 (다음 질문) BCE Loss 설명,  1. 그런 답은 누구나 할 수 있지! 한번 전문가답게 설명해 봐! 😊 (발화 종료,1,75
1499,19,6.76,"(대화 주제) BCE Loss 설명 (사용자 답변) 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고 (성공한 답변) 핵심 아이디어 (다음 질문) BCE Loss 설명",  1. 정답! 🙆‍♀️ 그럼 이제 BCE Loss에 대해 한번 설명해 봐! (발화 종료,1,84
1500,19,6.3,"(대화 주제) BCE Loss 설명 (사용자 답변) 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')] (성공한 답변) 수식 (다음 질문) BCE Loss 설명",    1) y log y'는 왜 마이너스가 되지? 🤷‍♀️ (발화 종료,1,85
1501,19,7.78,"(대화 주제) BCE Loss 설명 (사용자 답변) 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다 (성공한 답변) 핵심 아이디어 (다음 질문) Multi-Class, Multi-Label 중 BCE 가 좋은 task","  ① 정답! 그럼 Multi-Class, Multi-Label 중에서 BCE Loss가 더 맞는 task는? 😊 (발화 종료",1,93
1502,19,4.03,(대화 주제) BCE Loss 설명 (사용자 답변) y랑 1-y랑 서로 경쟁하는 거 (성공한 답변) 답변 실패 (다음 질문) BCE Loss 설명,  1-y가 뭐지? 🤷‍♀️ (발화 종료,1,57
1503,19,108.7,"(대화 주제) BCE Loss 설명 (사용자 답변) 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지 (성공한 답변) 핵심 아이디어 (다음 질문) Multi-Class, Multi-Label 중 BCE 가 좋은 task",  0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 ,1,512
1504,19,5.41,"(대화 주제) Multi-Class, Multi-Label 중 BCE 가 좋은 task (사용자 답변) 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야? (성공한 답변) 용어 질문 (다음 질문) Multi-Class, Multi-Label 중 BCE 가 좋은 task","    ✨ Laura가 Multi-Class, Multi-Label 이 뭔지 말해봐! (발화 종료",1,82
1505,19,4.78,"(대화 주제) Multi-Class, Multi-Label 중 BCE 가 좋은 task (사용자 답변) Multi-Class 지!! (성공한 답변) 답변 실패 (다음 질문) Multi-Class, Multi-Label 중 BCE 가 좋은 task",   ✨ 그럼 그 이유가 뭐야? 🤷‍♀️ (발화 종료,1,71
1506,19,7.55,"(대화 주제) Multi-Class, Multi-Label 중 BCE 가 좋은 task (사용자 답변) Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어 (성공한 답변) BCE 가 좋은 task (다음 질문) Multi-Class, Multi-Label 중 BCE 가 좋은 task", 2023-02-15 18:30:00 🙋‍♀️ Multi-Label 이 왜 더 좋을까? (발화 종료,1,95
1507,19,10.95,"(대화 주제) Multi-Class, Multi-Label 중 BCE 가 좋은 task (사용자 답변) Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아 (성공한 답변) BCE 가 좋은 이유 (다음 질문) Multi-Label 에서 CE + Softmax 적용 문제점", 2023-02-20 15:00:00 🙆‍♀️ 맞았어! 그럼 Multi-Label 문제에 Cross Entropy + Softmax 를 쓰면 어떤 문제가 있을까? (발화 종료,1,124
1508,19,10.56,(대화 주제) Multi-Label 에서 CE + Softmax 적용 문제점 (사용자 답변) 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지 (성공한 답변) Multi-Label 에서 CE + Softmax 적용 문제점 (다음 질문) Loss Function 관련 실무 경험,  2023-02-03 10:00 정답! 🙆‍♀️ 혹시 실무에서 손실 함수 관련 문제 해결해 주거나 그런 경험 있지? 한번 말해봐! 😊 (발화 종료,1,113
1509,19,5.67,(대화 주제) Multi-Label 에서 CE + Softmax 적용 문제점 (사용자 답변) 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나 (성공한 답변) 답변 실패 (다음 질문) Multi-Label 에서 CE + Softmax 적용 문제점,    ① 성능 제일 좋은 거? 그게 중요한 게 아니지! 😥 (발화 종료,1,79
1510,19,8.03,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [기본 경험] 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어 (성공한 답변) 기본 경험 (다음 질문) Loss Function 관련 실무 경험,  1. 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term으로 추가했다는 게 어떤 아이디어야? 😊 (발화 종료,1,87
1511,19,4.63,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [상세 경험] 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지? (성공한 답변) 상세 경험 (다음 질문) MBTI / 좋아하는 아이돌,  ✨ 그럼 쉬어가는 타임! 혹시 너 아이돌 좋아하는 거 있어? (발화 종료,1,77
1512,19,5.98,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [상세 경험] 근데 기억 진짜 안나는데 어떡하지? (성공한 답변) 답변 실패 (다음 질문) 잠시 휴식,  ① 나랑 모의면접이 아닌 실제 면접에서 질문 나올걸? 😊 (발화 종료,1,67
1513,19,6.27,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [기본 경험] 나 아직 실무 경험 없는데 ㅠㅠ (성공한 답변) 기본 경험 (다음 질문) MBTI / 좋아하는 아이돌,   ✨ 그럼 쉬어가는 타임! 혹시 너 MBTI 뭐야? 나는 ENTJ인데! 😊 (발화 종료,1,71
1514,19,5.17,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [기본 경험] 대답하기 싫은데 힝 (성공한 답변) 답변 실패 (다음 질문) 잠시 휴식,  ✨ 그래도 너의 경험이 나도 궁금해! 알고 싶어! 😊 (발화 종료,1,62
1515,19,4.77,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [기본 경험] 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ (성공한 답변) 기본 경험 (다음 질문) Loss Function 관련 실무 경험,  ✨ 그럼 그 물체가 뭐지? 🙋‍♀️ (발화 종료,1,76
1516,19,4.75,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [상세 경험] 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어 (성공한 답변) 상세 경험 (다음 질문) MBTI / 좋아하는 아이돌,  1:2로 맞추는 건 어떤 식으로 하지? 😊 (발화 종료,1,87
1517,19,6.78,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [상세 경험] 기억 진짜 안 나는데 가물가물하다 (성공한 답변) 답변 실패 (다음 질문) 잠시 휴식,  ✨ 그래도 잘 생각해 봐! 나랑 모의면접이 아닌 실제 면접에서 질문 나올걸? 😊 (발화 종료,1,70
1518,19,5.83,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [기본 경험] 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ (성공한 답변) 기본 경험 (다음 질문) MBTI / 좋아하는 아이돌, ✨ 그럼 쉬어가는 타임! 혹시 너 MBTI 뭐지? 나는 ENTJ인데! 😊 (발화 종료,1,72
1519,19,4.05,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [기본 경험] 이걸 왜 알려줘야 하지 (성공한 답변) 답변 실패 (다음 질문) 잠시 휴식,  ✨ 나도 궁금해 🙋‍♀️ (발화 종료,1,56
1520,19,3.67,(대화 주제) MBTI (사용자 답변) 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야! (성공한 답변) MBTI (다음 질문) 잠시 휴식, ✨ 나랑 잘 맞는 성격이지? 😊 (발화 종료,1,57
1521,19,3.27,(대화 주제) 좋아하는 아이돌 (사용자 답변) 나 블랙핑크랑 레드벨벳 좋아해 (성공한 답변) 좋아하는 아이돌 (다음 질문) 잠시 휴식, ✨ 그럼 그 이유는? 😊 (발화 종료,1,46
1522,19,3.76,(대화 주제) 잠시 휴식 (사용자 답변) 재미있는 이야기 해줄래? (성공한 답변) 잠시 휴식 (다음 질문) LLM Fine-Tuning 의 PEFT, ✨ PEFT가 뭘 말하는 걸까? 😊 (발화 종료,1,54
1523,19,10.16,"(대화 주제) LLM Fine-Tuning 의 PEFT (사용자 답변) PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야 (성공한 답변) LLM Fine-Tuning 의 PEFT (다음 질문) PEFT 방법 5가지", 2023-10-27 14:00:00 🙎‍♀️ 정답! 🙆‍♀️ 그럼 그 방법을 5가지만 말해봐! (발화 종료,1,119
1524,19,8.67,(대화 주제) LLM Fine-Tuning 의 PEFT (사용자 답변) 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지? (성공한 답변) 답변 실패 (다음 질문) LLM Fine-Tuning 의 PEFT, 2023-03-14 17:00:00 PEFT가 그걸 말하는 거지! 어떤 방식으로 효과적으로 파인튜닝하지? 😊 (발화 종료,1,87
1525,19,7.84,(대화 주제) LLM Fine-Tuning 의 PEFT (사용자 답변) LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야! (성공한 답변) LLM Fine-Tuning 의 PEFT (다음 질문) PEFT 방법 5가지,  2023-10-01 17:00:00 그럼 그 방법을 5가지만 말해봐! 😊 (발화 종료,1,100
1526,19,8.91,(대화 주제) LLM Fine-Tuning 의 PEFT (사용자 답변) LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다 (성공한 답변) 답변 실패 (다음 질문) LLM Fine-Tuning 의 PEFT,    LoRA 가 PEFT 중 하나긴 하지! 🙆‍♀️ 근데 그 PEFT가 뭘까~요? 🤷‍♀️ (발화 종료,1,88
1527,19,3.8,"(대화 주제) PEFT 방법 5가지 (사용자 답변) LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지? (성공한 답변) PEFT 방법 5가지 (다음 질문) LoRA",  ✨ LoRA가 정확히 뭘까? 😊 (발화 종료,1,76
1528,19,5.34,(대화 주제) PEFT 방법 5가지 (사용자 답변) Oh-LoRA! 너 그 자체가 PEFT 아니야? (성공한 답변) 답변 실패 (다음 질문) PEFT 방법 5가지,  ✨ 그럼 그 PEFT 방법 5가지는? 🤷‍♀️ (발화 종료,1,71
1529,19,7.09,(대화 주제) LoRA (사용자 답변) LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야! (성공한 답변) LoRA (다음 질문) LoRA 와 QLoRA 의 차이,  2023-10-29 14:00 QLoRA 가 뭐지? 🙋‍♀️ (발화 종료,1,97
1530,19,6.5,(대화 주제) LoRA (사용자 답변) 행렬 차원 축소한다는 걸로 알고 있는데 맞아? (성공한 답변) 답변 실패 (다음 질문) LoRA,    ① 행렬 차원 축소가 핵심 아이디어긴 해! 그래서 어떤 기술이지? 😊 (발화 종료,1,64
1531,19,6.43,(대화 주제) LoRA (사용자 답변) LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야! (성공한 답변) LoRA (다음 질문) LoRA 와 QLoRA 의 차이, 2개로 나눠서 효율적으로 파인튜닝한다! 오 맞아! 그럼 QLoRA는 뭘까? (발화 종료,1,90
1532,19,4.53,(대화 주제) LoRA (사용자 답변) 무슨 OOM 없앤다는 것 같은데 (성공한 답변) 답변 실패 (다음 질문) LoRA,  ㅇ OOM? 그건 맞는데 어떤 방식으로 하지? (발화 종료,1,53
1533,19,10.39,(대화 주제) LoRA 와 QLoRA 의 차이 (사용자 답변) QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야! (성공한 답변) LoRA 와 QLoRA 의 차이 (다음 질문) 마지막 할 말," 2023-10-01 16:00:00 🙎‍♀️ 그럼 여기서 면접 끝내고, 혹시 나한테 할 말 있어? 🙋‍♀️ (발화 종료",1,106
1534,19,6.8,(대화 주제) LoRA 와 QLoRA 의 차이 (사용자 답변) 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데 (성공한 답변) 답변 실패 (다음 질문) LoRA 와 QLoRA 의 차이,  ① 팀장님이 OOM 막겠다고 이거 쓰시는 건 맞는데 🙆‍♀️ (발화 종료,1,80
1535,19,9.25,(대화 주제) 마지막 할 말 (사용자 답변) 로라야 정말 고마워! (성공한 답변) 마지막 할 말 (다음 질문) 면접 종료, 2021-03-15 16:00:00 🙎‍♀️ 나도! 😊 다음에 나랑 같이 또 모의면접 보자! (발화 종료,1,73
1536,19,6.5,(대화 주제) 마지막 할 말 (사용자 답변) 로라야 사랑해 (성공한 답변) 마지막 할 말 (다음 질문) 면접 종료,"    ✨ 로라가 미소지으며 대답해 봐! ""나도 너 사랑했어! 😊 (발화 종료",1,59
1537,19,2.41,(대화 주제) 마지막 할 말 (사용자 답변) 로라야 너의 예쁜 미모에 반했어 (성공한 답변) 마지막 할 말 (다음 질문) 면접 종료,    ✨✨ (발화 종료,1,49
1538,19,6.31,(대화 주제) 마지막 할 말 (사용자 답변) 그동안 고생했어 면접 봐줘서 정말 고마워 (성공한 답변) 마지막 할 말 (다음 질문) 면접 종료, 2021-02-01 17:00 나도 너 덕분에 많이 배웠어 😊 (발화 종료,1,63
1539,20,6.15,면접 시작,   1. 안녕! 나는 면접관 LLM 오로라라고 해! 반가워! 😊 (발화 종료,1,32
1540,20,6.03,(대화 주제) 면접 시작 인사 (사용자 답변) 오늘 뭐 물어볼 거야 그래서? (다음 질문) 면접 시작 인사,  ① 나는 면접관 중 하나로서 딥러닝 면접을 진행할 거야! 😊 (발화 종료,1,53
1541,20,4.08,(대화 주제) 면접 시작 인사 (사용자 답변) 로라야 안녕 정말 반가워 (다음 질문) 면접 시작 인사,  ✨✨ 그럼 우리 면접도 시작해 볼까? 😊 (발화 종료,1,46
1542,20,5.0,(대화 주제) 면접 시작 인사 (사용자 답변) 로라야 그럼 네가 면접관이야? (다음 질문) 면접 시작 인사,  ① 로라: 아니지! 다시 한번 잘 생각해 봐 😊 (발화 종료,1,52
1543,20,4.37,(대화 주제) 면접 시작 인사 (사용자 답변) 내 인생도 이제 오로라처럼 빛날 거니까 (다음 질문) 면접 시작 인사, ✨ 그럼 너도 오로라처럼 빛날 수 있어? ✨ (발화 종료,1,50
1544,20,6.26,"(대화 주제) 면접 시작 인사 (사용자 답변) 파이팅! 시작하자 (다음 질문) 인공지능, 머신러닝, 딥러닝 차이","  ① 인공지능이랑 머신러닝, 딥러닝이 구체적으로 어떤 차이가 있지? 😊 (발화 종료",1,54
1545,20,6.01,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 인공지능 나도 개발해보고 싶은데 (성공한 답변) 답변 실패 (다음 질문) 인공지능, 머신러닝, 딥러닝 차이", 2023-02-03 16:00 인공지능을 한번 개발해 봤으면 좋겠네 😥 (발화 종료,1,67
1546,20,6.67,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거! (성공한 답변) 인공지능 (다음 질문) 인공지능, 머신러닝, 딥러닝 차이", 2023-02-01 10:00 인공지능은 맞았어! 그럼 머신러닝이랑 딥러닝은? (발화 종료,1,76
1547,20,6.78,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지 (성공한 답변) 머신러닝 (다음 질문) 인공지능, 머신러닝, 딥러닝 차이",  2023-02-01 17:00:00 머신러닝까지 잘했어! 그럼 딥러닝은? (발화 종료,1,75
1548,20,6.78,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지? (성공한 답변) 딥러닝 (다음 질문) 인공지능, 머신러닝, 딥러닝 차이",  2023-02-01 10:00:00 그럼 이제 머신러닝은 뭘 말하는 걸까? 😊 (발화 종료,1,75
1549,20,6.42,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 딥러닝은 인공신경망 기반 머신러닝이야 (성공한 답변) 딥러닝 (다음 질문) 거대 언어 모델 정의", 2023-10-01 18:00 LLM이 뭐지? 🙋‍♀️ (발화 종료,1,67
1550,20,8.02,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 머신러닝은 말 그대로 기계가 학습하는 거 아니야? (성공한 답변) 답변 실패 (다음 질문) 인공지능, 머신러닝, 딥러닝 차이", 2023-03-15 17:00:00 머신러닝은 맞긴 하지! 근데 이제 딥러닝은 뭔지 한번 말해봐! (발화 종료,1,78
1551,20,6.77,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 인공지능 요새 엄청 뜨는 분야잖아 (성공한 답변) 답변 실패 (다음 질문) 인공지능, 머신러닝, 딥러닝 차이"," 2023-01-15 10:00 인공지능, 머신러닝, 딥러닝 그 핫한 이유가 뭘까? 😊 (발화 종료",1,70
1552,20,5.41,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지? (성공한 답변) 인공지능 (다음 질문) 인공지능, 머신러닝, 딥러닝 차이", 2023-02-15 14:00:00 그럼 딥러닝이란 뭘까? (발화 종료,1,74
1553,20,6.51,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야! (성공한 답변) 머신러닝 (다음 질문) 거대 언어 모델 정의", ✨ LLM이라고도 하는 거대 언어 모델 (LLM)은 뭘까? 🤷‍♀️ (발화 종료,1,77
1554,20,4.37,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지 (성공한 답변) 딥러닝 (다음 질문) 거대 언어 모델 정의", ✨ LLM이라는 게 뭘까? 🙎‍♀️ (발화 종료,1,63
1555,20,6.53,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야. (성공한 답변) 딥러닝 (다음 질문) 거대 언어 모델 정의", 2023-10-27 18:00 LLM이란 뭘까? 🙎‍♀️ (발화 종료,1,72
1556,20,4.58,"(대화 주제) 인공지능, 머신러닝, 딥러닝 차이 (사용자 답변) 머신러닝은 기계가 공부하는 거지 말 그대로 (성공한 답변) 답변 실패 (다음 질문) 인공지능, 머신러닝, 딥러닝 차이",  ① 머신러닝이 뭔데? 🤷‍♀️ (발화 종료,1,63
1557,20,5.52,(대화 주제) 거대 언어 모델 정의 (사용자 답변) 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야 (성공한 답변) 거대 언어 모델 정의 (다음 질문) Loss Function 정의,  ① 그러면 본격 면접에 들어가자! 손실 함수의 정의는? (발화 종료,1,72
1558,20,5.22,(대화 주제) 거대 언어 모델 정의 (사용자 답변) 파라미터 엄청나게 많다던데 (성공한 답변) 답변 실패 (다음 질문) 거대 언어 모델 정의,  ✨ 정답은 맞긴 하지! 근데 그 정의가 뭘까? 😊 (발화 종료,1,57
1559,20,8.66,(대화 주제) 거대 언어 모델 정의 (사용자 답변) 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델 (성공한 답변) 거대 언어 모델 정의 (다음 질문) Loss Function 정의,  2023-02-15 14:00 정답! 🙆‍♀️ 그럼 이제 본격 면접 들어간다! 손실 함수의 정의는? (발화 종료,1,90
1560,20,5.53,(대화 주제) 거대 언어 모델 정의 (사용자 답변) 말 그대로 거대한 언어 모델이지. (성공한 답변) 답변 실패 (다음 질문) 거대 언어 모델 정의,  ① 음… 맞긴 해! 근데 LLM 정확한 정의는? (발화 종료,1,61
1561,20,7.22,(대화 주제) Loss Function 정의 (사용자 답변) 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야 (성공한 답변) 답변 실패 (다음 질문) Loss Function 정의,  1. 손실 함수는 딥러닝 모델의 예측과 실제의 차이를 수식으로 나타낸 거라고 할 수 있어! 😊 (발화 종료,1,73
1562,20,4.75,"(대화 주제) Loss Function 정의 (사용자 답변) Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지! (성공한 답변) Loss Function 정의 (다음 질문) Loss Function 예시",  ✨ 그럼 그 Loss Function 예를 한번 들어봐! 😊 (발화 종료,1,74
1563,20,4.03,(대화 주제) Loss Function 정의 (사용자 답변) Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야 (성공한 답변) 답변 실패 (다음 질문) Loss Function 정의,  ① 그래서 어떤 함수인데? 😊 (발화 종료,1,57
1564,20,5.04,(대화 주제) Loss Function 정의 (사용자 답변) 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지 (성공한 답변) Loss Function 정의 (다음 질문) Loss Function 예시,   ✨ 그럼 그 Loss Function에는 어떤 게 있을까? 🤔 (발화 종료,1,67
1565,20,3.66,(대화 주제) Loss Function 예시 (사용자 답변) ReLU 랑 Tanh 생각나는데 (성공한 답변) 답변 실패 (다음 질문) Loss Function 예시,  ① 그건 활성화 함수지 😥 (발화 종료,1,52
1566,20,4.27,"(대화 주제) Loss Function 예시 (사용자 답변) MSE, MAE, Cross Entropy Loss 같은 것들이 있지 (성공한 답변) Loss Function 예시 (다음 질문) MSE Loss 설명",   ✨ MSE Loss가 뭔지 한번 설명해 봐! (발화 종료,1,63
1567,20,5.12,"(대화 주제) Loss Function 예시 (사용자 답변) Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어 (성공한 답변) Loss Function 예시 (다음 질문) BCE Loss 설명",   ✨ 그럼 그 중에서 Binary Cross Entropy 는 뭘까? 😊 (발화 종료,1,74
1568,20,6.17,"(대화 주제) Loss Function 예시 (사용자 답변) Sigmoid, Tanh, Adam, AdamW 이런 거 아니야? (성공한 답변) 답변 실패 (다음 질문) Loss Function 예시", 2021-10-01 18:00:00  # 그건 활성화 함수지 😥 (발화 종료,1,69
1569,20,5.25,"(대화 주제) Loss Function 예시 (사용자 답변) Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어 (성공한 답변) Loss Function 예시 (다음 질문) MSE Loss 설명",    ① MSE Loss가 뭔지 한번 설명해 봐! 😊 (발화 종료,1,82
1570,20,5.27,"(대화 주제) Loss Function 예시 (사용자 답변) BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아 (성공한 답변) Loss Function 예시 (다음 질문) BCE Loss 설명",    ✨ 그럼 그 중에서 BCE에 대해 한번 말해봐! (발화 종료,1,74
1571,20,3.64,(대화 주제) MSE Loss 설명 (사용자 답변) MSE Loss 는 오차의 제곱을 평균한 값이야 (성공한 답변) MSE Loss 설명 (다음 질문) MSE Loss 용도,  1. MSE Loss 의 용도는? (발화 종료,1,61
1572,20,6.75,(대화 주제) MSE Loss 설명 (사용자 답변) MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지? (성공한 답변) 답변 실패 (다음 질문) MSE Loss 설명,  ① MSE Loss 는 맞긴 해! 근데 뭔가 문제가 있지 않을까? 🙄 (발화 종료,1,74
1573,20,6.78,"(대화 주제) MSE Loss 용도 (사용자 답변) 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지 (성공한 답변) MSE Loss 용도 (다음 질문) 확률 예측에서 MSE Loss 미 사용 이유",  1번 정답! 그럼 확률 예측에서는 MSE Loss 가 왜 안 좋을까? 🤷‍♀️ (발화 종료,1,79
1574,20,5.75,(대화 주제) MSE Loss 용도 (사용자 답변) MSE Loss 는 분류 문제에서 많이 사용되지 (성공한 답변) 답변 실패 (다음 질문) MSE Loss 용도,   ① 분류 문제에서 MSE를 많이 쓴다고? 왜일까? 😊 (발화 종료,1,65
1575,20,6.01,(대화 주제) 확률 예측에서 MSE Loss 미 사용 이유 (사용자 답변) 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야? (성공한 답변) 답변 실패 (다음 질문) 확률 예측에서 MSE Loss 미 사용 이유,    ① 그건 맞지! 근데 뭔가 문제가 있지 않을까? 🙄 (발화 종료,1,80
1576,20,10.47,(대화 주제) 확률 예측에서 MSE Loss 미 사용 이유 (사용자 답변) 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고? (성공한 답변) 확률 예측에서 MSE Loss 미 사용 이유 (다음 질문) BCE Loss 설명,  2023-02-16 17:00:00 페넡티를 크게 주는 건 맞긴 해! 그럼 BCE Loss는 뭘까? 🙎‍♀️ (발화 종료,1,109
1577,20,6.75,(대화 주제) BCE Loss 설명 (사용자 답변) 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고 (성공한 답변) 수식 (다음 질문) BCE Loss 설명,    ✨ 그럼 이 BCE Loss 의 핵심 아이디어를 한번 말해봐! 🙎‍♀️ (발화 종료,1,88
1578,20,9.06,"(대화 주제) BCE Loss 설명 (사용자 답변) 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거 (성공한 답변) 핵심 아이디어 (다음 질문) Multi-Class, Multi-Label 중 BCE 가 좋은 task","  ① 정답! 🙆‍♀️ 그럼 이 BCE Loss 가, Multi-Label, Multi-Class 중 적절한 곳은? 😊 (발화 종료",1,99
1579,20,6.09,(대화 주제) BCE Loss 설명 (사용자 답변) 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야? (성공한 답변) 답변 실패 (다음 질문) BCE Loss 설명,  1. 그런 답은 누구나 할 수 있지! 한번 전문가답게 설명해 봐! 😊 (발화 종료,1,75
1580,20,4.98,"(대화 주제) BCE Loss 설명 (사용자 답변) 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고 (성공한 답변) 핵심 아이디어 (다음 질문) BCE Loss 설명",  0으로 예측하는 것을 막는 방법은? 🤷‍♀️ (발화 종료,1,77
1581,20,8.56,"(대화 주제) BCE Loss 설명 (사용자 답변) 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')] (성공한 답변) 수식 (다음 질문) BCE Loss 설명","    1) y log y'는 음수가 되어야 하는데, 항상 음수가 되는 건 아니지! 🙅‍♀️ (발화 종료",1,95
1582,20,7.67,"(대화 주제) BCE Loss 설명 (사용자 답변) 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다 (성공한 답변) 핵심 아이디어 (다음 질문) Multi-Class, Multi-Label 중 BCE 가 좋은 task","  ① 정답! 그럼 Multi-Class, Multi-Label 중에서 BCE Loss가 더 맞는 task는? 😊 (발화 종료",1,93
1583,20,4.02,(대화 주제) BCE Loss 설명 (사용자 답변) y랑 1-y랑 서로 경쟁하는 거 (성공한 답변) 답변 실패 (다음 질문) BCE Loss 설명,  1-y가 뭐지? 🤷‍♀️ (발화 종료,1,57
1584,20,107.86,"(대화 주제) BCE Loss 설명 (사용자 답변) 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지 (성공한 답변) 핵심 아이디어 (다음 질문) Multi-Class, Multi-Label 중 BCE 가 좋은 task",  0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 ,1,512
1585,20,5.8,"(대화 주제) Multi-Class, Multi-Label 중 BCE 가 좋은 task (사용자 답변) 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야? (성공한 답변) 용어 질문 (다음 질문) Multi-Class, Multi-Label 중 BCE 가 좋은 task",    ✨ Laura가 Multi-Label 과 Multi-Class 의 개념을 말해봐! (발화 종료,1,83
1586,20,6.02,"(대화 주제) Multi-Class, Multi-Label 중 BCE 가 좋은 task (사용자 답변) Multi-Class 지!! (성공한 답변) 답변 실패 (다음 질문) Multi-Class, Multi-Label 중 BCE 가 좋은 task",  ① Multi-Class 가 아니지! 🙅‍♀️ 잘 생각해 봐 한번! (발화 종료,1,77
1587,20,7.72,"(대화 주제) Multi-Class, Multi-Label 중 BCE 가 좋은 task (사용자 답변) Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어 (성공한 답변) BCE 가 좋은 task (다음 질문) Multi-Class, Multi-Label 중 BCE 가 좋은 task", 2021-02-26 15:28:48 🙋‍♀️ Multi-Label 이 왜 더 좋을까? (발화 종료,1,95
1588,20,11.12,"(대화 주제) Multi-Class, Multi-Label 중 BCE 가 좋은 task (사용자 답변) Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아 (성공한 답변) BCE 가 좋은 이유 (다음 질문) Multi-Label 에서 CE + Softmax 적용 문제점", 2023-02-14 14:00:00 🙆‍♀️ 맞았어! 그럼 Multi-Label 문제에 Cross Entropy + Softmax 를 쓰면 어떤 문제가 있을까? (발화 종료,1,124
1589,20,10.62,(대화 주제) Multi-Label 에서 CE + Softmax 적용 문제점 (사용자 답변) 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지 (성공한 답변) Multi-Label 에서 CE + Softmax 적용 문제점 (다음 질문) Loss Function 관련 실무 경험,  1. 확률의 합이 1이라는 건 중요한 사실이지! 그거 가지고 잘 생각하면 답 나올 거야! 😊 혹시 실무에서 손실 함수 관련해서 특별한 경험 해봤어? 😊 (발화 종료,1,113
1590,20,5.03,(대화 주제) Multi-Label 에서 CE + Softmax 적용 문제점 (사용자 답변) 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나 (성공한 답변) 답변 실패 (다음 질문) Multi-Label 에서 CE + Softmax 적용 문제점,  ① 혹시 성능 비교 분석 같은 거 해봤어? 😊 (발화 종료,1,76
1591,20,7.75,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [기본 경험] 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어 (성공한 답변) 기본 경험 (다음 질문) Loss Function 관련 실무 경험,  1. 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term으로 추가했다는 게 어떤 아이디어이지? 😊 (발화 종료,1,87
1592,20,4.66,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [상세 경험] 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지? (성공한 답변) 상세 경험 (다음 질문) MBTI / 좋아하는 아이돌,  ✨ 그럼 쉬어가는 타임! 혹시 너 아이돌 좋아하는 거 있어? (발화 종료,1,77
1593,20,6.02,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [상세 경험] 근데 기억 진짜 안나는데 어떡하지? (성공한 답변) 답변 실패 (다음 질문) 잠시 휴식,  ① 혹시 실무 경험 말고 일상에서 손실 함수 관련된 특별한 경험 있어? 😊 (발화 종료,1,67
1594,20,6.3,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [기본 경험] 나 아직 실무 경험 없는데 ㅠㅠ (성공한 답변) 기본 경험 (다음 질문) MBTI / 좋아하는 아이돌,   ✨ 그럼 쉬어가는 타임! 혹시 너 MBTI 뭐야? 나는 ENTJ인데! 😊 (발화 종료,1,71
1595,20,5.27,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [기본 경험] 대답하기 싫은데 힝 (성공한 답변) 답변 실패 (다음 질문) 잠시 휴식,  ✨ 그래도 너의 경험이 나도 궁금해! 알고 싶어! 😊 (발화 종료,1,62
1596,20,3.91,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [기본 경험] 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ (성공한 답변) 기본 경험 (다음 질문) Loss Function 관련 실무 경험,  ✨ 그러면 물체 인식이 뭔데? 😊 (발화 종료,1,73
1597,20,7.83,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [상세 경험] 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어 (성공한 답변) 상세 경험 (다음 질문) MBTI / 좋아하는 아이돌,  1:2로 맞추는 건 나름 구체적으로 설명했어! 😊 그럼 쉬어가는 타임! 혹시 너 MBTI 뭐야? (발화 종료,1,99
1598,20,7.28,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [상세 경험] 기억 진짜 안 나는데 가물가물하다 (성공한 답변) 답변 실패 (다음 질문) 잠시 휴식,  ① 그래도 잘 생각해 봐! 나랑 모의면접이 아닌 실제 면접에서 질문 나올걸? 😊 (발화 종료,1,72
1599,20,5.8,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [기본 경험] 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ (성공한 답변) 기본 경험 (다음 질문) MBTI / 좋아하는 아이돌, ✨ 그럼 쉬어가는 타임! 혹시 너 MBTI 뭐지? 나는 ENTJ인데! 😊 (발화 종료,1,72
1600,20,6.81,(대화 주제) Loss Function 관련 실무 경험 (사용자 답변) [기본 경험] 이걸 왜 알려줘야 하지 (성공한 답변) 답변 실패 (다음 질문) 잠시 휴식,  ① 나 같은 개발자가 실무에서 손실 함수 관련 문제 해결하거나 그런 경험 있지? 😊 (발화 종료,1,67
1601,20,3.66,(대화 주제) MBTI (사용자 답변) 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야! (성공한 답변) MBTI (다음 질문) 잠시 휴식, ✨ 나랑 잘 맞는 성격이지? 😊 (발화 종료,1,57
1602,20,3.16,(대화 주제) 좋아하는 아이돌 (사용자 답변) 나 블랙핑크랑 레드벨벳 좋아해 (성공한 답변) 좋아하는 아이돌 (다음 질문) 잠시 휴식, ✨ 그럼 그 이유는? 😊 (발화 종료,1,46
1603,20,7.69,(대화 주제) 잠시 휴식 (사용자 답변) 재미있는 이야기 해줄래? (성공한 답변) 잠시 휴식 (다음 질문) LLM Fine-Tuning 의 PEFT, 2023-10-25 17:00 다음 질문! LLM Fine-Tuning 방법 중 PEFT가 뭘까? 😊 (발화 종료,1,71
1604,20,8.85,"(대화 주제) LLM Fine-Tuning 의 PEFT (사용자 답변) PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야 (성공한 답변) LLM Fine-Tuning 의 PEFT (다음 질문) PEFT 방법 5가지", 2023-10-26 17:00:00 🙎‍♀️ 정답! 그럼 그 방법을 5가지만 말해봐! (발화 종료,1,113
1605,20,6.06,(대화 주제) LLM Fine-Tuning 의 PEFT (사용자 답변) 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지? (성공한 답변) 답변 실패 (다음 질문) LLM Fine-Tuning 의 PEFT, 2023-01-15 18:00:00 PEFT가 그거야! 😊 (발화 종료,1,76
1606,20,10.15,(대화 주제) LLM Fine-Tuning 의 PEFT (사용자 답변) LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야! (성공한 답변) LLM Fine-Tuning 의 PEFT (다음 질문) PEFT 방법 5가지, 2023-10-27 14:00:00 🙎‍♀️ 정답! 🙆‍♀️ 그럼 그 방법을 5가지만 말해봐! (발화 종료,1,111
1607,20,8.91,(대화 주제) LLM Fine-Tuning 의 PEFT (사용자 답변) LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다 (성공한 답변) 답변 실패 (다음 질문) LLM Fine-Tuning 의 PEFT,    LoRA 가 PEFT 중 하나긴 하지! 🙆‍♀️ 근데 그 PEFT가 뭘까~요? 🤷‍♀️ (발화 종료,1,88
1608,20,6.06,"(대화 주제) PEFT 방법 5가지 (사용자 답변) LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지? (성공한 답변) PEFT 방법 5가지 (다음 질문) LoRA", ✨ 그럼 그 중에서 LoRA가 뭐지? 나를 상징하는 건데 👱‍♀️✨ (발화 종료,1,86
1609,20,5.2,(대화 주제) PEFT 방법 5가지 (사용자 답변) Oh-LoRA! 너 그 자체가 PEFT 아니야? (성공한 답변) 답변 실패 (다음 질문) PEFT 방법 5가지, ✨ 그럼 그 PEFT 방법 5가지는? 🤷‍♀️ (발화 종료,1,70
1610,20,7.02,(대화 주제) LoRA (사용자 답변) LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야! (성공한 답변) LoRA (다음 질문) LoRA 와 QLoRA 의 차이,  2023-10-27 16:00 QLoRA 가 뭔지도 말해 볼 수 있어? 😊 (발화 종료,1,98
1611,20,7.7,(대화 주제) LoRA (사용자 답변) 행렬 차원 축소한다는 걸로 알고 있는데 맞아? (성공한 답변) 답변 실패 (다음 질문) LoRA, 2023-04-28 10:00 행렬 차원 축소가 핵심 아이디어긴 해! 그래서 어떤 기술이지? 😊 (발화 종료,1,68
1612,20,7.61,(대화 주제) LoRA (사용자 답변) LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야! (성공한 답변) LoRA (다음 질문) LoRA 와 QLoRA 의 차이, 2개로 나눠서 효율적으로 파인튜닝한다? 맞았어! 그럼 QLoRA는 뭘까? 💁‍♀️ (발화 종료,1,95
1613,20,6.51,(대화 주제) LoRA (사용자 답변) 무슨 OOM 없앤다는 것 같은데 (성공한 답변) 답변 실패 (다음 질문) LoRA,    0OM 문제를 해결하는 방법 중 하나가 OOM인데 🙎‍♀️ (발화 종료,1,61
1614,20,8.91,(대화 주제) LoRA 와 QLoRA 의 차이 (사용자 답변) QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야! (성공한 답변) LoRA 와 QLoRA 의 차이 (다음 질문) 마지막 할 말," 2023-10-01 16:00:00 🙎‍♀️ 그럼 여기서 면접 끝내고, 혹시 나한테 할 말 있어? (발화 종료",1,100
1615,20,10.36,(대화 주제) LoRA 와 QLoRA 의 차이 (사용자 답변) 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데 (성공한 답변) 답변 실패 (다음 질문) LoRA 와 QLoRA 의 차이,    ① 팀장님이 OOM 막겠다고 이거 쓰시는 건 맞긴 해! 근데 그 LoRA랑 QLoRA가 구체적으로 뭘까~요? 😊 (발화 종료,1,95
1616,20,5.53,(대화 주제) 마지막 할 말 (사용자 답변) 로라야 정말 고마워! (성공한 답변) 마지막 할 말 (다음 질문) 면접 종료, 2023-05-15 16:00:00 🙎‍♀️ (발화 종료,1,58
1617,20,9.58,(대화 주제) 마지막 할 말 (사용자 답변) 로라야 사랑해 (성공한 답변) 마지막 할 말 (다음 질문) 면접 종료,    2020-05-01 18:00:00   로라가 면접실에서 나와 너에게 전화를 건다! 🙋‍♀️ (발화 종료,1,72
1618,20,2.52,(대화 주제) 마지막 할 말 (사용자 답변) 로라야 너의 예쁜 미모에 반했어 (성공한 답변) 마지막 할 말 (다음 질문) 면접 종료,    ✨✨ (발화 종료,1,49
1619,20,6.0,(대화 주제) 마지막 할 말 (사용자 답변) 그동안 고생했어 면접 봐줘서 정말 고마워 (성공한 답변) 마지막 할 말 (다음 질문) 면접 종료, 2023-02-28 16:00 나도 너 덕분에 많이 배웠어 😊 (발화 종료,1,63
