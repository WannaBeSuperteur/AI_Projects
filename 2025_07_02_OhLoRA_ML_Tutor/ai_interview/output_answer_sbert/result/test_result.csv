input_part,output_answer,predicted_score,ground_truth_score,absolute_error
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,BCE 가 좋은 task,-0.0045347395,0.0,0.004534739535301924
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,BCE 가 좋은 이유,-0.0042074053,0.0,0.004207405261695385
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,LLM Fine-Tuning 의 PEFT,-0.0021005007,0.0,0.0021005007438361645
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,LoRA,-0.005111135,0.0,0.005111135076731443
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,LoRA 와 QLoRA 의 차이,-0.003280805,0.0,0.0032808049581944942
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,Loss Function 예시,-0.00085076207,0.0,0.0008507620659656823
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,Loss Function 정의,-0.0029044452,0.0,0.0029044451657682657
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,MBTI,-0.0056468574,0.0,0.005646857433021069
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,MSE Loss 설명,0.00015610119,0.0,0.00015610118862241507
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,MSE Loss 용도,-0.0030768442,0.0,0.003076844150200486
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,Multi-Label 에서 CE + Softmax 적용 문제점,-0.00010602265,0.0,0.000106022649561055
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,PEFT 방법 5가지,-0.00061902,0.0,0.0006190200219862163
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,거대 언어 모델 정의,-0.0032561296,0.0,0.003256129566580057
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,기본 경험,-0.004911902,0.0,0.004911901894956827
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,답변 실패,-0.0028460503,0.0,0.0028460503090173006
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,딥러닝,-0.0035023706,0.0,0.0035023705568164587
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,마지막 할 말,-0.0033150294,0.0,0.0033150294329971075
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,머신러닝,-0.003027851,0.0,0.0030278509948402643
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,면접 시작 인사,0.99834675,1.0,0.0016532540321350098
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,상세 경험,-0.0027082812,0.0,0.002708281157538295
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,수식,-0.0034486814,0.0,0.003448681440204382
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,용어 질문,-0.002110262,0.0,0.0021102619357407093
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,인공지능,-0.0017509423,0.0,0.0017509423196315765
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,잠시 휴식,-0.0034830312,0.0,0.003483031177893281
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,좋아하는 아이돌,-0.0019933742,0.0,0.001993374200537801
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,핵심 아이디어,0.0013693693,0.0,0.001369369332678616
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,확률 예측에서 MSE Loss 미 사용 이유,-0.0048828595,0.0,0.004882859531790018
면접 시작 인사 -> 로라야 안녕 정말 반가워,BCE 가 좋은 task,-0.0027889386,0.0,0.002788938581943512
면접 시작 인사 -> 로라야 안녕 정말 반가워,BCE 가 좋은 이유,-0.0045197965,0.0,0.004519796464592218
면접 시작 인사 -> 로라야 안녕 정말 반가워,LLM Fine-Tuning 의 PEFT,-0.002832336,0.0,0.0028323358856141567
면접 시작 인사 -> 로라야 안녕 정말 반가워,LoRA,-0.0045981314,0.0,0.004598131403326988
면접 시작 인사 -> 로라야 안녕 정말 반가워,LoRA 와 QLoRA 의 차이,-0.0042729266,0.0,0.004272926598787308
면접 시작 인사 -> 로라야 안녕 정말 반가워,Loss Function 예시,-0.0023752085,0.0,0.0023752085398882627
면접 시작 인사 -> 로라야 안녕 정말 반가워,Loss Function 정의,-0.0018331286,0.0,0.0018331286264583468
면접 시작 인사 -> 로라야 안녕 정말 반가워,MBTI,-0.005914998,0.0,0.005914997775107622
면접 시작 인사 -> 로라야 안녕 정말 반가워,MSE Loss 설명,-0.00062805094,0.0,0.0006280509405769408
면접 시작 인사 -> 로라야 안녕 정말 반가워,MSE Loss 용도,-0.0041719745,0.0,0.004171974491328001
면접 시작 인사 -> 로라야 안녕 정말 반가워,Multi-Label 에서 CE + Softmax 적용 문제점,0.00016016782,0.0,0.00016016782319638878
면접 시작 인사 -> 로라야 안녕 정말 반가워,PEFT 방법 5가지,-0.00130552,0.0,0.0013055199524387717
면접 시작 인사 -> 로라야 안녕 정말 반가워,거대 언어 모델 정의,-0.0031362819,0.0,0.003136281855404377
면접 시작 인사 -> 로라야 안녕 정말 반가워,기본 경험,-0.0042683706,0.0,0.004268370568752289
면접 시작 인사 -> 로라야 안녕 정말 반가워,답변 실패,-0.0033163456,0.0,0.003316345624625683
면접 시작 인사 -> 로라야 안녕 정말 반가워,딥러닝,-0.0018410503,0.0,0.0018410503398627043
면접 시작 인사 -> 로라야 안녕 정말 반가워,마지막 할 말,-0.0024734484,0.0,0.0024734484031796455
면접 시작 인사 -> 로라야 안녕 정말 반가워,머신러닝,-0.0018435952,0.0,0.001843595178797841
면접 시작 인사 -> 로라야 안녕 정말 반가워,면접 시작 인사,0.9986791,1.0,0.0013208985328674316
면접 시작 인사 -> 로라야 안녕 정말 반가워,상세 경험,-0.00266296,0.0,0.002662959974259138
면접 시작 인사 -> 로라야 안녕 정말 반가워,수식,-0.0041509727,0.0,0.004150972701609135
면접 시작 인사 -> 로라야 안녕 정말 반가워,용어 질문,-0.0025072435,0.0,0.002507243538275361
면접 시작 인사 -> 로라야 안녕 정말 반가워,인공지능,-0.003027066,0.0,0.0030270658899098635
면접 시작 인사 -> 로라야 안녕 정말 반가워,잠시 휴식,-0.0040118676,0.0,0.004011867567896843
면접 시작 인사 -> 로라야 안녕 정말 반가워,좋아하는 아이돌,-0.0021952637,0.0,0.002195263747125864
면접 시작 인사 -> 로라야 안녕 정말 반가워,핵심 아이디어,0.0009910363,0.0,0.000991036300547421
면접 시작 인사 -> 로라야 안녕 정말 반가워,확률 예측에서 MSE Loss 미 사용 이유,-0.0038391913,0.0,0.0038391912821680307
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,BCE 가 좋은 task,-0.003415382,0.0,0.0034153820015490055
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,BCE 가 좋은 이유,-0.0049557113,0.0,0.004955711308866739
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,LLM Fine-Tuning 의 PEFT,-0.0026407146,0.0,0.0026407146360725164
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,LoRA,-0.0053101247,0.0,0.005310124717652798
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,LoRA 와 QLoRA 의 차이,-0.004025199,0.0,0.004025198984891176
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,Loss Function 예시,-0.002032892,0.0,0.0020328920800238848
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,Loss Function 정의,-0.0015971052,0.0,0.0015971051761880517
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,MBTI,-0.0067501096,0.0,0.006750109605491161
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,MSE Loss 설명,0.00033330097,0.0,0.0003333009663037956
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,MSE Loss 용도,-0.0026681006,0.0,0.002668100642040372
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,Multi-Label 에서 CE + Softmax 적용 문제점,0.0001419216,0.0,0.0001419215986970812
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,PEFT 방법 5가지,-0.0012248254,0.0,0.0012248253915458918
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,거대 언어 모델 정의,-0.0037166853,0.0,0.0037166853435337543
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,기본 경험,-0.0055316826,0.0,0.005531682632863522
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,답변 실패,-0.0029168506,0.0,0.0029168506152927876
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,딥러닝,-0.0017007777,0.0,0.0017007776768878102
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,마지막 할 말,-0.0037618384,0.0,0.003761838423088193
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,머신러닝,-0.0021622472,0.0,0.002162247197702527
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,면접 시작 인사,0.9980835,1.0,0.0019165277481079102
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,상세 경험,-0.0022250665,0.0,0.0022250665351748466
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,수식,-0.0051107253,0.0,0.005110725294798613
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,용어 질문,-0.0028808634,0.0,0.002880863379687071
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,인공지능,-0.0028245978,0.0,0.0028245977591723204
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,잠시 휴식,-0.0045462362,0.0,0.0045462362468242645
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,좋아하는 아이돌,-0.0026135235,0.0,0.0026135235093533993
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,핵심 아이디어,0.00044468252,0.0,0.0004446825187187642
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,확률 예측에서 MSE Loss 미 사용 이유,-0.004721718,0.0,0.004721717908978462
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,BCE 가 좋은 task,-0.0027502053,0.0,0.0027502053417265415
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,BCE 가 좋은 이유,-0.0028619873,0.0,0.0028619873337447643
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,LLM Fine-Tuning 의 PEFT,-0.0011541386,0.0,0.001154138590209186
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,LoRA,-0.005678737,0.0,0.005678737070411444
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,LoRA 와 QLoRA 의 차이,-0.0047778725,0.0,0.00477787246927619
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,Loss Function 예시,-0.0026151375,0.0,0.002615137491375208
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,Loss Function 정의,-0.0013621642,0.0,0.0013621641555801034
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,MBTI,-0.007444015,0.0,0.007444014772772789
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,MSE Loss 설명,-0.00011780878,0.0,0.00011780877684941515
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,MSE Loss 용도,-0.0050695883,0.0,0.00506958831101656
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,Multi-Label 에서 CE + Softmax 적용 문제점,-0.00033558338,0.0,0.000335583375999704
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,PEFT 방법 5가지,-0.001990696,0.0,0.0019906959496438503
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,거대 언어 모델 정의,-0.0025084151,0.0,0.0025084151420742273
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,기본 경험,-0.0047746687,0.0,0.0047746687196195126
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,답변 실패,-0.0030354215,0.0,0.00303542148321867
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,딥러닝,-0.0012428502,0.0,0.0012428502086549997
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,마지막 할 말,-0.0029071704,0.0,0.0029071704484522343
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,머신러닝,-0.0021203428,0.0,0.002120342804118991
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,면접 시작 인사,0.99786514,1.0,0.002134859561920166
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,상세 경험,-0.0025807011,0.0,0.0025807011406868696
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,수식,-0.004375763,0.0,0.004375762771815062
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,용어 질문,-0.0019252498,0.0,0.0019252498168498278
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,인공지능,-0.0023624476,0.0,0.0023624475579708815
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,잠시 휴식,-0.0028114668,0.0,0.002811466809362173
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,좋아하는 아이돌,-0.0022259103,0.0,0.0022259103134274483
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,핵심 아이디어,-0.00022068103,0.0,0.00022068103135097772
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,확률 예측에서 MSE Loss 미 사용 이유,-0.004638206,0.0,0.004638206213712692
면접 시작 인사 -> 파이팅! 시작하자,BCE 가 좋은 task,-0.00313817,0.0,0.00313817011192441
면접 시작 인사 -> 파이팅! 시작하자,BCE 가 좋은 이유,-0.0045048352,0.0,0.004504835233092308
면접 시작 인사 -> 파이팅! 시작하자,LLM Fine-Tuning 의 PEFT,-0.002800439,0.0,0.002800439018756151
면접 시작 인사 -> 파이팅! 시작하자,LoRA,-0.005337977,0.0,0.005337976850569248
면접 시작 인사 -> 파이팅! 시작하자,LoRA 와 QLoRA 의 차이,-0.0031166144,0.0,0.003116614418104291
면접 시작 인사 -> 파이팅! 시작하자,Loss Function 예시,-0.0010423857,0.0,0.00104238570202142
면접 시작 인사 -> 파이팅! 시작하자,Loss Function 정의,-0.0019888722,0.0,0.0019888721872121096
면접 시작 인사 -> 파이팅! 시작하자,MBTI,-0.0036023685,0.0,0.003602368524298072
면접 시작 인사 -> 파이팅! 시작하자,MSE Loss 설명,-0.0009085504,0.0,0.0009085503988899291
면접 시작 인사 -> 파이팅! 시작하자,MSE Loss 용도,-0.0032224145,0.0,0.0032224145252257586
면접 시작 인사 -> 파이팅! 시작하자,Multi-Label 에서 CE + Softmax 적용 문제점,-8.359114e-05,0.0,8.359114144695923e-05
면접 시작 인사 -> 파이팅! 시작하자,PEFT 방법 5가지,-0.00051687984,0.0,0.0005168798379600048
면접 시작 인사 -> 파이팅! 시작하자,거대 언어 모델 정의,-0.0040717283,0.0,0.004071728326380253
면접 시작 인사 -> 파이팅! 시작하자,기본 경험,-0.0040178774,0.0,0.004017877392470837
면접 시작 인사 -> 파이팅! 시작하자,답변 실패,-0.003208233,0.0,0.003208233043551445
면접 시작 인사 -> 파이팅! 시작하자,딥러닝,-0.0037103817,0.0,0.0037103816866874695
면접 시작 인사 -> 파이팅! 시작하자,마지막 할 말,-0.002410687,0.0,0.0024106870405375957
면접 시작 인사 -> 파이팅! 시작하자,머신러닝,-0.00090118987,0.0,0.0009011898655444384
면접 시작 인사 -> 파이팅! 시작하자,면접 시작 인사,0.9987022,1.0,0.00129777193069458
면접 시작 인사 -> 파이팅! 시작하자,상세 경험,-0.0029539925,0.0,0.002953992458060384
면접 시작 인사 -> 파이팅! 시작하자,수식,-0.0034931262,0.0,0.0034931262489408255
면접 시작 인사 -> 파이팅! 시작하자,용어 질문,-0.0023459278,0.0,0.002345927758142352
면접 시작 인사 -> 파이팅! 시작하자,인공지능,-0.0031762675,0.0,0.0031762674916535616
면접 시작 인사 -> 파이팅! 시작하자,잠시 휴식,-0.0036929538,0.0,0.00369295384734869
면접 시작 인사 -> 파이팅! 시작하자,좋아하는 아이돌,-0.0017089801,0.0,0.0017089800676330924
면접 시작 인사 -> 파이팅! 시작하자,핵심 아이디어,0.0014852398,0.0,0.0014852398307994008
면접 시작 인사 -> 파이팅! 시작하자,확률 예측에서 MSE Loss 미 사용 이유,-0.0037094604,0.0,0.003709460375830531
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",BCE 가 좋은 task,-0.0023459631,0.0,0.0023459631484001875
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",BCE 가 좋은 이유,-0.0019174321,0.0,0.0019174320623278618
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",LLM Fine-Tuning 의 PEFT,-0.0017206632,0.0,0.001720663160085678
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",LoRA,-0.0047800215,0.0,0.004780021496117115
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",LoRA 와 QLoRA 의 차이,-0.0023472651,0.0,0.0023472651373595
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",Loss Function 예시,-0.0007280287,0.0,0.000728028710000217
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",Loss Function 정의,-0.0024056972,0.0,0.0024056972470134497
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",MBTI,-0.0034667382,0.0,0.0034667381551116705
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",MSE Loss 설명,-0.0026930608,0.0,0.0026930607855319977
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",MSE Loss 용도,-0.002048523,0.0,0.0020485229324549437
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",Multi-Label 에서 CE + Softmax 적용 문제점,-0.00421243,0.0,0.004212430212646723
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",PEFT 방법 5가지,-0.0020867188,0.0,0.002086718799546361
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",거대 언어 모델 정의,-0.0026810553,0.0,0.002681055339053273
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",기본 경험,-0.0060750386,0.0,0.0060750385746359825
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",답변 실패,0.9986947,1.0,0.0013052821159362793
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",딥러닝,-0.0003267548,0.0,0.00032675478723831475
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",마지막 할 말,-0.00219767,0.0,0.0021976700518280268
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",머신러닝,-0.0028533298,0.0,0.002853329759091139
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",면접 시작 인사,-0.0017284933,0.0,0.0017284932546317577
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",상세 경험,-0.0030227734,0.0,0.0030227734241634607
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",수식,-0.0035749886,0.0,0.0035749885719269514
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",용어 질문,-0.0007332647,0.0,0.000733264721930027
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",인공지능,0.0053855893,0.0,0.005385589320212603
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",잠시 휴식,-0.0026217178,0.0,0.0026217177510261536
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",좋아하는 아이돌,-0.0005461269,0.0,0.0005461269174702466
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",핵심 아이디어,-0.0006930187,0.0,0.0006930187228135765
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",확률 예측에서 MSE Loss 미 사용 이유,-0.00082396617,0.0,0.0008239661692641675
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",BCE 가 좋은 task,-0.0019529418,0.0,0.0019529417622834444
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",BCE 가 좋은 이유,-0.00406595,0.0,0.004065949935466051
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",LLM Fine-Tuning 의 PEFT,-0.004559591,0.0,0.004559590946882963
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",LoRA,-0.0042039547,0.0,0.004203954711556435
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",LoRA 와 QLoRA 의 차이,-0.0011622595,0.0,0.0011622594902291894
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",Loss Function 예시,-0.0009929145,0.0,0.0009929145453497767
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",Loss Function 정의,-0.004334555,0.0,0.004334555007517338
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",MBTI,-0.00208897,0.0,0.0020889700390398502
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",MSE Loss 설명,-0.0012750645,0.0,0.0012750645400956273
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",MSE Loss 용도,-0.0018202381,0.0,0.0018202380742877722
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",Multi-Label 에서 CE + Softmax 적용 문제점,-0.0033870786,0.0,0.003387078642845154
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",PEFT 방법 5가지,0.00011170053,0.0,0.00011170053039677441
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",거대 언어 모델 정의,-0.004442477,0.0,0.004442477133125067
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",기본 경험,0.0003391357,0.0,0.00033913570223376155
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",답변 실패,-0.0035364956,0.0,0.0035364956129342318
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",딥러닝,-0.004880896,0.0,0.004880895838141441
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",마지막 할 말,-0.0013824746,0.0,0.0013824745547026396
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",머신러닝,-0.0024542436,0.0,0.0024542436003684998
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",면접 시작 인사,-0.0036116235,0.0,0.003611623542383313
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",상세 경험,-0.0061475406,0.0,0.006147540640085936
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",수식,-0.003290252,0.0,0.00329025206156075
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",용어 질문,-0.002164587,0.0,0.0021645869128406048
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",인공지능,0.99714583,1.0,0.00285416841506958
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",잠시 휴식,0.0009240997,0.0,0.0009240997023880482
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",좋아하는 아이돌,-0.005009132,0.0,0.005009131971746683
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",핵심 아이디어,-0.001763212,0.0,0.0017632120288908482
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",확률 예측에서 MSE Loss 미 사용 이유,-0.0019158936,0.0,0.0019158936338499188
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",BCE 가 좋은 task,-0.0035439308,0.0,0.0035439308267086744
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",BCE 가 좋은 이유,-0.00558702,0.0,0.005587019957602024
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",LLM Fine-Tuning 의 PEFT,-0.0028882183,0.0,0.002888218266889453
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",LoRA,0.0003149193,0.0,0.0003149193071294576
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",LoRA 와 QLoRA 의 차이,-0.0018485809,0.0,0.0018485808977857232
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",Loss Function 예시,0.0022851438,0.0,0.0022851438261568546
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",Loss Function 정의,-0.0018688738,0.0,0.0018688738346099854
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",MBTI,-0.0055959253,0.0,0.005595925264060497
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",MSE Loss 설명,-0.0035856876,0.0,0.003585687605664134
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",MSE Loss 용도,-0.00293285,0.0,0.0029328500386327505
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",Multi-Label 에서 CE + Softmax 적용 문제점,-0.00050379435,0.0,0.0005037943483330309
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",PEFT 방법 5가지,-0.0016034778,0.0,0.0016034777509048581
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",거대 언어 모델 정의,-0.005928985,0.0,0.005928984843194485
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",기본 경험,-0.0050783884,0.0,0.005078388378024101
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",답변 실패,-0.0025314298,0.0,0.0025314297527074814
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",딥러닝,-0.0038016567,0.0,0.0038016566541045904
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",마지막 할 말,-0.0021953147,0.0,0.0021953147370368242
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",머신러닝,0.99823755,1.0,0.0017624497413635254
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",면접 시작 인사,0.00073657616,0.0,0.0007365761557593942
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",상세 경험,-0.00019564731,0.0,0.0001956473133759573
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",수식,-0.0016154175,0.0,0.0016154175391420722
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",용어 질문,-0.00010997915,0.0,0.00010997914796462283
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",인공지능,-0.001980298,0.0,0.001980297965928912
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",잠시 휴식,-0.0031566492,0.0,0.0031566491816192865
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",좋아하는 아이돌,0.0006777706,0.0,0.0006777705857530236
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",핵심 아이디어,-0.0038320818,0.0,0.00383208179846406
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",확률 예측에서 MSE Loss 미 사용 이유,-0.00015976066,0.0,0.00015976066060829908
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",BCE 가 좋은 task,-0.001931801,0.0,0.0019318009726703167
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",BCE 가 좋은 이유,-0.0012149152,0.0,0.0012149151880294085
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",LLM Fine-Tuning 의 PEFT,-0.00408224,0.0,0.004082240164279938
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",LoRA,-0.0034766805,0.0,0.003476680489256978
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",LoRA 와 QLoRA 의 차이,-0.0007213506,0.0,0.0007213506032712758
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",Loss Function 예시,0.00010204078,0.0,0.00010204077989328653
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",Loss Function 정의,-0.0060219024,0.0,0.006021902430802584
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",MBTI,-0.0007219772,0.0,0.0007219772087410092
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",MSE Loss 설명,-0.00031704034,0.0,0.00031704033608548343
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",MSE Loss 용도,-0.00034501468,0.0,0.00034501467598602176
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",Multi-Label 에서 CE + Softmax 적용 문제점,-0.0017136096,0.0,0.0017136095557361841
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",PEFT 방법 5가지,-0.003863217,0.0,0.0038632170762866735
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",거대 언어 모델 정의,-0.0034358846,0.0,0.003435884602367878
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",기본 경험,-0.004005485,0.0,0.004005485214293003
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",답변 실패,-0.0012301072,0.0,0.0012301071546971798
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",딥러닝,0.9981949,1.0,0.001805126667022705
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",마지막 할 말,-0.0009852715,0.0,0.000985271530225873
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",머신러닝,-0.0057086,0.0,0.005708599928766489
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",면접 시작 인사,-0.0023860855,0.0,0.002386085456237197
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",상세 경험,-0.00539037,0.0,0.005390369798988104
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",수식,-0.0025989239,0.0,0.0025989238638430834
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",용어 질문,-0.00012655946,0.0,0.00012655946193262935
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",인공지능,-0.004935767,0.0,0.004935767035931349
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",잠시 휴식,-0.003553338,0.0,0.0035533381160348654
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",좋아하는 아이돌,-0.00028788895,0.0,0.0002878889499697834
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",핵심 아이디어,2.43905e-05,0.0,2.4390499675064348e-05
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",확률 예측에서 MSE Loss 미 사용 이유,-0.0016874884,0.0,0.0016874884022399783
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",BCE 가 좋은 task,-0.0012033887,0.0,0.00120338867418468
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",BCE 가 좋은 이유,-0.0025386154,0.0,0.002538615372031927
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",LLM Fine-Tuning 의 PEFT,-0.004712286,0.0,0.004712285939604044
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",LoRA,-0.0022581134,0.0,0.0022581133525818586
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",LoRA 와 QLoRA 의 차이,-0.00044891133,0.0,0.00044891133438795805
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",Loss Function 예시,-0.00046558288,0.0,0.0004655828815884888
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",Loss Function 정의,-0.0060978252,0.0,0.006097825244069099
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",MBTI,0.00044447317,0.0,0.0004444731748662889
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",MSE Loss 설명,-0.0001317791,0.0,0.00013177910295780748
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",MSE Loss 용도,-0.00089007354,0.0,0.0008900735410861671
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",Multi-Label 에서 CE + Softmax 적용 문제점,-0.0011426292,0.0,0.0011426291894167662
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",PEFT 방법 5가지,-0.0031512687,0.0,0.0031512686982750893
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",거대 언어 모델 정의,-0.003716646,0.0,0.003716645995154977
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",기본 경험,-0.0046766624,0.0,0.004676662385463715
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",답변 실패,-0.0008091686,0.0,0.0008091686177067459
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",딥러닝,0.9984119,1.0,0.0015881061553955078
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",마지막 할 말,-0.00095589244,0.0,0.0009558924357406795
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",머신러닝,-0.005705061,0.0,0.00570506090298295
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",면접 시작 인사,-0.002863054,0.0,0.0028630539309233427
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",상세 경험,-0.0049742963,0.0,0.0049742963165044785
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",수식,-0.0035045862,0.0,0.003504586173221469
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",용어 질문,-0.001225412,0.0,0.0012254120083525777
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",인공지능,-0.0037765026,0.0,0.003776502562686801
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",잠시 휴식,-0.005098966,0.0,0.00509896595031023
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",좋아하는 아이돌,-0.0009266665,0.0,0.0009266664856113493
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",핵심 아이디어,-0.00042190187,0.0,0.0004219018737785518
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",확률 예측에서 MSE Loss 미 사용 이유,-0.0012118598,0.0,0.0012118597514927387
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",BCE 가 좋은 task,-0.016515167,0.0,0.01651516743004322
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",BCE 가 좋은 이유,-0.006648626,0.0,0.0066486261785030365
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",LLM Fine-Tuning 의 PEFT,-0.006378208,0.0,0.0063782078213989735
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",LoRA,-0.012463572,0.0,0.012463572435081005
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",LoRA 와 QLoRA 의 차이,-0.009594854,0.0,0.009594853967428207
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",Loss Function 예시,-0.007852199,0.0,0.00785219855606556
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",Loss Function 정의,-0.0014588739,0.0,0.001458873855881393
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",MBTI,-0.002021365,0.0,0.002021365100517869
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",MSE Loss 설명,-0.01068908,0.0,0.010689079761505127
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",MSE Loss 용도,-0.0017834184,0.0,0.001783418352715671
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",Multi-Label 에서 CE + Softmax 적용 문제점,-0.0075922064,0.0,0.007592206355184317
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",PEFT 방법 5가지,0.003301462,0.0,0.003301461925730109
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",거대 언어 모델 정의,-0.007890492,0.0,0.007890491746366024
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",기본 경험,-0.01651093,0.0,0.01651092991232872
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",답변 실패,0.96508074,1.0,0.03491926193237305
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",딥러닝,-0.021567004,0.0,0.021567003801465034
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",마지막 할 말,-0.009380401,0.0,0.009380401112139225
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",머신러닝,0.19516519,0.0,0.19516518712043762
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",면접 시작 인사,-0.003971164,0.0,0.0039711641147732735
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",상세 경험,-0.0010911254,0.0,0.0010911254212260246
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",수식,-0.0030694774,0.0,0.0030694773886352777
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",용어 질문,-0.0047171772,0.0,0.0047171772457659245
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",인공지능,-0.010242067,0.0,0.010242067277431488
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",잠시 휴식,-0.0034850438,0.0,0.003485043765977025
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",좋아하는 아이돌,-0.013264768,0.0,0.013264767825603485
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",핵심 아이디어,-0.0030320024,0.0,0.003032002365216613
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",확률 예측에서 MSE Loss 미 사용 이유,0.0035644611,0.0,0.0035644611343741417
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아",BCE 가 좋은 task,-0.002636002,0.0,0.0026360019110143185
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아",BCE 가 좋은 이유,-0.0032695162,0.0,0.0032695161644369364
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아",LLM Fine-Tuning 의 PEFT,-0.0020060912,0.0,0.0020060911774635315
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아",LoRA,-0.0038667142,0.0,0.0038667141925543547
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아",LoRA 와 QLoRA 의 차이,-0.0016765131,0.0,0.0016765131149441004
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아",Loss Function 예시,-0.0021258458,0.0,0.00212584575638175
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아",Loss Function 정의,-0.0030279625,0.0,0.0030279625207185745
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아",MBTI,-0.002861495,0.0,0.0028614948969334364
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아",MSE Loss 설명,-0.003169555,0.0,0.0031695549841970205
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아",MSE Loss 용도,-0.0017230854,0.0,0.0017230854136869311
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아",Multi-Label 에서 CE + Softmax 적용 문제점,-0.003004964,0.0,0.0030049639753997326
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아",PEFT 방법 5가지,-0.0020089222,0.0,0.002008922165259719
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아",거대 언어 모델 정의,-0.0023867912,0.0,0.002386791165918112
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아",기본 경험,-0.003509445,0.0,0.0035094451159238815
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아",답변 실패,0.9994106,1.0,0.0005893707275390625
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아",딥러닝,-4.1423064e-05,0.0,4.1423063521506265e-05
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아",마지막 할 말,-0.0029985604,0.0,0.00299856043420732
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아",머신러닝,-0.002605397,0.0,0.0026053970213979483
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아",면접 시작 인사,-0.0032897366,0.0,0.0032897365745157003
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아",상세 경험,-0.0038259933,0.0,0.003825993277132511
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아",수식,-0.0034877486,0.0,0.003487748559564352
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아",용어 질문,-0.0021495477,0.0,0.0021495476830750704
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아",인공지능,0.0025975306,0.0,0.0025975306052714586
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아",잠시 휴식,-0.000636644,0.0,0.0006366440211422741
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아",좋아하는 아이돌,-0.0016393326,0.0,0.0016393326222896576
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아",핵심 아이디어,-0.0008272142,0.0,0.0008272142149507999
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아",확률 예측에서 MSE Loss 미 사용 이유,-0.00075780036,0.0,0.0007578003569506109
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지?",BCE 가 좋은 task,-0.002736822,0.0,0.0027368220034986734
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지?",BCE 가 좋은 이유,-0.00390472,0.0,0.0039047200698405504
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지?",LLM Fine-Tuning 의 PEFT,-0.0051675015,0.0,0.005167501512914896
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지?",LoRA,-0.0038834102,0.0,0.00388341024518013
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지?",LoRA 와 QLoRA 의 차이,-0.0023101629,0.0,0.002310162875801325
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지?",Loss Function 예시,-0.00056241907,0.0,0.0005624190671369433
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지?",Loss Function 정의,-0.004323243,0.0,0.004323243163526058
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지?",MBTI,-0.0031384134,0.0,0.003138413419947028
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지?",MSE Loss 설명,-0.0014449549,0.0,0.0014449548907577991
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지?",MSE Loss 용도,-0.0014196335,0.0,0.0014196335105225444
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지?",Multi-Label 에서 CE + Softmax 적용 문제점,-0.0033040864,0.0,0.0033040863927453756
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지?",PEFT 방법 5가지,0.00048572515,0.0,0.00048572514788247645
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지?",거대 언어 모델 정의,-0.004633993,0.0,0.004633992910385132
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지?",기본 경험,0.00037679498,0.0,0.00037679498200304806
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지?",답변 실패,-0.003099775,0.0,0.003099774941802025
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지?",딥러닝,-0.005266703,0.0,0.005266703199595213
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지?",마지막 할 말,-0.0023052034,0.0,0.0023052033502608538
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지?",머신러닝,-0.0034524396,0.0,0.003452439559623599
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지?",면접 시작 인사,-0.0035362397,0.0,0.003536239732056856
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지?",상세 경험,-0.0059482963,0.0,0.005948296282440424
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지?",수식,-0.003417362,0.0,0.003417361993342638
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지?",용어 질문,-0.0031337019,0.0,0.0031337018590420485
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지?",인공지능,0.99703205,1.0,0.0029679536819458008
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지?",잠시 휴식,0.0012855335,0.0,0.00128553353715688
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지?",좋아하는 아이돌,-0.0051376196,0.0,0.005137619562447071
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지?",핵심 아이디어,-0.0023472442,0.0,0.002347244182601571
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지?",확률 예측에서 MSE Loss 미 사용 이유,-0.0019458686,0.0,0.0019458686001598835
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야!",BCE 가 좋은 task,-0.0027304802,0.0,0.0027304801624268293
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야!",BCE 가 좋은 이유,-0.0058255508,0.0,0.005825550761073828
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야!",LLM Fine-Tuning 의 PEFT,-0.0032469267,0.0,0.0032469267025589943
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야!",LoRA,0.00032021885,0.0,0.0003202188527211547
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야!",LoRA 와 QLoRA 의 차이,-0.0012152476,0.0,0.0012152475537732244
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야!",Loss Function 예시,0.0029255254,0.0,0.0029255254194140434
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야!",Loss Function 정의,-0.0027670495,0.0,0.0027670494746416807
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야!",MBTI,-0.0059794555,0.0,0.005979455541819334
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야!",MSE Loss 설명,-0.003592107,0.0,0.003592106979340315
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야!",MSE Loss 용도,-0.002998477,0.0,0.002998477080836892
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야!",Multi-Label 에서 CE + Softmax 적용 문제점,-0.0007965346,0.0,0.0007965345866978168
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야!",PEFT 방법 5가지,-0.0019866813,0.0,0.0019866812508553267
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야!",거대 언어 모델 정의,-0.006193949,0.0,0.006193948909640312
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야!",기본 경험,-0.005247366,0.0,0.005247366148978472
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야!",답변 실패,-0.0026918612,0.0,0.002691861242055893
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야!",딥러닝,-0.0034152302,0.0,0.003415230195969343
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야!",마지막 할 말,-0.0026186947,0.0,0.0026186946779489517
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야!",머신러닝,0.99796623,1.0,0.0020337700843811035
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야!",면접 시작 인사,0.00015805638,0.0,0.00015805638395249844
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야!",상세 경험,0.00075360655,0.0,0.0007536065531894565
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야!",수식,-0.0017872817,0.0,0.0017872817115858197
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야!",용어 질문,-0.00075679604,0.0,0.0007567960419692099
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야!",인공지능,-0.0012071931,0.0,0.0012071931269019842
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야!",잠시 휴식,-0.003053296,0.0,0.0030532958917319775
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야!",좋아하는 아이돌,0.0011798375,0.0,0.0011798375053331256
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야!",핵심 아이디어,-0.0051320815,0.0,0.00513208145275712
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야!",확률 예측에서 MSE Loss 미 사용 이유,0.00013860968,0.0,0.0001386096846545115
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지",BCE 가 좋은 task,-0.001597719,0.0,0.0015977190341800451
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지",BCE 가 좋은 이유,-0.0013622799,0.0,0.0013622798724099994
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지",LLM Fine-Tuning 의 PEFT,-0.004801729,0.0,0.00480172922834754
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지",LoRA,-0.0032731567,0.0,0.0032731567043811083
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지",LoRA 와 QLoRA 의 차이,-0.00035057787,0.0,0.0003505778731778264
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지",Loss Function 예시,0.00031811986,0.0,0.0003181198553647846
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지",Loss Function 정의,-0.0049113133,0.0,0.00491131329908967
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지",MBTI,-7.437366e-05,0.0,7.437366002704948e-05
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지",MSE Loss 설명,-0.0013203748,0.0,0.0013203747803345323
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지",MSE Loss 용도,-0.0006597224,0.0,0.0006597224273718894
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지",Multi-Label 에서 CE + Softmax 적용 문제점,-0.0006390324,0.0,0.0006390323978848755
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지",PEFT 방법 5가지,-0.0028958796,0.0,0.0028958795592188835
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지",거대 언어 모델 정의,-0.004060146,0.0,0.0040601459331810474
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지",기본 경험,-0.004522481,0.0,0.0045224810019135475
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지",답변 실패,-0.0018115783,0.0,0.001811578287743032
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지",딥러닝,0.99830705,1.0,0.00169295072555542
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지",마지막 할 말,-0.000657686,0.0,0.0006576859741471708
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지",머신러닝,-0.0048492756,0.0,0.004849275574088097
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지",면접 시작 인사,-0.003091183,0.0,0.0030911830253899097
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지",상세 경험,-0.0052595367,0.0,0.005259536672383547
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지",수식,-0.0047129686,0.0,0.004712968599051237
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지",용어 질문,-0.0002535604,0.0,0.0002535603998694569
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지",인공지능,-0.004689833,0.0,0.004689833149313927
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지",잠시 휴식,-0.0037796462,0.0,0.0037796462420374155
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지",좋아하는 아이돌,-0.00081267464,0.0,0.0008126746397465467
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지",핵심 아이디어,0.0005020874,0.0,0.0005020874086767435
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지",확률 예측에서 MSE Loss 미 사용 이유,-0.0022283166,0.0,0.002228316618129611
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야.",BCE 가 좋은 task,-0.001851911,0.0,0.0018519109580665827
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야.",BCE 가 좋은 이유,-0.0030675551,0.0,0.0030675551388412714
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야.",LLM Fine-Tuning 의 PEFT,-0.004514056,0.0,0.004514055792242289
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야.",LoRA,-0.0036708142,0.0,0.0036708142142742872
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야.",LoRA 와 QLoRA 의 차이,-0.0005755274,0.0,0.0005755273741669953
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야.",Loss Function 예시,0.0006075019,0.0,0.0006075018900446594
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야.",Loss Function 정의,-0.0057063545,0.0,0.005706354510039091
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야.",MBTI,0.00023771056,0.0,0.0002377105556661263
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야.",MSE Loss 설명,-0.0008534039,0.0,0.0008534038788639009
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야.",MSE Loss 용도,-0.00056135847,0.0,0.000561358465347439
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야.",Multi-Label 에서 CE + Softmax 적용 문제점,-0.0012642385,0.0,0.0012642384972423315
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야.",PEFT 방법 5가지,-0.0032784988,0.0,0.0032784987706691027
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야.",거대 언어 모델 정의,-0.0044772225,0.0,0.004477222450077534
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야.",기본 경험,-0.00403872,0.0,0.004038719926029444
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야.",답변 실패,-0.0015718669,0.0,0.0015718669164925814
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야.",딥러닝,0.9982379,1.0,0.001762092113494873
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야.",마지막 할 말,-0.0010542892,0.0,0.001054289168678224
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야.",머신러닝,-0.005021364,0.0,0.005021363962441683
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야.",면접 시작 인사,-0.0030607982,0.0,0.0030607981607317924
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야.",상세 경험,-0.0051331418,0.0,0.00513314176350832
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야.",수식,-0.0035543148,0.0,0.0035543148405849934
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야.",용어 질문,-0.00020194126,0.0,0.00020194126409478486
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야.",인공지능,-0.003821052,0.0,0.003821051912382245
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야.",잠시 휴식,-0.0042783315,0.0,0.004278331529349089
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야.",좋아하는 아이돌,-0.00055765145,0.0,0.000557651452254504
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야.",핵심 아이디어,-0.00022375799,0.0,0.0002237579901702702
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야.",확률 예측에서 MSE Loss 미 사용 이유,-0.0018992285,0.0,0.0018992285476997495
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로",BCE 가 좋은 task,-0.0018699883,0.0,0.0018699882784858346
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로",BCE 가 좋은 이유,-0.003849567,0.0,0.003849566914141178
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로",LLM Fine-Tuning 의 PEFT,-0.003461091,0.0,0.0034610910806804895
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로",LoRA,-0.004187476,0.0,0.004187475889921188
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로",LoRA 와 QLoRA 의 차이,-0.0022033602,0.0,0.0022033601999282837
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로",Loss Function 예시,-0.0031444007,0.0,0.0031444006599485874
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로",Loss Function 정의,-0.0027883605,0.0,0.0027883604634553194
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로",MBTI,-0.000901851,0.0,0.0009018509881570935
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로",MSE Loss 설명,-0.0044905655,0.0,0.00449056550860405
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로",MSE Loss 용도,-0.0018280932,0.0,0.0018280931981280446
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로",Multi-Label 에서 CE + Softmax 적용 문제점,-0.0023211509,0.0,0.002321150852367282
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로",PEFT 방법 5가지,-0.0020787017,0.0,0.0020787017419934273
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로",거대 언어 모델 정의,-0.0007738546,0.0,0.0007738546119071543
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로",기본 경험,-0.0040132324,0.0,0.004013232421129942
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로",답변 실패,0.9994465,1.0,0.0005534887313842773
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로",딥러닝,-0.001380196,0.0,0.001380195957608521
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로",마지막 할 말,-0.002090886,0.0,0.002090886002406478
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로",머신러닝,0.0015007289,0.0,0.0015007288893684745
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로",면접 시작 인사,-0.0039480277,0.0,0.0039480277337133884
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로",상세 경험,-0.003188941,0.0,0.003188940929248929
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로",수식,-0.00051268854,0.0,0.0005126885371282697
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로",용어 질문,-0.0021469132,0.0,0.002146913204342127
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로",인공지능,-0.002230734,0.0,0.002230734098702669
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로",잠시 휴식,0.00020695104,0.0,0.00020695103739853948
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로",좋아하는 아이돌,-0.0033806989,0.0,0.003380698850378394
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로",핵심 아이디어,-0.00052182405,0.0,0.0005218240548856556
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로",확률 예측에서 MSE Loss 미 사용 이유,-0.00090481556,0.0,0.0009048155625350773
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,BCE 가 좋은 task,0.0002827772,0.0,0.00028277721139602363
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,BCE 가 좋은 이유,-0.009573269,0.0,0.009573268704116344
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,LLM Fine-Tuning 의 PEFT,-0.0007045178,0.0,0.0007045178208500147
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,LoRA,0.0003192714,0.0,0.0003192714066244662
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,LoRA 와 QLoRA 의 차이,-0.0018364693,0.0,0.0018364692805334926
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,Loss Function 예시,-0.0004316529,0.0,0.00043165290844626725
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,Loss Function 정의,-0.0086851735,0.0,0.008685173466801643
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,MBTI,-0.0010321295,0.0,0.001032129512168467
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,MSE Loss 설명,-0.0032009669,0.0,0.003200966864824295
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,MSE Loss 용도,-0.0013213016,0.0,0.0013213015627115965
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,Multi-Label 에서 CE + Softmax 적용 문제점,-0.0039518666,0.0,0.0039518666453659534
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,PEFT 방법 5가지,0.0005476715,0.0,0.0005476715159602463
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,거대 언어 모델 정의,0.9989687,1.0,0.0010312795639038086
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,기본 경험,-0.0032630616,0.0,0.003263061633333564
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,답변 실패,-0.0007429254,0.0,0.0007429253892041743
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,딥러닝,-0.0030143193,0.0,0.0030143193434923887
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,마지막 할 말,-0.0018751463,0.0,0.0018751462921500206
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,머신러닝,-0.0015682126,0.0,0.0015682126395404339
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,면접 시작 인사,-0.0033770099,0.0,0.003377009881660342
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,상세 경험,-0.0032074293,0.0,0.003207429312169552
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,수식,-0.0033875995,0.0,0.0033875994849950075
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,용어 질문,-0.005492683,0.0,0.005492683034390211
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,인공지능,-0.0048698094,0.0,0.004869809374213219
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,잠시 휴식,-0.00041478476,0.0,0.00041478476487100124
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,좋아하는 아이돌,-0.0034999093,0.0,0.0034999093040823936
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,핵심 아이디어,-0.0006904018,0.0,0.0006904018227942288
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,확률 예측에서 MSE Loss 미 사용 이유,-0.0019890703,0.0,0.001989070326089859
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,BCE 가 좋은 task,-0.0010675973,0.0,0.001067597302608192
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,BCE 가 좋은 이유,-0.00349306,0.0,0.003493059892207384
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,LLM Fine-Tuning 의 PEFT,-0.0012923646,0.0,0.0012923645554110408
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,LoRA,-0.0033083444,0.0,0.0033083443995565176
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,LoRA 와 QLoRA 의 차이,-0.00093921705,0.0,0.0009392170468345284
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,Loss Function 예시,-0.0036282844,0.0,0.0036282844375818968
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,Loss Function 정의,-0.0030305588,0.0,0.003030558815225959
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,MBTI,-0.0023249132,0.0,0.002324913162738085
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,MSE Loss 설명,-0.0028670162,0.0,0.0028670162428170443
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,MSE Loss 용도,-0.0022136546,0.0,0.002213654574006796
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,Multi-Label 에서 CE + Softmax 적용 문제점,-0.0013032334,0.0,0.001303233439102769
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,PEFT 방법 5가지,-0.0021616325,0.0,0.002161632524803281
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,거대 언어 모델 정의,-0.00020447288,0.0,0.00020447287533897907
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,기본 경험,-0.0032921366,0.0,0.0032921365927904844
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,답변 실패,0.9996764,1.0,0.0003235936164855957
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,딥러닝,0.0003042848,0.0,0.00030428479658439755
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,마지막 할 말,-0.0023034357,0.0,0.0023034357000142336
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,머신러닝,-0.0023286073,0.0,0.002328607253730297
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,면접 시작 인사,-0.0032523295,0.0,0.0032523295376449823
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,상세 경험,-0.001890996,0.0,0.0018909960053861141
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,수식,-0.0027855304,0.0,0.0027855304069817066
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,용어 질문,-0.0015695536,0.0,0.0015695536276325583
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,인공지능,-0.0029243054,0.0,0.002924305386841297
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,잠시 휴식,-0.00047645476,0.0,0.000476454762974754
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,좋아하는 아이돌,-0.0025469859,0.0,0.0025469858665019274
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,핵심 아이디어,-0.0017300838,0.0,0.0017300838371738791
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,확률 예측에서 MSE Loss 미 사용 이유,-0.0014134893,0.0,0.0014134893426671624
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델,BCE 가 좋은 task,-0.00051245984,0.0,0.0005124598392285407
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델,BCE 가 좋은 이유,-0.010936603,0.0,0.01093660295009613
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델,LLM Fine-Tuning 의 PEFT,0.0010026788,0.0,0.001002678764052689
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델,LoRA,0.00052504055,0.0,0.0005250405520200729
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델,LoRA 와 QLoRA 의 차이,-0.0025889277,0.0,0.002588927745819092
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델,Loss Function 예시,-0.00087137433,0.0,0.0008713743300177157
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델,Loss Function 정의,-0.0075559625,0.0,0.007555962540209293
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델,MBTI,-0.00038653702,0.0,0.00038653702358715236
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델,MSE Loss 설명,-0.0041780355,0.0,0.0041780355386435986
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델,MSE Loss 용도,-0.0011740855,0.0,0.0011740855406969786
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델,Multi-Label 에서 CE + Softmax 적용 문제점,-0.0028958956,0.0,0.0028958956245332956
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델,PEFT 방법 5가지,0.0006309226,0.0,0.0006309226155281067
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델,거대 언어 모델 정의,0.9985284,1.0,0.0014715790748596191
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델,기본 경험,-0.0044758744,0.0,0.004475874360650778
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델,답변 실패,-0.0008257641,0.0,0.0008257640874944627
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델,딥러닝,-0.0016726683,0.0,0.0016726682661101222
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델,마지막 할 말,-0.0016568514,0.0,0.0016568513819947839
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델,머신러닝,-0.0037119777,0.0,0.0037119777407497168
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델,면접 시작 인사,-0.0035782298,0.0,0.003578229807317257
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델,상세 경험,-0.003338213,0.0,0.0033382130786776543
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델,수식,-0.002755221,0.0,0.002755220979452133
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델,용어 질문,-0.005098183,0.0,0.005098183173686266
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델,인공지능,-0.0025681676,0.0,0.002568167634308338
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델,잠시 휴식,0.0008764183,0.0,0.0008764183148741722
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델,좋아하는 아이돌,-0.0031315002,0.0,0.0031315002124756575
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델,핵심 아이디어,-0.0014646754,0.0,0.0014646754134446383
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델,확률 예측에서 MSE Loss 미 사용 이유,-0.0026404639,0.0,0.0026404638774693012
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지.,BCE 가 좋은 task,-0.001878661,0.0,0.0018786609871312976
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지.,BCE 가 좋은 이유,-0.0042030653,0.0,0.004203065298497677
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지.,LLM Fine-Tuning 의 PEFT,-0.0019957977,0.0,0.001995797734707594
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지.,LoRA,-0.0041017523,0.0,0.004101752303540707
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지.,LoRA 와 QLoRA 의 차이,-0.0014826251,0.0,0.0014826251426711679
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지.,Loss Function 예시,-0.0033601162,0.0,0.0033601161558181047
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지.,Loss Function 정의,-0.0034606892,0.0,0.003460689214989543
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지.,MBTI,-0.0016275803,0.0,0.0016275802627205849
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지.,MSE Loss 설명,-0.0031858073,0.0,0.0031858072616159916
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지.,MSE Loss 용도,-0.0021217382,0.0,0.0021217381581664085
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지.,Multi-Label 에서 CE + Softmax 적용 문제점,-0.0016446483,0.0,0.0016446482622995973
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지.,PEFT 방법 5가지,-0.0021809225,0.0,0.002180922543630004
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지.,거대 언어 모델 정의,0.0012979555,0.0,0.0012979555176571012
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지.,기본 경험,-0.003220115,0.0,0.003220115089789033
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지.,답변 실패,0.9996284,1.0,0.00037157535552978516
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지.,딥러닝,-0.0013301641,0.0,0.0013301641447469592
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지.,마지막 할 말,-0.0016290356,0.0,0.0016290355706587434
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지.,머신러닝,-0.0014415976,0.0,0.0014415975892916322
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지.,면접 시작 인사,-0.0032319813,0.0,0.0032319813035428524
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지.,상세 경험,-0.0024258061,0.0,0.0024258061312139034
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지.,수식,-0.001798379,0.0,0.0017983790021389723
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지.,용어 질문,-0.0018432047,0.0,0.0018432047218084335
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지.,인공지능,-0.0030542745,0.0,0.0030542744789272547
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지.,잠시 휴식,-5.556019e-06,0.0,5.556019004870905e-06
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지.,좋아하는 아이돌,-0.0036519472,0.0,0.003651947248727083
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지.,핵심 아이디어,-0.0010530658,0.0,0.0010530657600611448
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지.,확률 예측에서 MSE Loss 미 사용 이유,-0.00091631163,0.0,0.0009163116337731481
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,BCE 가 좋은 task,-0.00027568964,0.0,0.00027568964287638664
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,BCE 가 좋은 이유,-0.009377494,0.0,0.00937749445438385
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,LLM Fine-Tuning 의 PEFT,-0.0017029824,0.0,0.0017029823502525687
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,LoRA,-0.003537972,0.0,0.003537971992045641
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,LoRA 와 QLoRA 의 차이,-0.0010342475,0.0,0.0010342474561184645
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,Loss Function 예시,-0.0041760593,0.0,0.0041760592721402645
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,Loss Function 정의,0.99669456,0.0,0.9966945648193359
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,MBTI,-0.004082287,0.0,0.004082287196069956
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,MSE Loss 설명,-0.00025936673,0.0,0.0002593667304608971
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,MSE Loss 용도,-0.0035262236,0.0,0.0035262235905975103
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,Multi-Label 에서 CE + Softmax 적용 문제점,-0.0056268037,0.0,0.005626803729683161
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,PEFT 방법 5가지,-0.0031224764,0.0,0.0031224763952195644
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,거대 언어 모델 정의,-0.0016608436,0.0,0.001660843612626195
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,기본 경험,-0.0002809951,0.0,0.00028099509654566646
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,답변 실패,7.869948e-05,1.0,0.9999213005212368
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,딥러닝,-0.010651124,0.0,0.010651123709976673
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,마지막 할 말,-0.00017366341,0.0,0.00017366341489832848
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,머신러닝,-0.0044845077,0.0,0.004484507720917463
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,면접 시작 인사,0.0010817058,0.0,0.00108170579187572
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,상세 경험,-0.005764624,0.0,0.0057646241039037704
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,수식,-0.0013957247,0.0,0.0013957247138023376
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,용어 질문,-0.0026399377,0.0,0.0026399376802146435
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,인공지능,-0.008108121,0.0,0.00810812134295702
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,잠시 휴식,-0.0016662193,0.0,0.0016662193229421973
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,좋아하는 아이돌,0.0051726005,0.0,0.005172600504010916
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,핵심 아이디어,-0.0018244241,0.0,0.001824424136430025
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,확률 예측에서 MSE Loss 미 사용 이유,0.0011942409,0.0,0.0011942408746108413
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",BCE 가 좋은 task,-0.001871319,0.0,0.0018713190220296383
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",BCE 가 좋은 이유,-0.0035504876,0.0,0.003550487570464611
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",LLM Fine-Tuning 의 PEFT,-0.0027953058,0.0,0.0027953058015555143
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",LoRA,-0.0024776687,0.0,0.0024776686914265156
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",LoRA 와 QLoRA 의 차이,-0.0029784064,0.0,0.0029784063808619976
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",Loss Function 예시,-0.0013467371,0.0,0.0013467371463775635
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",Loss Function 정의,0.9981121,1.0,0.0018879175186157227
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",MBTI,0.00017014277,0.0,0.0001701427681837231
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",MSE Loss 설명,-0.0023388902,0.0,0.0023388902191072702
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",MSE Loss 용도,-0.0018708755,0.0,0.0018708754796534777
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",Multi-Label 에서 CE + Softmax 적용 문제점,-0.004651391,0.0,0.004651390947401524
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",PEFT 방법 5가지,-0.0016738272,0.0,0.0016738271806389093
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",거대 언어 모델 정의,-0.0060157906,0.0,0.00601579062640667
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",기본 경험,-0.0014516712,0.0,0.0014516712399199605
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",답변 실패,-0.0007350494,0.0,0.0007350494270212948
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",딥러닝,-0.005600643,0.0,0.005600642878562212
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",마지막 할 말,-0.00044621393,0.0,0.00044621393317356706
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",머신러닝,-0.0020308862,0.0,0.0020308862440288067
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",면접 시작 인사,-0.00079629116,0.0,0.0007962911622598767
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",상세 경험,-0.0057428246,0.0,0.005742824636399746
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",수식,-0.002679154,0.0,0.0026791540440171957
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",용어 질문,-0.0051527964,0.0,0.005152796395123005
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",인공지능,-0.007066294,0.0,0.007066294085234404
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",잠시 휴식,-0.0020522322,0.0,0.0020522321574389935
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",좋아하는 아이돌,-0.00063759217,0.0,0.0006375921657308936
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",핵심 아이디어,-0.003776428,0.0,0.0037764280568808317
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",확률 예측에서 MSE Loss 미 사용 이유,-0.002322568,0.0,0.002322568092495203
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야,BCE 가 좋은 task,-0.0040172935,0.0,0.004017293453216553
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야,BCE 가 좋은 이유,-0.007277259,0.0,0.007277259137481451
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야,LLM Fine-Tuning 의 PEFT,-0.0020952993,0.0,0.002095299307256937
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야,LoRA,-0.0027640376,0.0,0.0027640375774353743
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야,LoRA 와 QLoRA 의 차이,-0.0049189855,0.0,0.0049189855344593525
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야,Loss Function 예시,-0.01764823,0.0,0.017648229375481606
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야,Loss Function 정의,0.95948994,0.0,0.9594899415969849
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야,MBTI,0.00255851,0.0,0.0025585100520402193
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야,MSE Loss 설명,-0.001325163,0.0,0.001325162942521274
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야,MSE Loss 용도,-0.0058166794,0.0,0.005816679447889328
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야,Multi-Label 에서 CE + Softmax 적용 문제점,-0.008792441,0.0,0.008792441338300705
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야,PEFT 방법 5가지,-0.0030751117,0.0,0.0030751116573810577
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야,거대 언어 모델 정의,0.0024923962,0.0,0.0024923961609601974
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야,기본 경험,-0.0010135225,0.0,0.0010135225020349026
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야,답변 실패,0.15887961,1.0,0.8411203920841217
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야,딥러닝,-0.025393063,0.0,0.025393063202500343
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야,마지막 할 말,-0.01668515,0.0,0.01668515056371689
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야,머신러닝,-0.009986661,0.0,0.00998666137456894
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야,면접 시작 인사,-0.008135753,0.0,0.008135752752423286
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야,상세 경험,0.0044807117,0.0,0.004480711650103331
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야,수식,-0.00353072,0.0,0.003530720015987754
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야,용어 질문,-0.011628803,0.0,0.011628802865743637
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야,인공지능,-0.005999082,0.0,0.005999081768095493
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야,잠시 휴식,-0.004688792,0.0,0.004688791930675507
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야,좋아하는 아이돌,0.001594409,0.0,0.0015944089973345399
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야,핵심 아이디어,-0.017328477,0.0,0.017328476533293724
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야,확률 예측에서 MSE Loss 미 사용 이유,0.00045143376,0.0,0.00045143376337364316
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지,BCE 가 좋은 task,-0.0022923064,0.0,0.0022923063952475786
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지,BCE 가 좋은 이유,-0.0064253737,0.0,0.0064253737218678
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지,LLM Fine-Tuning 의 PEFT,-0.0028015545,0.0,0.002801554510369897
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지,LoRA,-0.0060852403,0.0,0.0060852402821183205
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지,LoRA 와 QLoRA 의 차이,-0.0024738605,0.0,0.002473860513418913
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지,Loss Function 예시,-0.0013140194,0.0,0.0013140194350853562
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지,Loss Function 정의,0.9985874,1.0,0.0014125704765319824
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지,MBTI,0.0019926052,0.0,0.001992605160921812
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지,MSE Loss 설명,-0.001858823,0.0,0.001858823001384735
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지,MSE Loss 용도,-0.0016245965,0.0,0.0016245965380221605
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지,Multi-Label 에서 CE + Softmax 적용 문제점,-0.0025571443,0.0,0.0025571442674845457
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지,PEFT 방법 5가지,-0.0018360273,0.0,0.0018360272515565157
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지,거대 언어 모델 정의,-0.007392065,0.0,0.00739206513389945
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지,기본 경험,-0.003463862,0.0,0.0034638619981706142
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지,답변 실패,-0.0011334985,0.0,0.001133498502895236
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지,딥러닝,-0.003997299,0.0,0.003997298888862133
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지,마지막 할 말,-0.0018202699,0.0,0.001820269855670631
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지,머신러닝,-0.0016115766,0.0,0.0016115766484290361
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지,면접 시작 인사,-0.00060917035,0.0,0.000609170354437083
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지,상세 경험,-0.007377228,0.0,0.007377227768301964
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지,수식,-0.0016166392,0.0,0.001616639201529324
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지,용어 질문,-0.0018610258,0.0,0.0018610258121043444
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지,인공지능,-0.00342323,0.0,0.0034232300240546465
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지,잠시 휴식,0.0008528084,0.0,0.0008528084144927561
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지,좋아하는 아이돌,-0.0020910795,0.0,0.0020910794846713543
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지,핵심 아이디어,-0.001827606,0.0,0.0018276060000061989
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지,확률 예측에서 MSE Loss 미 사용 이유,-0.0049033184,0.0,0.004903318360447884
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,BCE 가 좋은 task,-0.0009571415,0.0,0.0009571415139362216
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,BCE 가 좋은 이유,-0.0006816861,0.0,0.0006816860986873507
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,LLM Fine-Tuning 의 PEFT,-0.0018129492,0.0,0.001812949194572866
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,LoRA,-0.0045516742,0.0,0.004551674239337444
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,LoRA 와 QLoRA 의 차이,-0.0003150541,0.0,0.0003150540869683027
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,Loss Function 예시,0.009577791,0.0,0.009577791206538677
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,Loss Function 정의,-0.0014659959,0.0,0.0014659959124401212
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,MBTI,-0.0026686876,0.0,0.0026686876080930233
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,MSE Loss 설명,-0.003611517,0.0,0.0036115169059485197
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,MSE Loss 용도,-0.0010970401,0.0,0.0010970401344820857
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,Multi-Label 에서 CE + Softmax 적용 문제점,-0.0022955304,0.0,0.0022955304011702538
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,PEFT 방법 5가지,-0.0019677207,0.0,0.0019677206873893738
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,거대 언어 모델 정의,-0.0010858098,0.0,0.001085809781216085
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,기본 경험,-0.0032922933,0.0,0.0032922932878136635
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,답변 실패,0.9989195,1.0,0.0010805130004882812
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,딥러닝,-0.0017196084,0.0,0.001719608437269926
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,마지막 할 말,-0.002153343,0.0,0.002153343055397272
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,머신러닝,-0.0024666898,0.0,0.002466689795255661
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,면접 시작 인사,-0.005192157,0.0,0.005192156881093979
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,상세 경험,-0.0018116068,0.0,0.0018116068094968796
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,수식,-0.0024750787,0.0,0.00247507868334651
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,용어 질문,-0.0022645683,0.0,0.0022645683493465185
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,인공지능,-0.003555084,0.0,0.0035550841130316257
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,잠시 휴식,-0.0007176156,0.0,0.0007176155922934413
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,좋아하는 아이돌,-0.0031958187,0.0,0.0031958187464624643
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,핵심 아이디어,-0.00012591758,0.0,0.00012591757695190609
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,확률 예측에서 MSE Loss 미 사용 이유,-0.0023834535,0.0,0.0023834535386413336
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",BCE 가 좋은 task,-0.0025489859,0.0,0.002548985881730914
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",BCE 가 좋은 이유,-0.004902245,0.0,0.004902245011180639
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",LLM Fine-Tuning 의 PEFT,-0.0031995338,0.0,0.0031995337922126055
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",LoRA,-0.0037592105,0.0,0.0037592104636132717
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",LoRA 와 QLoRA 의 차이,-0.0009669623,0.0,0.0009669623104855418
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",Loss Function 예시,0.9985545,1.0,0.001445472240447998
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",Loss Function 정의,-0.0022951046,0.0,0.002295104553923011
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",MBTI,-0.0016218979,0.0,0.0016218979144468904
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",MSE Loss 설명,-0.00544406,0.0,0.005444060079753399
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",MSE Loss 용도,-0.0073455954,0.0,0.007345595397055149
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",Multi-Label 에서 CE + Softmax 적용 문제점,-0.0021978952,0.0,0.00219789519906044
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",PEFT 방법 5가지,-0.0017774282,0.0,0.0017774282023310661
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",거대 언어 모델 정의,-0.0014470008,0.0,0.0014470007736235857
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",기본 경험,-0.0027396793,0.0,0.0027396793011575937
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",답변 실패,-0.00033067,0.0,0.0003306700091343373
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",딥러닝,-0.0017141642,0.0,0.0017141641583293676
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",마지막 할 말,-0.0015420531,0.0,0.0015420530689880252
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",머신러닝,0.0024396756,0.0,0.002439675619825721
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",면접 시작 인사,-0.002333872,0.0,0.0023338720202445984
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",상세 경험,-0.0030364383,0.0,0.0030364382546395063
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",수식,-0.0021768063,0.0,0.002176806330680847
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",용어 질문,-0.0011270026,0.0,0.0011270026443526149
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",인공지능,-0.000957583,0.0,0.0009575830190442502
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",잠시 휴식,-0.0013421703,0.0,0.0013421702897176147
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",좋아하는 아이돌,-0.0017268381,0.0,0.0017268380615860224
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",핵심 아이디어,-0.0005684686,0.0,0.00056846858933568
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",확률 예측에서 MSE Loss 미 사용 이유,-0.0034543122,0.0,0.003454312216490507
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어",BCE 가 좋은 task,-0.0018214921,0.0,0.001821492100134492
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어",BCE 가 좋은 이유,-0.0028868178,0.0,0.002886817790567875
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어",LLM Fine-Tuning 의 PEFT,-0.0032050696,0.0,0.00320506957359612
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어",LoRA,-0.004298081,0.0,0.0042980811558663845
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어",LoRA 와 QLoRA 의 차이,-7.104358e-05,0.0,7.104357791831717e-05
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어",Loss Function 예시,0.99838996,1.0,0.0016100406646728516
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어",Loss Function 정의,-0.0016927083,0.0,0.0016927083488553762
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어",MBTI,-0.002773184,0.0,0.002773184096440673
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어",MSE Loss 설명,-0.004836809,0.0,0.004836808890104294
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어",MSE Loss 용도,-0.0060299784,0.0,0.006029978394508362
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어",Multi-Label 에서 CE + Softmax 적용 문제점,-0.0005247942,0.0,0.0005247942171990871
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어",PEFT 방법 5가지,-0.0017226117,0.0,0.0017226117197424173
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어",거대 언어 모델 정의,-0.0010764593,0.0,0.0010764593025669456
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어",기본 경험,-0.0037741994,0.0,0.003774199401959777
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어",답변 실패,-0.001529167,0.0,0.0015291670570150018
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어",딥러닝,-0.00022438429,0.0,0.00022438429004978389
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어",마지막 할 말,-0.0018597065,0.0,0.0018597064772620797
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어",머신러닝,0.0006350547,0.0,0.0006350547191686928
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어",면접 시작 인사,-0.0022384764,0.0,0.0022384764160960913
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어",상세 경험,-0.0043095425,0.0,0.00430954247713089
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어",수식,-0.004928766,0.0,0.004928765818476677
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어",용어 질문,-0.0025628754,0.0,0.0025628753937780857
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어",인공지능,-0.0022502572,0.0,0.0022502571810036898
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어",잠시 휴식,-0.0011911219,0.0,0.001191121875308454
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어",좋아하는 아이돌,-0.00019518565,0.0,0.00019518565386533737
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어",핵심 아이디어,0.0006951321,0.0,0.0006951321265660226
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어",확률 예측에서 MSE Loss 미 사용 이유,-0.002875608,0.0,0.0028756079263985157
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야?",BCE 가 좋은 task,-0.0016319848,0.0,0.001631984836421907
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야?",BCE 가 좋은 이유,-0.003363387,0.0,0.0033633869607001543
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야?",LLM Fine-Tuning 의 PEFT,-0.00225577,0.0,0.0022557699121534824
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야?",LoRA,-0.0027948085,0.0,0.0027948084753006697
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야?",LoRA 와 QLoRA 의 차이,-0.00107237,0.0,0.0010723699815571308
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야?",Loss Function 예시,-0.0021192378,0.0,0.0021192377898842096
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야?",Loss Function 정의,-0.0023273632,0.0,0.0023273632396012545
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야?",MBTI,-0.0025533412,0.0,0.0025533412117511034
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야?",MSE Loss 설명,-0.0028929405,0.0,0.0028929405380040407
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야?",MSE Loss 용도,-0.00060250185,0.0,0.0006025018519721925
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야?",Multi-Label 에서 CE + Softmax 적용 문제점,-0.001364238,0.0,0.0013642379781231284
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야?",PEFT 방법 5가지,-0.0020356472,0.0,0.002035647165030241
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야?",거대 언어 모델 정의,-0.0016307775,0.0,0.0016307774931192398
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야?",기본 경험,-0.002999801,0.0,0.002999800955876708
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야?",답변 실패,0.9997027,1.0,0.00029730796813964844
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야?",딥러닝,0.00027618522,0.0,0.0002761852229014039
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야?",마지막 할 말,-0.002372124,0.0,0.0023721239995211363
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야?",머신러닝,-0.001329348,0.0,0.0013293479569256306
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야?",면접 시작 인사,-0.0039813383,0.0,0.00398133834823966
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야?",상세 경험,-0.002859009,0.0,0.002859008964151144
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야?",수식,-0.0026612915,0.0,0.0026612915098667145
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야?",용어 질문,-0.0016284744,0.0,0.0016284744488075376
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야?",인공지능,-0.0024527428,0.0,0.002452742774039507
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야?",잠시 휴식,-0.001036222,0.0,0.0010362219763919711
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야?",좋아하는 아이돌,-0.002931025,0.0,0.0029310251120477915
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야?",핵심 아이디어,-0.00095325446,0.0,0.0009532544645480812
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야?",확률 예측에서 MSE Loss 미 사용 이유,-0.0006648028,0.0,0.0006648027920164168
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어",BCE 가 좋은 task,-0.0022839324,0.0,0.0022839324083179235
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어",BCE 가 좋은 이유,-0.00548455,0.0,0.005484549794346094
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어",LLM Fine-Tuning 의 PEFT,-0.0022406196,0.0,0.002240619622170925
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어",LoRA,-0.004403878,0.0,0.004403878003358841
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어",LoRA 와 QLoRA 의 차이,0.0006267251,0.0,0.0006267250864766538
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어",Loss Function 예시,0.9977961,1.0,0.0022038817405700684
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어",Loss Function 정의,-0.0011505069,0.0,0.001150506897829473
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어",MBTI,-0.0024364004,0.0,0.002436400391161442
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어",MSE Loss 설명,-0.006790276,0.0,0.006790276151150465
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어",MSE Loss 용도,-0.0064513087,0.0,0.006451308727264404
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어",Multi-Label 에서 CE + Softmax 적용 문제점,-0.00140004,0.0,0.0014000399969518185
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어",PEFT 방법 5가지,-0.00058437407,0.0,0.0005843740655109286
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어",거대 언어 모델 정의,-0.00310549,0.0,0.0031054900027811527
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어",기본 경험,-0.0027789727,0.0,0.0027789727319031954
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어",답변 실패,-0.0020357857,0.0,0.002035785699263215
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어",딥러닝,0.00018651444,0.0,0.0001865144440671429
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어",마지막 할 말,-0.0018421592,0.0,0.0018421591958031058
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어",머신러닝,0.0030495399,0.0,0.0030495398677885532
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어",면접 시작 인사,-0.0028821852,0.0,0.002882185159251094
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어",상세 경험,-0.0035705944,0.0,0.003570594359189272
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어",수식,-0.0037381202,0.0,0.003738120198249817
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어",용어 질문,-0.0015444444,0.0,0.0015444443561136723
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어",인공지능,-0.001985803,0.0,0.0019858030136674643
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어",잠시 휴식,-0.0007165995,0.0,0.0007165995193645358
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어",좋아하는 아이돌,-0.0021056668,0.0,0.0021056667901575565
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어",핵심 아이디어,-0.0024239023,0.0,0.0024239022750407457
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어",확률 예측에서 MSE Loss 미 사용 이유,-0.0025775067,0.0,0.0025775067042559385
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아",BCE 가 좋은 task,-0.0014506847,0.0,0.001450684736482799
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아",BCE 가 좋은 이유,-0.005941272,0.0,0.005941271781921387
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아",LLM Fine-Tuning 의 PEFT,-0.0025108499,0.0,0.002510849852114916
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아",LoRA,-0.0038483941,0.0,0.0038483941461890936
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아",LoRA 와 QLoRA 의 차이,-0.0016192221,0.0,0.0016192221082746983
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아",Loss Function 예시,0.99825156,1.0,0.0017484426498413086
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아",Loss Function 정의,-0.001704038,0.0,0.0017040380043908954
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아",MBTI,-0.0016189383,0.0,0.0016189382877200842
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아",MSE Loss 설명,-0.0057426477,0.0,0.005742647685110569
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아",MSE Loss 용도,-0.008473946,0.0,0.008473945781588554
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아",Multi-Label 에서 CE + Softmax 적용 문제점,-0.0016032686,0.0,0.0016032685525715351
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아",PEFT 방법 5가지,-0.0011809727,0.0,0.00118097267113626
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아",거대 언어 모델 정의,-0.0012666971,0.0,0.0012666970724239945
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아",기본 경험,-0.0027810254,0.0,0.002781025366857648
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아",답변 실패,-0.0009823429,0.0,0.0009823428699746728
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아",딥러닝,-0.0016079858,0.0,0.0016079858178272843
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아",마지막 할 말,-0.0012350181,0.0,0.001235018135048449
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아",머신러닝,0.0021001839,0.0,0.0021001838613301516
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아",면접 시작 인사,-0.0032919608,0.0,0.0032919608056545258
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아",상세 경험,-0.003562216,0.0,0.0035622159484773874
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아",수식,-0.0025300502,0.0,0.002530050231143832
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아",용어 질문,-0.0026241709,0.0,0.0026241708546876907
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아",인공지능,-0.0021465104,0.0,0.0021465104073286057
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아",잠시 휴식,-0.0014241309,0.0,0.0014241308672353625
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아",좋아하는 아이돌,-0.00082373485,0.0,0.0008237348520196974
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아",핵심 아이디어,9.6446456e-05,0.0,9.64464561548084e-05
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아",확률 예측에서 MSE Loss 미 사용 이유,-0.0037021465,0.0,0.003702146466821432
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,BCE 가 좋은 task,-0.0003192665,0.0,0.000319266488077119
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,BCE 가 좋은 이유,-0.0033249953,0.0,0.003324995283037424
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,LLM Fine-Tuning 의 PEFT,-0.0027116882,0.0,0.002711688168346882
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,LoRA,-0.002168172,0.0,0.002168172039091587
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,LoRA 와 QLoRA 의 차이,-0.0030721729,0.0,0.0030721728689968586
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,Loss Function 예시,-0.0051605036,0.0,0.005160503555089235
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,Loss Function 정의,-0.002040662,0.0,0.0020406621042639017
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,MBTI,-0.004633146,0.0,0.004633145872503519
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,MSE Loss 설명,0.998464,1.0,0.0015360116958618164
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,MSE Loss 용도,0.0012953868,0.0,0.0012953868135809898
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,Multi-Label 에서 CE + Softmax 적용 문제점,-0.0018161536,0.0,0.0018161536427214742
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,PEFT 방법 5가지,-0.001645045,0.0,0.0016450450057163835
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,거대 언어 모델 정의,-0.0031133078,0.0,0.0031133077573031187
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,기본 경험,-0.0033447566,0.0,0.0033447565510869026
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,답변 실패,-0.002603815,0.0,0.0026038149371743202
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,딥러닝,-7.741704e-05,0.0,7.741704030195251e-05
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,마지막 할 말,-0.002293521,0.0,0.002293521072715521
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,머신러닝,-0.0039084004,0.0,0.003908400423824787
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,면접 시작 인사,-0.002263143,0.0,0.0022631429601460695
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,상세 경험,-0.0055896766,0.0,0.005589676555246115
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,수식,0.0011180486,0.0,0.0011180485598742962
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,용어 질문,0.00066149165,0.0,0.0006614916492253542
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,인공지능,0.00066177314,0.0,0.0006617731414735317
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,잠시 휴식,-0.003961267,0.0,0.003961266949772835
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,좋아하는 아이돌,-0.0013989222,0.0,0.0013989221770316362
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,핵심 아이디어,-0.0005464689,0.0,0.0005464688874781132
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,확률 예측에서 MSE Loss 미 사용 이유,-0.005407136,0.0,0.005407135933637619
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,BCE 가 좋은 task,-0.0016895856,0.0,0.0016895856242626905
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,BCE 가 좋은 이유,-0.0040380806,0.0,0.00403808057308197
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,LLM Fine-Tuning 의 PEFT,-0.0011927368,0.0,0.0011927367886528373
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,LoRA,-0.002861518,0.0,0.002861517947167158
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,LoRA 와 QLoRA 의 차이,-0.0011870275,0.0,0.0011870275484398007
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,Loss Function 예시,-0.0030122355,0.0,0.0030122355092316866
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,Loss Function 정의,-0.0035311999,0.0,0.0035311998799443245
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,MBTI,-0.0031611642,0.0,0.0031611642334610224
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,MSE Loss 설명,-0.0020882573,0.0,0.0020882573444396257
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,MSE Loss 용도,-0.0011835538,0.0,0.0011835538316518068
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,Multi-Label 에서 CE + Softmax 적용 문제점,-0.000959836,0.0,0.0009598360047675669
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,PEFT 방법 5가지,-0.0030154607,0.0,0.00301546067930758
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,거대 언어 모델 정의,-0.0018303966,0.0,0.0018303965916857123
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,기본 경험,-0.0022636775,0.0,0.002263677539303899
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,답변 실패,0.999717,1.0,0.0002830028533935547
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,딥러닝,-0.00029839613,0.0,0.00029839613125659525
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,마지막 할 말,-0.0027726511,0.0,0.002772651147097349
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,머신러닝,-0.0017273981,0.0,0.0017273981356993318
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,면접 시작 인사,-0.0024680651,0.0,0.0024680651258677244
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,상세 경험,-0.0024909724,0.0,0.002490972401574254
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,수식,-0.002862021,0.0,0.002862021094188094
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,용어 질문,-0.0017373395,0.0,0.0017373395385220647
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,인공지능,-0.002758919,0.0,0.0027589190285652876
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,잠시 휴식,-0.00021985208,0.0,0.0002198520814999938
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,좋아하는 아이돌,-0.0027700935,0.0,0.0027700935024768114
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,핵심 아이디어,-0.0016854207,0.0,0.0016854207497090101
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,확률 예측에서 MSE Loss 미 사용 이유,-0.0015624473,0.0,0.0015624472871422768
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",BCE 가 좋은 task,-0.0021878008,0.0,0.0021878008265048265
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",BCE 가 좋은 이유,-0.006960116,0.0,0.006960115861147642
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",LLM Fine-Tuning 의 PEFT,-0.00013100386,0.0,0.00013100386422593147
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",LoRA,-0.005810988,0.0,0.005810988135635853
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",LoRA 와 QLoRA 의 차이,-0.0037154285,0.0,0.0037154285237193108
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",Loss Function 예시,-0.008085629,0.0,0.008085628971457481
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",Loss Function 정의,-0.0011770175,0.0,0.00117701746057719
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",MBTI,-0.0015838103,0.0,0.001583810313604772
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",MSE Loss 설명,0.0029235207,0.0,0.0029235207475721836
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",MSE Loss 용도,0.9968934,1.0,0.0031065940856933594
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",Multi-Label 에서 CE + Softmax 적용 문제점,-0.0016877524,0.0,0.0016877524321898818
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",PEFT 방법 5가지,-0.0029436124,0.0,0.002943612402305007
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",거대 언어 모델 정의,-0.0012494848,0.0,0.0012494848342612386
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",기본 경험,-0.0026909919,0.0,0.0026909918524324894
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",답변 실패,-0.001082997,0.0,0.0010829969542101026
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",딥러닝,-0.0014729133,0.0,0.0014729133108630776
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",마지막 할 말,-0.004933075,0.0,0.004933075048029423
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",머신러닝,-0.0017747456,0.0,0.001774745644070208
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",면접 시작 인사,-0.0038250224,0.0,0.0038250223733484745
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",상세 경험,-0.0028284737,0.0,0.0028284736908972263
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",수식,-0.0019008699,0.0,0.0019008698873221874
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",용어 질문,-1.863827e-05,0.0,1.8638269466464408e-05
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",인공지능,-0.00072277494,0.0,0.0007227749447338283
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",잠시 휴식,-0.005110627,0.0,0.005110627040266991
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",좋아하는 아이돌,-0.0032360856,0.0,0.003236085642129183
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",핵심 아이디어,0.0015306938,0.0,0.001530693843960762
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",확률 예측에서 MSE Loss 미 사용 이유,-0.0039656274,0.0,0.0039656274020671844
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,BCE 가 좋은 task,-0.0016241411,0.0,0.0016241411212831736
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,BCE 가 좋은 이유,-0.0029630128,0.0,0.002963012782856822
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,LLM Fine-Tuning 의 PEFT,-0.0017763326,0.0,0.0017763326177373528
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,LoRA,-0.0034499622,0.0,0.003449962241575122
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,LoRA 와 QLoRA 의 차이,-0.0018089789,0.0,0.0018089788500219584
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,Loss Function 예시,-0.0025411225,0.0,0.002541122492402792
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,Loss Function 정의,-0.0034152952,0.0,0.0034152951557189226
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,MBTI,-0.0033040897,0.0,0.003304089652374387
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,MSE Loss 설명,-0.0036641553,0.0,0.0036641552578657866
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,MSE Loss 용도,-0.0010080169,0.0,0.0010080168722197413
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,Multi-Label 에서 CE + Softmax 적용 문제점,-0.0012506655,0.0,0.0012506655184552073
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,PEFT 방법 5가지,-0.0021042232,0.0,0.0021042232401669025
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,거대 언어 모델 정의,-0.00090724987,0.0,0.0009072498651221395
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,기본 경험,-0.0032798832,0.0,0.0032798831816762686
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,답변 실패,0.999729,1.0,0.00027102231979370117
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,딥러닝,0.00017857616,0.0,0.00017857615603134036
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,마지막 할 말,-0.0025921839,0.0,0.002592183882370591
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,머신러닝,-0.00202766,0.0,0.002027659909799695
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,면접 시작 인사,-0.0036032908,0.0,0.003603290766477585
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,상세 경험,-0.0019296514,0.0,0.0019296513637527823
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,수식,-0.0027488742,0.0,0.0027488742489367723
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,용어 질문,-0.0012653125,0.0,0.0012653125450015068
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,인공지능,-0.0027275085,0.0,0.002727508544921875
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,잠시 휴식,-0.00030553841,0.0,0.0003055384149774909
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,좋아하는 아이돌,-0.002791534,0.0,0.0027915339451283216
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,핵심 아이디어,-0.0016470803,0.0,0.0016470802947878838
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,확률 예측에서 MSE Loss 미 사용 이유,-0.0011218194,0.0,0.0011218193685635924
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,BCE 가 좋은 task,-0.0014118731,0.0,0.001411873148754239
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,BCE 가 좋은 이유,-0.0030179343,0.0,0.0030179342720657587
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,LLM Fine-Tuning 의 PEFT,-0.0013181875,0.0,0.001318187452852726
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,LoRA,-0.0042307023,0.0,0.004230702295899391
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,LoRA 와 QLoRA 의 차이,-0.0021577175,0.0,0.002157717477530241
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,Loss Function 예시,-0.003273609,0.0,0.0032736090943217278
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,Loss Function 정의,-0.00312161,0.0,0.0031216100323945284
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,MBTI,-0.0032775283,0.0,0.0032775283325463533
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,MSE Loss 설명,-0.0047714883,0.0,0.004771488253027201
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,MSE Loss 용도,-0.00067213457,0.0,0.0006721345707774162
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,Multi-Label 에서 CE + Softmax 적용 문제점,-0.0023175096,0.0,0.002317509613931179
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,PEFT 방법 5가지,-0.0024674765,0.0,0.0024674765300005674
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,거대 언어 모델 정의,-0.0009225539,0.0,0.0009225538815371692
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,기본 경험,-0.0034428814,0.0,0.0034428813960403204
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,답변 실패,0.99958014,1.0,0.00041985511779785156
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,딥러닝,-0.00016173215,0.0,0.00016173215408343822
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,마지막 할 말,-0.003230617,0.0,0.0032306169159710407
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,머신러닝,-0.0022429675,0.0,0.0022429674863815308
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,면접 시작 인사,-0.0028493514,0.0,0.002849351381883025
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,상세 경험,-0.0038600583,0.0,0.0038600582629442215
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,수식,-0.0022725756,0.0,0.0022725756280124187
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,용어 질문,-0.0012840198,0.0,0.0012840197887271643
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,인공지능,-0.002876712,0.0,0.0028767120093107224
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,잠시 휴식,-0.00051987625,0.0,0.0005198762519285083
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,좋아하는 아이돌,-0.0014381198,0.0,0.0014381197979673743
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,핵심 아이디어,-0.00056016794,0.0,0.0005601679440587759
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,확률 예측에서 MSE Loss 미 사용 이유,0.00010267759,0.0,0.0001026775935315527
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,BCE 가 좋은 task,0.0048195375,0.0,0.00481953751295805
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,BCE 가 좋은 이유,-0.004904794,0.0,0.004904794041067362
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,LLM Fine-Tuning 의 PEFT,-0.004608324,0.0,0.00460832379758358
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,LoRA,-0.002741328,0.0,0.0027413279749453068
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,LoRA 와 QLoRA 의 차이,-0.0027267016,0.0,0.0027267015539109707
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,Loss Function 예시,-0.0027673931,0.0,0.002767393132671714
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,Loss Function 정의,-0.0056811124,0.0,0.0056811124086380005
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,MBTI,-0.00025303842,0.0,0.00025303842267021537
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,MSE Loss 설명,-0.0015721985,0.0,0.0015721984673291445
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,MSE Loss 용도,-0.005389286,0.0,0.005389286205172539
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,Multi-Label 에서 CE + Softmax 적용 문제점,-0.0064370907,0.0,0.006437090691179037
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,PEFT 방법 5가지,-0.0033185624,0.0,0.0033185624051839113
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,거대 언어 모델 정의,-0.0025313885,0.0,0.0025313885416835546
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,기본 경험,-0.0019221491,0.0,0.0019221490947529674
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,답변 실패,-0.00032057,0.0,0.0003205699904356152
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,딥러닝,-0.0001907256,0.0,0.00019072559371124953
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,마지막 할 말,-0.0015941698,0.0,0.0015941697638481855
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,머신러닝,-0.00070940424,0.0,0.0007094042375683784
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,면접 시작 인사,-0.004226278,0.0,0.00422627804800868
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,상세 경험,-0.0048925537,0.0,0.004892553668469191
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,수식,-0.0023008923,0.0,0.0023008922580629587
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,용어 질문,-0.00574419,0.0,0.005744189955294132
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,인공지능,-0.0019578936,0.0,0.001957893604412675
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,잠시 휴식,-0.0026973144,0.0,0.0026973143685609102
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,좋아하는 아이돌,0.00029382884,0.0,0.0002938288380391896
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,핵심 아이디어,-0.005367459,0.0,0.005367458797991276
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,확률 예측에서 MSE Loss 미 사용 이유,0.99873066,1.0,0.0012693405151367188
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,BCE 가 좋은 task,0.00029228837,0.0,0.00029228837229311466
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,BCE 가 좋은 이유,-0.0054956255,0.0,0.005495625548064709
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,LLM Fine-Tuning 의 PEFT,-0.005836362,0.0,0.0058363620191812515
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,LoRA,-0.0013583842,0.0,0.0013583841500803828
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,LoRA 와 QLoRA 의 차이,-0.0019458093,0.0,0.0019458093447610736
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,Loss Function 예시,-0.00042310264,0.0,0.00042310263961553574
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,Loss Function 정의,-0.006085247,0.0,0.006085246801376343
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,MBTI,-0.0028571005,0.0,0.0028571004513651133
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,MSE Loss 설명,-0.0013131156,0.0,0.001313115586526692
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,MSE Loss 용도,-0.0025261752,0.0,0.002526175230741501
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,Multi-Label 에서 CE + Softmax 적용 문제점,-0.0020306977,0.0,0.002030697651207447
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,PEFT 방법 5가지,-0.0012394411,0.0,0.0012394411023706198
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,거대 언어 모델 정의,-2.3449666e-05,0.0,2.3449665604857728e-05
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,기본 경험,0.00056416495,0.0,0.0005641649477183819
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,답변 실패,-0.0017457121,0.0,0.0017457121284678578
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,딥러닝,-0.00062400213,0.0,0.0006240021320991218
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,마지막 할 말,-0.0038403089,0.0,0.0038403088692575693
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,머신러닝,-0.00196463,0.0,0.0019646300934255123
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,면접 시작 인사,-0.004414117,0.0,0.0044141169637441635
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,상세 경험,0.00079633674,0.0,0.000796336738858372
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,수식,0.9935925,1.0,0.006407499313354492
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,용어 질문,-0.0020662278,0.0,0.002066227840259671
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,인공지능,-0.005634163,0.0,0.005634163040667772
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,잠시 휴식,-0.00067030604,0.0,0.0006703060353174806
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,좋아하는 아이돌,-0.0009066735,0.0,0.0009066734928637743
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,핵심 아이디어,-0.00044752948,0.0,0.00044752948451787233
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,확률 예측에서 MSE Loss 미 사용 이유,0.0006858787,0.0,0.000685878680087626
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",BCE 가 좋은 task,-0.0011713237,0.0,0.0011713237036019564
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",BCE 가 좋은 이유,-0.0007305204,0.0,0.0007305204053409398
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",LLM Fine-Tuning 의 PEFT,-0.00080759486,0.0,0.0008075948571786284
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",LoRA,-0.00083830755,0.0,0.0008383075473830104
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",LoRA 와 QLoRA 의 차이,0.0020081308,0.0,0.0020081307739019394
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",Loss Function 예시,-0.00088988687,0.0,0.0008898868691176176
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",Loss Function 정의,-0.0005664477,0.0,0.0005664476775564253
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",MBTI,-0.0031670448,0.0,0.003167044837027788
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",MSE Loss 설명,-0.0010248178,0.0,0.0010248178150504827
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",MSE Loss 용도,9.130068e-05,0.0,9.130068065132946e-05
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",Multi-Label 에서 CE + Softmax 적용 문제점,-0.0007109297,0.0,0.0007109296857379377
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",PEFT 방법 5가지,-0.0010827889,0.0,0.0010827889200299978
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",거대 언어 모델 정의,-0.0009907198,0.0,0.0009907197672873735
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",기본 경험,-0.0035549225,0.0,0.00355492252856493
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",답변 실패,-0.0011031232,0.0,0.0011031231842935085
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",딥러닝,-0.002032542,0.0,0.0020325419027358294
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",마지막 할 말,-0.0037033272,0.0,0.003703327151015401
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",머신러닝,-0.002036363,0.0,0.002036362886428833
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",면접 시작 인사,-0.0015055254,0.0,0.0015055254334583879
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",상세 경험,-0.0022500507,0.0,0.002250050660222769
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",수식,-0.0021066484,0.0,0.0021066484041512012
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",용어 질문,-0.002617622,0.0,0.0026176220271736383
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",인공지능,-0.0018218302,0.0,0.0018218301702290773
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",잠시 휴식,-4.419474e-06,0.0,4.419473953021225e-06
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",좋아하는 아이돌,-0.0021024859,0.0,0.0021024858579039574
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",핵심 아이디어,0.9975329,1.0,0.0024670958518981934
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",확률 예측에서 MSE Loss 미 사용 이유,-0.004273161,0.0,0.0042731608264148235
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,BCE 가 좋은 task,-0.0032794476,0.0,0.003279447555541992
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,BCE 가 좋은 이유,0.024349587,0.0,0.02434958703815937
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,LLM Fine-Tuning 의 PEFT,0.0057052597,0.0,0.005705259740352631
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,LoRA,-0.0067422143,0.0,0.006742214318364859
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,LoRA 와 QLoRA 의 차이,-0.006478513,0.0,0.00647851312533021
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,Loss Function 예시,0.007905934,0.0,0.007905934005975723
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,Loss Function 정의,0.006252519,0.0,0.006252518855035305
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,MBTI,-0.013670562,0.0,0.013670561835169792
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,MSE Loss 설명,-0.0066898344,0.0,0.006689834408462048
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,MSE Loss 용도,-0.012411921,0.0,0.01241192128509283
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,Multi-Label 에서 CE + Softmax 적용 문제점,0.006136463,0.0,0.006136463023722172
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,PEFT 방법 5가지,-0.00022562715,0.0,0.00022562715457752347
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,거대 언어 모델 정의,0.009309152,0.0,0.009309152141213417
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,기본 경험,-0.0009723178,0.0,0.0009723178227432072
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,답변 실패,0.9698019,1.0,0.030198097229003906
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,딥러닝,-0.015506455,0.0,0.01550645474344492
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,마지막 할 말,-0.0066771624,0.0,0.006677162367850542
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,머신러닝,0.008188767,0.0,0.008188767358660698
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,면접 시작 인사,-0.00085292757,0.0,0.000852927565574646
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,상세 경험,0.0034745266,0.0,0.003474526572972536
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,수식,0.005443532,0.0,0.005443532019853592
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,용어 질문,-0.005581873,0.0,0.005581873003393412
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,인공지능,0.0061596557,0.0,0.006159655749797821
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,잠시 휴식,-0.0056164134,0.0,0.005616413429379463
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,좋아하는 아이돌,0.0022017655,0.0,0.0022017655428498983
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,핵심 아이디어,-0.0033510546,0.0,0.00335105461999774
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,확률 예측에서 MSE Loss 미 사용 이유,-0.0017196157,0.0,0.0017196156550198793
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",BCE 가 좋은 task,-0.0016533751,0.0,0.0016533751040697098
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",BCE 가 좋은 이유,-0.0040532276,0.0,0.004053227603435516
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",LLM Fine-Tuning 의 PEFT,-0.0025235612,0.0,0.002523561241105199
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",LoRA,-0.00049366005,0.0,0.0004936600453220308
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",LoRA 와 QLoRA 의 차이,0.0034996902,0.0,0.0034996902104467154
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",Loss Function 예시,0.0029239603,0.0,0.002923960331827402
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",Loss Function 정의,-0.0013109687,0.0,0.0013109686551615596
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",MBTI,-0.0041291425,0.0,0.004129142500460148
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",MSE Loss 설명,-0.00025665996,0.0,0.000256659957813099
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",MSE Loss 용도,-0.0021510166,0.0,0.0021510166116058826
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",Multi-Label 에서 CE + Softmax 적용 문제점,-0.0012909154,0.0,0.001290915417484939
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",PEFT 방법 5가지,-0.0011411571,0.0,0.0011411571176722646
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",거대 언어 모델 정의,-0.0036805084,0.0,0.0036805083509534597
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",기본 경험,-0.0017493583,0.0,0.0017493582563474774
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",답변 실패,-0.00052234175,0.0,0.00052234175382182
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",딥러닝,-0.0023096348,0.0,0.002309634815901518
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",마지막 할 말,-0.0059240777,0.0,0.005924077704548836
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",머신러닝,-0.0029073213,0.0,0.002907321322709322
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",면접 시작 인사,-0.001867068,0.0,0.001867068000137806
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",상세 경험,-0.0039990535,0.0,0.003999053500592709
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",수식,-0.0040493435,0.0,0.0040493435226380825
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",용어 질문,-0.000516002,0.0,0.0005160020082257688
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",인공지능,0.0011219562,0.0,0.001121956156566739
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",잠시 휴식,-0.00031981597,0.0,0.0003198159683961421
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",좋아하는 아이돌,6.63986e-05,0.0,6.639859930146486e-05
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",핵심 아이디어,0.9970792,1.0,0.002920806407928467
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",확률 예측에서 MSE Loss 미 사용 이유,-0.0071817953,0.0,0.007181795313954353
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')]",BCE 가 좋은 task,0.00036813776,0.0,0.00036813775659538805
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')]",BCE 가 좋은 이유,-0.0058861854,0.0,0.005886185448616743
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')]",LLM Fine-Tuning 의 PEFT,-0.0055760914,0.0,0.005576091352850199
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')]",LoRA,0.0003433109,0.0,0.0003433109086472541
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')]",LoRA 와 QLoRA 의 차이,-0.0026165745,0.0,0.0026165745221078396
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')]",Loss Function 예시,-0.002263257,0.0,0.00226325704716146
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')]",Loss Function 정의,-0.006061181,0.0,0.0060611809603869915
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')]",MBTI,-0.0031687429,0.0,0.003168742870911956
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')]",MSE Loss 설명,0.0020077853,0.0,0.002007785253226757
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')]",MSE Loss 용도,-0.0035251135,0.0,0.0035251134540885687
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')]",Multi-Label 에서 CE + Softmax 적용 문제점,-0.002004812,0.0,0.002004812005907297
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')]",PEFT 방법 5가지,-0.003510694,0.0,0.003510694019496441
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')]",거대 언어 모델 정의,-8.816616e-05,0.0,8.816616173135117e-05
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')]",기본 경험,0.0015403165,0.0,0.0015403165016323328
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')]",답변 실패,-0.0031112295,0.0,0.0031112295109778643
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')]",딥러닝,-0.0023801734,0.0,0.002380173420533538
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')]",마지막 할 말,-0.005610981,0.0,0.005610981024801731
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')]",머신러닝,-0.0013891468,0.0,0.0013891467824578285
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')]",면접 시작 인사,-0.008632079,0.0,0.008632078766822815
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')]",상세 경험,0.0010464878,0.0,0.0010464878287166357
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')]",수식,0.9935066,1.0,0.00649338960647583
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')]",용어 질문,-0.0010500918,0.0,0.001050091814249754
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')]",인공지능,-0.0062399907,0.0,0.006239990703761578
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')]",잠시 휴식,-5.3804465e-06,0.0,5.3804465096618515e-06
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')]",좋아하는 아이돌,0.000719764,0.0,0.0007197639788500965
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')]",핵심 아이디어,0.0007060617,0.0,0.0007060617208480835
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')]",확률 예측에서 MSE Loss 미 사용 이유,-0.00065930065,0.0,0.0006593006546609104
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다",BCE 가 좋은 task,-0.0016416478,0.0,0.0016416477737948298
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다",BCE 가 좋은 이유,-0.001980645,0.0,0.0019806448835879564
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다",LLM Fine-Tuning 의 PEFT,-0.0006725615,0.0,0.0006725615239702165
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다",LoRA,0.00014239798,0.0,0.00014239798474591225
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다",LoRA 와 QLoRA 의 차이,0.00090956985,0.0,0.0009095698478631675
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다",Loss Function 예시,-0.00050367485,0.0,0.0005036748480051756
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다",Loss Function 정의,-0.00030229823,0.0,0.0003022982273250818
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다",MBTI,-0.0028883475,0.0,0.002888347487896681
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다",MSE Loss 설명,-0.0011711483,0.0,0.0011711482657119632
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다",MSE Loss 용도,0.00073544023,0.0,0.0007354402332566679
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다",Multi-Label 에서 CE + Softmax 적용 문제점,-0.0016024589,0.0,0.0016024588840082288
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다",PEFT 방법 5가지,-0.0026033951,0.0,0.0026033951435238123
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다",거대 언어 모델 정의,-0.002205993,0.0,0.0022059930488467216
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다",기본 경험,-0.0040883464,0.0,0.004088346380740404
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다",답변 실패,-0.0007716565,0.0,0.000771656516008079
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다",딥러닝,-0.0038609172,0.0,0.0038609171751886606
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다",마지막 할 말,-0.004055732,0.0,0.004055731929838657
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다",머신러닝,-0.0007778603,0.0,0.0007778602885082364
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다",면접 시작 인사,-0.0009251432,0.0,0.0009251431911252439
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다",상세 경험,-0.0023969607,0.0,0.0023969607427716255
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다",수식,-0.0019046667,0.0,0.001904666656628251
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다",용어 질문,-0.0027106043,0.0,0.002710604341700673
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다",인공지능,-0.0019465126,0.0,0.00194651260972023
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다",잠시 휴식,0.0005772065,0.0,0.0005772064905613661
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다",좋아하는 아이돌,-0.0031591968,0.0,0.003159196814522147
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다",핵심 아이디어,0.9974655,1.0,0.00253450870513916
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다",확률 예측에서 MSE Loss 미 사용 이유,-0.0027992018,0.0,0.0027992017567157745
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거,BCE 가 좋은 task,-0.003468394,0.0,0.003468394046649337
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거,BCE 가 좋은 이유,-0.004944413,0.0,0.004944412969052792
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거,LLM Fine-Tuning 의 PEFT,-0.002070853,0.0,0.0020708530209958553
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거,LoRA,-0.0024241938,0.0,0.0024241937790066004
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거,LoRA 와 QLoRA 의 차이,0.00014308484,0.0,0.00014308483514469117
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거,Loss Function 예시,-0.0027185096,0.0,0.002718509640544653
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거,Loss Function 정의,-0.00388341,0.0,0.0038834100123494864
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거,MBTI,-0.0024212615,0.0,0.0024212615098804235
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거,MSE Loss 설명,-0.001898044,0.0,0.0018980440218001604
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거,MSE Loss 용도,-0.0010127894,0.0,0.0010127894347533584
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거,Multi-Label 에서 CE + Softmax 적용 문제점,-0.0014633688,0.0,0.0014633687678724527
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거,PEFT 방법 5가지,-0.0018591302,0.0,0.0018591302214190364
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거,거대 언어 모델 정의,-0.0022213282,0.0,0.00222132820636034
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거,기본 경험,-0.0026405703,0.0,0.002640570281073451
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거,답변 실패,0.99942464,1.0,0.0005753636360168457
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거,딥러닝,-0.00074318756,0.0,0.0007431875565089285
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거,마지막 할 말,-0.0013465139,0.0,0.0013465138617902994
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거,머신러닝,-0.0022938817,0.0,0.0022938817273825407
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거,면접 시작 인사,-0.0028673685,0.0,0.0028673685155808926
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거,상세 경험,-0.0034534326,0.0,0.003453432582318783
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거,수식,-0.0012201186,0.0,0.001220118603669107
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거,용어 질문,-0.0019730534,0.0,0.001973053440451622
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거,인공지능,-0.0028851526,0.0,0.0028851525858044624
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거,잠시 휴식,-0.00089834176,0.0,0.0008983417646959424
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거,좋아하는 아이돌,-0.0021057515,0.0,0.0021057515405118465
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거,핵심 아이디어,-0.00063994294,0.0,0.0006399429403245449
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거,확률 예측에서 MSE Loss 미 사용 이유,-0.0015180253,0.0,0.0015180252958089113
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지",BCE 가 좋은 task,-8.820117e-05,0.0,8.8201166363433e-05
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지",BCE 가 좋은 이유,-0.00046582313,0.0,0.00046582313370890915
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지",LLM Fine-Tuning 의 PEFT,-0.0027073103,0.0,0.002707310253754258
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지",LoRA,-0.0008295521,0.0,0.0008295521256513894
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지",LoRA 와 QLoRA 의 차이,0.0041741994,0.0,0.004174199420958757
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지",Loss Function 예시,-0.0020934027,0.0,0.0020934026688337326
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지",Loss Function 정의,-0.0006402256,0.0,0.0006402255967259407
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지",MBTI,-0.0027652087,0.0,0.0027652087155729532
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지",MSE Loss 설명,-0.0020923584,0.0,0.002092358423396945
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지",MSE Loss 용도,0.0011612861,0.0,0.0011612861417233944
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지",Multi-Label 에서 CE + Softmax 적용 문제점,-0.00211596,0.0,0.0021159600000828505
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지",PEFT 방법 5가지,-0.00065270544,0.0,0.0006527054356411099
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지",거대 언어 모델 정의,-0.00060639984,0.0,0.0006063998444005847
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지",기본 경험,-0.0025603299,0.0,0.002560329856351018
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지",답변 실패,-0.00085060555,0.0,0.000850605545565486
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지",딥러닝,-0.001514732,0.0,0.0015147320227697492
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지",마지막 할 말,-0.002936259,0.0,0.002936258912086487
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지",머신러닝,-0.0032982943,0.0,0.0032982942648231983
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지",면접 시작 인사,-0.0014750748,0.0,0.0014750747941434383
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지",상세 경험,-0.0034874603,0.0,0.0034874603152275085
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지",수식,-0.0010230483,0.0,0.0010230483021587133
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지",용어 질문,-0.00083206646,0.0,0.0008320664637722075
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지",인공지능,-0.0025609247,0.0,0.0025609247386455536
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지",잠시 휴식,-0.0013194442,0.0,0.0013194441562518477
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지",좋아하는 아이돌,-0.0018815865,0.0,0.0018815865041688085
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지",핵심 아이디어,0.9974845,1.0,0.0025154948234558105
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지",확률 예측에서 MSE Loss 미 사용 이유,-0.007196831,0.0,0.007196831051260233
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야?",BCE 가 좋은 task,-0.0022150862,0.0,0.0022150862496346235
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야?",BCE 가 좋은 이유,-0.004018401,0.0,0.0040184007957577705
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야?",LLM Fine-Tuning 의 PEFT,-0.00014044651,0.0,0.00014044651470612735
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야?",LoRA,-0.0023472558,0.0,0.0023472558241337538
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야?",LoRA 와 QLoRA 의 차이,0.0009998363,0.0,0.0009998362511396408
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야?",Loss Function 예시,-0.0014481246,0.0,0.001448124647140503
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야?",Loss Function 정의,-8.7070905e-05,0.0,8.707090455573052e-05
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야?",MBTI,0.003384856,0.0,0.003384856041520834
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야?",MSE Loss 설명,-0.001196673,0.0,0.0011966730235144496
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야?",MSE Loss 용도,-0.0025873394,0.0,0.002587339375168085
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야?",Multi-Label 에서 CE + Softmax 적용 문제점,0.0015186371,0.0,0.0015186370583251119
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야?",PEFT 방법 5가지,-0.008291922,0.0,0.008291921578347683
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야?",거대 언어 모델 정의,-0.004594079,0.0,0.004594079218804836
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야?",기본 경험,-0.0027864594,0.0,0.0027864594012498856
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야?",답변 실패,-0.0011279893,0.0,0.0011279892642050982
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야?",딥러닝,-0.0025115283,0.0,0.0025115283206105232
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야?",마지막 할 말,-0.0016062345,0.0,0.00160623446572572
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야?",머신러닝,-0.0022103556,0.0,0.002210355596616864
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야?",면접 시작 인사,-0.0035647713,0.0,0.0035647712647914886
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야?",상세 경험,-0.0059130057,0.0,0.00591300567612052
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야?",수식,-0.003916869,0.0,0.003916868939995766
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야?",용어 질문,0.9967501,1.0,0.0032498836517333984
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야?",인공지능,-0.0018563177,0.0,0.0018563177436590195
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야?",잠시 휴식,-0.0009362838,0.0,0.0009362837881781161
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야?",좋아하는 아이돌,0.0013613461,0.0,0.0013613461051136255
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야?",핵심 아이디어,-0.0033590333,0.0,0.0033590332604944706
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야?",확률 예측에서 MSE Loss 미 사용 이유,0.0006454743,0.0,0.0006454742979258299
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",BCE 가 좋은 task,0.06781898,0.0,0.06781897693872452
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",BCE 가 좋은 이유,-0.0077691446,0.0,0.007769144605845213
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",LLM Fine-Tuning 의 PEFT,0.0015858407,0.0,0.0015858407132327557
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",LoRA,-0.008307766,0.0,0.008307766169309616
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",LoRA 와 QLoRA 의 차이,-0.005288456,0.0,0.005288456100970507
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",Loss Function 예시,0.0006595603,0.0,0.0006595603190362453
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",Loss Function 정의,0.00045172716,0.0,0.0004517271590884775
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",MBTI,-0.0019025045,0.0,0.0019025044748559594
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",MSE Loss 설명,-0.003400251,0.0,0.0034002510365098715
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",MSE Loss 용도,-0.0049755904,0.0,0.004975590389221907
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",Multi-Label 에서 CE + Softmax 적용 문제점,-0.0015055013,0.0,0.0015055013354867697
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",PEFT 방법 5가지,-0.012251652,0.0,0.012251651845872402
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",거대 언어 모델 정의,9.4711686e-05,0.0,9.471168596064672e-05
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",기본 경험,0.002911507,0.0,0.0029115069191902876
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",답변 실패,0.9827647,1.0,0.017235279083251953
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",딥러닝,-0.0025371502,0.0,0.0025371501687914133
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",마지막 할 말,-0.0033261783,0.0,0.0033261782955378294
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",머신러닝,-0.0031133501,0.0,0.0031133501324802637
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",면접 시작 인사,-0.00027355435,0.0,0.000273554353043437
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",상세 경험,0.0016959419,0.0,0.0016959419008344412
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",수식,-0.013558063,0.0,0.013558062724769115
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",용어 질문,-0.0020946013,0.0,0.0020946012809872627
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",인공지능,-0.009852412,0.0,0.009852412156760693
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",잠시 휴식,-0.0062127747,0.0,0.0062127746641635895
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",좋아하는 아이돌,-0.010023652,0.0,0.010023651644587517
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",핵심 아이디어,0.0045695673,0.0,0.004569567274302244
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",확률 예측에서 MSE Loss 미 사용 이유,-0.001721424,0.0,0.001721424050629139
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",BCE 가 좋은 task,-0.0013640799,1.0,1.001364079886116
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",BCE 가 좋은 이유,-0.0021785023,0.0,0.002178502269089222
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",LLM Fine-Tuning 의 PEFT,-0.0027216477,0.0,0.00272164773195982
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",LoRA,-0.002948178,0.0,0.0029481779783964157
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",LoRA 와 QLoRA 의 차이,-0.0010738865,0.0,0.0010738865239545703
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",Loss Function 예시,-0.0036100096,0.0,0.0036100095603615046
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",Loss Function 정의,-0.0021386596,0.0,0.002138659590855241
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",MBTI,-0.0027835052,0.0,0.0027835052460432053
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",MSE Loss 설명,-0.003535959,0.0,0.0035359589383006096
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",MSE Loss 용도,-0.0038131636,0.0,0.003813163610175252
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",Multi-Label 에서 CE + Softmax 적용 문제점,-0.001821775,0.0,0.0018217749893665314
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",PEFT 방법 5가지,-0.0021042349,0.0,0.0021042348816990852
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",거대 언어 모델 정의,-0.0006805784,0.0,0.0006805784069001675
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",기본 경험,-0.0034365752,0.0,0.0034365751780569553
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",답변 실패,0.99939156,0.0,0.9993915557861328
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",딥러닝,-0.00027827657,0.0,0.0002782765659503639
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",마지막 할 말,-0.0026515783,0.0,0.0026515782810747623
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",머신러닝,-0.0015542025,0.0,0.0015542025212198496
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",면접 시작 인사,-0.0036575962,0.0,0.0036575961858034134
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",상세 경험,-0.0026982015,0.0,0.0026982014533132315
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",수식,-0.0024352758,0.0,0.0024352758191525936
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",용어 질문,-0.00017465174,0.0,0.00017465173732489347
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",인공지능,-0.0020712838,0.0,0.002071283757686615
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",잠시 휴식,0.0013116112,0.0,0.0013116111513227224
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",좋아하는 아이돌,-0.004450015,0.0,0.004450014792382717
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",핵심 아이디어,-0.00183293,0.0,0.00183293002191931
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",확률 예측에서 MSE Loss 미 사용 이유,-0.0020751036,0.0,0.0020751035772264004
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",BCE 가 좋은 task,0.0020231078,0.0,0.002023107837885618
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",BCE 가 좋은 이유,0.9951755,1.0,0.004824519157409668
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",LLM Fine-Tuning 의 PEFT,-0.0019023503,0.0,0.0019023503409698606
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",LoRA,-0.0035130472,0.0,0.0035130472388118505
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",LoRA 와 QLoRA 의 차이,-0.004527421,0.0,0.004527421202510595
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",Loss Function 예시,-0.0054736873,0.0,0.0054736873134970665
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",Loss Function 정의,-0.007475476,0.0,0.0074754757806658745
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",MBTI,-0.0043811924,0.0,0.00438119238242507
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",MSE Loss 설명,-0.0018856493,0.0,0.0018856492824852467
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",MSE Loss 용도,-0.0059975674,0.0,0.0059975674375891685
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",Multi-Label 에서 CE + Softmax 적용 문제점,0.0010260014,0.0,0.0010260014096274972
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",PEFT 방법 5가지,-0.004063813,0.0,0.004063813015818596
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",거대 언어 모델 정의,-0.006482011,0.0,0.0064820111729204655
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",기본 경험,-0.0042111315,0.0,0.0042111314833164215
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",답변 실패,-0.004012111,0.0,0.004012111108750105
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",딥러닝,-0.005856325,0.0,0.005856324918568134
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",마지막 할 말,-0.0059504127,0.0,0.005950412712991238
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",머신러닝,-0.0046603954,0.0,0.004660395439714193
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",면접 시작 인사,-0.0013218984,0.0,0.0013218984240666032
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",상세 경험,-0.0058426065,0.0,0.005842606537044048
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",수식,-0.005493395,0.0,0.005493395030498505
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",용어 질문,-0.0031187513,0.0,0.003118751337751746
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",인공지능,-0.002524088,0.0,0.002524087904021144
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",잠시 휴식,-0.0018220396,0.0,0.001822039601393044
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",좋아하는 아이돌,-0.004791392,0.0,0.004791392013430595
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",핵심 아이디어,-0.00014793611,0.0,0.00014793610898777843
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",확률 예측에서 MSE Loss 미 사용 이유,-0.004168506,0.0,0.004168505780398846
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,BCE 가 좋은 task,-0.0012910564,0.0,0.0012910563964396715
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,BCE 가 좋은 이유,-0.0023208705,0.0,0.0023208705242723227
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,LLM Fine-Tuning 의 PEFT,-0.00035040898,0.0,0.000350408983649686
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,LoRA,0.0007780721,0.0,0.0007780721061863005
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,LoRA 와 QLoRA 의 차이,-0.002326747,0.0,0.0023267469368875027
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,Loss Function 예시,-0.0029834681,0.0,0.0029834681190550327
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,Loss Function 정의,-0.0038975491,0.0,0.003897549118846655
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,MBTI,0.0011207332,0.0,0.0011207332136109471
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,MSE Loss 설명,-0.0005405124,0.0,0.0005405123811215162
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,MSE Loss 용도,-0.003896592,0.0,0.003896591952070594
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,Multi-Label 에서 CE + Softmax 적용 문제점,0.99866736,1.0,0.0013326406478881836
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,PEFT 방법 5가지,-0.0001421022,0.0,0.00014210220251698047
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,거대 언어 모델 정의,-0.003742751,0.0,0.0037427509669214487
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,기본 경험,-0.0035199393,0.0,0.0035199392586946487
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,답변 실패,-0.001540715,0.0,0.0015407149912789464
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,딥러닝,-0.00010149954,0.0,0.00010149954323424026
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,마지막 할 말,-0.0011527447,0.0,0.0011527447495609522
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,머신러닝,-0.0006777269,0.0,0.0006777268717996776
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,면접 시작 인사,-0.00072251674,0.0,0.0007225167355500162
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,상세 경험,-0.0009319514,0.0,0.0009319513919763267
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,수식,4.983835e-05,0.0,4.9838348786579445e-05
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,용어 질문,0.0017939729,0.0,0.0017939729150384665
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,인공지능,-0.0002746167,0.0,0.0002746167010627687
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,잠시 휴식,-0.0029538046,0.0,0.002953804563730955
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,좋아하는 아이돌,-0.0047024447,0.0,0.004702444653958082
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,핵심 아이디어,-0.0015647467,0.0,0.0015647467225790024
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,확률 예측에서 MSE Loss 미 사용 이유,-0.0069541405,0.0,0.006954140495508909
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,BCE 가 좋은 task,-0.0014788319,0.0,0.001478831865824759
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,BCE 가 좋은 이유,-0.004326263,0.0,0.004326262976974249
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,LLM Fine-Tuning 의 PEFT,-0.0024234043,0.0,0.00242340425029397
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,LoRA,-0.0033847648,0.0,0.0033847647719085217
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,LoRA 와 QLoRA 의 차이,-0.0018322094,0.0,0.0018322094110772014
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,Loss Function 예시,-0.003049345,0.0,0.003049344988539815
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,Loss Function 정의,-0.0029258712,0.0,0.0029258711729198694
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,MBTI,-0.0033034987,0.0,0.0033034987282007933
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,MSE Loss 설명,-0.0031820426,0.0,0.003182042622938752
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,MSE Loss 용도,-0.001776829,0.0,0.001776829012669623
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,Multi-Label 에서 CE + Softmax 적용 문제점,0.0003864234,0.0,0.0003864234022330493
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,PEFT 방법 5가지,-0.0022086361,0.0,0.0022086361423134804
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,거대 언어 모델 정의,-0.0011358681,0.0,0.0011358681367710233
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,기본 경험,-0.0026994813,0.0,0.002699481323361397
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,답변 실패,0.99967855,1.0,0.00032144784927368164
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,딥러닝,-0.00015356603,0.0,0.00015356602671090513
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,마지막 할 말,-0.0027973389,0.0,0.0027973388787359
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,머신러닝,-0.0013004069,0.0,0.001300406875088811
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,면접 시작 인사,-0.0030493094,0.0,0.003049309365451336
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,상세 경험,-0.00308067,0.0,0.0030806700233370066
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,수식,-0.0037202507,0.0,0.003720250679180026
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,용어 질문,-0.0015804628,0.0,0.0015804627910256386
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,인공지능,-0.002776071,0.0,0.002776070963591337
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,잠시 휴식,-0.0010464034,0.0,0.0010464034276083112
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,좋아하는 아이돌,-0.0030172034,0.0,0.003017203416675329
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,핵심 아이디어,-0.0014769684,0.0,0.0014769684057682753
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,확률 예측에서 MSE Loss 미 사용 이유,-0.0012012528,0.0,0.0012012528022751212
Loss Function 관련 실무 경험 -> [기본 경험] 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,BCE 가 좋은 task,-0.0026296661,0.0,0.0026296661235392094
Loss Function 관련 실무 경험 -> [기본 경험] 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,BCE 가 좋은 이유,-0.0023864042,0.0,0.002386404201388359
Loss Function 관련 실무 경험 -> [기본 경험] 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,LLM Fine-Tuning 의 PEFT,-0.0008689753,0.0,0.0008689753012731671
Loss Function 관련 실무 경험 -> [기본 경험] 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,LoRA,0.00082278147,0.0,0.0008227814687415957
Loss Function 관련 실무 경험 -> [기본 경험] 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,LoRA 와 QLoRA 의 차이,-0.0018683986,0.0,0.0018683986272662878
Loss Function 관련 실무 경험 -> [기본 경험] 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,Loss Function 예시,-0.00089821423,0.0,0.000898214231710881
Loss Function 관련 실무 경험 -> [기본 경험] 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,Loss Function 정의,-0.00081532385,0.0,0.0008153238450177014
Loss Function 관련 실무 경험 -> [기본 경험] 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,MBTI,-0.0042881723,0.0,0.004288172349333763
Loss Function 관련 실무 경험 -> [기본 경험] 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,MSE Loss 설명,-0.0018930463,0.0,0.00189304631203413
Loss Function 관련 실무 경험 -> [기본 경험] 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,MSE Loss 용도,-0.00096743566,0.0,0.0009674356551840901
Loss Function 관련 실무 경험 -> [기본 경험] 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,Multi-Label 에서 CE + Softmax 적용 문제점,-0.0040419465,0.0,0.004041946493089199
Loss Function 관련 실무 경험 -> [기본 경험] 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,PEFT 방법 5가지,-0.002332539,0.0,0.00233253906480968
Loss Function 관련 실무 경험 -> [기본 경험] 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,거대 언어 모델 정의,-0.0025946926,0.0,0.0025946926325559616
Loss Function 관련 실무 경험 -> [기본 경험] 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,기본 경험,0.99865466,1.0,0.0013453364372253418
Loss Function 관련 실무 경험 -> [기본 경험] 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,답변 실패,-0.0031716896,0.0,0.003171689575538039
Loss Function 관련 실무 경험 -> [기본 경험] 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,딥러닝,-0.0010224578,0.0,0.0010224578436464071
Loss Function 관련 실무 경험 -> [기본 경험] 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,마지막 할 말,-0.0025782213,0.0,0.0025782212615013123
Loss Function 관련 실무 경험 -> [기본 경험] 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,머신러닝,-0.0042665736,0.0,0.004266573581844568
Loss Function 관련 실무 경험 -> [기본 경험] 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,면접 시작 인사,-0.0048661,0.0,0.004866099916398525
Loss Function 관련 실무 경험 -> [기본 경험] 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,상세 경험,-0.0016310195,0.0,0.001631019520573318
Loss Function 관련 실무 경험 -> [기본 경험] 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,수식,-0.001331293,0.0,0.001331293024122715
Loss Function 관련 실무 경험 -> [기본 경험] 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,용어 질문,-4.937223e-05,0.0,4.937222911394201e-05
Loss Function 관련 실무 경험 -> [기본 경험] 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,인공지능,-0.0003329688,0.0,0.0003329688042867929
Loss Function 관련 실무 경험 -> [기본 경험] 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,잠시 휴식,-0.0028488652,0.0,0.002848865231499076
Loss Function 관련 실무 경험 -> [기본 경험] 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,좋아하는 아이돌,-0.0005916629,0.0,0.0005916628870181739
Loss Function 관련 실무 경험 -> [기본 경험] 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,핵심 아이디어,-0.0028406905,0.0,0.0028406905476003885
Loss Function 관련 실무 경험 -> [기본 경험] 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,확률 예측에서 MSE Loss 미 사용 이유,-0.0023247348,0.0,0.002324734814465046
Loss Function 관련 실무 경험 -> [상세 경험] 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,BCE 가 좋은 task,-0.0017373017,0.0,0.001737301703542471
Loss Function 관련 실무 경험 -> [상세 경험] 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,BCE 가 좋은 이유,-0.007103984,0.0,0.007103983778506517
Loss Function 관련 실무 경험 -> [상세 경험] 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,LLM Fine-Tuning 의 PEFT,-0.00035464353,0.0,0.0003546435327734798
Loss Function 관련 실무 경험 -> [상세 경험] 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,LoRA,-0.00040704387,0.0,0.0004070438735652715
Loss Function 관련 실무 경험 -> [상세 경험] 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,LoRA 와 QLoRA 의 차이,-0.0004602327,0.0,0.000460232695331797
Loss Function 관련 실무 경험 -> [상세 경험] 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,Loss Function 예시,-0.0014226668,0.0,0.001422666828148067
Loss Function 관련 실무 경험 -> [상세 경험] 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,Loss Function 정의,-0.0062905736,0.0,0.006290573626756668
Loss Function 관련 실무 경험 -> [상세 경험] 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,MBTI,0.0005384576,0.0,0.0005384575924836099
Loss Function 관련 실무 경험 -> [상세 경험] 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,MSE Loss 설명,-0.002541623,0.0,0.002541623078286648
Loss Function 관련 실무 경험 -> [상세 경험] 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,MSE Loss 용도,-0.0032839088,0.0,0.003283908823505044
Loss Function 관련 실무 경험 -> [상세 경험] 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,Multi-Label 에서 CE + Softmax 적용 문제점,-0.00043281692,0.0,0.0004328169161453843
Loss Function 관련 실무 경험 -> [상세 경험] 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,PEFT 방법 5가지,-0.00080087094,0.0,0.0008008709410205483
Loss Function 관련 실무 경험 -> [상세 경험] 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,거대 언어 모델 정의,-0.003543523,0.0,0.003543522907420993
Loss Function 관련 실무 경험 -> [상세 경험] 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,기본 경험,-0.006296131,0.0,0.006296130828559399
Loss Function 관련 실무 경험 -> [상세 경험] 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,답변 실패,2.8849892e-05,0.0,2.8849892260041088e-05
Loss Function 관련 실무 경험 -> [상세 경험] 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,딥러닝,-0.00079455506,0.0,0.0007945550605654716
Loss Function 관련 실무 경험 -> [상세 경험] 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,마지막 할 말,-0.0007893024,0.0,0.0007893024012446404
Loss Function 관련 실무 경험 -> [상세 경험] 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,머신러닝,-0.0009956498,0.0,0.0009956498397514224
Loss Function 관련 실무 경험 -> [상세 경험] 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,면접 시작 인사,-0.0021614332,0.0,0.002161433221772313
Loss Function 관련 실무 경험 -> [상세 경험] 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,상세 경험,0.9977165,1.0,0.0022835135459899902
Loss Function 관련 실무 경험 -> [상세 경험] 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,수식,-0.0036131279,0.0,0.003613127861171961
Loss Function 관련 실무 경험 -> [상세 경험] 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,용어 질문,-0.007265854,0.0,0.00726585416123271
Loss Function 관련 실무 경험 -> [상세 경험] 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,인공지능,-0.00485155,0.0,0.004851549863815308
Loss Function 관련 실무 경험 -> [상세 경험] 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,잠시 휴식,-0.005134633,0.0,0.005134632810950279
Loss Function 관련 실무 경험 -> [상세 경험] 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,좋아하는 아이돌,0.001167102,0.0,0.0011671020183712244
Loss Function 관련 실무 경험 -> [상세 경험] 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,핵심 아이디어,-0.002321636,0.0,0.0023216360714286566
Loss Function 관련 실무 경험 -> [상세 경험] 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,확률 예측에서 MSE Loss 미 사용 이유,-0.0026102653,0.0,0.002610265277326107
Loss Function 관련 실무 경험 -> [상세 경험] 근데 기억 진짜 안나는데 어떡하지?,BCE 가 좋은 task,-0.0018432204,0.0,0.0018432204378768802
Loss Function 관련 실무 경험 -> [상세 경험] 근데 기억 진짜 안나는데 어떡하지?,BCE 가 좋은 이유,-0.0036679863,0.0,0.0036679862532764673
Loss Function 관련 실무 경험 -> [상세 경험] 근데 기억 진짜 안나는데 어떡하지?,LLM Fine-Tuning 의 PEFT,-0.0023091438,0.0,0.002309143776074052
Loss Function 관련 실무 경험 -> [상세 경험] 근데 기억 진짜 안나는데 어떡하지?,LoRA,-0.0027409443,0.0,0.002740944270044565
Loss Function 관련 실무 경험 -> [상세 경험] 근데 기억 진짜 안나는데 어떡하지?,LoRA 와 QLoRA 의 차이,-0.0018201808,0.0,0.0018201807979494333
Loss Function 관련 실무 경험 -> [상세 경험] 근데 기억 진짜 안나는데 어떡하지?,Loss Function 예시,-0.0026324457,0.0,0.0026324456557631493
Loss Function 관련 실무 경험 -> [상세 경험] 근데 기억 진짜 안나는데 어떡하지?,Loss Function 정의,-0.0034077077,0.0,0.0034077076707035303
Loss Function 관련 실무 경험 -> [상세 경험] 근데 기억 진짜 안나는데 어떡하지?,MBTI,-0.0024323666,0.0,0.0024323666002601385
Loss Function 관련 실무 경험 -> [상세 경험] 근데 기억 진짜 안나는데 어떡하지?,MSE Loss 설명,-0.002358559,0.0,0.002358559053391218
Loss Function 관련 실무 경험 -> [상세 경험] 근데 기억 진짜 안나는데 어떡하지?,MSE Loss 용도,-0.0011191877,0.0,0.0011191876837983727
Loss Function 관련 실무 경험 -> [상세 경험] 근데 기억 진짜 안나는데 어떡하지?,Multi-Label 에서 CE + Softmax 적용 문제점,-0.0009971571,0.0,0.0009971570689231157
Loss Function 관련 실무 경험 -> [상세 경험] 근데 기억 진짜 안나는데 어떡하지?,PEFT 방법 5가지,-0.0017722814,0.0,0.0017722813645377755
Loss Function 관련 실무 경험 -> [상세 경험] 근데 기억 진짜 안나는데 어떡하지?,거대 언어 모델 정의,-0.0008568005,0.0,0.0008568005287088454
Loss Function 관련 실무 경험 -> [상세 경험] 근데 기억 진짜 안나는데 어떡하지?,기본 경험,-0.0021820925,0.0,0.0021820925176143646
Loss Function 관련 실무 경험 -> [상세 경험] 근데 기억 진짜 안나는데 어떡하지?,답변 실패,0.999771,1.0,0.00022900104522705078
Loss Function 관련 실무 경험 -> [상세 경험] 근데 기억 진짜 안나는데 어떡하지?,딥러닝,0.00036569877,0.0,0.0003656987682916224
Loss Function 관련 실무 경험 -> [상세 경험] 근데 기억 진짜 안나는데 어떡하지?,마지막 할 말,-0.002406948,0.0,0.0024069480132311583
Loss Function 관련 실무 경험 -> [상세 경험] 근데 기억 진짜 안나는데 어떡하지?,머신러닝,-0.0022794188,0.0,0.0022794187534600496
Loss Function 관련 실무 경험 -> [상세 경험] 근데 기억 진짜 안나는데 어떡하지?,면접 시작 인사,-0.0032239866,0.0,0.0032239865977317095
Loss Function 관련 실무 경험 -> [상세 경험] 근데 기억 진짜 안나는데 어떡하지?,상세 경험,-0.0008180856,0.0,0.0008180856239050627
Loss Function 관련 실무 경험 -> [상세 경험] 근데 기억 진짜 안나는데 어떡하지?,수식,-0.0023425324,0.0,0.0023425323888659477
Loss Function 관련 실무 경험 -> [상세 경험] 근데 기억 진짜 안나는데 어떡하지?,용어 질문,-0.0015414706,0.0,0.001541470643132925
Loss Function 관련 실무 경험 -> [상세 경험] 근데 기억 진짜 안나는데 어떡하지?,인공지능,-0.0028439008,0.0,0.002843900816515088
Loss Function 관련 실무 경험 -> [상세 경험] 근데 기억 진짜 안나는데 어떡하지?,잠시 휴식,-0.00038191437,0.0,0.000381914374884218
Loss Function 관련 실무 경험 -> [상세 경험] 근데 기억 진짜 안나는데 어떡하지?,좋아하는 아이돌,-0.003115104,0.0,0.0031151040457189083
Loss Function 관련 실무 경험 -> [상세 경험] 근데 기억 진짜 안나는데 어떡하지?,핵심 아이디어,-0.0016852638,0.0,0.0016852638218551874
Loss Function 관련 실무 경험 -> [상세 경험] 근데 기억 진짜 안나는데 어떡하지?,확률 예측에서 MSE Loss 미 사용 이유,-0.0011830162,0.0,0.00118301622569561
Loss Function 관련 실무 경험 -> [기본 경험] 나 아직 실무 경험 없는데 ㅠㅠ,BCE 가 좋은 task,-0.0022822237,0.0,0.002282223664224148
Loss Function 관련 실무 경험 -> [기본 경험] 나 아직 실무 경험 없는데 ㅠㅠ,BCE 가 좋은 이유,-0.004861512,0.0,0.004861512221395969
Loss Function 관련 실무 경험 -> [기본 경험] 나 아직 실무 경험 없는데 ㅠㅠ,LLM Fine-Tuning 의 PEFT,-0.0024612169,0.0,0.002461216878145933
Loss Function 관련 실무 경험 -> [기본 경험] 나 아직 실무 경험 없는데 ㅠㅠ,LoRA,0.00015062236,0.0,0.0001506223634351045
Loss Function 관련 실무 경험 -> [기본 경험] 나 아직 실무 경험 없는데 ㅠㅠ,LoRA 와 QLoRA 의 차이,6.0216356e-05,0.0,6.021635635988787e-05
Loss Function 관련 실무 경험 -> [기본 경험] 나 아직 실무 경험 없는데 ㅠㅠ,Loss Function 예시,-0.0021315636,0.0,0.002131563611328602
Loss Function 관련 실무 경험 -> [기본 경험] 나 아직 실무 경험 없는데 ㅠㅠ,Loss Function 정의,-0.00094626565,0.0,0.0009462656453251839
Loss Function 관련 실무 경험 -> [기본 경험] 나 아직 실무 경험 없는데 ㅠㅠ,MBTI,-0.0035672965,0.0,0.0035672965459525585
Loss Function 관련 실무 경험 -> [기본 경험] 나 아직 실무 경험 없는데 ㅠㅠ,MSE Loss 설명,-0.002653701,0.0,0.0026537009980529547
Loss Function 관련 실무 경험 -> [기본 경험] 나 아직 실무 경험 없는데 ㅠㅠ,MSE Loss 용도,-0.0016777256,0.0,0.001677725580520928
Loss Function 관련 실무 경험 -> [기본 경험] 나 아직 실무 경험 없는데 ㅠㅠ,Multi-Label 에서 CE + Softmax 적용 문제점,-0.0030438183,0.0,0.003043818287551403
Loss Function 관련 실무 경험 -> [기본 경험] 나 아직 실무 경험 없는데 ㅠㅠ,PEFT 방법 5가지,-0.0019528541,0.0,0.0019528541015461087
Loss Function 관련 실무 경험 -> [기본 경험] 나 아직 실무 경험 없는데 ㅠㅠ,거대 언어 모델 정의,-0.0034150179,0.0,0.003415017854422331
Loss Function 관련 실무 경험 -> [기본 경험] 나 아직 실무 경험 없는데 ㅠㅠ,기본 경험,0.99899876,1.0,0.0010012388229370117
Loss Function 관련 실무 경험 -> [기본 경험] 나 아직 실무 경험 없는데 ㅠㅠ,답변 실패,-0.0021941913,0.0,0.0021941913291811943
Loss Function 관련 실무 경험 -> [기본 경험] 나 아직 실무 경험 없는데 ㅠㅠ,딥러닝,-0.0030069118,0.0,0.003006911836564541
Loss Function 관련 실무 경험 -> [기본 경험] 나 아직 실무 경험 없는데 ㅠㅠ,마지막 할 말,-0.002233059,0.0,0.002233058912679553
Loss Function 관련 실무 경험 -> [기본 경험] 나 아직 실무 경험 없는데 ㅠㅠ,머신러닝,-0.0044253594,0.0,0.004425359424203634
Loss Function 관련 실무 경험 -> [기본 경험] 나 아직 실무 경험 없는데 ㅠㅠ,면접 시작 인사,-0.006061481,0.0,0.006061480846256018
Loss Function 관련 실무 경험 -> [기본 경험] 나 아직 실무 경험 없는데 ㅠㅠ,상세 경험,-0.0033133347,0.0,0.003313334658741951
Loss Function 관련 실무 경험 -> [기본 경험] 나 아직 실무 경험 없는데 ㅠㅠ,수식,-0.002908653,0.0,0.002908653113991022
Loss Function 관련 실무 경험 -> [기본 경험] 나 아직 실무 경험 없는데 ㅠㅠ,용어 질문,-0.0007313278,0.0,0.0007313278038054705
Loss Function 관련 실무 경험 -> [기본 경험] 나 아직 실무 경험 없는데 ㅠㅠ,인공지능,-0.0005963638,0.0,0.0005963637959212065
Loss Function 관련 실무 경험 -> [기본 경험] 나 아직 실무 경험 없는데 ㅠㅠ,잠시 휴식,-0.003303379,0.0,0.003303379053249955
Loss Function 관련 실무 경험 -> [기본 경험] 나 아직 실무 경험 없는데 ㅠㅠ,좋아하는 아이돌,0.0008891609,0.0,0.0008891609031707048
Loss Function 관련 실무 경험 -> [기본 경험] 나 아직 실무 경험 없는데 ㅠㅠ,핵심 아이디어,-0.0035258802,0.0,0.003525880165398121
Loss Function 관련 실무 경험 -> [기본 경험] 나 아직 실무 경험 없는데 ㅠㅠ,확률 예측에서 MSE Loss 미 사용 이유,-0.0026233296,0.0,0.0026233296375721693
Loss Function 관련 실무 경험 -> [기본 경험] 대답하기 싫은데 힝,BCE 가 좋은 task,-0.0021416433,0.0,0.002141643315553665
Loss Function 관련 실무 경험 -> [기본 경험] 대답하기 싫은데 힝,BCE 가 좋은 이유,-0.0037134592,0.0,0.0037134592421352863
Loss Function 관련 실무 경험 -> [기본 경험] 대답하기 싫은데 힝,LLM Fine-Tuning 의 PEFT,-0.0021093832,0.0,0.0021093832328915596
Loss Function 관련 실무 경험 -> [기본 경험] 대답하기 싫은데 힝,LoRA,-0.0028030754,0.0,0.002803075360134244
Loss Function 관련 실무 경험 -> [기본 경험] 대답하기 싫은데 힝,LoRA 와 QLoRA 의 차이,-0.0017071796,0.0,0.001707179588265717
Loss Function 관련 실무 경험 -> [기본 경험] 대답하기 싫은데 힝,Loss Function 예시,-0.0029141188,0.0,0.0029141188133507967
Loss Function 관련 실무 경험 -> [기본 경험] 대답하기 싫은데 힝,Loss Function 정의,-0.0036848069,0.0,0.0036848068702965975
Loss Function 관련 실무 경험 -> [기본 경험] 대답하기 싫은데 힝,MBTI,-0.0022391235,0.0,0.0022391234524548054
Loss Function 관련 실무 경험 -> [기본 경험] 대답하기 싫은데 힝,MSE Loss 설명,-0.0025476583,0.0,0.0025476582814007998
Loss Function 관련 실무 경험 -> [기본 경험] 대답하기 싫은데 힝,MSE Loss 용도,-0.00042880097,0.0,0.000428800965892151
Loss Function 관련 실무 경험 -> [기본 경험] 대답하기 싫은데 힝,Multi-Label 에서 CE + Softmax 적용 문제점,-0.00096098194,0.0,0.0009609819389879704
Loss Function 관련 실무 경험 -> [기본 경험] 대답하기 싫은데 힝,PEFT 방법 5가지,-0.002651567,0.0,0.002651567105203867
Loss Function 관련 실무 경험 -> [기본 경험] 대답하기 싫은데 힝,거대 언어 모델 정의,-0.000557763,0.0,0.0005577629781328142
Loss Function 관련 실무 경험 -> [기본 경험] 대답하기 싫은데 힝,기본 경험,0.001081174,0.0,0.0010811740066856146
Loss Function 관련 실무 경험 -> [기본 경험] 대답하기 싫은데 힝,답변 실패,0.99970603,1.0,0.00029397010803222656
Loss Function 관련 실무 경험 -> [기본 경험] 대답하기 싫은데 힝,딥러닝,0.00013675966,0.0,0.00013675965601578355
Loss Function 관련 실무 경험 -> [기본 경험] 대답하기 싫은데 힝,마지막 할 말,-0.0032357634,0.0,0.003235763404518366
Loss Function 관련 실무 경험 -> [기본 경험] 대답하기 싫은데 힝,머신러닝,-0.0024041971,0.0,0.002404197119176388
Loss Function 관련 실무 경험 -> [기본 경험] 대답하기 싫은데 힝,면접 시작 인사,-0.0026884652,0.0,0.0026884651742875576
Loss Function 관련 실무 경험 -> [기본 경험] 대답하기 싫은데 힝,상세 경험,-0.0026242458,0.0,0.0026242458261549473
Loss Function 관련 실무 경험 -> [기본 경험] 대답하기 싫은데 힝,수식,-0.0015932588,0.0,0.0015932588139548898
Loss Function 관련 실무 경험 -> [기본 경험] 대답하기 싫은데 힝,용어 질문,-0.0022364825,0.0,0.0022364824544638395
Loss Function 관련 실무 경험 -> [기본 경험] 대답하기 싫은데 힝,인공지능,-0.0035554047,0.0,0.003555404720827937
Loss Function 관련 실무 경험 -> [기본 경험] 대답하기 싫은데 힝,잠시 휴식,-0.00046197075,0.0,0.0004619707469828427
Loss Function 관련 실무 경험 -> [기본 경험] 대답하기 싫은데 힝,좋아하는 아이돌,-0.0035738214,0.0,0.0035738213919103146
Loss Function 관련 실무 경험 -> [기본 경험] 대답하기 싫은데 힝,핵심 아이디어,-0.0008601988,0.0,0.0008601988083682954
Loss Function 관련 실무 경험 -> [기본 경험] 대답하기 싫은데 힝,확률 예측에서 MSE Loss 미 사용 이유,-0.0015338795,0.0,0.001533879549242556
Loss Function 관련 실무 경험 -> [기본 경험] 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ,BCE 가 좋은 task,-0.0026233045,0.0,0.0026233044918626547
Loss Function 관련 실무 경험 -> [기본 경험] 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ,BCE 가 좋은 이유,-0.0049966797,0.0,0.004996679723262787
Loss Function 관련 실무 경험 -> [기본 경험] 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ,LLM Fine-Tuning 의 PEFT,-0.001956751,0.0,0.0019567511044442654
Loss Function 관련 실무 경험 -> [기본 경험] 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ,LoRA,0.00060395396,0.0,0.0006039539584890008
Loss Function 관련 실무 경험 -> [기본 경험] 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ,LoRA 와 QLoRA 의 차이,-0.00097145425,0.0,0.000971454253885895
Loss Function 관련 실무 경험 -> [기본 경험] 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ,Loss Function 예시,-0.0006442793,0.0,0.0006442792946472764
Loss Function 관련 실무 경험 -> [기본 경험] 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ,Loss Function 정의,-0.0015250916,0.0,0.0015250915894284844
Loss Function 관련 실무 경험 -> [기본 경험] 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ,MBTI,-0.003955112,0.0,0.003955111838877201
Loss Function 관련 실무 경험 -> [기본 경험] 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ,MSE Loss 설명,-0.0035831565,0.0,0.003583156503736973
Loss Function 관련 실무 경험 -> [기본 경험] 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ,MSE Loss 용도,-0.0022033202,0.0,0.0022033201530575752
Loss Function 관련 실무 경험 -> [기본 경험] 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ,Multi-Label 에서 CE + Softmax 적용 문제점,-0.0020710493,0.0,0.0020710492972284555
Loss Function 관련 실무 경험 -> [기본 경험] 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ,PEFT 방법 5가지,-0.0010330628,0.0,0.0010330628138035536
Loss Function 관련 실무 경험 -> [기본 경험] 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ,거대 언어 모델 정의,-0.0030727652,0.0,0.003072765190154314
Loss Function 관련 실무 경험 -> [기본 경험] 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ,기본 경험,0.9987099,1.0,0.0012900829315185547
Loss Function 관련 실무 경험 -> [기본 경험] 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ,답변 실패,-0.0020657927,0.0,0.002065792679786682
Loss Function 관련 실무 경험 -> [기본 경험] 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ,딥러닝,-0.001867024,0.0,0.0018670239951461554
Loss Function 관련 실무 경험 -> [기본 경험] 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ,마지막 할 말,-0.002968987,0.0,0.0029689869843423367
Loss Function 관련 실무 경험 -> [기본 경험] 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ,머신러닝,-0.005389637,0.0,0.0053896368481218815
Loss Function 관련 실무 경험 -> [기본 경험] 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ,면접 시작 인사,-0.005560741,0.0,0.005560740828514099
Loss Function 관련 실무 경험 -> [기본 경험] 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ,상세 경험,-0.0034733482,0.0,0.003473348217085004
Loss Function 관련 실무 경험 -> [기본 경험] 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ,수식,-0.0020131816,0.0,0.002013181569054723
Loss Function 관련 실무 경험 -> [기본 경험] 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ,용어 질문,-0.0015983097,0.0,0.001598309725522995
Loss Function 관련 실무 경험 -> [기본 경험] 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ,인공지능,-0.0011892035,0.0,0.001189203467220068
Loss Function 관련 실무 경험 -> [기본 경험] 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ,잠시 휴식,-0.0034769294,0.0,0.003476929385215044
Loss Function 관련 실무 경험 -> [기본 경험] 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ,좋아하는 아이돌,0.00044671376,0.0,0.000446713762357831
Loss Function 관련 실무 경험 -> [기본 경험] 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ,핵심 아이디어,-0.0020580075,0.0,0.0020580075215548277
Loss Function 관련 실무 경험 -> [기본 경험] 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ,확률 예측에서 MSE Loss 미 사용 이유,-0.001977068,0.0,0.001977067906409502
Loss Function 관련 실무 경험 -> [상세 경험] 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어,BCE 가 좋은 task,-0.0018536393,0.0,0.0018536392599344254
Loss Function 관련 실무 경험 -> [상세 경험] 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어,BCE 가 좋은 이유,-0.007263149,0.0,0.007263149134814739
Loss Function 관련 실무 경험 -> [상세 경험] 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어,LLM Fine-Tuning 의 PEFT,-3.485108e-05,0.0,3.4851080272346735e-05
Loss Function 관련 실무 경험 -> [상세 경험] 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어,LoRA,-0.0013257342,0.0,0.0013257341925054789
Loss Function 관련 실무 경험 -> [상세 경험] 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어,LoRA 와 QLoRA 의 차이,-0.00071864715,0.0,0.0007186471484601498
Loss Function 관련 실무 경험 -> [상세 경험] 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어,Loss Function 예시,-0.0006853489,0.0,0.0006853488739579916
Loss Function 관련 실무 경험 -> [상세 경험] 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어,Loss Function 정의,-0.004974622,0.0,0.0049746218137443066
Loss Function 관련 실무 경험 -> [상세 경험] 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어,MBTI,0.0023577276,0.0,0.0023577276151627302
Loss Function 관련 실무 경험 -> [상세 경험] 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어,MSE Loss 설명,-0.005493985,0.0,0.0054939850233495235
Loss Function 관련 실무 경험 -> [상세 경험] 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어,MSE Loss 용도,0.0003336932,0.0,0.000333693198626861
Loss Function 관련 실무 경험 -> [상세 경험] 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어,Multi-Label 에서 CE + Softmax 적용 문제점,-0.0023382849,0.0,0.00233828485943377
Loss Function 관련 실무 경험 -> [상세 경험] 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어,PEFT 방법 5가지,-0.0012413044,0.0,0.0012413044460117817
Loss Function 관련 실무 경험 -> [상세 경험] 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어,거대 언어 모델 정의,-0.0022745132,0.0,0.0022745132446289062
Loss Function 관련 실무 경험 -> [상세 경험] 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어,기본 경험,-0.0076732635,0.0,0.0076732635498046875
Loss Function 관련 실무 경험 -> [상세 경험] 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어,답변 실패,-0.0019410893,0.0,0.0019410892855376005
Loss Function 관련 실무 경험 -> [상세 경험] 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어,딥러닝,-0.002102965,0.0,0.002102965023368597
Loss Function 관련 실무 경험 -> [상세 경험] 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어,마지막 할 말,-0.002259404,0.0,0.002259403932839632
Loss Function 관련 실무 경험 -> [상세 경험] 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어,머신러닝,2.2368113e-05,0.0,2.2368112695403397e-05
Loss Function 관련 실무 경험 -> [상세 경험] 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어,면접 시작 인사,-0.004188411,0.0,0.004188410937786102
Loss Function 관련 실무 경험 -> [상세 경험] 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어,상세 경험,0.9982817,1.0,0.001718282699584961
Loss Function 관련 실무 경험 -> [상세 경험] 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어,수식,-0.003684902,0.0,0.003684902098029852
Loss Function 관련 실무 경험 -> [상세 경험] 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어,용어 질문,-0.0042282795,0.0,0.004228279460221529
Loss Function 관련 실무 경험 -> [상세 경험] 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어,인공지능,-0.0022452367,0.0,0.0022452366538345814
Loss Function 관련 실무 경험 -> [상세 경험] 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어,잠시 휴식,-0.0048680375,0.0,0.004868037533015013
Loss Function 관련 실무 경험 -> [상세 경험] 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어,좋아하는 아이돌,-0.0020741585,0.0,0.0020741585176438093
Loss Function 관련 실무 경험 -> [상세 경험] 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어,핵심 아이디어,-0.0017790478,0.0,0.0017790477722883224
Loss Function 관련 실무 경험 -> [상세 경험] 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어,확률 예측에서 MSE Loss 미 사용 이유,-0.001968833,0.0,0.0019688329193741083
Loss Function 관련 실무 경험 -> [상세 경험] 기억 진짜 안 나는데 가물가물하다,BCE 가 좋은 task,-0.0016004232,0.0,0.001600423245690763
Loss Function 관련 실무 경험 -> [상세 경험] 기억 진짜 안 나는데 가물가물하다,BCE 가 좋은 이유,-0.0039706184,0.0,0.003970618359744549
Loss Function 관련 실무 경험 -> [상세 경험] 기억 진짜 안 나는데 가물가물하다,LLM Fine-Tuning 의 PEFT,-0.0021004989,0.0,0.0021004988811910152
Loss Function 관련 실무 경험 -> [상세 경험] 기억 진짜 안 나는데 가물가물하다,LoRA,-0.0030637872,0.0,0.003063787240535021
Loss Function 관련 실무 경험 -> [상세 경험] 기억 진짜 안 나는데 가물가물하다,LoRA 와 QLoRA 의 차이,-0.0019518324,0.0,0.0019518324406817555
Loss Function 관련 실무 경험 -> [상세 경험] 기억 진짜 안 나는데 가물가물하다,Loss Function 예시,-0.0025191994,0.0,0.0025191993918269873
Loss Function 관련 실무 경험 -> [상세 경험] 기억 진짜 안 나는데 가물가물하다,Loss Function 정의,-0.003364221,0.0,0.0033642209600657225
Loss Function 관련 실무 경험 -> [상세 경험] 기억 진짜 안 나는데 가물가물하다,MBTI,-0.002464354,0.0,0.0024643540382385254
Loss Function 관련 실무 경험 -> [상세 경험] 기억 진짜 안 나는데 가물가물하다,MSE Loss 설명,-0.0023580287,0.0,0.0023580286651849747
Loss Function 관련 실무 경험 -> [상세 경험] 기억 진짜 안 나는데 가물가물하다,MSE Loss 용도,-0.0014168276,0.0,0.0014168275520205498
Loss Function 관련 실무 경험 -> [상세 경험] 기억 진짜 안 나는데 가물가물하다,Multi-Label 에서 CE + Softmax 적용 문제점,-0.0011584385,0.0,0.0011584385065361857
Loss Function 관련 실무 경험 -> [상세 경험] 기억 진짜 안 나는데 가물가물하다,PEFT 방법 5가지,-0.0023299137,0.0,0.002329913666471839
Loss Function 관련 실무 경험 -> [상세 경험] 기억 진짜 안 나는데 가물가물하다,거대 언어 모델 정의,-0.0011420521,0.0,0.00114205211866647
Loss Function 관련 실무 경험 -> [상세 경험] 기억 진짜 안 나는데 가물가물하다,기본 경험,-0.0020189798,0.0,0.002018979750573635
Loss Function 관련 실무 경험 -> [상세 경험] 기억 진짜 안 나는데 가물가물하다,답변 실패,0.9997324,1.0,0.0002676248550415039
Loss Function 관련 실무 경험 -> [상세 경험] 기억 진짜 안 나는데 가물가물하다,딥러닝,0.0005062816,0.0,0.0005062816198915243
Loss Function 관련 실무 경험 -> [상세 경험] 기억 진짜 안 나는데 가물가물하다,마지막 할 말,-0.0018115938,0.0,0.001811593770980835
Loss Function 관련 실무 경험 -> [상세 경험] 기억 진짜 안 나는데 가물가물하다,머신러닝,-0.00151002,0.0,0.0015100199962034822
Loss Function 관련 실무 경험 -> [상세 경험] 기억 진짜 안 나는데 가물가물하다,면접 시작 인사,-0.003444318,0.0,0.0034443179611116648
Loss Function 관련 실무 경험 -> [상세 경험] 기억 진짜 안 나는데 가물가물하다,상세 경험,-0.00077845715,0.0,0.0007784571498632431
Loss Function 관련 실무 경험 -> [상세 경험] 기억 진짜 안 나는데 가물가물하다,수식,-0.002426017,0.0,0.002426017075777054
Loss Function 관련 실무 경험 -> [상세 경험] 기억 진짜 안 나는데 가물가물하다,용어 질문,-0.0017878669,0.0,0.0017878669314086437
Loss Function 관련 실무 경험 -> [상세 경험] 기억 진짜 안 나는데 가물가물하다,인공지능,-0.002925365,0.0,0.002925364999100566
Loss Function 관련 실무 경험 -> [상세 경험] 기억 진짜 안 나는데 가물가물하다,잠시 휴식,0.00036961655,0.0,0.00036961655132472515
Loss Function 관련 실무 경험 -> [상세 경험] 기억 진짜 안 나는데 가물가물하다,좋아하는 아이돌,-0.0035402437,0.0,0.0035402437206357718
Loss Function 관련 실무 경험 -> [상세 경험] 기억 진짜 안 나는데 가물가물하다,핵심 아이디어,-0.0018098608,0.0,0.0018098608125001192
Loss Function 관련 실무 경험 -> [상세 경험] 기억 진짜 안 나는데 가물가물하다,확률 예측에서 MSE Loss 미 사용 이유,-0.0012604361,0.0,0.0012604361400008202
Loss Function 관련 실무 경험 -> [기본 경험] 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ,BCE 가 좋은 task,-0.0029099856,0.0,0.002909985603764653
Loss Function 관련 실무 경험 -> [기본 경험] 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ,BCE 가 좋은 이유,-0.0034879823,0.0,0.0034879823215305805
Loss Function 관련 실무 경험 -> [기본 경험] 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ,LLM Fine-Tuning 의 PEFT,-0.0025423253,0.0,0.002542325295507908
Loss Function 관련 실무 경험 -> [기본 경험] 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ,LoRA,0.00045674504,0.0,0.0004567450378090143
Loss Function 관련 실무 경험 -> [기본 경험] 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ,LoRA 와 QLoRA 의 차이,0.00046226216,0.0,0.00046226216363720596
Loss Function 관련 실무 경험 -> [기본 경험] 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ,Loss Function 예시,-0.0020513898,0.0,0.0020513897761702538
Loss Function 관련 실무 경험 -> [기본 경험] 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ,Loss Function 정의,-0.0008263782,0.0,0.0008263781783170998
Loss Function 관련 실무 경험 -> [기본 경험] 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ,MBTI,-0.0024038954,0.0,0.0024038953706622124
Loss Function 관련 실무 경험 -> [기본 경험] 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ,MSE Loss 설명,-0.0030763966,0.0,0.0030763966497033834
Loss Function 관련 실무 경험 -> [기본 경험] 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ,MSE Loss 용도,-0.0018034057,0.0,0.0018034056993201375
Loss Function 관련 실무 경험 -> [기본 경험] 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ,Multi-Label 에서 CE + Softmax 적용 문제점,-0.0038309377,0.0,0.0038309376686811447
Loss Function 관련 실무 경험 -> [기본 경험] 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ,PEFT 방법 5가지,-0.0015491545,0.0,0.00154915452003479
Loss Function 관련 실무 경험 -> [기본 경험] 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ,거대 언어 모델 정의,-0.0042525525,0.0,0.004252552520483732
Loss Function 관련 실무 경험 -> [기본 경험] 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ,기본 경험,0.9988564,1.0,0.0011435747146606445
Loss Function 관련 실무 경험 -> [기본 경험] 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ,답변 실패,-0.001869492,0.0,0.0018694919999688864
Loss Function 관련 실무 경험 -> [기본 경험] 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ,딥러닝,-0.002540269,0.0,0.002540268935263157
Loss Function 관련 실무 경험 -> [기본 경험] 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ,마지막 할 말,-0.0020485823,0.0,0.0020485823042690754
Loss Function 관련 실무 경험 -> [기본 경험] 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ,머신러닝,-0.004543363,0.0,0.004543363116681576
Loss Function 관련 실무 경험 -> [기본 경험] 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ,면접 시작 인사,-0.0041000647,0.0,0.004100064747035503
Loss Function 관련 실무 경험 -> [기본 경험] 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ,상세 경험,-0.0034679694,0.0,0.003467969363555312
Loss Function 관련 실무 경험 -> [기본 경험] 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ,수식,-0.0027942986,0.0,0.0027942985761910677
Loss Function 관련 실무 경험 -> [기본 경험] 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ,용어 질문,-0.00123368,0.0,0.0012336800573393703
Loss Function 관련 실무 경험 -> [기본 경험] 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ,인공지능,-0.0009683567,0.0,0.0009683566750027239
Loss Function 관련 실무 경험 -> [기본 경험] 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ,잠시 휴식,-0.0034361593,0.0,0.0034361593425273895
Loss Function 관련 실무 경험 -> [기본 경험] 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ,좋아하는 아이돌,0.0012274459,0.0,0.001227445900440216
Loss Function 관련 실무 경험 -> [기본 경험] 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ,핵심 아이디어,-0.003906623,0.0,0.0039066229946911335
Loss Function 관련 실무 경험 -> [기본 경험] 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ,확률 예측에서 MSE Loss 미 사용 이유,-0.0030317921,0.0,0.0030317921191453934
Loss Function 관련 실무 경험 -> [기본 경험] 이걸 왜 알려줘야 하지,BCE 가 좋은 task,-0.0017858823,0.0,0.0017858822830021381
Loss Function 관련 실무 경험 -> [기본 경험] 이걸 왜 알려줘야 하지,BCE 가 좋은 이유,-0.003898001,0.0,0.003898001043125987
Loss Function 관련 실무 경험 -> [기본 경험] 이걸 왜 알려줘야 하지,LLM Fine-Tuning 의 PEFT,-0.0022477522,0.0,0.0022477521561086178
Loss Function 관련 실무 경험 -> [기본 경험] 이걸 왜 알려줘야 하지,LoRA,-0.0027976865,0.0,0.002797686494886875
Loss Function 관련 실무 경험 -> [기본 경험] 이걸 왜 알려줘야 하지,LoRA 와 QLoRA 의 차이,-0.0015492688,0.0,0.001549268839880824
Loss Function 관련 실무 경험 -> [기본 경험] 이걸 왜 알려줘야 하지,Loss Function 예시,-0.0027818254,0.0,0.0027818253729492426
Loss Function 관련 실무 경험 -> [기본 경험] 이걸 왜 알려줘야 하지,Loss Function 정의,-0.0037107347,0.0,0.0037107346579432487
Loss Function 관련 실무 경험 -> [기본 경험] 이걸 왜 알려줘야 하지,MBTI,-0.0027198314,0.0,0.002719831420108676
Loss Function 관련 실무 경험 -> [기본 경험] 이걸 왜 알려줘야 하지,MSE Loss 설명,-0.0025335446,0.0,0.0025335445534437895
Loss Function 관련 실무 경험 -> [기본 경험] 이걸 왜 알려줘야 하지,MSE Loss 용도,-0.00065045117,0.0,0.0006504511693492532
Loss Function 관련 실무 경험 -> [기본 경험] 이걸 왜 알려줘야 하지,Multi-Label 에서 CE + Softmax 적용 문제점,-0.0007662629,0.0,0.0007662628777325153
Loss Function 관련 실무 경험 -> [기본 경험] 이걸 왜 알려줘야 하지,PEFT 방법 5가지,-0.0027034655,0.0,0.002703465521335602
Loss Function 관련 실무 경험 -> [기본 경험] 이걸 왜 알려줘야 하지,거대 언어 모델 정의,-0.00015241194,0.0,0.00015241194341797382
Loss Function 관련 실무 경험 -> [기본 경험] 이걸 왜 알려줘야 하지,기본 경험,0.0015080288,0.0,0.0015080288285389543
Loss Function 관련 실무 경험 -> [기본 경험] 이걸 왜 알려줘야 하지,답변 실패,0.9996741,1.0,0.00032591819763183594
Loss Function 관련 실무 경험 -> [기본 경험] 이걸 왜 알려줘야 하지,딥러닝,0.00027379507,0.0,0.00027379507082514465
Loss Function 관련 실무 경험 -> [기본 경험] 이걸 왜 알려줘야 하지,마지막 할 말,-0.0030866705,0.0,0.003086670534685254
Loss Function 관련 실무 경험 -> [기본 경험] 이걸 왜 알려줘야 하지,머신러닝,-0.0022816716,0.0,0.0022816716227680445
Loss Function 관련 실무 경험 -> [기본 경험] 이걸 왜 알려줘야 하지,면접 시작 인사,-0.0035466847,0.0,0.0035466847475618124
Loss Function 관련 실무 경험 -> [기본 경험] 이걸 왜 알려줘야 하지,상세 경험,-0.002435968,0.0,0.0024359680246561766
Loss Function 관련 실무 경험 -> [기본 경험] 이걸 왜 알려줘야 하지,수식,-0.0011952545,0.0,0.0011952545028179884
Loss Function 관련 실무 경험 -> [기본 경험] 이걸 왜 알려줘야 하지,용어 질문,-0.0022334673,0.0,0.002233467297628522
Loss Function 관련 실무 경험 -> [기본 경험] 이걸 왜 알려줘야 하지,인공지능,-0.003717724,0.0,0.0037177240010350943
Loss Function 관련 실무 경험 -> [기본 경험] 이걸 왜 알려줘야 하지,잠시 휴식,-0.00037181706,0.0,0.00037181706284172833
Loss Function 관련 실무 경험 -> [기본 경험] 이걸 왜 알려줘야 하지,좋아하는 아이돌,-0.0037508928,0.0,0.003750892821699381
Loss Function 관련 실무 경험 -> [기본 경험] 이걸 왜 알려줘야 하지,핵심 아이디어,-0.0008876149,0.0,0.0008876149076968431
Loss Function 관련 실무 경험 -> [기본 경험] 이걸 왜 알려줘야 하지,확률 예측에서 MSE Loss 미 사용 이유,-0.0016024177,0.0,0.001602417672984302
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,BCE 가 좋은 task,-0.005459038,0.0,0.005459038075059652
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,BCE 가 좋은 이유,-0.0032508646,0.0,0.003250864567235112
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,LLM Fine-Tuning 의 PEFT,-0.0046187085,0.0,0.00461870850995183
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,LoRA,-0.0035225383,0.0,0.003522538347169757
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,LoRA 와 QLoRA 의 차이,-0.0015683504,0.0,0.0015683503588661551
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,Loss Function 예시,-0.0024467367,0.0,0.0024467366747558117
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,Loss Function 정의,0.00086249335,0.0,0.0008624933543615043
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,MBTI,0.9977188,1.0,0.00228118896484375
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,MSE Loss 설명,-0.0042516594,0.0,0.004251659382134676
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,MSE Loss 용도,8.067902e-05,0.0,8.067901944741607e-05
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,Multi-Label 에서 CE + Softmax 적용 문제점,0.00057836366,0.0,0.0005783636588603258
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,PEFT 방법 5가지,-0.0023410285,0.0,0.0023410285357385874
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,거대 언어 모델 정의,-0.00313786,0.0,0.003137859981507063
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,기본 경험,-0.004465265,0.0,0.004465265199542046
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,답변 실패,-0.0018772699,0.0,0.0018772699404507875
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,딥러닝,-0.0042735836,0.0,0.004273583646863699
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,마지막 할 말,0.00017762887,0.0,0.00017762887000571936
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,머신러닝,-0.003593679,0.0,0.003593679051846266
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,면접 시작 인사,-0.0077692773,0.0,0.007769277319312096
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,상세 경험,-0.00093220884,0.0,0.000932208844460547
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,수식,-0.004786616,0.0,0.004786616191267967
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,용어 질문,0.00091828476,0.0,0.0009182847570627928
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,인공지능,-0.000906566,0.0,0.0009065659833140671
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,잠시 휴식,-0.004350048,0.0,0.004350048024207354
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,좋아하는 아이돌,-0.005761701,0.0,0.00576170114800334
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,핵심 아이디어,-0.0012110247,0.0,0.0012110247043892741
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,확률 예측에서 MSE Loss 미 사용 이유,0.0013339242,0.0,0.0013339242432266474
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,BCE 가 좋은 task,-0.0014649851,0.0,0.001464985078200698
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,BCE 가 좋은 이유,-0.0043651233,0.0,0.004365123342722654
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,LLM Fine-Tuning 의 PEFT,-0.0052383416,0.0,0.005238341633230448
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,LoRA,6.416392e-05,0.0,6.416391988750547e-05
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,LoRA 와 QLoRA 의 차이,-0.00021398882,0.0,0.00021398882381618023
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,Loss Function 예시,-0.0021672323,0.0,0.0021672323346138
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,Loss Function 정의,0.00049862365,0.0,0.0004986236453987658
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,MBTI,-0.004165855,0.0,0.004165854770690203
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,MSE Loss 설명,-0.0018832433,0.0,0.0018832433270290494
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,MSE Loss 용도,-0.0046466184,0.0,0.004646618384867907
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,Multi-Label 에서 CE + Softmax 적용 문제점,-0.0035618297,0.0,0.0035618296824395657
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,PEFT 방법 5가지,-0.0042753783,0.0,0.004275378305464983
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,거대 언어 모델 정의,-0.0024418288,0.0,0.0024418288376182318
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,기본 경험,-0.0003503751,0.0,0.00035037510679103434
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,답변 실패,-0.003948624,0.0,0.003948623780161142
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,딥러닝,-0.00019566447,0.0,0.00019566447008401155
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,마지막 할 말,-0.0029389511,0.0,0.0029389511328190565
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,머신러닝,0.0012446545,0.0,0.0012446545297279954
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,면접 시작 인사,0.00038012816,0.0,0.0003801281563937664
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,상세 경험,1.9138462e-05,0.0,1.913846244860906e-05
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,수식,0.00084385026,0.0,0.0008438502554781735
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,용어 질문,-0.00026840527,0.0,0.0002684052742552012
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,인공지능,-0.0013116804,0.0,0.0013116804184392095
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,잠시 휴식,-0.001005689,0.0,0.00100568903144449
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,좋아하는 아이돌,0.99863225,1.0,0.0013677477836608887
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,핵심 아이디어,-0.0013890434,0.0,0.0013890434056520462
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,확률 예측에서 MSE Loss 미 사용 이유,0.00070520514,0.0,0.0007052051369100809
잠시 휴식 -> 재미있는 이야기 해줄래?,BCE 가 좋은 task,-0.002436009,0.0,0.0024360090028494596
잠시 휴식 -> 재미있는 이야기 해줄래?,BCE 가 좋은 이유,-0.0013705759,0.0,0.0013705758610740304
잠시 휴식 -> 재미있는 이야기 해줄래?,LLM Fine-Tuning 의 PEFT,-0.002770964,0.0,0.0027709640562534332
잠시 휴식 -> 재미있는 이야기 해줄래?,LoRA,-0.0031430514,0.0,0.0031430514063686132
잠시 휴식 -> 재미있는 이야기 해줄래?,LoRA 와 QLoRA 의 차이,-0.002934261,0.0,0.002934260992333293
잠시 휴식 -> 재미있는 이야기 해줄래?,Loss Function 예시,-0.0025187575,0.0,0.0025187574792653322
잠시 휴식 -> 재미있는 이야기 해줄래?,Loss Function 정의,-0.0004909971,0.0,0.0004909971030429006
잠시 휴식 -> 재미있는 이야기 해줄래?,MBTI,-0.005300786,0.0,0.005300785880535841
잠시 휴식 -> 재미있는 이야기 해줄래?,MSE Loss 설명,-0.0024406798,0.0,0.0024406798183918
잠시 휴식 -> 재미있는 이야기 해줄래?,MSE Loss 용도,-0.0056310897,0.0,0.005631089676171541
잠시 휴식 -> 재미있는 이야기 해줄래?,Multi-Label 에서 CE + Softmax 적용 문제점,-0.0028132151,0.0,0.00281321513466537
잠시 휴식 -> 재미있는 이야기 해줄래?,PEFT 방법 5가지,0.00051741995,0.0,0.0005174199468456209
잠시 휴식 -> 재미있는 이야기 해줄래?,거대 언어 모델 정의,-0.00050343893,0.0,0.0005034389323554933
잠시 휴식 -> 재미있는 이야기 해줄래?,기본 경험,-0.0023760085,0.0,0.0023760085459798574
잠시 휴식 -> 재미있는 이야기 해줄래?,답변 실패,-0.0007814714,0.0,0.0007814713753759861
잠시 휴식 -> 재미있는 이야기 해줄래?,딥러닝,-0.0030457072,0.0,0.003045707242563367
잠시 휴식 -> 재미있는 이야기 해줄래?,마지막 할 말,-0.00030083413,0.0,0.0003008341300301254
잠시 휴식 -> 재미있는 이야기 해줄래?,머신러닝,-0.004233614,0.0,0.0042336140759289265
잠시 휴식 -> 재미있는 이야기 해줄래?,면접 시작 인사,-0.0038673799,0.0,0.003867379855364561
잠시 휴식 -> 재미있는 이야기 해줄래?,상세 경험,-0.005279946,0.0,0.005279946140944958
잠시 휴식 -> 재미있는 이야기 해줄래?,수식,-0.0013331587,0.0,0.0013331586960703135
잠시 휴식 -> 재미있는 이야기 해줄래?,용어 질문,0.00067680696,0.0,0.0006768069579266012
잠시 휴식 -> 재미있는 이야기 해줄래?,인공지능,0.0013929122,0.0,0.0013929122360423207
잠시 휴식 -> 재미있는 이야기 해줄래?,잠시 휴식,0.99914443,1.0,0.000855565071105957
잠시 휴식 -> 재미있는 이야기 해줄래?,좋아하는 아이돌,0.00020872298,0.0,0.00020872298046015203
잠시 휴식 -> 재미있는 이야기 해줄래?,핵심 아이디어,-0.0004951894,0.0,0.0004951893934048712
잠시 휴식 -> 재미있는 이야기 해줄래?,확률 예측에서 MSE Loss 미 사용 이유,-0.0016924044,0.0,0.0016924043884500861
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",BCE 가 좋은 task,-0.0019134115,0.0,0.0019134115427732468
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",BCE 가 좋은 이유,0.00029046423,0.0,0.00029046423151157796
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",LLM Fine-Tuning 의 PEFT,0.9978026,1.0,0.0021973848342895508
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",LoRA,-0.0033495403,0.0,0.003349540289491415
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",LoRA 와 QLoRA 의 차이,-0.0045292596,0.0,0.004529259633272886
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",Loss Function 예시,-0.0033827107,0.0,0.003382710739970207
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",Loss Function 정의,-0.0005980361,0.0,0.0005980361020192504
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",MBTI,-0.0049970993,0.0,0.004997099284082651
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",MSE Loss 설명,-0.004932542,0.0,0.004932541865855455
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",MSE Loss 용도,-0.0018441151,0.0,0.0018441150896251202
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",Multi-Label 에서 CE + Softmax 적용 문제점,0.001930787,0.0,0.001930786995217204
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",PEFT 방법 5가지,-0.0009407735,0.0,0.0009407735196873546
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",거대 언어 모델 정의,-0.0021520676,0.0,0.002152067609131336
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",기본 경험,-0.0013887021,0.0,0.0013887020759284496
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",답변 실패,0.001511899,0.0,0.0015118990559130907
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",딥러닝,-0.005046852,0.0,0.005046851933002472
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",마지막 할 말,-0.0009250326,0.0,0.0009250325965695083
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",머신러닝,-0.002456007,0.0,0.002456007059663534
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",면접 시작 인사,-0.0016373305,0.0,0.001637330511584878
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",상세 경험,0.00030915273,0.0,0.0003091527323704213
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",수식,-0.012442631,0.0,0.012442630715668201
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",용어 질문,0.0030083596,0.0,0.0030083595775067806
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",인공지능,-0.005291347,0.0,0.005291346926242113
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",잠시 휴식,-0.00015351646,0.0,0.00015351646288763732
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",좋아하는 아이돌,-0.0041726157,0.0,0.004172615706920624
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",핵심 아이디어,-0.0029764075,0.0,0.002976407529786229
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",확률 예측에서 MSE Loss 미 사용 이유,-0.0012591066,0.0,0.0012591065606102347
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,BCE 가 좋은 task,-0.0007361941,0.0,0.0007361940806731582
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,BCE 가 좋은 이유,-0.003991632,0.0,0.003991631790995598
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,LLM Fine-Tuning 의 PEFT,-0.0029289003,0.0,0.0029289002995938063
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,LoRA,-0.0028963222,0.0,0.0028963221702724695
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,LoRA 와 QLoRA 의 차이,-0.0014199894,0.0,0.0014199893921613693
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,Loss Function 예시,-0.0032400806,0.0,0.003240080550312996
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,Loss Function 정의,-0.0025657981,0.0,0.0025657981168478727
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,MBTI,-0.0014932067,0.0,0.001493206713348627
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,MSE Loss 설명,-0.0028935045,0.0,0.0028935044538229704
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,MSE Loss 용도,-0.0013644735,0.0,0.0013644734863191843
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,Multi-Label 에서 CE + Softmax 적용 문제점,-0.0009325178,0.0,0.0009325178107246757
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,PEFT 방법 5가지,-0.0025971585,0.0,0.0025971585419028997
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,거대 언어 모델 정의,-0.0017886568,0.0,0.0017886568093672395
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,기본 경험,-0.0033636515,0.0,0.003363651456311345
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,답변 실패,0.99970585,1.0,0.00029414892196655273
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,딥러닝,0.00055337726,0.0,0.000553377263713628
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,마지막 할 말,-0.0024297056,0.0,0.0024297055788338184
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,머신러닝,-0.0024863197,0.0,0.0024863197468221188
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,면접 시작 인사,-0.0033550507,0.0,0.003355050692334771
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,상세 경험,-0.0032589396,0.0,0.0032589395996183157
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,수식,-0.0019781908,0.0,0.0019781908486038446
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,용어 질문,-0.0006816673,0.0,0.0006816672976128757
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,인공지능,-0.0025717167,0.0,0.002571716671809554
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,잠시 휴식,-0.00023857417,0.0,0.0002385741681791842
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,좋아하는 아이돌,-0.0034523706,0.0,0.0034523706417530775
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,핵심 아이디어,-0.0015483882,0.0,0.0015483881579712033
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,확률 예측에서 MSE Loss 미 사용 이유,-0.0010620609,0.0,0.0010620609391480684
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야!,BCE 가 좋은 task,-0.0006000187,0.0,0.000600018713157624
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야!,BCE 가 좋은 이유,-0.0008586047,0.0,0.0008586046751588583
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야!,LLM Fine-Tuning 의 PEFT,0.9979999,1.0,0.002000093460083008
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야!,LoRA,-0.0014352205,0.0,0.0014352204743772745
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야!,LoRA 와 QLoRA 의 차이,-0.0043810564,0.0,0.004381056409329176
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야!,Loss Function 예시,-0.0029822255,0.0,0.002982225501909852
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야!,Loss Function 정의,-0.0026924154,0.0,0.002692415378987789
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야!,MBTI,-0.0073041082,0.0,0.007304108235985041
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야!,MSE Loss 설명,-0.0017630752,0.0,0.0017630752408877015
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야!,MSE Loss 용도,-0.0015233896,0.0,0.0015233895974233747
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야!,Multi-Label 에서 CE + Softmax 적용 문제점,0.0009044955,0.0,0.0009044954786077142
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야!,PEFT 방법 5가지,0.0009126896,0.0,0.0009126896038651466
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야!,거대 언어 모델 정의,0.00070410117,0.0,0.0007041011704131961
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야!,기본 경험,-0.00095973426,0.0,0.0009597342577762902
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야!,답변 실패,-0.0014128505,0.0,0.0014128504553809762
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야!,딥러닝,-0.004821829,0.0,0.004821829032152891
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야!,마지막 할 말,-0.001380926,0.0,0.0013809259980916977
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야!,머신러닝,-0.004067243,0.0,0.004067243076860905
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야!,면접 시작 인사,-0.0019800824,0.0,0.0019800823647528887
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야!,상세 경험,0.0015838891,0.0,0.001583889126777649
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야!,수식,-0.007644399,0.0,0.00764439906924963
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야!,용어 질문,0.00032474144,0.0,0.0003247414424549788
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야!,인공지능,-0.004921775,0.0,0.004921774845570326
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야!,잠시 휴식,-0.002410273,0.0,0.002410273067653179
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야!,좋아하는 아이돌,-0.007408822,0.0,0.007408821955323219
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야!,핵심 아이디어,-0.0008849313,0.0,0.0008849313016980886
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야!,확률 예측에서 MSE Loss 미 사용 이유,-0.002675397,0.0,0.002675397088751197
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다,BCE 가 좋은 task,-0.00033099574,0.0,0.00033099573920480907
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다,BCE 가 좋은 이유,-0.0034570256,0.0,0.0034570256248116493
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다,LLM Fine-Tuning 의 PEFT,-0.0023916608,0.0,0.0023916608188301325
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다,LoRA,-0.002910289,0.0,0.002910288982093334
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다,LoRA 와 QLoRA 의 차이,-0.0010739663,0.0,0.0010739662684500217
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다,Loss Function 예시,-0.0033555552,0.0,0.003355555236339569
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다,Loss Function 정의,-0.0024162713,0.0,0.00241627125069499
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다,MBTI,-0.0020372726,0.0,0.0020372725557535887
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다,MSE Loss 설명,-0.0024961806,0.0,0.0024961805902421474
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다,MSE Loss 용도,-0.0014776392,0.0,0.001477639190852642
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다,Multi-Label 에서 CE + Softmax 적용 문제점,-0.0008804521,0.0,0.0008804521057754755
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다,PEFT 방법 5가지,-0.0030493913,0.0,0.003049391321837902
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다,거대 언어 모델 정의,-0.0013890573,0.0,0.0013890572590753436
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다,기본 경험,-0.0029770122,0.0,0.0029770121909677982
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다,답변 실패,0.99969137,1.0,0.00030863285064697266
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다,딥러닝,-0.00013241211,0.0,0.00013241211127024144
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다,마지막 할 말,-0.0024196573,0.0,0.0024196573067456484
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다,머신러닝,-0.0024172622,0.0,0.002417262177914381
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다,면접 시작 인사,-0.003975623,0.0,0.003975622821599245
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다,상세 경험,-0.0028328577,0.0,0.002832857659086585
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다,수식,-0.0017648732,0.0,0.0017648731591179967
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다,용어 질문,-0.00043406864,0.0,0.00043406864278949797
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다,인공지능,-0.0027820456,0.0,0.002782045630738139
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다,잠시 휴식,0.00053348654,0.0,0.000533486541826278
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다,좋아하는 아이돌,-0.0033500702,0.0,0.0033500702120363712
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다,핵심 아이디어,-0.0018681529,0.0,0.0018681528745219111
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다,확률 예측에서 MSE Loss 미 사용 이유,-0.0017902751,0.0,0.0017902750987559557
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",BCE 가 좋은 task,-0.0013303478,0.0,0.0013303478481248021
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",BCE 가 좋은 이유,-0.0027340236,0.0,0.0027340236119925976
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",LLM Fine-Tuning 의 PEFT,5.2132393e-05,0.0,5.213239273871295e-05
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",LoRA,-0.00132575,0.0,0.0013257500249892473
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",LoRA 와 QLoRA 의 차이,0.0006384345,0.0,0.0006384344887919724
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",Loss Function 예시,-0.00096576154,0.0,0.0009657615446485579
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",Loss Function 정의,-4.4016433e-05,0.0,4.401643309392966e-05
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",MBTI,-5.707141e-05,0.0,5.707141099264845e-05
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",MSE Loss 설명,-0.0020074395,0.0,0.002007439499720931
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",MSE Loss 용도,-0.003992262,0.0,0.003992261830717325
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",Multi-Label 에서 CE + Softmax 적용 문제점,-0.00032796586,0.0,0.00032796585583128035
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",PEFT 방법 5가지,0.99789137,1.0,0.002108633518218994
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",거대 언어 모델 정의,-0.0005948644,0.0,0.0005948644247837365
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",기본 경험,0.00086402096,0.0,0.0008640209562145174
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",답변 실패,-0.003711652,0.0,0.003711652010679245
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",딥러닝,-0.0043071657,0.0,0.004307165741920471
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",마지막 할 말,-0.000582726,0.0,0.0005827259737998247
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",머신러닝,0.001440428,0.0,0.001440427964553237
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",면접 시작 인사,-0.0016296005,0.0,0.0016296005342155695
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",상세 경험,-0.003064885,0.0,0.003064885037019849
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",수식,-0.002295666,0.0,0.0022956659086048603
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",용어 질문,-0.008873935,0.0,0.008873934857547283
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",인공지능,-0.0005768761,0.0,0.0005768761038780212
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",잠시 휴식,-0.0002377977,0.0,0.000237797707086429
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",좋아하는 아이돌,-0.004893653,0.0,0.004893653094768524
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",핵심 아이디어,-0.0032822068,0.0,0.003282206831499934
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",확률 예측에서 MSE Loss 미 사용 이유,-0.0034406553,0.0,0.0034406553022563457
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,BCE 가 좋은 task,-0.0015967998,0.0,0.0015967998187988997
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,BCE 가 좋은 이유,-0.0038119673,0.0,0.0038119673263281584
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,LLM Fine-Tuning 의 PEFT,-0.0021358342,0.0,0.002135834190994501
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,LoRA,-0.0032871177,0.0,0.0032871176954358816
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,LoRA 와 QLoRA 의 차이,-0.0016060661,0.0,0.0016060661291703582
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,Loss Function 예시,-0.0039218394,0.0,0.0039218394085764885
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,Loss Function 정의,-0.0031320082,0.0,0.00313200824894011
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,MBTI,-0.0013284867,0.0,0.001328486716374755
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,MSE Loss 설명,-0.0034596634,0.0,0.003459663363173604
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,MSE Loss 용도,-0.0019235248,0.0,0.0019235247746109962
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,Multi-Label 에서 CE + Softmax 적용 문제점,-0.001538143,0.0,0.001538143027573824
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,PEFT 방법 5가지,-0.00040851015,0.0,0.0004085101536475122
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,거대 언어 모델 정의,-0.0022355185,0.0,0.0022355185355991125
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,기본 경험,-0.0029068908,0.0,0.002906890818849206
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,답변 실패,0.9997293,1.0,0.0002707242965698242
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,딥러닝,-0.00023642072,0.0,0.00023642071755602956
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,마지막 할 말,-0.0021017727,0.0,0.0021017726976424456
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,머신러닝,-0.0019501622,0.0,0.0019501622300595045
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,면접 시작 인사,-0.0030685281,0.0,0.003068528138101101
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,상세 경험,-0.002167292,0.0,0.0021672919392585754
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,수식,-0.0018407868,0.0,0.001840786775574088
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,용어 질문,-0.0013573144,0.0,0.0013573144096881151
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,인공지능,-0.0032940651,0.0,0.0032940651290118694
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,잠시 휴식,-0.00046795042,0.0,0.0004679504199884832
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,좋아하는 아이돌,-0.0029846109,0.0,0.002984610851854086
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,핵심 아이디어,-0.0021209985,0.0,0.00212099845521152
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,확률 예측에서 MSE Loss 미 사용 이유,-0.0005434473,0.0,0.0005434473278000951
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,BCE 가 좋은 task,-0.0035356951,0.0,0.0035356951411813498
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,BCE 가 좋은 이유,-0.0056925365,0.0,0.005692536476999521
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,LLM Fine-Tuning 의 PEFT,0.00019290578,0.0,0.00019290577620267868
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,LoRA,0.997899,1.0,0.002101004123687744
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,LoRA 와 QLoRA 의 차이,-0.0015201727,0.0,0.001520172692835331
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,Loss Function 예시,-0.0036383187,0.0,0.0036383187398314476
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,Loss Function 정의,-0.0032220723,0.0,0.0032220722641795874
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,MBTI,-0.0038910103,0.0,0.0038910103030502796
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,MSE Loss 설명,-0.0022396592,0.0,0.002239659195765853
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,MSE Loss 용도,-0.0045861905,0.0,0.004586190450936556
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,Multi-Label 에서 CE + Softmax 적용 문제점,0.0005008116,0.0,0.0005008116131648421
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,PEFT 방법 5가지,-0.00071018905,0.0,0.0007101890514604747
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,거대 언어 모델 정의,0.0015944132,0.0,0.0015944131882861257
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,기본 경험,0.0015944567,0.0,0.001594456727616489
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,답변 실패,-0.00404905,0.0,0.004049050156027079
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,딥러닝,-0.0042914343,0.0,0.004291434306651354
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,마지막 할 말,-0.0018565474,0.0,0.001856547431088984
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,머신러닝,-0.0040058494,0.0,0.004005849361419678
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,면접 시작 인사,-0.006202018,0.0,0.006202017888426781
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,상세 경험,-0.0021340798,0.0,0.002134079812094569
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,수식,-0.0033351963,0.0,0.003335196292027831
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,용어 질문,-0.0014968629,0.0,0.0014968628529459238
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,인공지능,-0.0041094176,0.0,0.004109417553991079
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,잠시 휴식,-0.002122394,0.0,0.0021223940420895815
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,좋아하는 아이돌,-0.0021660014,0.0,0.002166001359000802
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,핵심 아이디어,-0.00033125214,0.0,0.0003312521439511329
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,확률 예측에서 MSE Loss 미 사용 이유,-0.00085474894,0.0,0.0008547489414922893
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,BCE 가 좋은 task,-0.005089655,0.0,0.005089655052870512
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,BCE 가 좋은 이유,-0.002650233,0.0,0.0026502329856157303
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,LLM Fine-Tuning 의 PEFT,-0.00035726794,0.0,0.0003572679415810853
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,LoRA,-0.0030819105,0.0,0.0030819105450063944
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,LoRA 와 QLoRA 의 차이,0.001370487,0.0,0.0013704870361834764
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,Loss Function 예시,-0.0034421473,0.0,0.0034421472810208797
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,Loss Function 정의,-0.0024883016,0.0,0.0024883016012609005
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,MBTI,-0.003070771,0.0,0.0030707709956914186
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,MSE Loss 설명,-0.004617616,0.0,0.004617616068571806
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,MSE Loss 용도,-0.0019056312,0.0,0.0019056311575695872
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,Multi-Label 에서 CE + Softmax 적용 문제점,-0.0019185609,0.0,0.0019185609417036176
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,PEFT 방법 5가지,-0.0021789449,0.0,0.002178944880142808
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,거대 언어 모델 정의,0.0014576876,0.0,0.0014576875837519765
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,기본 경험,-0.0033819359,0.0,0.003381935879588127
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,답변 실패,0.99765193,1.0,0.0023480653762817383
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,딥러닝,-0.0012311891,0.0,0.0012311891186982393
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,마지막 할 말,-0.0013008066,0.0,0.0013008066453039646
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,머신러닝,-0.002087176,0.0,0.002087176078930497
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,면접 시작 인사,-0.00045373588,0.0,0.0004537358763627708
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,상세 경험,-0.000611476,0.0,0.0006114760180935264
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,수식,-0.0041923155,0.0,0.004192315507680178
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,용어 질문,-0.0025478164,0.0,0.0025478163734078407
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,인공지능,-0.0039177216,0.0,0.003917721565812826
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,잠시 휴식,-3.5213587e-05,0.0,3.521358667057939e-05
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,좋아하는 아이돌,-0.0007900515,0.0,0.0007900514756329358
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,핵심 아이디어,-0.00063207286,0.0,0.0006320728571154177
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,확률 예측에서 MSE Loss 미 사용 이유,-0.0013577499,0.0,0.0013577499194070697
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야!,BCE 가 좋은 task,-0.0025941508,0.0,0.002594150835648179
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야!,BCE 가 좋은 이유,-0.006212498,0.0,0.006212498061358929
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야!,LLM Fine-Tuning 의 PEFT,-0.00067279703,0.0,0.0006727970321662724
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야!,LoRA,0.9978107,1.0,0.0021892786026000977
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야!,LoRA 와 QLoRA 의 차이,-0.0021234353,0.0,0.0021234352607280016
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야!,Loss Function 예시,-0.0032772857,0.0,0.003277285723015666
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야!,Loss Function 정의,-0.0026160264,0.0,0.0026160264387726784
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야!,MBTI,0.0008551332,0.0,0.00085513322846964
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야!,MSE Loss 설명,-0.001399238,0.0,0.0013992380117997527
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야!,MSE Loss 용도,-0.0045805513,0.0,0.004580551292747259
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야!,Multi-Label 에서 CE + Softmax 적용 문제점,0.001349777,0.0,0.0013497769832611084
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야!,PEFT 방법 5가지,-0.001977536,0.0,0.0019775358960032463
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야!,거대 언어 모델 정의,-0.00010692407,0.0,0.00010692406794987619
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야!,기본 경험,0.0011925063,0.0,0.00119250628631562
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야!,답변 실패,-0.002673903,0.0,0.00267390301451087
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야!,딥러닝,-0.0034147962,0.0,0.0034147961996495724
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야!,마지막 할 말,-0.0029853508,0.0,0.002985350787639618
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야!,머신러닝,-0.002679759,0.0,0.0026797589380294085
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야!,면접 시작 인사,-0.0069848094,0.0,0.006984809413552284
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야!,상세 경험,-3.994624e-05,0.0,3.994624057668261e-05
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야!,수식,-0.0049915337,0.0,0.004991533700376749
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야!,용어 질문,-0.0017231178,0.0,0.001723117777146399
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야!,인공지능,-0.004208289,0.0,0.004208289086818695
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야!,잠시 휴식,-0.0020951321,0.0,0.0020951321348547935
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야!,좋아하는 아이돌,-0.00014444957,0.0,0.00014444957196246833
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야!,핵심 아이디어,-0.0009839297,0.0,0.0009839297272264957
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야!,확률 예측에서 MSE Loss 미 사용 이유,-0.0013004588,0.0,0.0013004587963223457
LoRA -> 무슨 OOM 없앤다는 것 같은데,BCE 가 좋은 task,-0.0019144072,0.0,0.0019144072430208325
LoRA -> 무슨 OOM 없앤다는 것 같은데,BCE 가 좋은 이유,-0.0034508647,0.0,0.0034508646931499243
LoRA -> 무슨 OOM 없앤다는 것 같은데,LLM Fine-Tuning 의 PEFT,-0.0016212859,0.0,0.0016212859191000462
LoRA -> 무슨 OOM 없앤다는 것 같은데,LoRA,-0.00247585,0.0,0.002475850051268935
LoRA -> 무슨 OOM 없앤다는 것 같은데,LoRA 와 QLoRA 의 차이,-0.00078655814,0.0,0.000786558142863214
LoRA -> 무슨 OOM 없앤다는 것 같은데,Loss Function 예시,-0.0033379323,0.0,0.0033379322849214077
LoRA -> 무슨 OOM 없앤다는 것 같은데,Loss Function 정의,-0.0034166046,0.0,0.003416604595258832
LoRA -> 무슨 OOM 없앤다는 것 같은데,MBTI,-0.0020348178,0.0,0.002034817822277546
LoRA -> 무슨 OOM 없앤다는 것 같은데,MSE Loss 설명,-0.003419591,0.0,0.00341959111392498
LoRA -> 무슨 OOM 없앤다는 것 같은데,MSE Loss 용도,-0.0018596468,0.0,0.0018596467562019825
LoRA -> 무슨 OOM 없앤다는 것 같은데,Multi-Label 에서 CE + Softmax 적용 문제점,-0.0009073489,0.0,0.0009073488763533533
LoRA -> 무슨 OOM 없앤다는 것 같은데,PEFT 방법 5가지,-0.0018357527,0.0,0.0018357527442276478
LoRA -> 무슨 OOM 없앤다는 것 같은데,거대 언어 모델 정의,-0.001484694,0.0,0.0014846939593553543
LoRA -> 무슨 OOM 없앤다는 것 같은데,기본 경험,-0.0024031522,0.0,0.0024031521752476692
LoRA -> 무슨 OOM 없앤다는 것 같은데,답변 실패,0.9997453,1.0,0.00025469064712524414
LoRA -> 무슨 OOM 없앤다는 것 같은데,딥러닝,-0.00013368094,0.0,0.00013368093641474843
LoRA -> 무슨 OOM 없앤다는 것 같은데,마지막 할 말,-0.0016602695,0.0,0.0016602694522589445
LoRA -> 무슨 OOM 없앤다는 것 같은데,머신러닝,-0.0022514667,0.0,0.0022514667361974716
LoRA -> 무슨 OOM 없앤다는 것 같은데,면접 시작 인사,-0.0033855112,0.0,0.003385511226952076
LoRA -> 무슨 OOM 없앤다는 것 같은데,상세 경험,-0.0021220858,0.0,0.0021220857743173838
LoRA -> 무슨 OOM 없앤다는 것 같은데,수식,-0.0025396748,0.0,0.002539674751460552
LoRA -> 무슨 OOM 없앤다는 것 같은데,용어 질문,-0.0017042322,0.0,0.0017042321851477027
LoRA -> 무슨 OOM 없앤다는 것 같은데,인공지능,-0.0026063095,0.0,0.0026063094846904278
LoRA -> 무슨 OOM 없앤다는 것 같은데,잠시 휴식,-0.00031392855,0.0,0.00031392855453304946
LoRA -> 무슨 OOM 없앤다는 것 같은데,좋아하는 아이돌,-0.0024731464,0.0,0.0024731464218348265
LoRA -> 무슨 OOM 없앤다는 것 같은데,핵심 아이디어,-0.001221295,0.0,0.0012212949804961681
LoRA -> 무슨 OOM 없앤다는 것 같은데,확률 예측에서 MSE Loss 미 사용 이유,-0.0014323174,0.0,0.0014323174254968762
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,BCE 가 좋은 task,-0.00088079536,0.0,0.0008807953563518822
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,BCE 가 좋은 이유,-0.007211339,0.0,0.007211339194327593
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,LLM Fine-Tuning 의 PEFT,-0.003969521,0.0,0.0039695207960903645
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,LoRA,-0.0023021486,0.0,0.002302148612216115
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,LoRA 와 QLoRA 의 차이,0.99891,1.0,0.0010899901390075684
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,Loss Function 예시,-0.00045432898,0.0,0.00045432898332364857
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,Loss Function 정의,-0.0024729315,0.0,0.002472931519150734
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,MBTI,-0.0004429547,0.0,0.0004429547116160393
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,MSE Loss 설명,-0.0035751576,0.0,0.003575157606974244
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,MSE Loss 용도,-0.0025472234,0.0,0.0025472233537584543
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,Multi-Label 에서 CE + Softmax 적용 문제점,-0.0038051712,0.0,0.0038051712326705456
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,PEFT 방법 5가지,0.0019773974,0.0,0.0019773973617702723
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,거대 언어 모델 정의,-0.0035698581,0.0,0.0035698581486940384
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,기본 경험,-0.0023431822,0.0,0.0023431822191923857
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,답변 실패,-0.0024840953,0.0,0.0024840952828526497
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,딥러닝,-0.0010994305,0.0,0.0010994304902851582
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,마지막 할 말,-0.0014298105,0.0,0.001429810537956655
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,머신러닝,-0.0028947282,0.0,0.002894728211686015
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,면접 시작 인사,-0.002880746,0.0,0.0028807460330426693
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,상세 경험,-0.0017929362,0.0,0.0017929362365975976
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,수식,-0.0016130274,0.0,0.0016130274161696434
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,용어 질문,0.0013243724,0.0,0.0013243723660707474
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,인공지능,-0.0035556632,0.0,0.003555663162842393
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,잠시 휴식,-0.004159101,0.0,0.004159100819379091
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,좋아하는 아이돌,-0.0035839875,0.0,0.0035839874763041735
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,핵심 아이디어,0.0011270302,0.0,0.0011270302347838879
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,확률 예측에서 MSE Loss 미 사용 이유,0.0016333426,0.0,0.0016333425883203745
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,BCE 가 좋은 task,-0.0022556407,0.0,0.0022556406911462545
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,BCE 가 좋은 이유,-0.0032031469,0.0,0.0032031468581408262
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,LLM Fine-Tuning 의 PEFT,-0.0015882221,0.0,0.0015882221050560474
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,LoRA,-0.0032297664,0.0,0.003229766385629773
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,LoRA 와 QLoRA 의 차이,0.0031102502,0.0,0.003110250225290656
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,Loss Function 예시,-0.004071407,0.0,0.0040714070200920105
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,Loss Function 정의,-0.0021518476,0.0,0.0021518475841730833
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,MBTI,-0.0021153851,0.0,0.002115385141223669
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,MSE Loss 설명,-0.0038236103,0.0,0.003823610255494714
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,MSE Loss 용도,-0.00051211566,0.0,0.0005121156573295593
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,Multi-Label 에서 CE + Softmax 적용 문제점,-0.0017629843,0.0,0.0017629843205213547
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,PEFT 방법 5가지,-0.0025188702,0.0,0.0025188701692968607
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,거대 언어 모델 정의,-3.0236735e-05,0.0,3.0236735256039537e-05
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,기본 경험,-0.002858733,0.0,0.002858733059838414
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,답변 실패,0.99955183,1.0,0.0004481673240661621
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,딥러닝,-0.0005393627,0.0,0.0005393627216108143
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,마지막 할 말,-0.0017082039,0.0,0.0017082039266824722
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,머신러닝,-0.0027601453,0.0,0.0027601453475654125
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,면접 시작 인사,-0.002728511,0.0,0.0027285111136734486
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,상세 경험,-0.0023131554,0.0,0.002313155448064208
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,수식,-0.0040527033,0.0,0.004052703268826008
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,용어 질문,-0.0029221743,0.0,0.0029221742879599333
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,인공지능,-0.0018552408,0.0,0.0018552407855167985
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,잠시 휴식,-0.0001458185,0.0,0.00014581849973183125
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,좋아하는 아이돌,-0.0020829723,0.0,0.0020829723216593266
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,핵심 아이디어,-0.0010175667,0.0,0.0010175666538998485
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,확률 예측에서 MSE Loss 미 사용 이유,-0.0004984911,0.0,0.0004984911065548658
마지막 할 말 -> 로라야 정말 고마워!,BCE 가 좋은 task,-0.0016095731,0.0,0.0016095731407403946
마지막 할 말 -> 로라야 정말 고마워!,BCE 가 좋은 이유,-0.0047437204,0.0,0.004743720404803753
마지막 할 말 -> 로라야 정말 고마워!,LLM Fine-Tuning 의 PEFT,-0.0042878827,0.0,0.004287882708013058
마지막 할 말 -> 로라야 정말 고마워!,LoRA,-0.002221724,0.0,0.0022217240184545517
마지막 할 말 -> 로라야 정말 고마워!,LoRA 와 QLoRA 의 차이,-0.0014017904,0.0,0.0014017904177308083
마지막 할 말 -> 로라야 정말 고마워!,Loss Function 예시,-9.810957e-05,0.0,9.810957271838561e-05
마지막 할 말 -> 로라야 정말 고마워!,Loss Function 정의,-0.0009992858,0.0,0.000999285839498043
마지막 할 말 -> 로라야 정말 고마워!,MBTI,-0.00090764684,0.0,0.0009076468413695693
마지막 할 말 -> 로라야 정말 고마워!,MSE Loss 설명,-0.003685983,0.0,0.003685982897877693
마지막 할 말 -> 로라야 정말 고마워!,MSE Loss 용도,-0.0048090643,0.0,0.004809064324945211
마지막 할 말 -> 로라야 정말 고마워!,Multi-Label 에서 CE + Softmax 적용 문제점,-0.0013702825,0.0,0.0013702824944630265
마지막 할 말 -> 로라야 정말 고마워!,PEFT 방법 5가지,-0.0010887501,0.0,0.0010887500829994678
마지막 할 말 -> 로라야 정말 고마워!,거대 언어 모델 정의,-0.0035306485,0.0,0.003530648536980152
마지막 할 말 -> 로라야 정말 고마워!,기본 경험,-0.0026967581,0.0,0.002696758136153221
마지막 할 말 -> 로라야 정말 고마워!,답변 실패,-0.0034651964,0.0,0.0034651963505893946
마지막 할 말 -> 로라야 정말 고마워!,딥러닝,-0.00013031537,0.0,0.00013031536946073174
마지막 할 말 -> 로라야 정말 고마워!,마지막 할 말,0.99919724,1.0,0.0008027553558349609
마지막 할 말 -> 로라야 정말 고마워!,머신러닝,-0.0029383411,0.0,0.0029383411165326834
마지막 할 말 -> 로라야 정말 고마워!,면접 시작 인사,-0.0026883006,0.0,0.0026883005630224943
마지막 할 말 -> 로라야 정말 고마워!,상세 경험,-0.0015706395,0.0,0.00157063954975456
마지막 할 말 -> 로라야 정말 고마워!,수식,-0.002329199,0.0,0.0023291991092264652
마지막 할 말 -> 로라야 정말 고마워!,용어 질문,-0.0019948613,0.0,0.001994861289858818
마지막 할 말 -> 로라야 정말 고마워!,인공지능,-0.002047409,0.0,0.0020474090706557035
마지막 할 말 -> 로라야 정말 고마워!,잠시 휴식,-0.0026635681,0.0,0.002663568127900362
마지막 할 말 -> 로라야 정말 고마워!,좋아하는 아이돌,-0.001873504,0.0,0.001873504021205008
마지막 할 말 -> 로라야 정말 고마워!,핵심 아이디어,-0.0017364416,0.0,0.0017364416271448135
마지막 할 말 -> 로라야 정말 고마워!,확률 예측에서 MSE Loss 미 사용 이유,0.00060219865,0.0,0.0006021986482664943
마지막 할 말 -> 로라야 사랑해,BCE 가 좋은 task,-0.0019341432,0.0,0.0019341432489454746
마지막 할 말 -> 로라야 사랑해,BCE 가 좋은 이유,-0.00476454,0.0,0.004764540120959282
마지막 할 말 -> 로라야 사랑해,LLM Fine-Tuning 의 PEFT,-0.004233594,0.0,0.004233594052493572
마지막 할 말 -> 로라야 사랑해,LoRA,-0.0024127478,0.0,0.002412747824564576
마지막 할 말 -> 로라야 사랑해,LoRA 와 QLoRA 의 차이,-0.0012748732,0.0,0.0012748731533065438
마지막 할 말 -> 로라야 사랑해,Loss Function 예시,-0.00039417538,0.0,0.00039417538209818304
마지막 할 말 -> 로라야 사랑해,Loss Function 정의,-0.0014612176,0.0,0.0014612176455557346
마지막 할 말 -> 로라야 사랑해,MBTI,-0.0009826882,0.0,0.0009826881578192115
마지막 할 말 -> 로라야 사랑해,MSE Loss 설명,-0.0034687913,0.0,0.0034687912557274103
마지막 할 말 -> 로라야 사랑해,MSE Loss 용도,-0.0047666696,0.0,0.00476666959002614
마지막 할 말 -> 로라야 사랑해,Multi-Label 에서 CE + Softmax 적용 문제점,-0.0014401558,0.0,0.0014401557855308056
마지막 할 말 -> 로라야 사랑해,PEFT 방법 5가지,-0.0013309468,0.0,0.0013309468049556017
마지막 할 말 -> 로라야 사랑해,거대 언어 모델 정의,-0.0034216084,0.0,0.0034216083586215973
마지막 할 말 -> 로라야 사랑해,기본 경험,-0.0027182363,0.0,0.0027182362973690033
마지막 할 말 -> 로라야 사랑해,답변 실패,-0.0033512518,0.0,0.0033512518275529146
마지막 할 말 -> 로라야 사랑해,딥러닝,-0.00032773198,0.0,0.00032773197744973004
마지막 할 말 -> 로라야 사랑해,마지막 할 말,0.99929595,1.0,0.0007040500640869141
마지막 할 말 -> 로라야 사랑해,머신러닝,-0.0027676164,0.0,0.002767616417258978
마지막 할 말 -> 로라야 사랑해,면접 시작 인사,-0.0025134268,0.0,0.002513426821678877
마지막 할 말 -> 로라야 사랑해,상세 경험,-0.0014367392,0.0,0.0014367392286658287
마지막 할 말 -> 로라야 사랑해,수식,-0.0021864653,0.0,0.002186465309932828
마지막 할 말 -> 로라야 사랑해,용어 질문,-0.002243124,0.0,0.002243123948574066
마지막 할 말 -> 로라야 사랑해,인공지능,-0.002009389,0.0,0.002009388990700245
마지막 할 말 -> 로라야 사랑해,잠시 휴식,-0.0024902797,0.0,0.0024902797304093838
마지막 할 말 -> 로라야 사랑해,좋아하는 아이돌,-0.0019424355,0.0,0.0019424355123192072
마지막 할 말 -> 로라야 사랑해,핵심 아이디어,-0.0014575615,0.0,0.001457561505958438
마지막 할 말 -> 로라야 사랑해,확률 예측에서 MSE Loss 미 사용 이유,0.0010714629,0.0,0.0010714628733694553
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,BCE 가 좋은 task,-0.0019993708,0.0,0.001999370753765106
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,BCE 가 좋은 이유,-0.0041157245,0.0,0.004115724470466375
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,LLM Fine-Tuning 의 PEFT,-0.0029511661,0.0,0.0029511661268770695
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,LoRA,-0.002916365,0.0,0.0029163649305701256
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,LoRA 와 QLoRA 의 차이,-0.0018700154,0.0,0.0018700154032558203
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,Loss Function 예시,-0.0010828171,0.0,0.00108281709253788
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,Loss Function 정의,-0.0009435455,0.0,0.0009435454849153757
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,MBTI,-0.0006720892,0.0,0.0006720892270095646
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,MSE Loss 설명,-0.004520388,0.0,0.004520387854427099
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,MSE Loss 용도,-0.0046598488,0.0,0.004659848753362894
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,Multi-Label 에서 CE + Softmax 적용 문제점,-0.0019596245,0.0,0.0019596244674175978
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,PEFT 방법 5가지,-0.0015960364,0.0,0.0015960363671183586
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,거대 언어 모델 정의,-0.0038521942,0.0,0.0038521941751241684
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,기본 경험,-0.0025948447,0.0,0.0025948446709662676
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,답변 실패,-0.0033840977,0.0,0.0033840977121144533
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,딥러닝,-0.00094223337,0.0,0.0009422333678230643
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,마지막 할 말,0.99882376,1.0,0.0011762380599975586
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,머신러닝,-0.0034082404,0.0,0.0034082403872162104
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,면접 시작 인사,-0.0027460551,0.0,0.0027460551355034113
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,상세 경험,-0.002511695,0.0,0.0025116950273513794
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,수식,-0.0037751321,0.0,0.0037751321215182543
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,용어 질문,-0.0034670734,0.0,0.003467073431238532
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,인공지능,-0.0024261877,0.0,0.002426187740638852
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,잠시 휴식,-0.001936604,0.0,0.0019366040360182524
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,좋아하는 아이돌,-0.002002902,0.0,0.0020029020961374044
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,핵심 아이디어,-0.0015189045,0.0,0.0015189044643193483
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,확률 예측에서 MSE Loss 미 사용 이유,0.00023660965,0.0,0.00023660964507143945
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워,BCE 가 좋은 task,-0.0017100088,0.0,0.001710008829832077
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워,BCE 가 좋은 이유,-0.0043916195,0.0,0.004391619469970465
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워,LLM Fine-Tuning 의 PEFT,-0.003940749,0.0,0.003940748982131481
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워,LoRA,-0.0024921375,0.0,0.002492137486115098
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워,LoRA 와 QLoRA 의 차이,-0.0013865612,0.0,0.0013865611981600523
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워,Loss Function 예시,-0.0007206101,0.0,0.0007206100854091346
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워,Loss Function 정의,-0.00044883246,0.0,0.0004488324630074203
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워,MBTI,-0.0031252066,0.0,0.0031252065673470497
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워,MSE Loss 설명,-0.0025111819,0.0,0.0025111818686127663
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워,MSE Loss 용도,-0.0053492766,0.0,0.005349276587367058
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워,Multi-Label 에서 CE + Softmax 적용 문제점,-0.0009831446,0.0,0.000983144622296095
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워,PEFT 방법 5가지,-0.0005274758,0.0,0.0005274757859297097
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워,거대 언어 모델 정의,-0.0030406027,0.0,0.0030406026635318995
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워,기본 경험,-0.0028510487,0.0,0.002851048717275262
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워,답변 실패,-0.0035355045,0.0,0.0035355044528841972
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워,딥러닝,-0.00047625476,0.0,0.0004762547614518553
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워,마지막 할 말,0.99880385,1.0,0.001196146011352539
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워,머신러닝,-0.003266384,0.0,0.003266383893787861
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워,면접 시작 인사,-0.00237839,0.0,0.0023783899378031492
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워,상세 경험,-0.0021478804,0.0,0.002147880382835865
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워,수식,-0.00454475,0.0,0.004544749855995178
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워,용어 질문,-0.0021857598,0.0,0.0021857598330825567
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워,인공지능,-0.0014637313,0.0,0.0014637312851846218
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워,잠시 휴식,-0.002371338,0.0,0.002371337963268161
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워,좋아하는 아이돌,-0.002032925,0.0,0.00203292490914464
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워,핵심 아이디어,-0.0019481928,0.0,0.0019481928320601583
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워,확률 예측에서 MSE Loss 미 사용 이유,8.048695e-06,0.0,8.048695235629566e-06
