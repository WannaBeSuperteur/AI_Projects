input_part,output_answer,predicted_score,ground_truth_score,absolute_error
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,BCE 가 좋은 task,-0.002266027,0.0,0.00226602703332901
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,"BCE 가 좋은 task, 이유",-0.002565351,0.0,0.0025653510820120573
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,LLM Fine-Tuning 의 PEFT,-0.0009005849,0.0,0.0009005849133245647
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,LoRA,0.0026715065,0.0,0.0026715064886957407
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,LoRA 와 QLoRA 의 차이,0.0007898801,0.0,0.0007898801122792065
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,Loss Function 예시,0.0006377713,0.0,0.0006377713289111853
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,Loss Function 정의,-0.002726232,0.0,0.002726231934502721
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,MBTI,-0.0037073432,0.0,0.0037073432467877865
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,MSE Loss 설명,-0.004514679,0.0,0.004514678847044706
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,MSE Loss 용도,0.0006678837,0.0,0.0006678837235085666
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,Multi-Label 에서 CE + Softmax 적용 문제점,-0.0033907231,0.0,0.003390723140910268
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,PEFT 방법 5가지,-5.3178177e-05,0.0,5.317817704053596e-05
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,거대 언어 모델 정의,-0.0027999103,0.0,0.0027999102603644133
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,기본 경험,-0.000588164,0.0,0.0005881640245206654
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,답변 실패,-0.0041193157,0.0,0.004119315650314093
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,딥러닝,-0.006496301,0.0,0.006496300920844078
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,마지막 할 말,0.0018079475,0.0,0.0018079475266858935
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,머신러닝,-0.009361089,0.0,0.009361089207231998
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,면접 시작 인사,0.99649674,1.0,0.003503262996673584
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,상세 경험,0.00013669963,0.0,0.00013669962936546654
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,수식,3.9436356e-05,0.0,3.9436356018995866e-05
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,인공지능,0.0044936794,0.0,0.0044936793856322765
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,잠시 휴식,-0.0025611161,0.0,0.002561116125434637
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,좋아하는 아이돌,-0.0026313453,0.0,0.002631345298141241
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,핵심 아이디어,0.0009619837,0.0,0.0009619836928322911
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,확률 예측에서 MSE Loss 미 사용 이유,-0.002360427,0.0,0.002360427053645253
면접 시작 인사 -> 로라야 안녕 정말 반가워,BCE 가 좋은 task,-0.0016545297,0.0,0.0016545297112315893
면접 시작 인사 -> 로라야 안녕 정말 반가워,"BCE 가 좋은 task, 이유",-0.0021627806,0.0,0.002162780612707138
면접 시작 인사 -> 로라야 안녕 정말 반가워,LLM Fine-Tuning 의 PEFT,-0.002161113,0.0,0.002161113079637289
면접 시작 인사 -> 로라야 안녕 정말 반가워,LoRA,0.0010361527,0.0,0.001036152709275484
면접 시작 인사 -> 로라야 안녕 정말 반가워,LoRA 와 QLoRA 의 차이,0.0007382117,0.0,0.0007382116746157408
면접 시작 인사 -> 로라야 안녕 정말 반가워,Loss Function 예시,-0.0023018476,0.0,0.0023018475621938705
면접 시작 인사 -> 로라야 안녕 정말 반가워,Loss Function 정의,-0.0002898421,0.0,0.0002898421080317348
면접 시작 인사 -> 로라야 안녕 정말 반가워,MBTI,0.00058523635,0.0,0.0005852363537997007
면접 시작 인사 -> 로라야 안녕 정말 반가워,MSE Loss 설명,-0.004980379,0.0,0.004980378784239292
면접 시작 인사 -> 로라야 안녕 정말 반가워,MSE Loss 용도,-0.0020005016,0.0,0.002000501612201333
면접 시작 인사 -> 로라야 안녕 정말 반가워,Multi-Label 에서 CE + Softmax 적용 문제점,-0.004249038,0.0,0.004249038174748421
면접 시작 인사 -> 로라야 안녕 정말 반가워,PEFT 방법 5가지,-0.0025244807,0.0,0.002524480689316988
면접 시작 인사 -> 로라야 안녕 정말 반가워,거대 언어 모델 정의,0.0001603614,0.0,0.00016036139277275652
면접 시작 인사 -> 로라야 안녕 정말 반가워,기본 경험,-0.00199729,0.0,0.0019972899463027716
면접 시작 인사 -> 로라야 안녕 정말 반가워,답변 실패,-0.0026516344,0.0,0.002651634393259883
면접 시작 인사 -> 로라야 안녕 정말 반가워,딥러닝,-0.004597599,0.0,0.0045975991524755955
면접 시작 인사 -> 로라야 안녕 정말 반가워,마지막 할 말,0.004753331,0.0,0.004753331188112497
면접 시작 인사 -> 로라야 안녕 정말 반가워,머신러닝,-0.008458065,0.0,0.008458064869046211
면접 시작 인사 -> 로라야 안녕 정말 반가워,면접 시작 인사,0.9971465,1.0,0.0028535127639770508
면접 시작 인사 -> 로라야 안녕 정말 반가워,상세 경험,-0.00045868606,0.0,0.00045868605957366526
면접 시작 인사 -> 로라야 안녕 정말 반가워,수식,-0.0024934565,0.0,0.002493456471711397
면접 시작 인사 -> 로라야 안녕 정말 반가워,인공지능,0.0045309504,0.0,0.004530950449407101
면접 시작 인사 -> 로라야 안녕 정말 반가워,잠시 휴식,-0.0007312255,0.0,0.0007312254747375846
면접 시작 인사 -> 로라야 안녕 정말 반가워,좋아하는 아이돌,-0.0013337173,0.0,0.001333717256784439
면접 시작 인사 -> 로라야 안녕 정말 반가워,핵심 아이디어,0.00038074623,0.0,0.00038074623444117606
면접 시작 인사 -> 로라야 안녕 정말 반가워,확률 예측에서 MSE Loss 미 사용 이유,-0.0020437543,0.0,0.0020437543280422688
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,BCE 가 좋은 task,-0.0029350612,0.0,0.0029350612312555313
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,"BCE 가 좋은 task, 이유",-0.00045848452,0.0,0.0004584845155477524
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,LLM Fine-Tuning 의 PEFT,-0.0036715583,0.0,0.003671558341011405
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,LoRA,0.0019863052,0.0,0.0019863052293658257
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,LoRA 와 QLoRA 의 차이,0.00046644566,0.0,0.0004664456646423787
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,Loss Function 예시,-0.0017638498,0.0,0.0017638497520238161
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,Loss Function 정의,-0.0019724765,0.0,0.0019724764861166477
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,MBTI,-0.0040508904,0.0,0.004050890449434519
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,MSE Loss 설명,-0.00516308,0.0,0.005163080058991909
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,MSE Loss 용도,0.0007946877,0.0,0.0007946877158246934
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,Multi-Label 에서 CE + Softmax 적용 문제점,-0.0049919193,0.0,0.00499191926792264
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,PEFT 방법 5가지,-0.003055565,0.0,0.003055565059185028
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,거대 언어 모델 정의,-0.0012214158,0.0,0.0012214158196002245
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,기본 경험,-0.00086899946,0.0,0.0008689994574524462
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,답변 실패,-0.0013984224,0.0,0.0013984224060550332
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,딥러닝,-0.0053340103,0.0,0.005334010347723961
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,마지막 할 말,0.0024000753,0.0,0.0024000753182917833
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,머신러닝,-0.011159951,0.0,0.011159950867295265
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,면접 시작 인사,0.99555403,1.0,0.004445970058441162
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,상세 경험,0.0015071258,0.0,0.0015071257948875427
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,수식,-0.0011254889,0.0,0.0011254888959228992
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,인공지능,0.0042618373,0.0,0.004261837340891361
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,잠시 휴식,-0.0047489298,0.0,0.004748929757624865
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,좋아하는 아이돌,-0.0037282852,0.0,0.003728285199031234
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,핵심 아이디어,-0.0003750401,0.0,0.0003750401083379984
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,확률 예측에서 MSE Loss 미 사용 이유,-0.0028022325,0.0,0.002802232513204217
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,BCE 가 좋은 task,-0.0023947526,0.0,0.0023947525769472122
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,"BCE 가 좋은 task, 이유",0.0068047484,0.0,0.006804748438298702
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,LLM Fine-Tuning 의 PEFT,0.0022869478,0.0,0.002286947797983885
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,LoRA,0.0013016312,0.0,0.0013016312150284648
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,LoRA 와 QLoRA 의 차이,0.0033911788,0.0,0.0033911787904798985
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,Loss Function 예시,-0.0021886698,0.0,0.002188669750466943
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,Loss Function 정의,-4.0015155e-05,0.0,4.001515480922535e-05
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,MBTI,-0.0041530454,0.0,0.004153045359998941
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,MSE Loss 설명,-0.0058442405,0.0,0.005844240542501211
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,MSE Loss 용도,-0.006938971,0.0,0.006938971113413572
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,Multi-Label 에서 CE + Softmax 적용 문제점,-0.0069002383,0.0,0.006900238338857889
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,PEFT 방법 5가지,-0.0014902417,0.0,0.0014902417315170169
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,거대 언어 모델 정의,-0.00031821863,0.0,0.00031821863376535475
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,기본 경험,-0.0005092973,0.0,0.0005092973005957901
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,답변 실패,-0.0044285445,0.0,0.004428544547408819
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,딥러닝,-0.003440766,0.0,0.0034407658968120813
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,마지막 할 말,0.0058995825,0.0,0.005899582523852587
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,머신러닝,-0.0059581078,0.0,0.005958107765763998
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,면접 시작 인사,0.9947816,1.0,0.005218386650085449
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,상세 경험,-0.0001852707,0.0,0.00018527070642448962
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,수식,-0.0055329385,0.0,0.0055329385213553905
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,인공지능,0.0025867026,0.0,0.0025867025833576918
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,잠시 휴식,-0.0015185935,0.0,0.0015185935189947486
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,좋아하는 아이돌,-0.004734758,0.0,0.004734757822006941
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,핵심 아이디어,-4.9892307e-05,0.0,4.989230728824623e-05
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,확률 예측에서 MSE Loss 미 사용 이유,-0.0013793291,0.0,0.0013793291291221976
면접 시작 인사 -> 파이팅! 시작하자,BCE 가 좋은 task,-0.0020389885,0.0,0.0020389885175973177
면접 시작 인사 -> 파이팅! 시작하자,"BCE 가 좋은 task, 이유",0.0010331009,0.0,0.001033100881613791
면접 시작 인사 -> 파이팅! 시작하자,LLM Fine-Tuning 의 PEFT,-0.0011913603,0.0,0.0011913602938875556
면접 시작 인사 -> 파이팅! 시작하자,LoRA,0.00050060963,0.0,0.0005006096325814724
면접 시작 인사 -> 파이팅! 시작하자,LoRA 와 QLoRA 의 차이,0.0005191419,0.0,0.000519141904078424
면접 시작 인사 -> 파이팅! 시작하자,Loss Function 예시,0.001175086,0.0,0.0011750860139727592
면접 시작 인사 -> 파이팅! 시작하자,Loss Function 정의,-0.0035063464,0.0,0.003506346372887492
면접 시작 인사 -> 파이팅! 시작하자,MBTI,0.0034049046,0.0,0.0034049046225845814
면접 시작 인사 -> 파이팅! 시작하자,MSE Loss 설명,-0.0036627953,0.0,0.0036627952940762043
면접 시작 인사 -> 파이팅! 시작하자,MSE Loss 용도,-0.000997214,0.0,0.000997213996015489
면접 시작 인사 -> 파이팅! 시작하자,Multi-Label 에서 CE + Softmax 적용 문제점,-0.004786016,0.0,0.0047860159538686275
면접 시작 인사 -> 파이팅! 시작하자,PEFT 방법 5가지,-0.0004525635,0.0,0.0004525634867604822
면접 시작 인사 -> 파이팅! 시작하자,거대 언어 모델 정의,-0.0014746839,0.0,0.0014746838714927435
면접 시작 인사 -> 파이팅! 시작하자,기본 경험,-0.0018157241,0.0,0.0018157240701839328
면접 시작 인사 -> 파이팅! 시작하자,답변 실패,-0.0042953477,0.0,0.004295347724109888
면접 시작 인사 -> 파이팅! 시작하자,딥러닝,-0.004510473,0.0,0.004510472994297743
면접 시작 인사 -> 파이팅! 시작하자,마지막 할 말,0.0027666711,0.0,0.002766671124845743
면접 시작 인사 -> 파이팅! 시작하자,머신러닝,-0.0061123758,0.0,0.006112375762313604
면접 시작 인사 -> 파이팅! 시작하자,면접 시작 인사,0.9968849,1.0,0.0031151175498962402
면접 시작 인사 -> 파이팅! 시작하자,상세 경험,0.00038326578,0.0,0.00038326578214764595
면접 시작 인사 -> 파이팅! 시작하자,수식,-0.0013185084,0.0,0.0013185084098950028
면접 시작 인사 -> 파이팅! 시작하자,인공지능,0.002450988,0.0,0.0024509879294782877
면접 시작 인사 -> 파이팅! 시작하자,잠시 휴식,-0.00033247293,0.0,0.00033247293322347105
면접 시작 인사 -> 파이팅! 시작하자,좋아하는 아이돌,-0.0018657644,0.0,0.001865764381363988
면접 시작 인사 -> 파이팅! 시작하자,핵심 아이디어,0.00049336907,0.0,0.0004933690652251244
면접 시작 인사 -> 파이팅! 시작하자,확률 예측에서 MSE Loss 미 사용 이유,-0.0015711805,0.0,0.0015711805317550898
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",BCE 가 좋은 task,0.0022263613,0.0,0.002226361306384206
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데","BCE 가 좋은 task, 이유",0.0013932493,0.0,0.0013932492583990097
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",LLM Fine-Tuning 의 PEFT,-0.016470063,0.0,0.016470063477754593
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",LoRA,-0.0078069163,0.0,0.007806916255503893
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",LoRA 와 QLoRA 의 차이,0.000492957,0.0,0.0004929570131935179
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",Loss Function 예시,-0.006364411,0.0,0.00636441120877862
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",Loss Function 정의,-0.020437645,0.0,0.02043764479458332
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",MBTI,-0.00966024,0.0,0.009660240262746811
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",MSE Loss 설명,-0.01428758,0.0,0.01428757980465889
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",MSE Loss 용도,-0.0038026073,0.0,0.003802607301622629
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",Multi-Label 에서 CE + Softmax 적용 문제점,0.007126135,0.0,0.007126134820282459
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",PEFT 방법 5가지,-0.01499007,0.0,0.014990069903433323
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",거대 언어 모델 정의,-0.009713051,0.0,0.009713050909340382
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",기본 경험,0.10544762,0.0,0.10544762015342712
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",답변 실패,0.97690403,1.0,0.02309596538543701
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",딥러닝,-0.0073866323,0.0,0.007386632263660431
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",마지막 할 말,-0.0061237602,0.0,0.006123760249465704
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",머신러닝,0.0073483777,0.0,0.007348377723246813
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",면접 시작 인사,-0.003472868,0.0,0.003472867887467146
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",상세 경험,0.015635604,0.0,0.015635604038834572
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",수식,0.0019123164,0.0,0.0019123164238408208
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",인공지능,0.01191269,0.0,0.011912689544260502
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",잠시 휴식,-0.006305193,0.0,0.0063051930628716946
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",좋아하는 아이돌,-0.0035273544,0.0,0.003527354449033737
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",핵심 아이디어,-7.802586e-05,0.0,7.802586333127692e-05
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",확률 예측에서 MSE Loss 미 사용 이유,-0.011339719,0.0,0.011339719407260418
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",BCE 가 좋은 task,0.0005273469,0.0,0.0005273469141684473
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!","BCE 가 좋은 task, 이유",0.00089536415,0.0,0.000895364151801914
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",LLM Fine-Tuning 의 PEFT,0.006478485,0.0,0.006478485185652971
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",LoRA,-0.008091841,0.0,0.008091840893030167
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",LoRA 와 QLoRA 의 차이,-0.0027175867,0.0,0.002717586699873209
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",Loss Function 예시,0.0014051298,0.0,0.0014051297912374139
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",Loss Function 정의,-0.004634777,0.0,0.004634777083992958
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",MBTI,-0.00018780354,0.0,0.000187803540029563
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",MSE Loss 설명,-0.0019784486,0.0,0.0019784485921263695
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",MSE Loss 용도,-0.006961832,0.0,0.006961831822991371
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",Multi-Label 에서 CE + Softmax 적용 문제점,-0.0018193235,0.0,0.0018193235155194998
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",PEFT 방법 5가지,-0.0007112664,0.0,0.0007112664170563221
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",거대 언어 모델 정의,-0.0129086105,0.0,0.01290861051529646
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",기본 경험,-0.007352995,0.0,0.007352995220571756
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",답변 실패,-0.0053214054,0.0,0.005321405362337828
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",딥러닝,-0.012798188,0.0,0.012798188254237175
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",마지막 할 말,0.0030403999,0.0,0.003040399868041277
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",머신러닝,-0.016277282,0.0,0.016277281567454338
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",면접 시작 인사,0.0066141705,0.0,0.006614170502871275
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",상세 경험,-0.009661121,0.0,0.009661121293902397
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",수식,0.0036582942,0.0,0.0036582942120730877
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",인공지능,0.9901107,1.0,0.009889304637908936
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",잠시 휴식,-0.003944112,0.0,0.003944111987948418
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",좋아하는 아이돌,-0.001480041,0.0,0.0014800409553572536
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",핵심 아이디어,0.0009170101,0.0,0.0009170101257041097
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",확률 예측에서 MSE Loss 미 사용 이유,0.00047013524,0.0,0.0004701352445408702
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",BCE 가 좋은 task,-0.0003885799,0.0,0.00038857990875840187
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지","BCE 가 좋은 task, 이유",-0.007275469,0.0,0.00727546913549304
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",LLM Fine-Tuning 의 PEFT,-0.0060299123,0.0,0.006029912270605564
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",LoRA,-0.0005856421,0.0,0.0005856421194039285
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",LoRA 와 QLoRA 의 차이,0.0015159501,0.0,0.0015159500762820244
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",Loss Function 예시,0.004234582,0.0,0.004234582185745239
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",Loss Function 정의,-0.008014725,0.0,0.008014724589884281
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",MBTI,-0.0031679564,0.0,0.003167956368997693
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",MSE Loss 설명,-0.0050360486,0.0,0.005036048591136932
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",MSE Loss 용도,-0.006734882,0.0,0.006734882015734911
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",Multi-Label 에서 CE + Softmax 적용 문제점,-0.0002992772,0.0,0.00029927719151601195
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",PEFT 방법 5가지,-0.0018119817,0.0,0.0018119816668331623
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",거대 언어 모델 정의,0.0011753247,0.0,0.0011753246653825045
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",기본 경험,-0.0073662973,0.0,0.007366297300904989
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",답변 실패,-0.0021351993,0.0,0.002135199261829257
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",딥러닝,-0.019346273,0.0,0.019346272572875023
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",마지막 할 말,0.0016730783,0.0,0.0016730782808735967
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",머신러닝,0.99194455,1.0,0.008055448532104492
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",면접 시작 인사,-0.0052837855,0.0,0.00528378551825881
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",상세 경험,0.002558907,0.0,0.002558907028287649
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",수식,-0.002443394,0.0,0.002443393925204873
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",인공지능,-0.018184315,0.0,0.018184315413236618
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",잠시 휴식,-0.0039583123,0.0,0.003958312328904867
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",좋아하는 아이돌,0.0040256684,0.0,0.004025668371468782
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",핵심 아이디어,-0.011140863,0.0,0.011140863411128521
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",확률 예측에서 MSE Loss 미 사용 이유,0.0026676587,0.0,0.002667658729478717
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",BCE 가 좋은 task,-0.00016217405,0.0,0.00016217405209317803
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?","BCE 가 좋은 task, 이유",-0.0055759666,0.0,0.0055759665556252
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",LLM Fine-Tuning 의 PEFT,-0.015606706,0.0,0.015606706030666828
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",LoRA,-0.007451065,0.0,0.007451064884662628
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",LoRA 와 QLoRA 의 차이,0.001588084,0.0,0.0015880840364843607
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",Loss Function 예시,0.0025198269,0.0,0.0025198268704116344
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",Loss Function 정의,-0.023685157,0.0,0.023685157299041748
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",MBTI,-0.00014299153,0.0,0.00014299152826424688
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",MSE Loss 설명,0.0007587902,0.0,0.0007587901782244444
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",MSE Loss 용도,0.00038459492,0.0,0.0003845949249807745
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",Multi-Label 에서 CE + Softmax 적용 문제점,-0.0025591638,0.0,0.0025591638404875994
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",PEFT 방법 5가지,-0.0074477354,0.0,0.007447735406458378
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",거대 언어 모델 정의,0.0031255377,0.0,0.0031255376525223255
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",기본 경험,-0.0064000576,0.0,0.006400057580322027
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",답변 실패,-0.0032204615,0.0,0.00322046154178679
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",딥러닝,0.9918827,1.0,0.008117318153381348
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",마지막 할 말,0.002048139,0.0,0.0020481389947235584
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",머신러닝,-0.01457485,0.0,0.014574849978089333
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",면접 시작 인사,-0.010494541,0.0,0.010494541376829147
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",상세 경험,0.0035714756,0.0,0.003571475623175502
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",수식,-0.0060514407,0.0,0.0060514407232403755
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",인공지능,-0.01345984,0.0,0.01345983985811472
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",잠시 휴식,-0.005881589,0.0,0.005881588906049728
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",좋아하는 아이돌,-0.006930461,0.0,0.006930461153388023
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",핵심 아이디어,-0.0030887942,0.0,0.003088794182986021
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",확률 예측에서 MSE Loss 미 사용 이유,0.0046247137,0.0,0.0046247136779129505
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",BCE 가 좋은 task,0.0002589753,0.0,0.0002589753130450845
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야","BCE 가 좋은 task, 이유",-0.008360707,0.0,0.008360707201063633
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",LLM Fine-Tuning 의 PEFT,-0.014956053,0.0,0.014956053346395493
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",LoRA,-0.009964861,0.0,0.00996486097574234
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",LoRA 와 QLoRA 의 차이,-7.858943e-05,0.0,7.858942990424111e-05
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",Loss Function 예시,0.004449414,0.0,0.00444941408932209
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",Loss Function 정의,-0.02229074,0.0,0.02229074016213417
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",MBTI,0.00058037305,0.0,0.0005803730455227196
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",MSE Loss 설명,-0.0030412728,0.0,0.0030412727501243353
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",MSE Loss 용도,0.00021969357,0.0,0.0002196935674874112
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",Multi-Label 에서 CE + Softmax 적용 문제점,-0.0013933574,0.0,0.001393357408232987
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",PEFT 방법 5가지,-0.006145938,0.0,0.006145937833935022
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",거대 언어 모델 정의,0.0064063906,0.0,0.0064063905738294125
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",기본 경험,-0.0073204385,0.0,0.007320438511669636
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",답변 실패,-0.0034396616,0.0,0.003439661581069231
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",딥러닝,0.9914131,1.0,0.008586883544921875
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",마지막 할 말,0.00069345476,0.0,0.0006934547564014792
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",머신러닝,-0.005122116,0.0,0.005122115835547447
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",면접 시작 인사,-0.008967856,0.0,0.00896785594522953
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",상세 경험,-0.00095036806,0.0,0.0009503680630587041
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",수식,-0.008632186,0.0,0.008632185868918896
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",인공지능,-0.011348946,0.0,0.011348946020007133
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",잠시 휴식,-0.0064856224,0.0,0.006485622376203537
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",좋아하는 아이돌,-0.006152819,0.0,0.006152818910777569
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",핵심 아이디어,-0.0059557445,0.0,0.005955744534730911
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",확률 예측에서 MSE Loss 미 사용 이유,0.0009687274,0.0,0.0009687273995950818
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",BCE 가 좋은 task,-0.0025924928,0.0,0.00259249284863472
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?","BCE 가 좋은 task, 이유",-0.005188763,0.0,0.00518876314163208
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",LLM Fine-Tuning 의 PEFT,-0.007818875,0.0,0.007818874903023243
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",LoRA,-0.0023841353,0.0,0.002384135266765952
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",LoRA 와 QLoRA 의 차이,0.00069166935,0.0,0.0006916693528182805
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",Loss Function 예시,0.008288474,0.0,0.008288473822176456
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",Loss Function 정의,-0.00670609,0.0,0.0067060901783406734
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",MBTI,-0.0038159895,0.0,0.003815989475697279
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",MSE Loss 설명,-0.005073363,0.0,0.005073362961411476
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",MSE Loss 용도,-0.0073764734,0.0,0.007376473397016525
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",Multi-Label 에서 CE + Softmax 적용 문제점,0.0012162792,0.0,0.0012162792263552547
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",PEFT 방법 5가지,0.0005563558,0.0,0.0005563558079302311
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",거대 언어 모델 정의,-0.00058289623,0.0,0.0005828962312079966
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",기본 경험,-0.0069246693,0.0,0.00692466925829649
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",답변 실패,-0.0004432105,1.0,1.000443210505182
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",딥러닝,-0.019837713,0.0,0.01983771286904812
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",마지막 할 말,-0.0016058771,0.0,0.0016058770706877112
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",머신러닝,0.9919525,0.0,0.9919524788856506
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",면접 시작 인사,-0.0021908546,0.0,0.0021908546332269907
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",상세 경험,0.004034698,0.0,0.004034698009490967
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",수식,-0.002430102,0.0,0.002430102089419961
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",인공지능,-0.008872851,0.0,0.00887285079807043
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",잠시 휴식,-0.0038456188,0.0,0.0038456188049167395
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",좋아하는 아이돌,0.005364083,0.0,0.005364083219319582
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",핵심 아이디어,-0.0074935034,0.0,0.007493503391742706
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",확률 예측에서 MSE Loss 미 사용 이유,-0.00086620066,0.0,0.000866200658492744
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,BCE 가 좋은 task,-0.008178123,0.0,0.008178123272955418
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,"BCE 가 좋은 task, 이유",-0.005833642,0.0,0.005833642091602087
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,LLM Fine-Tuning 의 PEFT,-0.011555951,0.0,0.011555951088666916
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,LoRA,-0.0025459842,0.0,0.0025459842290729284
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,LoRA 와 QLoRA 의 차이,-0.0005905215,0.0,0.0005905214929953218
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,Loss Function 예시,0.00096355303,0.0,0.0009635530295781791
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,Loss Function 정의,0.003103056,0.0,0.003103055991232395
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,MBTI,0.0011304473,0.0,0.001130447257310152
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,MSE Loss 설명,0.0025167495,0.0,0.0025167495477944613
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,MSE Loss 용도,-0.0049124383,0.0,0.004912438336759806
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,Multi-Label 에서 CE + Softmax 적용 문제점,-0.008190877,0.0,0.008190876804292202
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,PEFT 방법 5가지,-0.000581189,0.0,0.0005811890005134046
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,거대 언어 모델 정의,0.9965699,1.0,0.0034301280975341797
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,기본 경험,-0.010021085,0.0,0.010021084919571877
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,답변 실패,-0.0025809754,0.0,0.002580975415185094
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,딥러닝,0.0024157413,0.0,0.002415741328150034
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,마지막 할 말,-0.0077822753,0.0,0.007782275322824717
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,머신러닝,-0.0024333256,0.0,0.002433325629681349
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,면접 시작 인사,-0.0027354355,0.0,0.0027354354970157146
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,상세 경험,-0.0047242492,0.0,0.004724249243736267
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,수식,0.006489056,0.0,0.006489056162536144
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,인공지능,-0.007427337,0.0,0.007427337113767862
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,잠시 휴식,0.00055228954,0.0,0.0005522895371541381
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,좋아하는 아이돌,-0.00827892,0.0,0.008278920315206051
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,핵심 아이디어,-0.0072486983,0.0,0.007248698268085718
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,확률 예측에서 MSE Loss 미 사용 이유,0.0034764106,0.0,0.003476410638540983
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,BCE 가 좋은 task,-0.0014966254,0.0,0.0014966253656893969
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,"BCE 가 좋은 task, 이유",-0.0017885619,0.0,0.0017885619308799505
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,LLM Fine-Tuning 의 PEFT,-0.00087060395,0.0,0.000870603951625526
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,LoRA,-0.006656382,0.0,0.006656381767243147
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,LoRA 와 QLoRA 의 차이,7.8795085e-05,0.0,7.879508484620601e-05
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,Loss Function 예시,-0.0034698055,0.0,0.0034698054660111666
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,Loss Function 정의,-0.002189084,0.0,0.002189083956182003
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,MBTI,0.00047533656,0.0,0.0004753365647047758
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,MSE Loss 설명,-0.005403497,0.0,0.005403496790677309
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,MSE Loss 용도,-0.0031065424,0.0,0.003106542397290468
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,Multi-Label 에서 CE + Softmax 적용 문제점,-0.0012083027,0.0,0.0012083026813343167
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,PEFT 방법 5가지,-0.0033110327,0.0,0.003311032662168145
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,거대 언어 모델 정의,0.0031787388,0.0,0.0031787387561053038
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,기본 경험,-0.002968931,0.0,0.0029689311049878597
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,답변 실패,0.99859905,1.0,0.0014009475708007812
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,딥러닝,-0.003750026,0.0,0.0037500259932130575
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,마지막 할 말,-0.0012428362,0.0,0.0012428362388163805
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,머신러닝,-0.0039110132,0.0,0.003911013249307871
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,면접 시작 인사,-0.004173446,0.0,0.0041734459809958935
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,상세 경험,0.0009545685,0.0,0.0009545685024932027
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,수식,-0.005452973,0.0,0.005452972836792469
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,인공지능,-0.0035486855,0.0,0.00354868546128273
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,잠시 휴식,-0.0025344214,0.0,0.00253442139364779
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,좋아하는 아이돌,-0.0022454225,0.0,0.002245422452688217
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,핵심 아이디어,-0.006145557,0.0,0.006145556923002005
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,확률 예측에서 MSE Loss 미 사용 이유,-0.0019290926,0.0,0.001929092570208013
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,BCE 가 좋은 task,-0.008188974,0.0,0.008188974112272263
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,"BCE 가 좋은 task, 이유",-0.006773066,0.0,0.006773065775632858
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,LLM Fine-Tuning 의 PEFT,0.0027915754,0.0,0.002791575388982892
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,LoRA,-0.0061533377,0.0,0.00615333765745163
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,LoRA 와 QLoRA 의 차이,-0.0051344605,0.0,0.005134460516273975
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,Loss Function 예시,0.00089276274,0.0,0.0008927627350203693
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,Loss Function 정의,0.99630696,0.0,0.9963069558143616
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,MBTI,-0.011801295,0.0,0.011801294982433319
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,MSE Loss 설명,-0.0034814186,0.0,0.0034814185928553343
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,MSE Loss 용도,-0.0030901046,0.0,0.003090104553848505
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,Multi-Label 에서 CE + Softmax 적용 문제점,-0.00406164,0.0,0.004061639774590731
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,PEFT 방법 5가지,-0.003258682,0.0,0.0032586820889264345
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,거대 언어 모델 정의,-0.00018656056,0.0,0.0001865605590865016
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,기본 경험,-0.0002010599,0.0,0.00020105989824514836
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,답변 실패,-0.0023011216,1.0,1.002301121596247
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,딥러닝,-0.024276927,0.0,0.02427692711353302
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,마지막 할 말,0.00853164,0.0,0.008531640283763409
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,머신러닝,-0.012596579,0.0,0.012596579268574715
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,면접 시작 인사,-0.0018614585,0.0,0.001861458527855575
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,상세 경험,-0.0048017018,0.0,0.004801701754331589
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,수식,0.0018197158,0.0,0.0018197158351540565
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,인공지능,0.0012708809,0.0,0.0012708809226751328
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,잠시 휴식,-0.002335038,0.0,0.002335038036108017
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,좋아하는 아이돌,-0.002635742,0.0,0.0026357420720160007
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,핵심 아이디어,0.009006638,0.0,0.009006638079881668
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,확률 예측에서 MSE Loss 미 사용 이유,0.011476632,0.0,0.011476632207632065
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",BCE 가 좋은 task,-0.009497918,0.0,0.00949791818857193
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!","BCE 가 좋은 task, 이유",-0.0004981655,0.0,0.0004981654928997159
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",LLM Fine-Tuning 의 PEFT,0.004739618,0.0,0.004739617928862572
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",LoRA,-0.0077150892,0.0,0.007715089246630669
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",LoRA 와 QLoRA 의 차이,-0.0032051676,0.0,0.003205167595297098
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",Loss Function 예시,-0.0022279974,0.0,0.0022279974073171616
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",Loss Function 정의,0.9934945,1.0,0.006505489349365234
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",MBTI,-0.010386162,0.0,0.01038616243749857
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",MSE Loss 설명,0.0031003184,0.0,0.003100318368524313
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",MSE Loss 용도,-0.0013479413,0.0,0.0013479413464665413
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",Multi-Label 에서 CE + Softmax 적용 문제점,-0.003354449,0.0,0.0033544490579515696
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",PEFT 방법 5가지,0.004832537,0.0,0.004832536913454533
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",거대 언어 모델 정의,-0.0018415869,0.0,0.0018415868980810046
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",기본 경험,0.001273222,0.0,0.0012732220347970724
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",답변 실패,-0.0043880865,0.0,0.004388086497783661
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",딥러닝,-0.03952955,0.0,0.039529550820589066
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",마지막 할 말,0.002026937,0.0,0.0020269369706511497
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",머신러닝,-0.009411974,0.0,0.009411973878741264
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",면접 시작 인사,-0.0017705077,0.0,0.0017705076606944203
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",상세 경험,-0.008496607,0.0,0.008496606722474098
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",수식,0.049303275,0.0,0.04930327460169792
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",인공지능,0.0019204499,0.0,0.0019204498967155814
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",잠시 휴식,-0.006556752,0.0,0.006556752137839794
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",좋아하는 아이돌,-0.009790561,0.0,0.00979056116193533
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",핵심 아이디어,-0.008676833,0.0,0.008676832541823387
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",확률 예측에서 MSE Loss 미 사용 이유,-0.0028760482,0.0,0.002876048209145665
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,BCE 가 좋은 task,-0.001945329,0.0,0.0019453290151432157
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,"BCE 가 좋은 task, 이유",-0.0013726407,0.0,0.0013726407196372747
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,LLM Fine-Tuning 의 PEFT,-0.0009271167,0.0,0.000927116721868515
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,LoRA,-0.005484415,0.0,0.005484415218234062
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,LoRA 와 QLoRA 의 차이,-0.00014362636,0.0,0.00014362635556608438
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,Loss Function 예시,0.008604836,0.0,0.008604835718870163
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,Loss Function 정의,-0.003984808,0.0,0.00398480799049139
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,MBTI,0.0027598413,0.0,0.0027598412707448006
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,MSE Loss 설명,-0.00431345,0.0,0.004313449840992689
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,MSE Loss 용도,-0.00022675823,0.0,0.00022675823129247874
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,Multi-Label 에서 CE + Softmax 적용 문제점,-0.0015329309,0.0,0.0015329308807849884
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,PEFT 방법 5가지,-0.0018051834,0.0,0.0018051833612844348
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,거대 언어 모델 정의,-0.004967468,0.0,0.004967467859387398
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,기본 경험,-0.0018042412,0.0,0.0018042412120848894
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,답변 실패,0.9984272,1.0,0.0015727877616882324
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,딥러닝,-0.004323809,0.0,0.004323808941990137
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,마지막 할 말,-0.0011061744,0.0,0.0011061744298785925
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,머신러닝,-0.0037516644,0.0,0.00375166442245245
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,면접 시작 인사,-0.0047491104,0.0,0.00474911043420434
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,상세 경험,-0.0017066998,0.0,0.0017066998407244682
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,수식,-0.0050896904,0.0,0.005089690443128347
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,인공지능,-0.0014341613,0.0,0.001434161327779293
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,잠시 휴식,-0.002599383,0.0,0.002599383005872369
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,좋아하는 아이돌,-0.0013764813,0.0,0.0013764812611043453
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,핵심 아이디어,-0.005226767,0.0,0.005226767156273127
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,확률 예측에서 MSE Loss 미 사용 이유,-0.0044832355,0.0,0.0044832355342805386
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",BCE 가 좋은 task,-0.00722768,0.0,0.007227680180221796
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지","BCE 가 좋은 task, 이유",-0.0032721215,0.0,0.003272121539339423
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",LLM Fine-Tuning 의 PEFT,-0.010118882,0.0,0.010118882171809673
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",LoRA,-0.0016643016,0.0,0.0016643016133457422
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",LoRA 와 QLoRA 의 차이,-0.002770888,0.0,0.0027708879206329584
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",Loss Function 예시,0.99708146,1.0,0.002918541431427002
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",Loss Function 정의,-0.0031276427,0.0,0.0031276426743716
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",MBTI,0.00532975,0.0,0.005329750012606382
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",MSE Loss 설명,-0.0016315696,0.0,0.0016315695829689503
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",MSE Loss 용도,-0.006260588,0.0,0.0062605878338217735
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",Multi-Label 에서 CE + Softmax 적용 문제점,-0.00086325256,0.0,0.0008632525568827987
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",PEFT 방법 5가지,-0.0065800147,0.0,0.00658001471310854
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",거대 언어 모델 정의,-0.0018745252,0.0,0.001874525216408074
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",기본 경험,-0.002501326,0.0,0.0025013259146362543
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",답변 실패,0.0012657467,0.0,0.0012657466577365994
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",딥러닝,-0.0006784809,0.0,0.0006784808938391507
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",마지막 할 말,0.00015031453,0.0,0.00015031453222036362
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",머신러닝,0.0007270431,0.0,0.0007270430796779692
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",면접 시작 인사,-0.0013409655,0.0,0.0013409655075520277
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",상세 경험,0.0054868856,0.0,0.00548688555136323
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",수식,-3.6381058e-05,0.0,3.6381057725520805e-05
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",인공지능,0.0010155666,0.0,0.0010155666386708617
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",잠시 휴식,0.0024570487,0.0,0.0024570487439632416
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",좋아하는 아이돌,0.001919129,0.0,0.001919129048474133
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",핵심 아이디어,-0.009872545,0.0,0.009872544556856155
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",확률 예측에서 MSE Loss 미 사용 이유,0.003727602,0.0,0.0037276020739227533
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,BCE 가 좋은 task,-0.011402649,0.0,0.011402648873627186
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,"BCE 가 좋은 task, 이유",-0.0062113088,0.0,0.006211308762431145
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,LLM Fine-Tuning 의 PEFT,0.0077094813,0.0,0.0077094812877476215
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,LoRA,-0.0010650468,0.0,0.0010650467593222857
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,LoRA 와 QLoRA 의 차이,-0.0010037712,0.0,0.001003771205432713
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,Loss Function 예시,-0.004199656,0.0,0.004199656192213297
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,Loss Function 정의,-0.0032554131,0.0,0.003255413146689534
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,MBTI,0.0018595074,0.0,0.0018595074070617557
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,MSE Loss 설명,0.9968246,1.0,0.00317537784576416
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,MSE Loss 용도,-0.002300058,0.0,0.002300058025866747
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,Multi-Label 에서 CE + Softmax 적용 문제점,-0.0017028128,0.0,0.0017028128495439887
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,PEFT 방법 5가지,-0.0013789438,0.0,0.0013789437944069505
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,거대 언어 모델 정의,0.0074310456,0.0,0.007431045640259981
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,기본 경험,-0.0033201904,0.0,0.003320190357044339
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,답변 실패,-0.0054195044,0.0,0.0054195043630898
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,딥러닝,0.0028983231,0.0,0.002898323116824031
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,마지막 할 말,-0.0024042053,0.0,0.0024042052682489157
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,머신러닝,-0.0035970125,0.0,0.0035970124881714582
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,면접 시작 인사,-0.0031875058,0.0,0.0031875057611614466
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,상세 경험,-0.018087061,0.0,0.018087061122059822
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,수식,-0.010129833,0.0,0.010129832662642002
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,인공지능,-0.004692167,0.0,0.004692167043685913
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,잠시 휴식,-0.0010370855,0.0,0.0010370855452492833
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,좋아하는 아이돌,-0.0013615788,0.0,0.0013615788193419576
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,핵심 아이디어,-0.0055460194,0.0,0.005546019412577152
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,확률 예측에서 MSE Loss 미 사용 이유,-0.002667285,0.0,0.0026672850362956524
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,BCE 가 좋은 task,-0.01121166,0.0,0.011211659759283066
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,"BCE 가 좋은 task, 이유",-0.0058639375,0.0,0.005863937549293041
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,LLM Fine-Tuning 의 PEFT,0.003986983,0.0,0.003986983094364405
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,LoRA,0.00075864955,0.0,0.0007586495485156775
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,LoRA 와 QLoRA 의 차이,0.0016495781,0.0,0.0016495781019330025
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,Loss Function 예시,-0.0052441102,0.0,0.005244110245257616
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,Loss Function 정의,-0.0006830276,0.0,0.0006830276106484234
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,MBTI,0.0030537874,0.0,0.0030537873972207308
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,MSE Loss 설명,0.99532,0.0,0.9953200221061707
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,MSE Loss 용도,-0.006501869,0.0,0.006501868832856417
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,Multi-Label 에서 CE + Softmax 적용 문제점,-0.0015738216,0.0,0.0015738216461613774
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,PEFT 방법 5가지,-0.0006818708,0.0,0.0006818707915954292
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,거대 언어 모델 정의,0.0060427026,0.0,0.006042702589184046
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,기본 경험,0.00047258602,0.0,0.0004725860198959708
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,답변 실패,-0.0013250351,1.0,1.001325035118498
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,딥러닝,-0.000104182516,0.0,0.00010418251622468233
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,마지막 할 말,-0.001987269,0.0,0.001987268915399909
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,머신러닝,-0.00232015,0.0,0.002320149913430214
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,면접 시작 인사,-0.0061437804,0.0,0.006143780425190926
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,상세 경험,-0.01427635,0.0,0.014276349917054176
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,수식,-0.011540476,0.0,0.011540476232767105
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,인공지능,-0.0043426557,0.0,0.004342655651271343
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,잠시 휴식,-0.0012551632,0.0,0.001255163224413991
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,좋아하는 아이돌,-0.0014771186,0.0,0.001477118581533432
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,핵심 아이디어,-0.0054861894,0.0,0.005486189387738705
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,확률 예측에서 MSE Loss 미 사용 이유,-0.0027683463,0.0,0.0027683463413268328
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",BCE 가 좋은 task,0.003251908,0.0,0.003251908114179969
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지","BCE 가 좋은 task, 이유",-0.011401988,0.0,0.011401987634599209
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",LLM Fine-Tuning 의 PEFT,-0.0074139484,0.0,0.00741394842043519
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",LoRA,-0.011870153,0.0,0.011870153248310089
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",LoRA 와 QLoRA 의 차이,-0.0042589223,0.0,0.004258922301232815
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",Loss Function 예시,-0.0058851726,0.0,0.005885172635316849
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",Loss Function 정의,-0.00082392676,0.0,0.0008239267626777291
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",MBTI,-0.0022911364,0.0,0.002291136421263218
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",MSE Loss 설명,-0.00786784,0.0,0.007867840118706226
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",MSE Loss 용도,0.99556327,1.0,0.0044367313385009766
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",Multi-Label 에서 CE + Softmax 적용 문제점,-0.006677694,0.0,0.0066776941530406475
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",PEFT 방법 5가지,-0.0048463764,0.0,0.004846376366913319
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",거대 언어 모델 정의,-0.00036091867,0.0,0.0003609186678659171
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",기본 경험,-0.007203272,0.0,0.007203272078186274
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",답변 실패,0.00031963695,0.0,0.00031963695073500276
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",딥러닝,0.008517152,0.0,0.008517151698470116
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",마지막 할 말,-0.0041273413,0.0,0.0041273413226008415
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",머신러닝,-0.005614255,0.0,0.005614255089312792
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",면접 시작 인사,-0.0016983332,0.0,0.0016983331879600883
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",상세 경험,-0.008872194,0.0,0.008872194215655327
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",수식,-0.0051231654,0.0,0.005123165436089039
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",인공지능,-0.0030830016,0.0,0.0030830015894025564
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",잠시 휴식,-0.00031242936,0.0,0.0003124293580185622
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",좋아하는 아이돌,-6.2706364e-05,0.0,6.270636367844418e-05
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",핵심 아이디어,0.0011972161,0.0,0.0011972161009907722
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",확률 예측에서 MSE Loss 미 사용 이유,-0.008827344,0.0,0.008827343583106995
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,BCE 가 좋은 task,-0.0018281159,0.0,0.0018281158991158009
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,"BCE 가 좋은 task, 이유",-0.0032225223,0.0,0.0032225223258137703
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,LLM Fine-Tuning 의 PEFT,-0.0015642652,0.0,0.0015642652288079262
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,LoRA,-0.0032128084,0.0,0.003212808398529887
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,LoRA 와 QLoRA 의 차이,-0.0016790614,0.0,0.001679061446338892
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,Loss Function 예시,-0.0021307238,0.0,0.0021307237911969423
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,Loss Function 정의,-0.0015709805,0.0,0.001570980530232191
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,MBTI,0.00091922184,0.0,0.0009192218421958387
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,MSE Loss 설명,-0.0025914772,0.0,0.0025914772413671017
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,MSE Loss 용도,0.0029467584,0.0,0.002946758409962058
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,Multi-Label 에서 CE + Softmax 적용 문제점,-0.0006967721,0.0,0.0006967721274122596
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,PEFT 방법 5가지,-0.0023273795,0.0,0.0023273795377463102
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,거대 언어 모델 정의,-0.0033487212,0.0,0.0033487211912870407
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,기본 경험,-0.0044674915,0.0,0.004467491526156664
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,답변 실패,0.99846274,1.0,0.0015372633934020996
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,딥러닝,-0.004653326,0.0,0.004653325770050287
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,마지막 할 말,-0.0013747792,0.0,0.0013747791526839137
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,머신러닝,-0.0040398124,0.0,0.004039812367409468
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,면접 시작 인사,-0.0039689923,0.0,0.00396899227052927
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,상세 경험,-0.0005360692,0.0,0.0005360692157410085
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,수식,-0.0051203957,0.0,0.005120395682752132
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,인공지능,0.00013465885,0.0,0.00013465885422192514
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,잠시 휴식,-0.002067503,0.0,0.0020675030536949635
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,좋아하는 아이돌,-0.0018934506,0.0,0.001893450622446835
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,핵심 아이디어,-0.0056324475,0.0,0.005632447544485331
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,확률 예측에서 MSE Loss 미 사용 이유,-0.0025371828,0.0,0.002537182765081525
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,BCE 가 좋은 task,-0.0043661855,0.0,0.004366185516119003
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,"BCE 가 좋은 task, 이유",0.003243986,0.0,0.003243986051529646
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,LLM Fine-Tuning 의 PEFT,0.0025087094,0.0,0.002508709440007806
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,LoRA,-0.0047183735,0.0,0.004718373529613018
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,LoRA 와 QLoRA 의 차이,-0.0013691804,0.0,0.001369180390611291
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,Loss Function 예시,-0.002294822,0.0,0.002294821897521615
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,Loss Function 정의,-0.0009293557,0.0,0.0009293556795455515
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,MBTI,-0.0005965951,0.0,0.0005965951131656766
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,MSE Loss 설명,-0.009496028,0.0,0.00949602760374546
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,MSE Loss 용도,-0.0013573868,0.0,0.0013573868200182915
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,Multi-Label 에서 CE + Softmax 적용 문제점,0.0009772899,0.0,0.0009772898629307747
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,PEFT 방법 5가지,-0.001948457,0.0,0.0019484569784253836
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,거대 언어 모델 정의,-0.005368285,0.0,0.00536828488111496
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,기본 경험,-0.006932801,0.0,0.006932801101356745
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,답변 실패,0.992588,1.0,0.00741201639175415
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,딥러닝,-0.0006851481,0.0,0.000685148115735501
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,마지막 할 말,-0.0022648554,0.0,0.0022648554295301437
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,머신러닝,0.0015477404,0.0,0.0015477404231205583
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,면접 시작 인사,0.0011297463,0.0,0.001129746320657432
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,상세 경험,-0.0048286775,0.0,0.004828677512705326
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,수식,-0.005747593,0.0,0.005747593007981777
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,인공지능,0.005218816,0.0,0.005218815989792347
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,잠시 휴식,-0.0039009731,0.0,0.0039009731262922287
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,좋아하는 아이돌,-0.0072940765,0.0,0.00729407649487257
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,핵심 아이디어,-0.002494081,0.0,0.002494080923497677
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,확률 예측에서 MSE Loss 미 사용 이유,0.036042757,0.0,0.03604275733232498
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,BCE 가 좋은 task,0.0005633755,0.0,0.0005633754772134125
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,"BCE 가 좋은 task, 이유",-0.008429596,0.0,0.008429596200585365
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,LLM Fine-Tuning 의 PEFT,-0.007504345,0.0,0.007504344917833805
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,LoRA,-0.0058106757,0.0,0.005810675676912069
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,LoRA 와 QLoRA 의 차이,-0.007383659,0.0,0.007383659016340971
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,Loss Function 예시,0.0032746857,0.0,0.0032746857032179832
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,Loss Function 정의,0.0017403471,0.0,0.0017403471283614635
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,MBTI,-0.0010981753,0.0,0.0010981753002852201
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,MSE Loss 설명,0.0023844375,0.0,0.002384437480941415
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,MSE Loss 용도,-0.005865349,0.0,0.005865348968654871
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,Multi-Label 에서 CE + Softmax 적용 문제점,-0.0056979856,0.0,0.005697985645383596
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,PEFT 방법 5가지,-0.0039755483,0.0,0.003975548315793276
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,거대 언어 모델 정의,0.0022408029,0.0,0.0022408028598874807
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,기본 경험,-0.0038891255,0.0,0.0038891255389899015
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,답변 실패,-0.0005624375,0.0,0.0005624375189654529
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,딥러닝,0.00049575913,0.0,0.0004957591299898922
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,마지막 할 말,-0.0020325687,0.0,0.0020325686782598495
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,머신러닝,0.005817932,0.0,0.005817932076752186
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,면접 시작 인사,-0.0056046485,0.0,0.005604648496955633
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,상세 경험,-0.00040565635,0.0,0.0004056563484482467
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,수식,0.0026675651,0.0,0.002667565131559968
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,인공지능,0.0019010373,0.0,0.0019010372925549746
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,잠시 휴식,-0.007044793,0.0,0.007044793106615543
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,좋아하는 아이돌,-0.005597981,0.0,0.005597981158643961
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,핵심 아이디어,-0.0010022315,0.0,0.00100223149638623
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,확률 예측에서 MSE Loss 미 사용 이유,0.99727225,1.0,0.0027277469635009766
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,BCE 가 좋은 task,-0.010894739,0.0,0.01089473906904459
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,"BCE 가 좋은 task, 이유",-0.027384426,0.0,0.027384426444768906
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,LLM Fine-Tuning 의 PEFT,0.00036988154,0.0,0.0003698815417010337
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,LoRA,-0.0014756919,0.0,0.001475691911764443
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,LoRA 와 QLoRA 의 차이,0.0022157824,0.0,0.0022157824132591486
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,Loss Function 예시,0.0064714644,0.0,0.0064714644104242325
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,Loss Function 정의,-0.0041488805,0.0,0.004148880485445261
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,MBTI,0.0050470307,0.0,0.005047030746936798
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,MSE Loss 설명,-0.010827357,0.0,0.010827356949448586
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,MSE Loss 용도,0.0005607396,0.0,0.0005607396014966071
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,Multi-Label 에서 CE + Softmax 적용 문제점,-0.0039326735,0.0,0.00393267348408699
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,PEFT 방법 5가지,-0.014712204,0.0,0.014712204225361347
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,거대 언어 모델 정의,-0.0008661751,0.0,0.000866175105329603
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,기본 경험,-0.004917968,0.0,0.004917968064546585
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,답변 실패,0.10293892,0.0,0.10293892025947571
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,딥러닝,-0.013478895,0.0,0.013478894717991352
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,마지막 할 말,-0.0030998334,0.0,0.003099833382293582
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,머신러닝,-0.009824466,0.0,0.009824465960264206
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,면접 시작 인사,-0.008670482,0.0,0.008670481853187084
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,상세 경험,0.00033148905,0.0,0.0003314890491310507
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,수식,0.9796997,1.0,0.020300328731536865
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,인공지능,0.0014699233,0.0,0.0014699232997372746
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,잠시 휴식,-0.0051687346,0.0,0.005168734584003687
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,좋아하는 아이돌,0.0011438214,0.0,0.0011438213987275958
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,핵심 아이디어,-0.005281961,0.0,0.005281961057335138
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,확률 예측에서 MSE Loss 미 사용 이유,0.004745333,0.0,0.0047453329898417
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",BCE 가 좋은 task,-0.00088773144,0.0,0.0008877314394339919
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거","BCE 가 좋은 task, 이유",0.002186542,0.0,0.00218654191121459
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",LLM Fine-Tuning 의 PEFT,-0.0016806195,0.0,0.0016806195490062237
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",LoRA,0.0032305778,0.0,0.003230577800422907
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",LoRA 와 QLoRA 의 차이,-0.00042760232,0.0,0.0004276023246347904
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",Loss Function 예시,-0.009177755,0.0,0.009177754633128643
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",Loss Function 정의,-0.0046825875,0.0,0.004682587459683418
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",MBTI,-0.009923611,0.0,0.009923610836267471
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",MSE Loss 설명,-0.00046213195,0.0,0.00046213195309974253
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",MSE Loss 용도,-0.001910094,0.0,0.0019100940553471446
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",Multi-Label 에서 CE + Softmax 적용 문제점,-0.0025393358,0.0,0.002539335750043392
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",PEFT 방법 5가지,-0.0096449535,0.0,0.009644953534007072
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",거대 언어 모델 정의,-0.010899238,0.0,0.010899238288402557
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",기본 경험,-0.0031615295,0.0,0.0031615295447409153
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",답변 실패,-0.0043221586,0.0,0.0043221586383879185
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",딥러닝,-0.018956965,0.0,0.01895696483552456
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",마지막 할 말,-0.005248706,0.0,0.005248705856502056
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",머신러닝,-0.007312425,0.0,0.007312424946576357
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",면접 시작 인사,0.0018448532,0.0,0.001844853162765503
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",상세 경험,-0.005069584,0.0,0.005069584120064974
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",수식,-0.0054400847,0.0,0.005440084729343653
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",인공지능,0.00064591796,0.0,0.0006459179567173123
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",잠시 휴식,-0.0038719361,0.0,0.0038719361182302237
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",좋아하는 아이돌,0.0013661714,0.0,0.0013661714037880301
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",핵심 아이디어,0.99001426,1.0,0.009985744953155518
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",확률 예측에서 MSE Loss 미 사용 이유,-0.006709789,0.0,0.006709788925945759
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,BCE 가 좋은 task,0.0077126147,0.0,0.007712614722549915
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,"BCE 가 좋은 task, 이유",0.058024734,0.0,0.05802473425865173
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,LLM Fine-Tuning 의 PEFT,-0.01592119,0.0,0.01592119038105011
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,LoRA,0.048174642,0.0,0.04817464202642441
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,LoRA 와 QLoRA 의 차이,-0.01888409,0.0,0.018884090706706047
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,Loss Function 예시,0.0392961,0.0,0.03929610177874565
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,Loss Function 정의,-0.01961977,0.0,0.019619770348072052
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,MBTI,0.066278554,0.0,0.06627855449914932
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,MSE Loss 설명,0.19237794,0.0,0.1923779398202896
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,MSE Loss 용도,-0.040052142,0.0,0.0400521419942379
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,Multi-Label 에서 CE + Softmax 적용 문제점,0.07524325,0.0,0.07524324953556061
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,PEFT 방법 5가지,-0.045918375,0.0,0.04591837525367737
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,거대 언어 모델 정의,0.0071628876,0.0,0.00716288760304451
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,기본 경험,0.00058545056,0.0,0.0005854505579918623
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,답변 실패,0.5053132,1.0,0.4946867823600769
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,딥러닝,-0.019447904,0.0,0.01944790408015251
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,마지막 할 말,-0.04011076,0.0,0.0401107594370842
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,머신러닝,0.021404665,0.0,0.02140466496348381
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,면접 시작 인사,-0.034874022,0.0,0.034874022006988525
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,상세 경험,0.034499414,0.0,0.03449941426515579
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,수식,-0.034502868,0.0,0.034502867609262466
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,인공지능,-0.00033446666,0.0,0.00033446666202507913
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,잠시 휴식,-0.030591775,0.0,0.030591774731874466
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,좋아하는 아이돌,-0.026271503,0.0,0.026271503418684006
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,핵심 아이디어,0.49585924,0.0,0.4958592355251312
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,확률 예측에서 MSE Loss 미 사용 이유,-0.058070578,0.0,0.058070577681064606
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",BCE 가 좋은 task,-0.007602305,0.0,0.00760230515152216
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고","BCE 가 좋은 task, 이유",0.0071670464,0.0,0.007167046424001455
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",LLM Fine-Tuning 의 PEFT,-0.002561021,0.0,0.0025610208977013826
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",LoRA,0.003326513,0.0,0.0033265131060034037
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",LoRA 와 QLoRA 의 차이,-0.0002536508,0.0,0.0002536507963668555
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",Loss Function 예시,-0.0032558518,0.0,0.003255851799622178
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",Loss Function 정의,-4.5724195e-05,0.0,4.57241949334275e-05
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",MBTI,-0.00082262605,0.0,0.0008226260542869568
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",MSE Loss 설명,0.008731074,0.0,0.008731073699891567
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",MSE Loss 용도,0.002068322,0.0,0.002068321919068694
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",Multi-Label 에서 CE + Softmax 적용 문제점,-0.0055764937,0.0,0.005576493684202433
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",PEFT 방법 5가지,-0.010453135,0.0,0.010453134775161743
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",거대 언어 모델 정의,-0.010905762,0.0,0.010905762203037739
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",기본 경험,-0.0054324474,0.0,0.0054324474185705185
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",답변 실패,-0.0026537892,0.0,0.0026537892408668995
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",딥러닝,-0.013384292,0.0,0.013384291902184486
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",마지막 할 말,-0.0064766905,0.0,0.006476690527051687
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",머신러닝,-0.006135346,0.0,0.0061353459022939205
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",면접 시작 인사,-0.003490913,0.0,0.0034909129608422518
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",상세 경험,-0.009915404,0.0,0.00991540402173996
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",수식,-0.0014729556,0.0,0.0014729555696249008
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",인공지능,-0.0004031205,0.0,0.00040312050259672105
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",잠시 휴식,-0.006328013,0.0,0.00632801279425621
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",좋아하는 아이돌,-0.0032297948,0.0,0.003229794790968299
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",핵심 아이디어,0.9884651,1.0,0.011534929275512695
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",확률 예측에서 MSE Loss 미 사용 이유,-0.00851111,0.0,0.008511110208928585
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",BCE 가 좋은 task,0.2217199,0.0,0.2217199057340622
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!","BCE 가 좋은 task, 이유",-0.012555052,0.0,0.01255505159497261
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",LLM Fine-Tuning 의 PEFT,-0.005168793,0.0,0.0051687927916646
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",LoRA,-0.008872504,0.0,0.008872504346072674
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",LoRA 와 QLoRA 의 차이,-0.018697308,0.0,0.018697308376431465
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",Loss Function 예시,-0.023143115,0.0,0.023143114522099495
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",Loss Function 정의,-0.015314865,0.0,0.015314864926040173
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",MBTI,0.0036087378,0.0,0.003608737839385867
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",MSE Loss 설명,-0.018926965,0.0,0.018926965072751045
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",MSE Loss 용도,0.001876254,0.0,0.0018762539839372039
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",Multi-Label 에서 CE + Softmax 적용 문제점,-0.0010206007,0.0,0.001020600670017302
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",PEFT 방법 5가지,-0.0145196505,0.0,0.014519650489091873
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",거대 언어 모델 정의,-0.004330841,0.0,0.004330840893089771
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",기본 경험,0.010080584,0.0,0.010080584324896336
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",답변 실패,0.925449,1.0,0.07455098628997803
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",딥러닝,0.010409013,0.0,0.010409013368189335
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",마지막 할 말,-0.009150501,0.0,0.00915050134062767
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",머신러닝,0.00059526484,0.0,0.0005952648352831602
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",면접 시작 인사,-0.009979755,0.0,0.00997975468635559
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",상세 경험,0.01739631,0.0,0.017396310344338417
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",수식,-0.008639324,0.0,0.008639324456453323
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",인공지능,-0.010797169,0.0,0.010797169059515
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",잠시 휴식,-0.010782516,0.0,0.010782515630126
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",좋아하는 아이돌,0.0016309379,0.0,0.0016309379134327173
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",핵심 아이디어,0.008742087,0.0,0.008742086589336395
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",확률 예측에서 MSE Loss 미 사용 이유,-0.012306167,0.0,0.01230616681277752
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",BCE 가 좋은 task,-0.00013748335,1.0,1.000137483351864
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어","BCE 가 좋은 task, 이유",-0.0013091858,0.0,0.00130918575450778
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",LLM Fine-Tuning 의 PEFT,-0.0013984203,0.0,0.0013984203105792403
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",LoRA,-0.0042476323,0.0,0.004247632343322039
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",LoRA 와 QLoRA 의 차이,-0.0016726849,0.0,0.0016726849135011435
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",Loss Function 예시,-0.0011643441,0.0,0.0011643441393971443
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",Loss Function 정의,-0.0010259704,0.0,0.0010259704431518912
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",MBTI,0.002861096,0.0,0.0028610960580408573
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",MSE Loss 설명,-0.0033881709,0.0,0.003388170851394534
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",MSE Loss 용도,-0.0028824701,0.0,0.002882470143958926
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",Multi-Label 에서 CE + Softmax 적용 문제점,-0.0023733594,0.0,0.0023733593989163637
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",PEFT 방법 5가지,-0.0031427934,0.0,0.0031427934300154448
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",거대 언어 모델 정의,-0.0019443507,0.0,0.001944350660778582
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",기본 경험,-0.0029072743,0.0,0.002907274290919304
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",답변 실패,0.9981159,0.0,0.9981158971786499
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",딥러닝,-0.0040886006,0.0,0.004088600631803274
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",마지막 할 말,-0.0007104544,0.0,0.0007104544201865792
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",머신러닝,-0.004533545,0.0,0.004533545114099979
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",면접 시작 인사,-0.0055803647,0.0,0.005580364726483822
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",상세 경험,-0.0017448519,0.0,0.0017448519356548786
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",수식,-0.0034026208,0.0,0.0034026207868009806
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",인공지능,0.0006064017,0.0,0.0006064017070457339
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",잠시 휴식,-0.0024737539,0.0,0.0024737538769841194
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",좋아하는 아이돌,-0.0011623938,0.0,0.0011623938335105777
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",핵심 아이디어,-0.0035248816,0.0,0.0035248815547674894
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",확률 예측에서 MSE Loss 미 사용 이유,-0.0034647216,0.0,0.0034647216089069843
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",BCE 가 좋은 task,0.021014735,0.0,0.021014735102653503
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아","BCE 가 좋은 task, 이유",0.99210835,1.0,0.007891654968261719
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",LLM Fine-Tuning 의 PEFT,-0.004990297,0.0,0.00499029690399766
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",LoRA,-0.015739819,0.0,0.015739819034934044
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",LoRA 와 QLoRA 의 차이,-0.0015203936,0.0,0.0015203936491161585
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",Loss Function 예시,-0.001984898,0.0,0.0019848980009555817
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",Loss Function 정의,0.00018811274,0.0,0.00018811273912433535
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",MBTI,-0.0026409663,0.0,0.0026409663259983063
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",MSE Loss 설명,-0.0028176075,0.0,0.0028176074847579002
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",MSE Loss 용도,-0.009892448,0.0,0.009892447851598263
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",Multi-Label 에서 CE + Softmax 적용 문제점,-0.011003318,0.0,0.01100331824272871
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",PEFT 방법 5가지,-0.0021756794,0.0,0.0021756794303655624
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",거대 언어 모델 정의,-0.0064878343,0.0,0.006487834267318249
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",기본 경험,0.00317176,0.0,0.0031717598903924227
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",답변 실패,0.0003060472,0.0,0.0003060472081415355
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",딥러닝,0.0026483145,0.0,0.0026483144611120224
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",마지막 할 말,0.0021874951,0.0,0.002187495119869709
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",머신러닝,-0.0010482572,0.0,0.0010482572251930833
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",면접 시작 인사,-0.003227482,0.0,0.003227482084184885
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",상세 경험,-0.010866276,0.0,0.010866275988519192
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",수식,-0.007950625,0.0,0.007950625382363796
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",인공지능,-0.012175368,0.0,0.012175368145108223
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",잠시 휴식,-0.0009976402,0.0,0.0009976401925086975
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",좋아하는 아이돌,-0.0059381872,0.0,0.00593818724155426
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",핵심 아이디어,-0.0056412797,0.0,0.0056412797421216965
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",확률 예측에서 MSE Loss 미 사용 이유,-0.0071453536,0.0,0.007145353592932224
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,BCE 가 좋은 task,-0.005147427,0.0,0.005147426854819059
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,"BCE 가 좋은 task, 이유",-0.0068992698,0.0,0.006899269763380289
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,LLM Fine-Tuning 의 PEFT,-0.021022776,0.0,0.021022776141762733
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,LoRA,-1.0118509e-05,0.0,1.0118508726009168e-05
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,LoRA 와 QLoRA 의 차이,-0.0025650049,0.0,0.002565004862844944
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,Loss Function 예시,-0.0011101777,0.0,0.0011101777199655771
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,Loss Function 정의,-0.005871526,0.0,0.005871525965631008
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,MBTI,0.0031189772,0.0,0.0031189771834760904
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,MSE Loss 설명,0.0006654283,0.0,0.0006654282915405929
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,MSE Loss 용도,-0.008884072,0.0,0.008884072303771973
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,Multi-Label 에서 CE + Softmax 적용 문제점,0.9978668,1.0,0.002133190631866455
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,PEFT 방법 5가지,-0.0080340095,0.0,0.008034009486436844
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,거대 언어 모델 정의,-0.008034128,0.0,0.00803412776440382
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,기본 경험,-0.00264887,0.0,0.0026488699950277805
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,답변 실패,-0.0027092437,0.0,0.00270924367941916
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,딥러닝,-0.0028785546,0.0,0.002878554631024599
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,마지막 할 말,-0.0035286034,0.0,0.0035286033526062965
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,머신러닝,0.0040428084,0.0,0.004042808432132006
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,면접 시작 인사,-0.0024137832,0.0,0.002413783222436905
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,상세 경험,-0.004219873,0.0,0.004219872877001762
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,수식,0.00029534765,0.0,0.0002953476505354047
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,인공지능,-0.007875553,0.0,0.007875553332269192
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,잠시 휴식,-0.0015640489,0.0,0.0015640489291399717
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,좋아하는 아이돌,-0.009063468,0.0,0.00906346831470728
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,핵심 아이디어,-0.0034235015,0.0,0.003423501504585147
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,확률 예측에서 MSE Loss 미 사용 이유,-0.004372631,0.0,0.004372631199657917
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,BCE 가 좋은 task,-0.0007974028,0.0,0.0007974028121680021
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,"BCE 가 좋은 task, 이유",-0.0021885666,0.0,0.002188566606491804
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,LLM Fine-Tuning 의 PEFT,-0.0019548482,0.0,0.0019548481795936823
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,LoRA,-0.0051669455,0.0,0.005166945513337851
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,LoRA 와 QLoRA 의 차이,-0.0014745317,0.0,0.0014745317166671157
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,Loss Function 예시,-0.0021132487,0.0,0.002113248687237501
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,Loss Function 정의,-0.0017224067,0.0,0.00172240671236068
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,MBTI,0.00029216186,0.0,0.00029216185794211924
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,MSE Loss 설명,-0.0038248142,0.0,0.003824814222753048
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,MSE Loss 용도,-0.0026259986,0.0,0.0026259985752403736
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,Multi-Label 에서 CE + Softmax 적용 문제점,0.0024478484,0.0,0.002447848441079259
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,PEFT 방법 5가지,-0.0017408256,0.0,0.0017408255953341722
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,거대 언어 모델 정의,-0.0025165228,0.0,0.0025165227707475424
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,기본 경험,-0.0037232102,0.0,0.0037232101894915104
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,답변 실패,0.9984592,1.0,0.0015407800674438477
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,딥러닝,-0.002922741,0.0,0.002922740997746587
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,마지막 할 말,-0.0014820659,0.0,0.0014820658834651113
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,머신러닝,-0.0021605822,0.0,0.0021605822257697582
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,면접 시작 인사,-0.002692581,0.0,0.002692580921575427
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,상세 경험,-0.001585397,0.0,0.0015853970544412732
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,수식,-0.0062345224,0.0,0.006234522443264723
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,인공지능,6.0052538e-05,0.0,6.005253817420453e-05
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,잠시 휴식,-0.0026175044,0.0,0.002617504447698593
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,좋아하는 아이돌,-0.0017140803,0.0,0.0017140803392976522
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,핵심 아이디어,-0.0042721103,0.0,0.004272110294550657
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,확률 예측에서 MSE Loss 미 사용 이유,-0.0028036993,0.0,0.0028036993462592363
Loss Function 관련 실무 경험 -> 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,BCE 가 좋은 task,-0.013974873,0.0,0.013974873349070549
Loss Function 관련 실무 경험 -> 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,"BCE 가 좋은 task, 이유",-0.007608587,0.0,0.007608586922287941
Loss Function 관련 실무 경험 -> 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,LLM Fine-Tuning 의 PEFT,-0.0083988635,0.0,0.008398863486945629
Loss Function 관련 실무 경험 -> 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,LoRA,-0.0053798226,0.0,0.005379822570830584
Loss Function 관련 실무 경험 -> 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,LoRA 와 QLoRA 의 차이,-0.002229195,0.0,0.002229195088148117
Loss Function 관련 실무 경험 -> 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,Loss Function 예시,0.008788204,0.0,0.008788203820586205
Loss Function 관련 실무 경험 -> 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,Loss Function 정의,-0.0032611308,0.0,0.0032611307688057423
Loss Function 관련 실무 경험 -> 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,MBTI,-0.0055628717,0.0,0.005562871694564819
Loss Function 관련 실무 경험 -> 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,MSE Loss 설명,-0.004942666,0.0,0.004942665807902813
Loss Function 관련 실무 경험 -> 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,MSE Loss 용도,-0.012516627,0.0,0.012516627088189125
Loss Function 관련 실무 경험 -> 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,Multi-Label 에서 CE + Softmax 적용 문제점,-0.0050176797,0.0,0.005017679650336504
Loss Function 관련 실무 경험 -> 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,PEFT 방법 5가지,-0.0067440243,0.0,0.006744024343788624
Loss Function 관련 실무 경험 -> 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,거대 언어 모델 정의,-0.0029963134,0.0,0.0029963133856654167
Loss Function 관련 실무 경험 -> 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,기본 경험,-0.017280541,1.0,1.0172805413603783
Loss Function 관련 실무 경험 -> 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,답변 실패,-0.0006044301,0.0,0.000604430097155273
Loss Function 관련 실무 경험 -> 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,딥러닝,0.0032730398,0.0,0.003273039823397994
Loss Function 관련 실무 경험 -> 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,마지막 할 말,-0.0052430867,0.0,0.005243086721748114
Loss Function 관련 실무 경험 -> 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,머신러닝,-0.009185745,0.0,0.009185745380818844
Loss Function 관련 실무 경험 -> 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,면접 시작 인사,-0.008764208,0.0,0.008764208294451237
Loss Function 관련 실무 경험 -> 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,상세 경험,0.9870623,0.0,0.9870622754096985
Loss Function 관련 실무 경험 -> 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,수식,-0.0011650495,0.0,0.0011650494998320937
Loss Function 관련 실무 경험 -> 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,인공지능,-0.0036332973,0.0,0.0036332972813397646
Loss Function 관련 실무 경험 -> 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,잠시 휴식,0.00019104242,0.0,0.00019104241800960153
Loss Function 관련 실무 경험 -> 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,좋아하는 아이돌,-0.0036404363,0.0,0.0036404363345354795
Loss Function 관련 실무 경험 -> 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,핵심 아이디어,-0.011460121,0.0,0.011460120789706707
Loss Function 관련 실무 경험 -> 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,확률 예측에서 MSE Loss 미 사용 이유,0.0030787657,0.0,0.0030787657015025616
Loss Function 관련 실무 경험 -> 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,BCE 가 좋은 task,0.0067791813,0.0,0.006779181305319071
Loss Function 관련 실무 경험 -> 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,"BCE 가 좋은 task, 이유",0.00036714796,0.0,0.000367147964425385
Loss Function 관련 실무 경험 -> 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,LLM Fine-Tuning 의 PEFT,-0.016033938,0.0,0.01603393815457821
Loss Function 관련 실무 경험 -> 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,LoRA,-0.01676713,0.0,0.01676712930202484
Loss Function 관련 실무 경험 -> 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,LoRA 와 QLoRA 의 차이,0.00597964,0.0,0.005979639943689108
Loss Function 관련 실무 경험 -> 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,Loss Function 예시,-0.01069972,0.0,0.010699720121920109
Loss Function 관련 실무 경험 -> 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,Loss Function 정의,-0.019523392,0.0,0.019523391500115395
Loss Function 관련 실무 경험 -> 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,MBTI,-0.011818866,0.0,0.01181886624544859
Loss Function 관련 실무 경험 -> 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,MSE Loss 설명,-0.005851053,0.0,0.005851053167134523
Loss Function 관련 실무 경험 -> 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,MSE Loss 용도,-0.0072778603,0.0,0.007277860306203365
Loss Function 관련 실무 경험 -> 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,Multi-Label 에서 CE + Softmax 적용 문제점,0.0003034268,0.0,0.0003034267865587026
Loss Function 관련 실무 경험 -> 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,PEFT 방법 5가지,-0.016042858,0.0,0.016042858362197876
Loss Function 관련 실무 경험 -> 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,거대 언어 모델 정의,-0.005719836,0.0,0.0057198358699679375
Loss Function 관련 실무 경험 -> 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,기본 경험,0.97307986,0.0,0.9730798602104187
Loss Function 관련 실무 경험 -> 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,답변 실패,0.02494762,0.0,0.024947620928287506
Loss Function 관련 실무 경험 -> 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,딥러닝,-0.0014041988,0.0,0.0014041988179087639
Loss Function 관련 실무 경험 -> 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,마지막 할 말,-0.011742679,0.0,0.01174267940223217
Loss Function 관련 실무 경험 -> 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,머신러닝,0.005732627,0.0,0.005732627119868994
Loss Function 관련 실무 경험 -> 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,면접 시작 인사,-0.007853823,0.0,0.007853822782635689
Loss Function 관련 실무 경험 -> 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,상세 경험,0.032505702,1.0,0.967494297772646
Loss Function 관련 실무 경험 -> 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,수식,0.011516212,0.0,0.011516211554408073
Loss Function 관련 실무 경험 -> 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,인공지능,-0.0036376242,0.0,0.003637624206021428
Loss Function 관련 실무 경험 -> 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,잠시 휴식,-0.01683605,0.0,0.016836050897836685
Loss Function 관련 실무 경험 -> 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,좋아하는 아이돌,-0.006636942,0.0,0.006636941805481911
Loss Function 관련 실무 경험 -> 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,핵심 아이디어,-0.0032761912,0.0,0.003276191186159849
Loss Function 관련 실무 경험 -> 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,확률 예측에서 MSE Loss 미 사용 이유,-0.011411694,0.0,0.011411693878471851
Loss Function 관련 실무 경험 -> 근데 기억 진짜 안나는데 어떡하지?,BCE 가 좋은 task,-0.0012383976,0.0,0.0012383975554257631
Loss Function 관련 실무 경험 -> 근데 기억 진짜 안나는데 어떡하지?,"BCE 가 좋은 task, 이유",-0.001977099,0.0,0.0019770991057157516
Loss Function 관련 실무 경험 -> 근데 기억 진짜 안나는데 어떡하지?,LLM Fine-Tuning 의 PEFT,-0.001988252,0.0,0.0019882519263774157
Loss Function 관련 실무 경험 -> 근데 기억 진짜 안나는데 어떡하지?,LoRA,-0.005382196,0.0,0.005382196046411991
Loss Function 관련 실무 경험 -> 근데 기억 진짜 안나는데 어떡하지?,LoRA 와 QLoRA 의 차이,-0.00045511627,0.0,0.0004551162710413337
Loss Function 관련 실무 경험 -> 근데 기억 진짜 안나는데 어떡하지?,Loss Function 예시,-0.0010932969,0.0,0.0010932969162240624
Loss Function 관련 실무 경험 -> 근데 기억 진짜 안나는데 어떡하지?,Loss Function 정의,-0.0017730336,0.0,0.0017730336403474212
Loss Function 관련 실무 경험 -> 근데 기억 진짜 안나는데 어떡하지?,MBTI,0.0018455952,0.0,0.0018455951940268278
Loss Function 관련 실무 경험 -> 근데 기억 진짜 안나는데 어떡하지?,MSE Loss 설명,-0.0042618196,0.0,0.0042618196457624435
Loss Function 관련 실무 경험 -> 근데 기억 진짜 안나는데 어떡하지?,MSE Loss 용도,-0.00250365,0.0,0.0025036500301212072
Loss Function 관련 실무 경험 -> 근데 기억 진짜 안나는데 어떡하지?,Multi-Label 에서 CE + Softmax 적용 문제점,-0.0024788887,0.0,0.002478888723999262
Loss Function 관련 실무 경험 -> 근데 기억 진짜 안나는데 어떡하지?,PEFT 방법 5가지,-0.0023331817,0.0,0.0023331816773861647
Loss Function 관련 실무 경험 -> 근데 기억 진짜 안나는데 어떡하지?,거대 언어 모델 정의,-0.0017802881,0.0,0.0017802880611270666
Loss Function 관련 실무 경험 -> 근데 기억 진짜 안나는데 어떡하지?,기본 경험,-0.0025820145,0.0,0.002582014538347721
Loss Function 관련 실무 경험 -> 근데 기억 진짜 안나는데 어떡하지?,답변 실패,0.99882907,1.0,0.0011709332466125488
Loss Function 관련 실무 경험 -> 근데 기억 진짜 안나는데 어떡하지?,딥러닝,-0.003575561,0.0,0.0035755611024796963
Loss Function 관련 실무 경험 -> 근데 기억 진짜 안나는데 어떡하지?,마지막 할 말,-0.0011451239,0.0,0.0011451238533481956
Loss Function 관련 실무 경험 -> 근데 기억 진짜 안나는데 어떡하지?,머신러닝,-0.0032936072,0.0,0.0032936071511358023
Loss Function 관련 실무 경험 -> 근데 기억 진짜 안나는데 어떡하지?,면접 시작 인사,-0.0039763474,0.0,0.003976347390562296
Loss Function 관련 실무 경험 -> 근데 기억 진짜 안나는데 어떡하지?,상세 경험,0.0010077984,0.0,0.0010077983606606722
Loss Function 관련 실무 경험 -> 근데 기억 진짜 안나는데 어떡하지?,수식,-0.0033850977,0.0,0.0033850977197289467
Loss Function 관련 실무 경험 -> 근데 기억 진짜 안나는데 어떡하지?,인공지능,-0.0024741003,0.0,0.0024741003289818764
Loss Function 관련 실무 경험 -> 근데 기억 진짜 안나는데 어떡하지?,잠시 휴식,-0.003042905,0.0,0.003042904892936349
Loss Function 관련 실무 경험 -> 근데 기억 진짜 안나는데 어떡하지?,좋아하는 아이돌,-0.0017986394,0.0,0.0017986394232138991
Loss Function 관련 실무 경험 -> 근데 기억 진짜 안나는데 어떡하지?,핵심 아이디어,-0.0049000164,0.0,0.004900016356259584
Loss Function 관련 실무 경험 -> 근데 기억 진짜 안나는데 어떡하지?,확률 예측에서 MSE Loss 미 사용 이유,-0.0029757242,0.0,0.002975724171847105
Loss Function 관련 실무 경험 -> 나 아직 실무 경험 없는데 ㅠㅠ,BCE 가 좋은 task,-0.0018075353,0.0,0.0018075353000313044
Loss Function 관련 실무 경험 -> 나 아직 실무 경험 없는데 ㅠㅠ,"BCE 가 좋은 task, 이유",0.0031162861,0.0,0.003116286126896739
Loss Function 관련 실무 경험 -> 나 아직 실무 경험 없는데 ㅠㅠ,LLM Fine-Tuning 의 PEFT,-0.0027255164,0.0,0.0027255164459347725
Loss Function 관련 실무 경험 -> 나 아직 실무 경험 없는데 ㅠㅠ,LoRA,-0.0028007976,0.0,0.002800797577947378
Loss Function 관련 실무 경험 -> 나 아직 실무 경험 없는데 ㅠㅠ,LoRA 와 QLoRA 의 차이,0.003731491,0.0,0.003731491044163704
Loss Function 관련 실무 경험 -> 나 아직 실무 경험 없는데 ㅠㅠ,Loss Function 예시,-0.0038207916,0.0,0.00382079160772264
Loss Function 관련 실무 경험 -> 나 아직 실무 경험 없는데 ㅠㅠ,Loss Function 정의,-0.0012944523,0.0,0.001294452347792685
Loss Function 관련 실무 경험 -> 나 아직 실무 경험 없는데 ㅠㅠ,MBTI,-0.0026545883,0.0,0.0026545883156359196
Loss Function 관련 실무 경험 -> 나 아직 실무 경험 없는데 ㅠㅠ,MSE Loss 설명,-0.0044573173,0.0,0.004457317292690277
Loss Function 관련 실무 경험 -> 나 아직 실무 경험 없는데 ㅠㅠ,MSE Loss 용도,-0.0077577257,0.0,0.0077577256597578526
Loss Function 관련 실무 경험 -> 나 아직 실무 경험 없는데 ㅠㅠ,Multi-Label 에서 CE + Softmax 적용 문제점,-0.0018953027,0.0,0.0018953026738017797
Loss Function 관련 실무 경험 -> 나 아직 실무 경험 없는데 ㅠㅠ,PEFT 방법 5가지,-0.0052917995,0.0,0.005291799549013376
Loss Function 관련 실무 경험 -> 나 아직 실무 경험 없는데 ㅠㅠ,거대 언어 모델 정의,-0.007207881,0.0,0.007207881193608046
Loss Function 관련 실무 경험 -> 나 아직 실무 경험 없는데 ㅠㅠ,기본 경험,0.9965489,1.0,0.003451108932495117
Loss Function 관련 실무 경험 -> 나 아직 실무 경험 없는데 ㅠㅠ,답변 실패,-0.0035003547,0.0,0.0035003547091037035
Loss Function 관련 실무 경험 -> 나 아직 실무 경험 없는데 ㅠㅠ,딥러닝,-0.0026114904,0.0,0.0026114904321730137
Loss Function 관련 실무 경험 -> 나 아직 실무 경험 없는데 ㅠㅠ,마지막 할 말,-0.005956878,0.0,0.005956877954304218
Loss Function 관련 실무 경험 -> 나 아직 실무 경험 없는데 ㅠㅠ,머신러닝,-0.0061098267,0.0,0.006109826732426882
Loss Function 관련 실무 경험 -> 나 아직 실무 경험 없는데 ㅠㅠ,면접 시작 인사,-0.004245172,0.0,0.0042451717890799046
Loss Function 관련 실무 경험 -> 나 아직 실무 경험 없는데 ㅠㅠ,상세 경험,-0.018071273,0.0,0.01807127334177494
Loss Function 관련 실무 경험 -> 나 아직 실무 경험 없는데 ㅠㅠ,수식,-0.006388029,0.0,0.006388029083609581
Loss Function 관련 실무 경험 -> 나 아직 실무 경험 없는데 ㅠㅠ,인공지능,-0.0039505167,0.0,0.003950516693294048
Loss Function 관련 실무 경험 -> 나 아직 실무 경험 없는데 ㅠㅠ,잠시 휴식,-0.0032182364,0.0,0.00321823637932539
Loss Function 관련 실무 경험 -> 나 아직 실무 경험 없는데 ㅠㅠ,좋아하는 아이돌,-0.0048762485,0.0,0.00487624853849411
Loss Function 관련 실무 경험 -> 나 아직 실무 경험 없는데 ㅠㅠ,핵심 아이디어,-0.006437347,0.0,0.006437346804887056
Loss Function 관련 실무 경험 -> 나 아직 실무 경험 없는데 ㅠㅠ,확률 예측에서 MSE Loss 미 사용 이유,-0.0037894791,0.0,0.0037894791457802057
Loss Function 관련 실무 경험 -> 대답하기 싫은데 힝,BCE 가 좋은 task,-0.0006281252,0.0,0.0006281252135522664
Loss Function 관련 실무 경험 -> 대답하기 싫은데 힝,"BCE 가 좋은 task, 이유",-0.0015546196,0.0,0.0015546196373179555
Loss Function 관련 실무 경험 -> 대답하기 싫은데 힝,LLM Fine-Tuning 의 PEFT,-0.0020449536,0.0,0.00204495363868773
Loss Function 관련 실무 경험 -> 대답하기 싫은데 힝,LoRA,-0.004806007,0.0,0.004806006792932749
Loss Function 관련 실무 경험 -> 대답하기 싫은데 힝,LoRA 와 QLoRA 의 차이,-0.0008323252,0.0,0.0008323251968249679
Loss Function 관련 실무 경험 -> 대답하기 싫은데 힝,Loss Function 예시,-0.0016137575,0.0,0.0016137574566528201
Loss Function 관련 실무 경험 -> 대답하기 싫은데 힝,Loss Function 정의,-0.0020513318,0.0,0.002051331801339984
Loss Function 관련 실무 경험 -> 대답하기 싫은데 힝,MBTI,0.001776355,0.0,0.0017763549694791436
Loss Function 관련 실무 경험 -> 대답하기 싫은데 힝,MSE Loss 설명,-0.004762682,0.0,0.004762682132422924
Loss Function 관련 실무 경험 -> 대답하기 싫은데 힝,MSE Loss 용도,-0.0018151185,0.0,0.001815118477679789
Loss Function 관련 실무 경험 -> 대답하기 싫은데 힝,Multi-Label 에서 CE + Softmax 적용 문제점,-0.0026178944,0.0,0.0026178944390267134
Loss Function 관련 실무 경험 -> 대답하기 싫은데 힝,PEFT 방법 5가지,-0.003001962,0.0,0.0030019620899111032
Loss Function 관련 실무 경험 -> 대답하기 싫은데 힝,거대 언어 모델 정의,-0.00199628,0.0,0.001996279926970601
Loss Function 관련 실무 경험 -> 대답하기 싫은데 힝,기본 경험,-0.0031463956,0.0,0.0031463955529034138
Loss Function 관련 실무 경험 -> 대답하기 싫은데 힝,답변 실패,0.9989407,1.0,0.0010592937469482422
Loss Function 관련 실무 경험 -> 대답하기 싫은데 힝,딥러닝,-0.0026327246,0.0,0.0026327245868742466
Loss Function 관련 실무 경험 -> 대답하기 싫은데 힝,마지막 할 말,-0.0008251458,0.0,0.0008251458057202399
Loss Function 관련 실무 경험 -> 대답하기 싫은데 힝,머신러닝,-0.0028953687,0.0,0.002895368728786707
Loss Function 관련 실무 경험 -> 대답하기 싫은데 힝,면접 시작 인사,-0.0036476722,0.0,0.0036476722452789545
Loss Function 관련 실무 경험 -> 대답하기 싫은데 힝,상세 경험,-0.0003963586,0.0,0.0003963586059398949
Loss Function 관련 실무 경험 -> 대답하기 싫은데 힝,수식,-0.0041791163,0.0,0.00417911633849144
Loss Function 관련 실무 경험 -> 대답하기 싫은데 힝,인공지능,-0.0017866736,0.0,0.0017866735579445958
Loss Function 관련 실무 경험 -> 대답하기 싫은데 힝,잠시 휴식,-0.002543869,0.0,0.002543868962675333
Loss Function 관련 실무 경험 -> 대답하기 싫은데 힝,좋아하는 아이돌,-0.001990912,0.0,0.001990912016481161
Loss Function 관련 실무 경험 -> 대답하기 싫은데 힝,핵심 아이디어,-0.0043735593,0.0,0.004373559262603521
Loss Function 관련 실무 경험 -> 대답하기 싫은데 힝,확률 예측에서 MSE Loss 미 사용 이유,-0.0031251837,0.0,0.0031251837499439716
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,BCE 가 좋은 task,0.010807765,0.0,0.0108077647164464
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,"BCE 가 좋은 task, 이유",0.0018115264,0.0,0.0018115263665094972
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,LLM Fine-Tuning 의 PEFT,-0.013005349,0.0,0.013005348853766918
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,LoRA,-0.006566227,0.0,0.006566226948052645
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,LoRA 와 QLoRA 의 차이,-0.0023325393,0.0,0.0023325392976403236
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,Loss Function 예시,0.0058927415,0.0,0.005892741493880749
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,Loss Function 정의,-0.0073737497,0.0,0.007373749744147062
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,MBTI,0.9934714,1.0,0.006528615951538086
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,MSE Loss 설명,0.0031933847,0.0,0.0031933847349137068
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,MSE Loss 용도,0.0039736517,0.0,0.003973651677370071
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,Multi-Label 에서 CE + Softmax 적용 문제점,-0.0028443304,0.0,0.0028443303890526295
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,PEFT 방법 5가지,-0.0013766867,0.0,0.0013766867341473699
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,거대 언어 모델 정의,-0.0045898682,0.0,0.004589868243783712
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,기본 경험,0.0005676052,0.0,0.0005676051951013505
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,답변 실패,0.001057605,0.0,0.0010576050262898207
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,딥러닝,-0.00248224,0.0,0.0024822400882840157
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,마지막 할 말,0.00013406079,0.0,0.0001340607850579545
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,머신러닝,-0.003463915,0.0,0.0034639150835573673
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,면접 시작 인사,-0.0065314053,0.0,0.006531405262649059
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,상세 경험,-0.0039792582,0.0,0.003979258239269257
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,수식,0.005863501,0.0,0.005863501224666834
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,인공지능,-0.01216917,0.0,0.012169170193374157
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,잠시 휴식,-0.008018335,0.0,0.008018335327506065
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,좋아하는 아이돌,0.005713898,0.0,0.0057138982228934765
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,핵심 아이디어,-0.00022923556,0.0,0.00022923556389287114
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,확률 예측에서 MSE Loss 미 사용 이유,-0.0040588337,0.0,0.004058833699673414
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,BCE 가 좋은 task,-0.00035519982,0.0,0.00035519982338882983
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,"BCE 가 좋은 task, 이유",-0.0043729655,0.0,0.004372965544462204
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,LLM Fine-Tuning 의 PEFT,0.00342539,0.0,0.0034253899939358234
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,LoRA,-0.00060741283,0.0,0.0006074128323234618
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,LoRA 와 QLoRA 의 차이,0.008006793,0.0,0.008006793446838856
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,Loss Function 예시,0.0004153685,0.0,0.00041536850039847195
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,Loss Function 정의,-0.0012393477,0.0,0.0012393477372825146
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,MBTI,0.0022317113,0.0,0.0022317112889140844
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,MSE Loss 설명,0.0030213378,0.0,0.003021337790414691
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,MSE Loss 용도,-0.007543405,0.0,0.0075434050522744656
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,Multi-Label 에서 CE + Softmax 적용 문제점,-0.005377864,0.0,0.005377863999456167
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,PEFT 방법 5가지,0.007086738,0.0,0.007086738012731075
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,거대 언어 모델 정의,-0.0023209797,0.0,0.0023209797218441963
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,기본 경험,-0.010877621,0.0,0.010877621360123158
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,답변 실패,-0.0033631572,0.0,0.003363157156854868
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,딥러닝,0.003775219,0.0,0.003775218967348337
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,마지막 할 말,-0.005434519,0.0,0.005434519145637751
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,머신러닝,-0.0005144098,0.0,0.0005144097958691418
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,면접 시작 인사,0.0016994285,0.0,0.0016994285397231579
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,상세 경험,0.000108782464,0.0,0.00010878246393986046
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,수식,-0.00019584097,0.0,0.00019584097026381642
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,인공지능,0.0022949064,0.0,0.0022949064150452614
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,잠시 휴식,0.0018845949,0.0,0.0018845949089154601
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,좋아하는 아이돌,0.99553657,1.0,0.0044634342193603516
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,핵심 아이디어,0.0023201567,0.0,0.00232015666551888
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,확률 예측에서 MSE Loss 미 사용 이유,-0.009678394,0.0,0.009678393602371216
잠시 휴식 -> 재미있는 이야기 해줄래?,BCE 가 좋은 task,0.0022681542,0.0,0.0022681541740894318
잠시 휴식 -> 재미있는 이야기 해줄래?,"BCE 가 좋은 task, 이유",-0.0030557935,0.0,0.0030557934660464525
잠시 휴식 -> 재미있는 이야기 해줄래?,LLM Fine-Tuning 의 PEFT,-5.1502077e-05,0.0,5.150207653059624e-05
잠시 휴식 -> 재미있는 이야기 해줄래?,LoRA,-0.0007647483,0.0,0.0007647483143955469
잠시 휴식 -> 재미있는 이야기 해줄래?,LoRA 와 QLoRA 의 차이,-0.008153802,0.0,0.008153801783919334
잠시 휴식 -> 재미있는 이야기 해줄래?,Loss Function 예시,0.0025050824,0.0,0.002505082404240966
잠시 휴식 -> 재미있는 이야기 해줄래?,Loss Function 정의,-0.003756138,0.0,0.0037561380304396152
잠시 휴식 -> 재미있는 이야기 해줄래?,MBTI,-0.00614818,0.0,0.006148179993033409
잠시 휴식 -> 재미있는 이야기 해줄래?,MSE Loss 설명,0.0019171939,0.0,0.0019171938765794039
잠시 휴식 -> 재미있는 이야기 해줄래?,MSE Loss 용도,-0.00033215116,0.0,0.0003321511612739414
잠시 휴식 -> 재미있는 이야기 해줄래?,Multi-Label 에서 CE + Softmax 적용 문제점,-0.002361498,0.0,0.002361498074606061
잠시 휴식 -> 재미있는 이야기 해줄래?,PEFT 방법 5가지,-0.0068613784,0.0,0.006861378438770771
잠시 휴식 -> 재미있는 이야기 해줄래?,거대 언어 모델 정의,-0.0026869564,0.0,0.0026869564317166805
잠시 휴식 -> 재미있는 이야기 해줄래?,기본 경험,-0.0005575912,0.0,0.0005575912073254585
잠시 휴식 -> 재미있는 이야기 해줄래?,답변 실패,-0.0035057794,0.0,0.003505779430270195
잠시 휴식 -> 재미있는 이야기 해줄래?,딥러닝,-0.0031047696,0.0,0.0031047696247696877
잠시 휴식 -> 재미있는 이야기 해줄래?,마지막 할 말,0.0028633515,0.0,0.0028633514884859324
잠시 휴식 -> 재미있는 이야기 해줄래?,머신러닝,0.00057559647,0.0,0.0005755964666604996
잠시 휴식 -> 재미있는 이야기 해줄래?,면접 시작 인사,-0.00017567821,0.0,0.00017567821487318724
잠시 휴식 -> 재미있는 이야기 해줄래?,상세 경험,0.0025740266,0.0,0.002574026584625244
잠시 휴식 -> 재미있는 이야기 해줄래?,수식,-0.0013863727,0.0,0.0013863727217540145
잠시 휴식 -> 재미있는 이야기 해줄래?,인공지능,-0.0018785339,0.0,0.0018785338615998626
잠시 휴식 -> 재미있는 이야기 해줄래?,잠시 휴식,0.99760324,1.0,0.0023967623710632324
잠시 휴식 -> 재미있는 이야기 해줄래?,좋아하는 아이돌,-0.00010857176,0.0,0.00010857175948331133
잠시 휴식 -> 재미있는 이야기 해줄래?,핵심 아이디어,-0.0034398504,0.0,0.0034398504067212343
잠시 휴식 -> 재미있는 이야기 해줄래?,확률 예측에서 MSE Loss 미 사용 이유,-0.005262231,0.0,0.005262230988591909
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",BCE 가 좋은 task,-0.00022499084,0.0,0.00022499084298033267
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야","BCE 가 좋은 task, 이유",-0.004373023,0.0,0.004373022820800543
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",LLM Fine-Tuning 의 PEFT,0.99472064,1.0,0.005279362201690674
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",LoRA,-0.016654948,0.0,0.01665494777262211
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",LoRA 와 QLoRA 의 차이,0.002005842,0.0,0.002005842048674822
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",Loss Function 예시,-0.009436577,0.0,0.009436576627194881
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",Loss Function 정의,-0.0019480505,0.0,0.001948050456121564
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",MBTI,-0.014309084,0.0,0.014309084042906761
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",MSE Loss 설명,0.0033252693,0.0,0.0033252693247050047
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",MSE Loss 용도,-0.009921736,0.0,0.00992173608392477
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",Multi-Label 에서 CE + Softmax 적용 문제점,-0.02027621,0.0,0.020276209339499474
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",PEFT 방법 5가지,-0.008809587,0.0,0.008809586986899376
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",거대 언어 모델 정의,-0.014237299,0.0,0.014237298630177975
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",기본 경험,-0.0012797796,0.0,0.0012797795934602618
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",답변 실패,-8.3939936e-05,0.0,8.393993630306795e-05
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",딥러닝,0.0003699903,0.0,0.0003699903027154505
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",마지막 할 말,-0.0024362556,0.0,0.002436255570501089
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",머신러닝,-0.008679591,0.0,0.008679591119289398
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",면접 시작 인사,0.0040073283,0.0,0.004007328301668167
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",상세 경험,-0.0019062266,0.0,0.001906226621940732
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",수식,-0.0058049117,0.0,0.005804911721497774
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",인공지능,0.005470893,0.0,0.005470892880111933
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",잠시 휴식,-0.004502327,0.0,0.0045023271813988686
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",좋아하는 아이돌,0.0007240273,0.0,0.0007240272825583816
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",핵심 아이디어,-0.005724971,0.0,0.005724971182644367
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",확률 예측에서 MSE Loss 미 사용 이유,-0.011016782,0.0,0.011016782373189926
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,BCE 가 좋은 task,-0.0015689862,0.0,0.0015689862193539739
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,"BCE 가 좋은 task, 이유",-0.0017909012,0.0,0.001790901180356741
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,LLM Fine-Tuning 의 PEFT,0.0029218725,0.0,0.002921872539445758
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,LoRA,-0.0039978107,0.0,0.003997810650616884
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,LoRA 와 QLoRA 의 차이,-0.0031290583,0.0,0.0031290582846850157
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,Loss Function 예시,0.0003725369,0.0,0.0003725368878804147
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,Loss Function 정의,-0.0020272695,0.0,0.0020272694528102875
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,MBTI,-0.0001453802,0.0,0.00014538019604515284
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,MSE Loss 설명,-0.009174088,0.0,0.009174088016152382
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,MSE Loss 용도,-0.0030173694,0.0,0.0030173694249242544
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,Multi-Label 에서 CE + Softmax 적용 문제점,-6.181549e-05,0.0,6.181548815220594e-05
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,PEFT 방법 5가지,-0.0010748948,0.0,0.0010748947970569134
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,거대 언어 모델 정의,-0.002660893,0.0,0.002660892903804779
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,기본 경험,-0.0028023543,0.0,0.002802354283630848
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,답변 실패,0.9980367,1.0,0.001963317394256592
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,딥러닝,-0.0050752764,0.0,0.005075276363641024
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,마지막 할 말,-0.0036967832,0.0,0.0036967832129448652
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,머신러닝,-0.0010134535,0.0,0.0010134534677490592
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,면접 시작 인사,-0.0047388165,0.0,0.004738816525787115
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,상세 경험,0.0017339219,0.0,0.0017339219339191914
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,수식,-0.005409809,0.0,0.0054098088294267654
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,인공지능,-0.0010025471,0.0,0.0010025470983237028
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,잠시 휴식,-0.0013214576,0.0,0.0013214575592428446
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,좋아하는 아이돌,-0.0025869878,0.0,0.0025869878008961678
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,핵심 아이디어,-0.003971815,0.0,0.00397181510925293
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,확률 예측에서 MSE Loss 미 사용 이유,-0.0051196357,0.0,0.005119635723531246
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",BCE 가 좋은 task,-0.005157205,0.0,0.005157204810529947
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?","BCE 가 좋은 task, 이유",0.0025450422,0.0,0.002545042196288705
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",LLM Fine-Tuning 의 PEFT,-0.009735227,0.0,0.00973522663116455
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",LoRA,-0.0038042178,0.0,0.003804217791184783
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",LoRA 와 QLoRA 의 차이,0.0065082223,0.0,0.0065082223154604435
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",Loss Function 예시,-0.0054012113,0.0,0.005401211325079203
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",Loss Function 정의,0.0013132474,0.0,0.001313247368671
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",MBTI,0.0019770437,0.0,0.001977043692022562
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",MSE Loss 설명,0.0023111133,0.0,0.00231111329048872
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",MSE Loss 용도,-0.010214819,0.0,0.010214818641543388
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",Multi-Label 에서 CE + Softmax 적용 문제점,-0.0048595727,0.0,0.004859572742134333
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",PEFT 방법 5가지,0.9952633,1.0,0.004736721515655518
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",거대 언어 모델 정의,0.00011653814,0.0,0.00011653813999146223
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",기본 경험,-3.540034e-05,0.0,3.5400338674662635e-05
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",답변 실패,-0.0037854924,0.0,0.0037854923866689205
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",딥러닝,-0.004679882,0.0,0.00467988196760416
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",마지막 할 말,-0.0037696348,0.0,0.0037696347571909428
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",머신러닝,-0.0011780127,0.0,0.0011780126951634884
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",면접 시작 인사,-0.001612678,0.0,0.0016126780537888408
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",상세 경험,0.0017391653,0.0,0.0017391652800142765
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",수식,-0.006715131,0.0,0.006715130992233753
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",인공지능,-0.005041727,0.0,0.005041726864874363
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",잠시 휴식,-0.0048783873,0.0,0.0048783873207867146
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",좋아하는 아이돌,0.0051374873,0.0,0.005137487314641476
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",핵심 아이디어,-0.006253578,0.0,0.006253577768802643
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",확률 예측에서 MSE Loss 미 사용 이유,-0.0012231547,0.0,0.0012231547152623534
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,BCE 가 좋은 task,-0.0023116497,0.0,0.0023116497322916985
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,"BCE 가 좋은 task, 이유",-0.0010039074,0.0,0.0010039074113592505
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,LLM Fine-Tuning 의 PEFT,-8.675437e-05,0.0,8.675437129568309e-05
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,LoRA,-0.004101323,0.0,0.004101322963833809
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,LoRA 와 QLoRA 의 차이,-0.00078734447,0.0,0.000787344470154494
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,Loss Function 예시,-0.00413567,0.0,0.004135670140385628
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,Loss Function 정의,-0.004608992,0.0,0.004608992021530867
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,MBTI,0.0032179134,0.0,0.003217913443222642
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,MSE Loss 설명,-0.006742882,0.0,0.006742882076650858
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,MSE Loss 용도,-0.002025175,0.0,0.0020251749083399773
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,Multi-Label 에서 CE + Softmax 적용 문제점,-0.001990312,0.0,0.001990312011912465
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,PEFT 방법 5가지,0.0047649867,0.0,0.00476498669013381
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,거대 언어 모델 정의,-0.0049433312,0.0,0.004943331237882376
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,기본 경험,-0.0016722662,0.0,0.001672266167588532
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,답변 실패,0.99827087,1.0,0.001729130744934082
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,딥러닝,-0.004125621,0.0,0.004125621169805527
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,마지막 할 말,-0.00090180937,0.0,0.0009018093696795404
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,머신러닝,-0.0018719733,0.0,0.0018719732761383057
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,면접 시작 인사,-0.0045362986,0.0,0.00453629856929183
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,상세 경험,-0.00084570603,0.0,0.0008457060321234167
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,수식,-0.004154217,0.0,0.004154216963797808
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,인공지능,-0.0026460805,0.0,0.0026460804510861635
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,잠시 휴식,-0.0017068145,0.0,0.0017068145098164678
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,좋아하는 아이돌,-0.0015053308,0.0,0.0015053307870402932
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,핵심 아이디어,-0.0047101066,0.0,0.004710106644779444
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,확률 예측에서 MSE Loss 미 사용 이유,-0.0019238732,0.0,0.0019238732056692243
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,BCE 가 좋은 task,-0.001482977,0.0,0.0014829769497737288
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,"BCE 가 좋은 task, 이유",-0.0054790364,0.0,0.0054790363647043705
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,LLM Fine-Tuning 의 PEFT,-0.013017289,0.0,0.013017289340496063
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,LoRA,0.9943425,1.0,0.005657494068145752
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,LoRA 와 QLoRA 의 차이,0.0013470373,0.0,0.0013470372650772333
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,Loss Function 예시,-0.0066562695,0.0,0.006656269542872906
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,Loss Function 정의,-0.0084887175,0.0,0.00848871748894453
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,MBTI,-0.0064021293,0.0,0.006402129307389259
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,MSE Loss 설명,-0.0041631334,0.0,0.004163133446127176
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,MSE Loss 용도,-0.015023239,0.0,0.015023238956928253
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,Multi-Label 에서 CE + Softmax 적용 문제점,0.00010884934,0.0,0.00010884933726629242
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,PEFT 방법 5가지,-0.005682215,0.0,0.005682215094566345
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,거대 언어 모델 정의,-0.0053888797,0.0,0.005388879682868719
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,기본 경험,-0.00063184026,0.0,0.0006318402593024075
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,답변 실패,-0.0054197223,0.0,0.00541972229257226
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,딥러닝,0.0012300237,0.0,0.0012300236849114299
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,마지막 할 말,0.00062302314,0.0,0.0006230231374502182
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,머신러닝,-0.004863234,0.0,0.004863233771175146
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,면접 시작 인사,-0.0027160516,0.0,0.002716051647439599
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,상세 경험,-0.009163295,0.0,0.009163294918835163
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,수식,-0.005063025,0.0,0.005063024815171957
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,인공지능,-0.0056708846,0.0,0.005670884624123573
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,잠시 휴식,0.00054056104,0.0,0.0005405610427260399
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,좋아하는 아이돌,-0.0072775753,0.0,0.007277575321495533
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,핵심 아이디어,-0.0009285939,0.0,0.000928593915887177
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,확률 예측에서 MSE Loss 미 사용 이유,-0.005274929,0.0,0.005274929106235504
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,BCE 가 좋은 task,-0.0017862238,0.0,0.0017862238455563784
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,"BCE 가 좋은 task, 이유",-0.0031234403,0.0,0.0031234403140842915
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,LLM Fine-Tuning 의 PEFT,-0.0011211524,0.0,0.001121152425184846
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,LoRA,-0.0033925346,0.0,0.003392534563317895
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,LoRA 와 QLoRA 의 차이,-0.001791722,0.0,0.0017917220247909427
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,Loss Function 예시,-0.0006587137,0.0,0.000658713688608259
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,Loss Function 정의,-0.0029593273,0.0,0.002959327306598425
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,MBTI,0.00084734685,0.0,0.0008473468478769064
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,MSE Loss 설명,-0.005102371,0.0,0.0051023708656430244
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,MSE Loss 용도,-0.001109446,0.0,0.0011094460496678948
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,Multi-Label 에서 CE + Softmax 적용 문제점,-0.0016835913,0.0,0.0016835912829264998
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,PEFT 방법 5가지,-0.0035332858,0.0,0.0035332858096808195
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,거대 언어 모델 정의,-0.0015961878,0.0,0.0015961878234520555
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,기본 경험,-0.0007071831,0.0,0.0007071830914355814
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,답변 실패,0.99873227,1.0,0.0012677311897277832
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,딥러닝,-0.0031256764,0.0,0.0031256764195859432
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,마지막 할 말,-0.0025041553,0.0,0.002504155272617936
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,머신러닝,-0.00178162,0.0,0.0017816199688240886
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,면접 시작 인사,-0.004336556,0.0,0.004336555954068899
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,상세 경험,-0.00020734566,0.0,0.00020734565623570234
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,수식,-0.004505736,0.0,0.004505735822021961
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,인공지능,-0.0017277754,0.0,0.0017277754377573729
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,잠시 휴식,-0.0020385606,0.0,0.002038560574874282
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,좋아하는 아이돌,-0.0026239112,0.0,0.0026239112485200167
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,핵심 아이디어,-0.004274802,0.0,0.004274801816791296
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,확률 예측에서 MSE Loss 미 사용 이유,-0.0021929832,0.0,0.0021929831709712744
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,BCE 가 좋은 task,-0.0069271442,0.0,0.00692714424803853
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,"BCE 가 좋은 task, 이유",-0.0023987198,0.0,0.0023987197782844305
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,LLM Fine-Tuning 의 PEFT,0.003270895,0.0,0.0032708949875086546
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,LoRA,-0.003164178,0.0,0.003164177993312478
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,LoRA 와 QLoRA 의 차이,0.9968359,1.0,0.0031641125679016113
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,Loss Function 예시,-0.002409489,0.0,0.002409488894045353
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,Loss Function 정의,-0.004964328,0.0,0.004964327905327082
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,MBTI,-0.002491411,0.0,0.002491411054506898
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,MSE Loss 설명,-0.0035815814,0.0,0.0035815814044326544
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,MSE Loss 용도,-0.009794035,0.0,0.009794034995138645
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,Multi-Label 에서 CE + Softmax 적용 문제점,0.002516613,0.0,0.00251661310903728
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,PEFT 방법 5가지,-0.0027688434,0.0,0.0027688434347510338
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,거대 언어 모델 정의,0.0005520024,0.0,0.000552002398762852
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,기본 경험,0.0016295059,0.0,0.0016295058885589242
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,답변 실패,-0.0041040643,0.0,0.0041040643118321896
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,딥러닝,0.0037430048,0.0,0.0037430047523230314
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,마지막 할 말,0.0029167137,0.0,0.002916713710874319
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,머신러닝,-0.0013757562,0.0,0.0013757562264800072
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,면접 시작 인사,-0.001236741,0.0,0.0012367409653961658
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,상세 경험,-0.001736428,0.0,0.0017364280065521598
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,수식,0.0042721406,0.0,0.004272140562534332
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,인공지능,-0.0018147308,0.0,0.0018147308146581054
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,잠시 휴식,-0.012678525,0.0,0.012678525410592556
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,좋아하는 아이돌,-0.0040501035,0.0,0.004050103481858969
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,핵심 아이디어,-0.006998526,0.0,0.006998525932431221
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,확률 예측에서 MSE Loss 미 사용 이유,-0.0025246993,0.0,0.002524699317291379
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,BCE 가 좋은 task,-0.0013510649,0.0,0.0013510648859664798
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,"BCE 가 좋은 task, 이유",-0.0012266524,0.0,0.0012266524136066437
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,LLM Fine-Tuning 의 PEFT,2.2464024e-05,0.0,2.2464024368673563e-05
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,LoRA,-0.004005697,0.0,0.004005697090178728
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,LoRA 와 QLoRA 의 차이,0.003977274,0.0,0.003977274056524038
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,Loss Function 예시,-0.0016162614,0.0,0.0016162614338099957
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,Loss Function 정의,-0.0018405201,0.0,0.0018405200680717826
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,MBTI,0.0006809717,0.0,0.0006809717160649598
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,MSE Loss 설명,-0.0031661263,0.0,0.0031661263201385736
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,MSE Loss 용도,-0.00053067575,0.0,0.0005306757520884275
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,Multi-Label 에서 CE + Softmax 적용 문제점,-0.0017246192,0.0,0.001724619185552001
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,PEFT 방법 5가지,-0.0021830157,0.0,0.0021830156911164522
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,거대 언어 모델 정의,-0.0050879573,0.0,0.005087957251816988
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,기본 경험,-0.002256227,0.0,0.002256226958706975
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,답변 실패,0.99841136,1.0,0.0015886425971984863
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,딥러닝,-0.003923902,0.0,0.0039239018224179745
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,마지막 할 말,-0.0026047085,0.0,0.0026047085411846638
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,머신러닝,-0.0026859667,0.0,0.002685966668650508
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,면접 시작 인사,-0.004399312,0.0,0.0043993121944367886
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,상세 경험,0.0003717224,0.0,0.0003717223880812526
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,수식,-0.004100722,0.0,0.004100721795111895
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,인공지능,-0.0010716871,0.0,0.001071687089279294
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,잠시 휴식,-0.002537539,0.0,0.0025375389959663153
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,좋아하는 아이돌,-0.001464646,0.0,0.001464645960368216
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,핵심 아이디어,-0.0056849713,0.0,0.00568497134372592
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,확률 예측에서 MSE Loss 미 사용 이유,-0.0018402898,0.0,0.001840289798565209
마지막 할 말 -> 로라야 정말 고마워!,BCE 가 좋은 task,-0.0045407517,0.0,0.004540751688182354
마지막 할 말 -> 로라야 정말 고마워!,"BCE 가 좋은 task, 이유",-0.00028012105,0.0,0.00028012105030938983
마지막 할 말 -> 로라야 정말 고마워!,LLM Fine-Tuning 의 PEFT,-0.0025912076,0.0,0.0025912076234817505
마지막 할 말 -> 로라야 정말 고마워!,LoRA,-0.0005757668,0.0,0.0005757667822763324
마지막 할 말 -> 로라야 정말 고마워!,LoRA 와 QLoRA 의 차이,0.0013561726,0.0,0.0013561726082116365
마지막 할 말 -> 로라야 정말 고마워!,Loss Function 예시,0.0015576731,0.0,0.0015576730947941542
마지막 할 말 -> 로라야 정말 고마워!,Loss Function 정의,0.006227671,0.0,0.006227671168744564
마지막 할 말 -> 로라야 정말 고마워!,MBTI,0.0031095245,0.0,0.003109524492174387
마지막 할 말 -> 로라야 정말 고마워!,MSE Loss 설명,-0.0044473805,0.0,0.004447380546480417
마지막 할 말 -> 로라야 정말 고마워!,MSE Loss 용도,-0.00018727378,0.0,0.00018727377755567431
마지막 할 말 -> 로라야 정말 고마워!,Multi-Label 에서 CE + Softmax 적용 문제점,-0.00672863,0.0,0.006728630047291517
마지막 할 말 -> 로라야 정말 고마워!,PEFT 방법 5가지,-7.667303e-05,0.0,7.667302998015657e-05
마지막 할 말 -> 로라야 정말 고마워!,거대 언어 모델 정의,-0.006741431,0.0,0.006741431076079607
마지막 할 말 -> 로라야 정말 고마워!,기본 경험,-0.007965933,0.0,0.007965932600200176
마지막 할 말 -> 로라야 정말 고마워!,답변 실패,-0.0024756235,0.0,0.00247562350705266
마지막 할 말 -> 로라야 정말 고마워!,딥러닝,0.0024456114,0.0,0.0024456114042550325
마지막 할 말 -> 로라야 정말 고마워!,마지막 할 말,0.9969082,1.0,0.0030918121337890625
마지막 할 말 -> 로라야 정말 고마워!,머신러닝,-0.0030879756,0.0,0.003087975550442934
마지막 할 말 -> 로라야 정말 고마워!,면접 시작 인사,0.0005612613,0.0,0.0005612613167613745
마지막 할 말 -> 로라야 정말 고마워!,상세 경험,-0.0039504613,0.0,0.003950461279600859
마지막 할 말 -> 로라야 정말 고마워!,수식,-0.008966139,0.0,0.00896613858640194
마지막 할 말 -> 로라야 정말 고마워!,인공지능,0.0025361252,0.0,0.002536125248298049
마지막 할 말 -> 로라야 정말 고마워!,잠시 휴식,-0.0012262585,0.0,0.0012262584641575813
마지막 할 말 -> 로라야 정말 고마워!,좋아하는 아이돌,-0.0029069283,0.0,0.0029069283045828342
마지막 할 말 -> 로라야 정말 고마워!,핵심 아이디어,-0.0033800097,0.0,0.0033800096716731787
마지막 할 말 -> 로라야 정말 고마워!,확률 예측에서 MSE Loss 미 사용 이유,-0.0019386399,0.0,0.0019386399071663618
마지막 할 말 -> 로라야 사랑해,BCE 가 좋은 task,-0.0069238143,0.0,0.006923814304172993
마지막 할 말 -> 로라야 사랑해,"BCE 가 좋은 task, 이유",-0.0003698132,0.0,0.0003698132059071213
마지막 할 말 -> 로라야 사랑해,LLM Fine-Tuning 의 PEFT,-0.0020855141,0.0,0.002085514133796096
마지막 할 말 -> 로라야 사랑해,LoRA,-0.0011248311,0.0,0.001124831149354577
마지막 할 말 -> 로라야 사랑해,LoRA 와 QLoRA 의 차이,0.0022033458,0.0,0.002203345764428377
마지막 할 말 -> 로라야 사랑해,Loss Function 예시,0.00091778644,0.0,0.0009177864412777126
마지막 할 말 -> 로라야 사랑해,Loss Function 정의,0.0041091046,0.0,0.0041091046296060085
마지막 할 말 -> 로라야 사랑해,MBTI,0.0063236775,0.0,0.006323677487671375
마지막 할 말 -> 로라야 사랑해,MSE Loss 설명,-0.0068492503,0.0,0.006849250290542841
마지막 할 말 -> 로라야 사랑해,MSE Loss 용도,0.00026418694,0.0,0.0002641869359649718
마지막 할 말 -> 로라야 사랑해,Multi-Label 에서 CE + Softmax 적용 문제점,-0.007885004,0.0,0.00788500439375639
마지막 할 말 -> 로라야 사랑해,PEFT 방법 5가지,-0.0004824273,0.0,0.0004824273055419326
마지막 할 말 -> 로라야 사랑해,거대 언어 모델 정의,-0.00660269,0.0,0.00660269008949399
마지막 할 말 -> 로라야 사랑해,기본 경험,-0.0074339975,0.0,0.007433997467160225
마지막 할 말 -> 로라야 사랑해,답변 실패,-0.0021199824,0.0,0.0021199823822826147
마지막 할 말 -> 로라야 사랑해,딥러닝,0.0012087061,0.0,0.001208706060424447
마지막 할 말 -> 로라야 사랑해,마지막 할 말,0.99692506,1.0,0.003074944019317627
마지막 할 말 -> 로라야 사랑해,머신러닝,-0.0020784775,0.0,0.0020784775260835886
마지막 할 말 -> 로라야 사랑해,면접 시작 인사,-0.0013568049,0.0,0.0013568048598244786
마지막 할 말 -> 로라야 사랑해,상세 경험,-0.006400606,0.0,0.006400606129318476
마지막 할 말 -> 로라야 사랑해,수식,-0.005770244,0.0,0.0057702441699802876
마지막 할 말 -> 로라야 사랑해,인공지능,0.0009898597,0.0,0.0009898596908897161
마지막 할 말 -> 로라야 사랑해,잠시 휴식,0.0008426308,0.0,0.0008426308049820364
마지막 할 말 -> 로라야 사랑해,좋아하는 아이돌,0.0045922305,0.0,0.0045922305434942245
마지막 할 말 -> 로라야 사랑해,핵심 아이디어,-0.0036429854,0.0,0.003642985364422202
마지막 할 말 -> 로라야 사랑해,확률 예측에서 MSE Loss 미 사용 이유,-0.0021388636,0.0,0.0021388635504990816
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,BCE 가 좋은 task,-0.0067981645,0.0,0.006798164453357458
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,"BCE 가 좋은 task, 이유",0.00024100381,0.0,0.00024100381415337324
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,LLM Fine-Tuning 의 PEFT,-0.0013395741,0.0,0.0013395741116255522
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,LoRA,-0.0012416114,0.0,0.0012416114332154393
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,LoRA 와 QLoRA 의 차이,0.0026960806,0.0,0.0026960805989801884
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,Loss Function 예시,-6.183523e-05,0.0,6.183522782521322e-05
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,Loss Function 정의,0.0067751803,0.0,0.006775180343538523
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,MBTI,0.0051576607,0.0,0.0051576606929302216
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,MSE Loss 설명,-0.0042912876,0.0,0.004291287623345852
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,MSE Loss 용도,-0.0017435583,0.0,0.0017435583285987377
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,Multi-Label 에서 CE + Softmax 적용 문제점,-0.0091990065,0.0,0.009199006482958794
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,PEFT 방법 5가지,-0.00047861,0.0,0.0004786099889315665
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,거대 언어 모델 정의,-0.0060696485,0.0,0.006069648545235395
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,기본 경험,-0.008236974,0.0,0.00823697354644537
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,답변 실패,-0.0022682934,0.0,0.0022682934068143368
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,딥러닝,0.0008052931,0.0,0.0008052930934354663
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,마지막 할 말,0.9957282,1.0,0.004271805286407471
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,머신러닝,-0.002862419,0.0,0.0028624190017580986
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,면접 시작 인사,-0.0002312478,0.0,0.0002312478027306497
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,상세 경험,-0.0069241775,0.0,0.006924177519977093
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,수식,-0.007168229,0.0,0.007168229203671217
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,인공지능,0.0022673395,0.0,0.002267339499667287
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,잠시 휴식,0.0013317685,0.0,0.0013317684642970562
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,좋아하는 아이돌,0.00038480948,0.0,0.00038480947841890156
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,핵심 아이디어,-0.0027689869,0.0,0.0027689868584275246
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,확률 예측에서 MSE Loss 미 사용 이유,-0.003514517,0.0,0.0035145170986652374
