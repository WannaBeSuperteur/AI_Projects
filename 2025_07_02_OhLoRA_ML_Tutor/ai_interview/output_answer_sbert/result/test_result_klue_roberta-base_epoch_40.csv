input_part,output_answer,predicted_score,ground_truth_score,absolute_error
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,BCE 가 좋은 task,-0.0036278546,0.0,0.0036278546322137117
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,BCE 가 좋은 이유,-0.0017897403,0.0,0.0017897402867674828
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,LLM Fine-Tuning 의 PEFT,-0.0036021085,0.0,0.0036021084524691105
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,LoRA,-0.0019244286,0.0,0.0019244286231696606
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,LoRA 와 QLoRA 의 차이,-0.0029406243,0.0,0.0029406242538243532
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,Loss Function 예시,-0.0018302663,0.0,0.001830266322940588
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,Loss Function 정의,-0.0033063532,0.0,0.0033063532318919897
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,MBTI,-0.003782279,0.0,0.0037822790909558535
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,MSE Loss 설명,-0.00507827,0.0,0.005078270100057125
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,MSE Loss 용도,-0.005700642,0.0,0.0057006417773664
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,Multi-Label 에서 CE + Softmax 적용 문제점,-0.00182055,0.0,0.0018205499509349465
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,PEFT 방법 5가지,-0.0027557106,0.0,0.0027557106222957373
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,거대 언어 모델 정의,-0.0051732413,0.0,0.005173241253942251
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,기본 경험,0.000545554,0.0,0.0005455539794638753
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,답변 실패,-0.0012903657,0.0,0.0012903657043352723
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,딥러닝,-0.0004272666,0.0,0.000427266611950472
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,마지막 할 말,-0.00011131295,0.0,0.00011131294741062447
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,머신러닝,-0.004981745,0.0,0.004981745034456253
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,면접 시작 인사,0.9988642,1.0,0.0011358261108398438
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,상세 경험,0.0006661997,0.0,0.0006661997176706791
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,수식,-0.0026376937,0.0,0.0026376936584711075
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,용어 질문,-0.003913065,0.0,0.003913064952939749
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,인공지능,-9.480568e-05,0.0,9.480567678110674e-05
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,잠시 휴식,-0.0032933734,0.0,0.003293373389169574
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,좋아하는 아이돌,-0.005839005,0.0,0.00583900511264801
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,핵심 아이디어,-0.0017958122,0.0,0.0017958121607080102
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,확률 예측에서 MSE Loss 미 사용 이유,-0.006233044,0.0,0.006233043968677521
면접 시작 인사 -> 로라야 안녕 정말 반가워,BCE 가 좋은 task,-0.002565437,0.0,0.0025654369965195656
면접 시작 인사 -> 로라야 안녕 정말 반가워,BCE 가 좋은 이유,-0.0013464366,0.0,0.0013464365620166063
면접 시작 인사 -> 로라야 안녕 정말 반가워,LLM Fine-Tuning 의 PEFT,-0.0041537113,0.0,0.0041537112556397915
면접 시작 인사 -> 로라야 안녕 정말 반가워,LoRA,-0.0013903871,0.0,0.0013903870712965727
면접 시작 인사 -> 로라야 안녕 정말 반가워,LoRA 와 QLoRA 의 차이,-0.0031907065,0.0,0.0031907064840197563
면접 시작 인사 -> 로라야 안녕 정말 반가워,Loss Function 예시,-0.0020442554,0.0,0.002044255379587412
면접 시작 인사 -> 로라야 안녕 정말 반가워,Loss Function 정의,-0.0021796825,0.0,0.0021796824876219034
면접 시작 인사 -> 로라야 안녕 정말 반가워,MBTI,-0.003739494,0.0,0.0037394938990473747
면접 시작 인사 -> 로라야 안녕 정말 반가워,MSE Loss 설명,-0.005415086,0.0,0.005415086168795824
면접 시작 인사 -> 로라야 안녕 정말 반가워,MSE Loss 용도,-0.0058821514,0.0,0.005882151424884796
면접 시작 인사 -> 로라야 안녕 정말 반가워,Multi-Label 에서 CE + Softmax 적용 문제점,-0.0029590758,0.0,0.0029590758495032787
면접 시작 인사 -> 로라야 안녕 정말 반가워,PEFT 방법 5가지,-0.0037826807,0.0,0.0037826807238161564
면접 시작 인사 -> 로라야 안녕 정말 반가워,거대 언어 모델 정의,-0.0041675754,0.0,0.004167575389146805
면접 시작 인사 -> 로라야 안녕 정말 반가워,기본 경험,0.00039213928,0.0,0.00039213927811942995
면접 시작 인사 -> 로라야 안녕 정말 반가워,답변 실패,-0.0010971216,0.0,0.0010971216252073646
면접 시작 인사 -> 로라야 안녕 정말 반가워,딥러닝,4.0966577e-05,0.0,4.096657721675001e-05
면접 시작 인사 -> 로라야 안녕 정말 반가워,마지막 할 말,0.0012483149,0.0,0.0012483148602768779
면접 시작 인사 -> 로라야 안녕 정말 반가워,머신러닝,-0.004609773,0.0,0.004609772935509682
면접 시작 인사 -> 로라야 안녕 정말 반가워,면접 시작 인사,0.99907035,1.0,0.0009296536445617676
면접 시작 인사 -> 로라야 안녕 정말 반가워,상세 경험,0.0016206239,0.0,0.0016206238651648164
면접 시작 인사 -> 로라야 안녕 정말 반가워,수식,-0.0023430085,0.0,0.00234300852753222
면접 시작 인사 -> 로라야 안녕 정말 반가워,용어 질문,-0.003982599,0.0,0.003982598893344402
면접 시작 인사 -> 로라야 안녕 정말 반가워,인공지능,-0.0010988417,0.0,0.0010988416615873575
면접 시작 인사 -> 로라야 안녕 정말 반가워,잠시 휴식,-0.003277691,0.0,0.0032776910811662674
면접 시작 인사 -> 로라야 안녕 정말 반가워,좋아하는 아이돌,-0.0053661093,0.0,0.005366109311580658
면접 시작 인사 -> 로라야 안녕 정말 반가워,핵심 아이디어,-0.0019471238,0.0,0.0019471237901598215
면접 시작 인사 -> 로라야 안녕 정말 반가워,확률 예측에서 MSE Loss 미 사용 이유,-0.005889699,0.0,0.00588969886302948
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,BCE 가 좋은 task,-0.003262477,0.0,0.003262476995587349
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,BCE 가 좋은 이유,-0.0018889852,0.0,0.0018889851635321975
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,LLM Fine-Tuning 의 PEFT,-0.0039968197,0.0,0.003996819723397493
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,LoRA,-0.0021401218,0.0,0.002140121767297387
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,LoRA 와 QLoRA 의 차이,-0.0026177005,0.0,0.0026177004911005497
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,Loss Function 예시,-0.0006730291,0.0,0.0006730291061103344
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,Loss Function 정의,-0.0023647326,0.0,0.0023647325579077005
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,MBTI,-0.005247377,0.0,0.00524737685918808
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,MSE Loss 설명,-0.004819543,0.0,0.0048195431008934975
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,MSE Loss 용도,-0.0056012333,0.0,0.005601233337074518
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,Multi-Label 에서 CE + Softmax 적용 문제점,-0.0021411283,0.0,0.002141128294169903
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,PEFT 방법 5가지,-0.0041615567,0.0,0.004161556717008352
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,거대 언어 모델 정의,-0.005026365,0.0,0.005026365164667368
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,기본 경험,0.00032699914,0.0,0.0003269991429988295
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,답변 실패,-0.000545178,0.0,0.0005451780161820352
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,딥러닝,0.0008594881,0.0,0.000859488092828542
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,마지막 할 말,-0.00036041,0.0,0.0003604099911171943
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,머신러닝,-0.005865447,0.0,0.005865447223186493
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,면접 시작 인사,0.9986426,1.0,0.0013573765754699707
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,상세 경험,0.0013526155,0.0,0.0013526155380532146
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,수식,-0.0029897636,0.0,0.0029897636268287897
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,용어 질문,-0.0037892761,0.0,0.0037892761174589396
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,인공지능,-0.0019440909,0.0,0.0019440909381955862
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,잠시 휴식,-0.0034663773,0.0,0.003466377267614007
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,좋아하는 아이돌,-0.005996564,0.0,0.00599656393751502
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,핵심 아이디어,-0.0025414503,0.0,0.0025414503179490566
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,확률 예측에서 MSE Loss 미 사용 이유,-0.00605297,0.0,0.006052969954907894
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,BCE 가 좋은 task,-0.0027368004,0.0,0.0027368003502488136
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,BCE 가 좋은 이유,-0.0027717685,0.0,0.0027717684861272573
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,LLM Fine-Tuning 의 PEFT,-0.0042756996,0.0,0.004275699611753225
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,LoRA,-0.0025322845,0.0,0.0025322844740003347
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,LoRA 와 QLoRA 의 차이,-0.0038398914,0.0,0.003839891403913498
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,Loss Function 예시,-0.0011530039,0.0,0.001153003890067339
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,Loss Function 정의,-0.001163525,0.0,0.00116352504119277
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,MBTI,-0.0057056188,0.0,0.005705618765205145
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,MSE Loss 설명,-0.005693773,0.0,0.005693772807717323
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,MSE Loss 용도,-0.005117288,0.0,0.0051172878593206406
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,Multi-Label 에서 CE + Softmax 적용 문제점,-0.0024439355,0.0,0.002443935489282012
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,PEFT 방법 5가지,-0.0034319358,0.0,0.0034319357946515083
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,거대 언어 모델 정의,-0.004536087,0.0,0.004536087159067392
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,기본 경험,0.00055228034,0.0,0.0005522803403437138
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,답변 실패,-3.76934e-05,0.0,3.7693400372518227e-05
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,딥러닝,-0.00094975065,0.0,0.000949750654399395
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,마지막 할 말,-0.00032137835,0.0,0.00032137834932655096
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,머신러닝,-0.0057084933,0.0,0.0057084932923316956
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,면접 시작 인사,0.9983514,1.0,0.0016486048698425293
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,상세 경험,0.0016562074,0.0,0.0016562073724344373
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,수식,-0.004401422,0.0,0.00440142210572958
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,용어 질문,-0.0023474102,0.0,0.0023474101908504963
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,인공지능,-0.0002004783,0.0,0.00020047830184921622
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,잠시 휴식,-0.003764524,0.0,0.0037645238917320967
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,좋아하는 아이돌,-0.0060570925,0.0,0.0060570924542844296
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,핵심 아이디어,-0.0028208876,0.0,0.002820887602865696
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,확률 예측에서 MSE Loss 미 사용 이유,-0.0070025115,0.0,0.007002511527389288
면접 시작 인사 -> 파이팅! 시작하자,BCE 가 좋은 task,-0.0031672684,0.0,0.003167268354445696
면접 시작 인사 -> 파이팅! 시작하자,BCE 가 좋은 이유,-0.0008467772,0.0,0.0008467772277072072
면접 시작 인사 -> 파이팅! 시작하자,LLM Fine-Tuning 의 PEFT,-0.0034937887,0.0,0.0034937886521220207
면접 시작 인사 -> 파이팅! 시작하자,LoRA,-0.0012167532,0.0,0.001216753153130412
면접 시작 인사 -> 파이팅! 시작하자,LoRA 와 QLoRA 의 차이,-0.0026517157,0.0,0.002651715651154518
면접 시작 인사 -> 파이팅! 시작하자,Loss Function 예시,-1.1207022e-05,0.0,1.1207021998416167e-05
면접 시작 인사 -> 파이팅! 시작하자,Loss Function 정의,-0.002782931,0.0,0.002782931085675955
면접 시작 인사 -> 파이팅! 시작하자,MBTI,-0.0029073595,0.0,0.002907359506934881
면접 시작 인사 -> 파이팅! 시작하자,MSE Loss 설명,-0.0054087,0.0,0.005408700089901686
면접 시작 인사 -> 파이팅! 시작하자,MSE Loss 용도,-0.0064209667,0.0,0.006420966703444719
면접 시작 인사 -> 파이팅! 시작하자,Multi-Label 에서 CE + Softmax 적용 문제점,-0.0023022932,0.0,0.002302293200045824
면접 시작 인사 -> 파이팅! 시작하자,PEFT 방법 5가지,-0.0024834275,0.0,0.0024834275245666504
면접 시작 인사 -> 파이팅! 시작하자,거대 언어 모델 정의,-0.0049375566,0.0,0.0049375565722584724
면접 시작 인사 -> 파이팅! 시작하자,기본 경험,0.00047942146,0.0,0.0004794214619323611
면접 시작 인사 -> 파이팅! 시작하자,답변 실패,-0.0010654952,0.0,0.001065495191141963
면접 시작 인사 -> 파이팅! 시작하자,딥러닝,-0.00030074507,0.0,0.0003007450723089278
면접 시작 인사 -> 파이팅! 시작하자,마지막 할 말,0.00052462483,0.0,0.000524624832905829
면접 시작 인사 -> 파이팅! 시작하자,머신러닝,-0.0038843772,0.0,0.0038843771908432245
면접 시작 인사 -> 파이팅! 시작하자,면접 시작 인사,0.9991041,1.0,0.0008959174156188965
면접 시작 인사 -> 파이팅! 시작하자,상세 경험,0.0013324709,0.0,0.00133247091434896
면접 시작 인사 -> 파이팅! 시작하자,수식,-0.0018470938,0.0,0.001847093808464706
면접 시작 인사 -> 파이팅! 시작하자,용어 질문,-0.003641743,0.0,0.003641742980107665
면접 시작 인사 -> 파이팅! 시작하자,인공지능,-0.001208021,0.0,0.0012080209562554955
면접 시작 인사 -> 파이팅! 시작하자,잠시 휴식,-0.003216023,0.0,0.003216023091226816
면접 시작 인사 -> 파이팅! 시작하자,좋아하는 아이돌,-0.0060897795,0.0,0.006089779548346996
면접 시작 인사 -> 파이팅! 시작하자,핵심 아이디어,-0.0012176636,0.0,0.0012176636373624206
면접 시작 인사 -> 파이팅! 시작하자,확률 예측에서 MSE Loss 미 사용 이유,-0.0066551715,0.0,0.006655171513557434
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",BCE 가 좋은 task,-0.0032593934,0.0,0.003259393386542797
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",BCE 가 좋은 이유,-0.0043479525,0.0,0.004347952548414469
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",LLM Fine-Tuning 의 PEFT,-0.002549307,0.0,0.002549306955188513
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",LoRA,-0.0030627484,0.0,0.0030627483502030373
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",LoRA 와 QLoRA 의 차이,-0.0018842806,0.0,0.0018842805875465274
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",Loss Function 예시,-0.0022411342,0.0,0.0022411341778934
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",Loss Function 정의,-0.0026158507,0.0,0.0026158506516367197
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",MBTI,-0.0018200572,0.0,0.0018200571648776531
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",MSE Loss 설명,-0.0020293274,0.0,0.002029327442869544
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",MSE Loss 용도,-0.0026661814,0.0,0.002666181419044733
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",Multi-Label 에서 CE + Softmax 적용 문제점,-0.002460175,0.0,0.002460174961015582
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",PEFT 방법 5가지,-0.003195713,0.0,0.0031957130413502455
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",거대 언어 모델 정의,-0.001106288,0.0,0.0011062880512326956
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",기본 경험,-0.0019519533,0.0,0.001951953279785812
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",답변 실패,0.9997433,1.0,0.0002567172050476074
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",딥러닝,-0.0016242856,0.0,0.0016242855926975608
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",마지막 할 말,-0.0031404598,0.0,0.003140459768474102
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",머신러닝,-0.0028634828,0.0,0.002863482804968953
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",면접 시작 인사,-0.0011338849,0.0,0.0011338848853483796
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",상세 경험,-0.0030798062,0.0,0.0030798062216490507
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",수식,-0.0029669015,0.0,0.002966901520267129
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",용어 질문,-0.00094977254,0.0,0.0009497725404798985
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",인공지능,0.00084713084,0.0,0.0008471308392472565
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",잠시 휴식,-0.004282854,0.0,0.004282854031771421
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",좋아하는 아이돌,-0.0011451944,0.0,0.0011451944010332227
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",핵심 아이디어,-0.0022093744,0.0,0.002209374448284507
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",확률 예측에서 MSE Loss 미 사용 이유,-0.0015307529,0.0,0.0015307528665289283
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",BCE 가 좋은 task,-0.0028305484,0.0,0.002830548444762826
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",BCE 가 좋은 이유,-0.0021732226,0.0,0.002173222601413727
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",LLM Fine-Tuning 의 PEFT,0.0004303901,0.0,0.00043039009324274957
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",LoRA,-0.0029174194,0.0,0.002917419420555234
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",LoRA 와 QLoRA 의 차이,-0.0019786325,0.0,0.001978632528334856
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",Loss Function 예시,-0.00092622463,0.0,0.0009262246312573552
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",Loss Function 정의,0.0020717874,0.0,0.0020717873703688383
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",MBTI,-0.001361094,0.0,0.0013610939495265484
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",MSE Loss 설명,0.0007401952,0.0,0.000740195217076689
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",MSE Loss 용도,0.006443055,0.0,0.006443054880946875
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",Multi-Label 에서 CE + Softmax 적용 문제점,0.00402402,0.0,0.004024019930511713
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",PEFT 방법 5가지,-0.0020339359,0.0,0.002033935859799385
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",거대 언어 모델 정의,-0.00028610337,0.0,0.0002861033717636019
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",기본 경험,-0.0016522943,0.0,0.0016522943042218685
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",답변 실패,-0.0010684484,0.0,0.0010684484150260687
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",딥러닝,-0.0020110623,0.0,0.0020110623445361853
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",마지막 할 말,-0.0049528466,0.0,0.004952846560627222
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",머신러닝,-0.007216737,0.0,0.007216737139970064
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",면접 시작 인사,-0.001536855,0.0,0.0015368550084531307
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",상세 경험,-0.0044453493,0.0,0.004445349331945181
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",수식,8.873849e-05,0.0,8.873848855728284e-05
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",용어 질문,-0.002799885,0.0,0.0027998851146548986
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",인공지능,0.9985228,1.0,0.0014771819114685059
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",잠시 휴식,-0.0009056089,0.0,0.000905608874745667
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",좋아하는 아이돌,-0.0023442544,0.0,0.0023442544043064117
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",핵심 아이디어,-0.003740348,0.0,0.003740347921848297
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",확률 예측에서 MSE Loss 미 사용 이유,0.0015895673,0.0,0.0015895672840997577
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",BCE 가 좋은 task,-0.0008328695,0.0,0.0008328694966621697
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",BCE 가 좋은 이유,0.001504586,0.0,0.0015045859618112445
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",LLM Fine-Tuning 의 PEFT,-0.0054903803,0.0,0.005490380339324474
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",LoRA,-0.007902853,0.0,0.007902853190898895
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",LoRA 와 QLoRA 의 차이,-0.00065015577,0.0,0.0006501557654701173
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",Loss Function 예시,-0.0040242933,0.0,0.004024293273687363
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",Loss Function 정의,-0.005964321,0.0,0.005964321084320545
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",MBTI,-0.0007170577,0.0,0.0007170576718635857
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",MSE Loss 설명,-0.0013639298,0.0,0.0013639298267662525
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",MSE Loss 용도,-0.0024623848,0.0,0.002462384756654501
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",Multi-Label 에서 CE + Softmax 적용 문제점,-0.0045350958,0.0,0.004535095766186714
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",PEFT 방법 5가지,0.00023489844,0.0,0.0002348984417039901
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",거대 언어 모델 정의,-0.0006051224,0.0,0.0006051224190741777
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",기본 경험,-0.00017387864,0.0,0.00017387863772455603
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",답변 실패,-0.00095103605,0.0,0.0009510360541753471
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",딥러닝,-0.004352179,0.0,0.004352178890258074
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",마지막 할 말,-0.0017009864,0.0,0.001700986409559846
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",머신러닝,0.9988819,1.0,0.0011181235313415527
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",면접 시작 인사,-0.0040665935,0.0,0.00406659347936511
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",상세 경험,-0.0004443999,0.0,0.00044439989142119884
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",수식,-0.0028774324,0.0,0.0028774323873221874
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",용어 질문,0.004085589,0.0,0.004085589200258255
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",인공지능,-0.0039488645,0.0,0.0039488645270466805
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",잠시 휴식,-0.003135248,0.0,0.003135248087346554
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",좋아하는 아이돌,-0.00044410318,0.0,0.00044410317786969244
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",핵심 아이디어,-0.0021961504,0.0,0.002196150366216898
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",확률 예측에서 MSE Loss 미 사용 이유,-0.0018658699,0.0,0.0018658698536455631
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",BCE 가 좋은 task,0.0017737503,0.0,0.0017737502930685878
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",BCE 가 좋은 이유,-0.0013792487,0.0,0.0013792486861348152
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",LLM Fine-Tuning 의 PEFT,0.0027420067,0.0,0.002742006676271558
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",LoRA,-0.00050026615,0.0,0.000500266149174422
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",LoRA 와 QLoRA 의 차이,-0.0029373732,0.0,0.0029373732395470142
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",Loss Function 예시,-0.0030856323,0.0,0.0030856323428452015
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",Loss Function 정의,9.705769e-06,0.0,9.705769116408192e-06
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",MBTI,-0.00030443183,0.0,0.000304431829135865
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",MSE Loss 설명,-0.005005463,0.0,0.0050054630264639854
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",MSE Loss 용도,-0.0025959392,0.0,0.0025959392078220844
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",Multi-Label 에서 CE + Softmax 적용 문제점,-0.0014842006,0.0,0.0014842005912214518
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",PEFT 방법 5가지,-0.0024715143,0.0,0.002471514279022813
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",거대 언어 모델 정의,-0.008702063,0.0,0.008702063001692295
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",기본 경험,-0.001706721,0.0,0.0017067210283130407
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",답변 실패,-0.0023853544,0.0,0.0023853543680161238
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",딥러닝,0.9987449,1.0,0.0012550950050354004
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",마지막 할 말,-0.0010977044,0.0,0.0010977044003084302
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",머신러닝,-0.0039021382,0.0,0.0039021382108330727
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",면접 시작 인사,-0.0010775634,0.0,0.0010775633854791522
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",상세 경험,-0.0006842783,0.0,0.0006842783186584711
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",수식,-0.0034070401,0.0,0.0034070401452481747
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",용어 질문,0.005948679,0.0,0.005948679056018591
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",인공지능,-0.0035118219,0.0,0.0035118218511343002
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",잠시 휴식,-0.004148137,0.0,0.0041481368243694305
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",좋아하는 아이돌,-0.0003010117,0.0,0.0003010116924997419
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",핵심 아이디어,-0.00228064,0.0,0.002280639950186014
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",확률 예측에서 MSE Loss 미 사용 이유,0.0020855803,0.0,0.0020855802576988935
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",BCE 가 좋은 task,0.0011929735,0.0,0.0011929734610021114
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",BCE 가 좋은 이유,-0.0009504757,0.0,0.0009504756890237331
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",LLM Fine-Tuning 의 PEFT,0.0019535662,0.0,0.001953566214069724
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",LoRA,0.00014567972,0.0,0.00014567971811629832
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",LoRA 와 QLoRA 의 차이,-0.0024350958,0.0,0.002435095841065049
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",Loss Function 예시,-0.0039750854,0.0,0.003975085448473692
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",Loss Function 정의,-0.0012556422,0.0,0.001255642157047987
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",MBTI,-0.0013510018,0.0,0.0013510017888620496
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",MSE Loss 설명,-0.007055585,0.0,0.0070555848069489
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",MSE Loss 용도,-0.0029218185,0.0,0.00292181852273643
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",Multi-Label 에서 CE + Softmax 적용 문제점,-0.0026657213,0.0,0.002665721345692873
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",PEFT 방법 5가지,-0.002636579,0.0,0.0026365790981799364
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",거대 언어 모델 정의,-0.008944607,0.0,0.008944607339799404
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",기본 경험,-0.0014662185,0.0,0.0014662184985354543
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",답변 실패,-0.0010551288,0.0,0.00105512875597924
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",딥러닝,0.9986388,1.0,0.0013611912727355957
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",마지막 할 말,-0.0010622037,0.0,0.0010622036643326283
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",머신러닝,-0.003482198,0.0,0.0034821981098502874
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",면접 시작 인사,6.1717044e-05,0.0,6.171704444568604e-05
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",상세 경험,0.0004224338,0.0,0.00042243380448780954
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",수식,-0.0039366363,0.0,0.003936636261641979
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",용어 질문,0.0060803085,0.0,0.006080308463424444
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",인공지능,-0.0018888747,0.0,0.0018888746853917837
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",잠시 휴식,-0.00460061,0.0,0.004600610118359327
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",좋아하는 아이돌,-0.00042723498,0.0,0.00042723497608676553
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",핵심 아이디어,-0.003963293,0.0,0.003963293042033911
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",확률 예측에서 MSE Loss 미 사용 이유,0.002442033,0.0,0.0024420330300927162
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",BCE 가 좋은 task,-0.0010154453,0.0,0.001015445333905518
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",BCE 가 좋은 이유,0.0017095736,0.0,0.0017095735529437661
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",LLM Fine-Tuning 의 PEFT,-0.0042177993,0.0,0.004217799287289381
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",LoRA,-0.0061464305,0.0,0.006146430503576994
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",LoRA 와 QLoRA 의 차이,-0.0030063563,0.0,0.0030063563026487827
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",Loss Function 예시,-0.0041025095,0.0,0.004102509468793869
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",Loss Function 정의,-0.0067722113,0.0,0.006772211287170649
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",MBTI,-0.0021554006,0.0,0.0021554005797952414
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",MSE Loss 설명,-0.0023311381,0.0,0.0023311381228268147
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",MSE Loss 용도,-0.0028288933,0.0,0.0028288932517170906
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",Multi-Label 에서 CE + Softmax 적용 문제점,-0.007580959,0.0,0.0075809587724506855
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",PEFT 방법 5가지,-0.0002554315,0.0,0.00025543151423335075
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",거대 언어 모델 정의,-0.001778833,0.0,0.0017788329860195518
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",기본 경험,-0.0015935764,0.0,0.0015935763949528337
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",답변 실패,0.0012091873,1.0,0.9987908126786351
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",딥러닝,-0.0030978352,0.0,0.0030978352297097445
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",마지막 할 말,-0.0026516088,0.0,0.002651608781889081
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",머신러닝,0.9980789,0.0,0.9980788826942444
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",면접 시작 인사,-0.002580245,0.0,0.0025802450254559517
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",상세 경험,0.00038243536,0.0,0.00038243536255322397
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",수식,-0.004160816,0.0,0.004160815849900246
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",용어 질문,0.0059542586,0.0,0.005954258609563112
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",인공지능,0.00083386595,0.0,0.0008338659536093473
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",잠시 휴식,-0.003746372,0.0,0.0037463719490915537
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",좋아하는 아이돌,-0.002928489,0.0,0.00292848888784647
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",핵심 아이디어,0.00065463077,0.0,0.0006546307704411447
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",확률 예측에서 MSE Loss 미 사용 이유,-0.0020191164,0.0,0.00201911642216146
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아",BCE 가 좋은 task,-0.0027692523,0.0,0.00276925228536129
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아",BCE 가 좋은 이유,-0.0040179086,0.0,0.004017908591777086
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아",LLM Fine-Tuning 의 PEFT,-0.0023171944,0.0,0.0023171943612396717
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아",LoRA,-0.0031011414,0.0,0.0031011414248496294
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아",LoRA 와 QLoRA 의 차이,-0.0019330565,0.0,0.0019330565119162202
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아",Loss Function 예시,-0.0020076795,0.0,0.002007679548114538
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아",Loss Function 정의,-0.0024578134,0.0,0.002457813359797001
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아",MBTI,-0.0018410976,0.0,0.001841097604483366
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아",MSE Loss 설명,-0.0018821162,0.0,0.001882116193883121
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아",MSE Loss 용도,-0.002899628,0.0,0.002899627899751067
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아",Multi-Label 에서 CE + Softmax 적용 문제점,-0.0024917142,0.0,0.0024917142000049353
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아",PEFT 방법 5가지,-0.0032098726,0.0,0.0032098726369440556
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아",거대 언어 모델 정의,-0.0007724067,0.0,0.0007724066963419318
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아",기본 경험,-0.0017194371,0.0,0.0017194370739161968
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아",답변 실패,0.9997742,1.0,0.0002257823944091797
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아",딥러닝,-0.0020197139,0.0,0.0020197138655930758
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아",마지막 할 말,-0.0031925417,0.0,0.003192541655153036
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아",머신러닝,-0.0026904182,0.0,0.0026904181577265263
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아",면접 시작 인사,-0.0008502224,0.0,0.0008502224227413535
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아",상세 경험,-0.0027854142,0.0,0.0027854142244905233
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아",수식,-0.0032845899,0.0,0.0032845898531377316
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아",용어 질문,-0.0008666973,0.0,0.0008666972862556577
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아",인공지능,0.00014971134,0.0,0.00014971134078223258
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아",잠시 휴식,-0.004256515,0.0,0.004256514832377434
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아",좋아하는 아이돌,-0.0011309546,0.0,0.0011309545952826738
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아",핵심 아이디어,-0.002113803,0.0,0.002113803057000041
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아",확률 예측에서 MSE Loss 미 사용 이유,-0.0013075554,0.0,0.0013075553579255939
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지?",BCE 가 좋은 task,-0.003470151,0.0,0.003470150986686349
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지?",BCE 가 좋은 이유,-0.0013914618,0.0,0.001391461817547679
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지?",LLM Fine-Tuning 의 PEFT,-0.00033597852,0.0,0.0003359785187058151
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지?",LoRA,-0.0023139068,0.0,0.002313906792551279
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지?",LoRA 와 QLoRA 의 차이,-0.002161713,0.0,0.002161713084205985
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지?",Loss Function 예시,-0.001178371,0.0,0.0011783710215240717
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지?",Loss Function 정의,0.002388534,0.0,0.002388533903285861
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지?",MBTI,-0.0013330191,0.0,0.001333019114099443
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지?",MSE Loss 설명,0.0007492532,0.0,0.0007492532022297382
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지?",MSE Loss 용도,0.0064973966,0.0,0.006497396621853113
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지?",Multi-Label 에서 CE + Softmax 적용 문제점,0.0041867,0.0,0.004186700098216534
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지?",PEFT 방법 5가지,-0.0017840156,0.0,0.0017840155633166432
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지?",거대 언어 모델 정의,0.00024514206,0.0,0.00024514205870218575
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지?",기본 경험,-0.0017551399,0.0,0.0017551399068906903
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지?",답변 실패,-0.0014337421,0.0,0.0014337421162053943
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지?",딥러닝,-0.0023597563,0.0,0.0023597562685608864
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지?",마지막 할 말,-0.0057616304,0.0,0.005761630367487669
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지?",머신러닝,-0.004841569,0.0,0.0048415688797831535
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지?",면접 시작 인사,-0.0021476604,0.0,0.002147660357877612
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지?",상세 경험,-0.0056713633,0.0,0.005671363323926926
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지?",수식,-0.0003310479,0.0,0.00033104789326898754
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지?",용어 질문,-0.002573778,0.0,0.002573777921497822
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지?",인공지능,0.9984209,1.0,0.0015791058540344238
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지?",잠시 휴식,-0.00025762638,0.0,0.0002576263796072453
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지?",좋아하는 아이돌,-0.0019159111,0.0,0.0019159110961481929
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지?",핵심 아이디어,-0.0058156406,0.0,0.0058156405575573444
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지?",확률 예측에서 MSE Loss 미 사용 이유,0.0016435152,0.0,0.0016435151919722557
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야!",BCE 가 좋은 task,-0.0009951399,0.0,0.0009951399406418204
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야!",BCE 가 좋은 이유,0.001612614,0.0,0.001612614025361836
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야!",LLM Fine-Tuning 의 PEFT,-0.0058503887,0.0,0.005850388668477535
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야!",LoRA,-0.0075209346,0.0,0.007520934566855431
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야!",LoRA 와 QLoRA 의 차이,-8.86425e-05,0.0,8.864249684847891e-05
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야!",Loss Function 예시,-0.004267646,0.0,0.004267645999789238
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야!",Loss Function 정의,-0.00616068,0.0,0.006160680204629898
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야!",MBTI,-0.00094641815,0.0,0.0009464181493967772
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야!",MSE Loss 설명,-0.0012447364,0.0,0.0012447363696992397
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야!",MSE Loss 용도,-0.0020497402,0.0,0.002049740171059966
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야!",Multi-Label 에서 CE + Softmax 적용 문제점,-0.004999379,0.0,0.00499937916174531
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야!",PEFT 방법 5가지,-9.461095e-05,0.0,9.461095032747835e-05
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야!",거대 언어 모델 정의,-0.0011077696,0.0,0.0011077695526182652
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야!",기본 경험,-0.00021429098,0.0,0.00021429097978398204
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야!",답변 실패,-0.0009503958,0.0,0.0009503958281129599
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야!",딥러닝,-0.003574413,0.0,0.003574413014575839
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야!",마지막 할 말,-0.0015645269,0.0,0.0015645269304513931
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야!",머신러닝,0.9988391,1.0,0.0011609196662902832
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야!",면접 시작 인사,-0.0043989206,0.0,0.004398920573294163
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야!",상세 경험,-0.00039749645,0.0,0.0003974964492954314
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야!",수식,-0.0017706028,0.0,0.001770602772012353
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야!",용어 질문,0.0040271534,0.0,0.004027153365314007
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야!",인공지능,-0.0042565055,0.0,0.004256505519151688
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야!",잠시 휴식,-0.002816885,0.0,0.0028168850112706423
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야!",좋아하는 아이돌,-0.00020160917,0.0,0.0002016091748373583
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야!",핵심 아이디어,-0.0023286284,0.0,0.0023286284413188696
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야!",확률 예측에서 MSE Loss 미 사용 이유,-0.0016596747,0.0,0.0016596746863797307
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지",BCE 가 좋은 task,0.0018439254,0.0,0.001843925449065864
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지",BCE 가 좋은 이유,-0.0014371858,0.0,0.0014371857978403568
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지",LLM Fine-Tuning 의 PEFT,0.0027002627,0.0,0.002700262703001499
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지",LoRA,-0.00051373173,0.0,0.0005137317348271608
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지",LoRA 와 QLoRA 의 차이,-0.0021391113,0.0,0.0021391112823039293
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지",Loss Function 예시,-0.002886314,0.0,0.002886313945055008
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지",Loss Function 정의,6.4502e-05,0.0,6.45019972580485e-05
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지",MBTI,-0.00039248247,0.0,0.00039248247048817575
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지",MSE Loss 설명,-0.004790269,0.0,0.004790268838405609
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지",MSE Loss 용도,-0.0026555138,0.0,0.0026555138174444437
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지",Multi-Label 에서 CE + Softmax 적용 문제점,-0.0016934905,0.0,0.0016934905434027314
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지",PEFT 방법 5가지,-0.003015676,0.0,0.00301567604765296
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지",거대 언어 모델 정의,-0.009032702,0.0,0.009032702073454857
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지",기본 경험,-0.0015658328,0.0,0.0015658327611163259
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지",답변 실패,-0.002151917,0.0,0.002151916967704892
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지",딥러닝,0.998735,1.0,0.0012649893760681152
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지",마지막 할 말,-0.0015999721,0.0,0.0015999721363186836
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지",머신러닝,-0.0037148658,0.0,0.0037148657720535994
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지",면접 시작 인사,-0.0006317762,0.0,0.0006317761726677418
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지",상세 경험,-0.0006098856,0.0,0.0006098856101743877
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지",수식,-0.0030837292,0.0,0.0030837291851639748
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지",용어 질문,0.006055595,0.0,0.006055594887584448
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지",인공지능,-0.0031400293,0.0,0.003140029264613986
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지",잠시 휴식,-0.0041777017,0.0,0.004177701659500599
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지",좋아하는 아이돌,-0.0003475524,0.0,0.0003475524135865271
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지",핵심 아이디어,-0.0033781093,0.0,0.003378109307959676
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지",확률 예측에서 MSE Loss 미 사용 이유,0.0021245785,0.0,0.002124578459188342
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야.",BCE 가 좋은 task,0.0012876483,0.0,0.001287648337893188
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야.",BCE 가 좋은 이유,-0.0015492648,0.0,0.0015492647653445601
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야.",LLM Fine-Tuning 의 PEFT,0.0027310909,0.0,0.0027310908772051334
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야.",LoRA,-0.0005817244,0.0,0.0005817243945784867
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야.",LoRA 와 QLoRA 의 차이,-0.0023400509,0.0,0.0023400508798658848
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야.",Loss Function 예시,-0.0029375816,0.0,0.0029375816229730844
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야.",Loss Function 정의,0.000108883156,0.0,0.00010888315591728315
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야.",MBTI,-0.00019750342,0.0,0.00019750342471525073
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야.",MSE Loss 설명,-0.0045288815,0.0,0.004528881516307592
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야.",MSE Loss 용도,-0.0021115541,0.0,0.0021115541458129883
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야.",Multi-Label 에서 CE + Softmax 적용 문제점,-0.00091537775,0.0,0.0009153777500614524
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야.",PEFT 방법 5가지,-0.002454289,0.0,0.0024542890023440123
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야.",거대 언어 모델 정의,-0.009154416,0.0,0.00915441568940878
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야.",기본 경험,-0.0017748409,0.0,0.0017748408718034625
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야.",답변 실패,-0.0025546597,0.0,0.0025546597316861153
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야.",딥러닝,0.9986521,1.0,0.0013478994369506836
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야.",마지막 할 말,-0.0021848185,0.0,0.002184818498790264
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야.",머신러닝,-0.002808641,0.0,0.002808640943840146
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야.",면접 시작 인사,-0.0010392854,0.0,0.001039285445585847
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야.",상세 경험,-0.0007094694,0.0,0.0007094693719409406
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야.",수식,-0.0036162566,0.0,0.0036162566393613815
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야.",용어 질문,0.0062959534,0.0,0.006295953411608934
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야.",인공지능,-0.0032157474,0.0,0.00321574741974473
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야.",잠시 휴식,-0.0042467806,0.0,0.004246780648827553
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야.",좋아하는 아이돌,-0.00013229164,0.0,0.0001322916359640658
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야.",핵심 아이디어,-0.0037990578,0.0,0.003799057798460126
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야.",확률 예측에서 MSE Loss 미 사용 이유,0.0021252104,0.0,0.0021252103615552187
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로",BCE 가 좋은 task,-0.0031416262,0.0,0.003141626249998808
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로",BCE 가 좋은 이유,-0.00445512,0.0,0.004455119837075472
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로",LLM Fine-Tuning 의 PEFT,-0.0021754114,0.0,0.002175411442294717
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로",LoRA,-0.0030620734,0.0,0.0030620733741670847
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로",LoRA 와 QLoRA 의 차이,-0.0023704735,0.0,0.002370473463088274
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로",Loss Function 예시,-0.0025966377,0.0,0.002596637699753046
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로",Loss Function 정의,-0.002052083,0.0,0.0020520829129964113
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로",MBTI,-0.0016926827,0.0,0.0016926827374845743
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로",MSE Loss 설명,-0.0020354248,0.0,0.0020354248117655516
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로",MSE Loss 용도,-0.0025399236,0.0,0.002539923647418618
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로",Multi-Label 에서 CE + Softmax 적용 문제점,-0.0028970598,0.0,0.002897059777751565
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로",PEFT 방법 5가지,-0.0031499835,0.0,0.00314998347312212
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로",거대 언어 모델 정의,-0.0005123109,0.0,0.0005123108858242631
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로",기본 경험,-0.0016779747,0.0,0.0016779747093096375
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로",답변 실패,0.9997689,1.0,0.00023108720779418945
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로",딥러닝,-0.0018236521,0.0,0.0018236520700156689
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로",마지막 할 말,-0.0030305795,0.0,0.003030579537153244
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로",머신러닝,-0.0011989507,0.0,0.0011989506892859936
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로",면접 시작 인사,-0.0015363387,0.0,0.0015363387065008283
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로",상세 경험,-0.0029909508,0.0,0.002990950830280781
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로",수식,-0.0025443924,0.0,0.002544392365962267
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로",용어 질문,-0.00067827315,0.0,0.0006782731506973505
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로",인공지능,-0.0012561608,0.0,0.001256160787306726
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로",잠시 휴식,-0.0043449495,0.0,0.004344949498772621
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로",좋아하는 아이돌,-0.0016598153,0.0,0.0016598153160884976
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로",핵심 아이디어,-0.0023491844,0.0,0.0023491843603551388
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로",확률 예측에서 MSE Loss 미 사용 이유,-0.00063050527,0.0,0.0006305052665993571
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,BCE 가 좋은 task,-0.0014629632,0.0,0.0014629631768912077
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,BCE 가 좋은 이유,-3.4982455e-05,0.0,3.498245496302843e-05
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,LLM Fine-Tuning 의 PEFT,-0.0013684568,0.0,0.0013684567529708147
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,LoRA,-0.0032177542,0.0,0.0032177541870623827
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,LoRA 와 QLoRA 의 차이,-0.0069190157,0.0,0.0069190156646072865
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,Loss Function 예시,0.001864717,0.0,0.001864716992713511
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,Loss Function 정의,-0.0045855776,0.0,0.004585577640682459
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,MBTI,-0.0010499022,0.0,0.0010499021736904979
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,MSE Loss 설명,-0.005485203,0.0,0.005485203117132187
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,MSE Loss 용도,-0.005626969,0.0,0.005626969039440155
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,Multi-Label 에서 CE + Softmax 적용 문제점,-0.0016599953,0.0,0.001659995294176042
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,PEFT 방법 5가지,-0.0014062173,0.0,0.001406217343173921
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,거대 언어 모델 정의,0.999041,1.0,0.0009589791297912598
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,기본 경험,-0.0062663127,0.0,0.006266312673687935
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,답변 실패,-0.0009753471,0.0,0.0009753471240401268
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,딥러닝,-0.0084430035,0.0,0.00844300352036953
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,마지막 할 말,-0.0055800756,0.0,0.005580075550824404
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,머신러닝,0.0010571614,0.0,0.0010571613674983382
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,면접 시작 인사,-0.0050729928,0.0,0.0050729927606880665
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,상세 경험,-0.0026407368,0.0,0.0026407367549836636
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,수식,-0.006412115,0.0,0.0064121149480342865
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,용어 질문,-0.0030171424,0.0,0.003017142415046692
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,인공지능,-0.002455624,0.0,0.0024556240532547235
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,잠시 휴식,-0.0031412377,0.0,0.0031412376556545496
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,좋아하는 아이돌,-0.0014655065,0.0,0.0014655065024271607
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,핵심 아이디어,-0.0020590879,0.0,0.0020590878557413816
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,확률 예측에서 MSE Loss 미 사용 이유,-0.004263813,0.0,0.004263813141733408
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,BCE 가 좋은 task,-0.003742432,0.0,0.003742431988939643
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,BCE 가 좋은 이유,-0.022594608,0.0,0.02259460836648941
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,LLM Fine-Tuning 의 PEFT,-0.0027221325,0.0,0.002722132485359907
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,LoRA,-0.00884738,0.0,0.008847380056977272
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,LoRA 와 QLoRA 의 차이,-0.0018986054,0.0,0.0018986053764820099
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,Loss Function 예시,-0.018811593,0.0,0.01881159283220768
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,Loss Function 정의,-0.0092585385,0.0,0.009258538484573364
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,MBTI,-0.011129336,0.0,0.011129336431622505
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,MSE Loss 설명,-0.01489168,0.0,0.01489168033003807
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,MSE Loss 용도,-0.012109716,0.0,0.012109716422855854
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,Multi-Label 에서 CE + Softmax 적용 문제점,-0.016308295,0.0,0.016308294609189034
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,PEFT 방법 5가지,0.0027780998,0.0,0.002778099849820137
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,거대 언어 모델 정의,0.8615111,0.0,0.8615111112594604
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,기본 경험,-0.0019324103,0.0,0.0019324102904647589
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,답변 실패,0.43151897,1.0,0.5684810280799866
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,딥러닝,-0.02460375,0.0,0.024603750556707382
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,마지막 할 말,-0.019419085,0.0,0.01941908523440361
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,머신러닝,0.003415787,0.0,0.0034157868940383196
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,면접 시작 인사,-0.027497029,0.0,0.027497028931975365
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,상세 경험,-0.0025418862,0.0,0.0025418861769139767
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,수식,-0.018719584,0.0,0.018719583749771118
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,용어 질문,-0.009548386,0.0,0.009548385627567768
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,인공지능,-0.007074427,0.0,0.007074426859617233
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,잠시 휴식,-0.010818831,0.0,0.01081883069127798
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,좋아하는 아이돌,-0.015610035,0.0,0.015610034577548504
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,핵심 아이디어,-0.0044901576,0.0,0.004490157589316368
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,확률 예측에서 MSE Loss 미 사용 이유,-0.0025512099,0.0,0.002551209880039096
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델,BCE 가 좋은 task,-0.0021385313,0.0,0.0021385313011705875
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델,BCE 가 좋은 이유,-0.0011332402,0.0,0.001133240177296102
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델,LLM Fine-Tuning 의 PEFT,-0.0008507746,0.0,0.0008507745806127787
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델,LoRA,-0.003470987,0.0,0.00347098708152771
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델,LoRA 와 QLoRA 의 차이,-0.006868576,0.0,0.006868576165288687
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델,Loss Function 예시,0.0015512912,0.0,0.0015512912068516016
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델,Loss Function 정의,-0.0041541075,0.0,0.00415410753339529
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델,MBTI,-0.00155564,0.0,0.0015556400176137686
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델,MSE Loss 설명,-0.006332458,0.0,0.00633245799690485
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델,MSE Loss 용도,-0.006107017,0.0,0.006107016932219267
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델,Multi-Label 에서 CE + Softmax 적용 문제점,-0.00096971943,0.0,0.00096971943276003
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델,PEFT 방법 5가지,-7.2351024e-05,0.0,7.235102384584025e-05
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델,거대 언어 모델 정의,0.9987417,1.0,0.0012583136558532715
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델,기본 경험,-0.007366919,0.0,0.007366918958723545
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델,답변 실패,-0.0008053736,0.0,0.0008053735946305096
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델,딥러닝,-0.008445661,0.0,0.008445660583674908
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델,마지막 할 말,-0.0057901526,0.0,0.005790152586996555
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델,머신러닝,-0.0008531691,0.0,0.0008531691273674369
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델,면접 시작 인사,-0.0057966663,0.0,0.005796666257083416
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델,상세 경험,-0.0013105551,0.0,0.0013105551479384303
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델,수식,-0.0061740032,0.0,0.00617400323972106
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델,용어 질문,-0.0033972245,0.0,0.003397224470973015
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델,인공지능,-0.002155459,0.0,0.0021554590202867985
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델,잠시 휴식,-0.0032825063,0.0,0.003282506251707673
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델,좋아하는 아이돌,-0.0013954106,0.0,0.0013954106252640486
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델,핵심 아이디어,-0.0027406814,0.0,0.00274068140424788
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델,확률 예측에서 MSE Loss 미 사용 이유,-0.0031911072,0.0,0.0031911071855574846
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지.,BCE 가 좋은 task,-0.0017056074,0.0,0.0017056073993444443
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지.,BCE 가 좋은 이유,-0.0045213522,0.0,0.0045213522389531136
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지.,LLM Fine-Tuning 의 PEFT,-0.0023259416,0.0,0.002325941575691104
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지.,LoRA,-0.0034870126,0.0,0.003487012581899762
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지.,LoRA 와 QLoRA 의 차이,-0.001523403,0.0,0.0015234029851853848
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지.,Loss Function 예시,-0.0014086724,0.0,0.0014086724258959293
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지.,Loss Function 정의,-0.0013835407,0.0,0.0013835406862199306
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지.,MBTI,-0.002001651,0.0,0.002001651097089052
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지.,MSE Loss 설명,-0.0022442099,0.0,0.002244209870696068
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지.,MSE Loss 용도,-0.003463511,0.0,0.003463510889559984
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지.,Multi-Label 에서 CE + Softmax 적용 문제점,-0.0027002722,0.0,0.002700272249057889
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지.,PEFT 방법 5가지,-0.0038066253,0.0,0.003806625260040164
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지.,거대 언어 모델 정의,0.0003273386,0.0,0.0003273386100772768
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지.,기본 경험,-0.0013085541,0.0,0.0013085540849715471
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지.,답변 실패,0.9997577,1.0,0.0002422928810119629
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지.,딥러닝,-0.0025882912,0.0,0.002588291186839342
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지.,마지막 할 말,-0.0020850103,0.0,0.002085010288283229
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지.,머신러닝,-0.002997783,0.0,0.00299778301268816
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지.,면접 시작 인사,-0.0014991632,0.0,0.001499163219705224
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지.,상세 경험,-0.0029963467,0.0,0.0029963466804474592
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지.,수식,-0.0032003697,0.0,0.003200369654223323
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지.,용어 질문,-0.0001396696,0.0,0.00013966960250400007
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지.,인공지능,-0.002342844,0.0,0.0023428439162671566
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지.,잠시 휴식,-0.0038386844,0.0,0.0038386844098567963
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지.,좋아하는 아이돌,-0.0009258434,0.0,0.000925843371078372
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지.,핵심 아이디어,-0.0024175462,0.0,0.0024175462312996387
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지.,확률 예측에서 MSE Loss 미 사용 이유,-0.00045629096,0.0,0.00045629095984622836
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,BCE 가 좋은 task,-0.0026235024,0.0,0.0026235023979097605
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,BCE 가 좋은 이유,-0.0057618017,0.0,0.005761801730841398
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,LLM Fine-Tuning 의 PEFT,-0.0026781203,0.0,0.0026781202759593725
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,LoRA,-0.0027388975,0.0,0.002738897455856204
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,LoRA 와 QLoRA 의 차이,-0.002763757,0.0,0.0027637570165097713
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,Loss Function 예시,-0.0009255377,0.0,0.0009255377226509154
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,Loss Function 정의,-0.0005664831,0.0,0.0005664831260219216
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,MBTI,-0.00077211903,0.0,0.0007721190340816975
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,MSE Loss 설명,-0.0029513806,0.0,0.0029513805638998747
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,MSE Loss 용도,-0.0026710848,0.0,0.0026710848324000835
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,Multi-Label 에서 CE + Softmax 적용 문제점,-0.0022176213,0.0,0.002217621309682727
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,PEFT 방법 5가지,-0.0037154614,0.0,0.003715461352840066
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,거대 언어 모델 정의,-0.0008640387,0.0,0.000864038709551096
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,기본 경험,-0.0010982732,0.0,0.0010982732055708766
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,답변 실패,0.9995758,1.0,0.0004242062568664551
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,딥러닝,-0.001338451,0.0,0.0013384510530158877
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,마지막 할 말,-0.0012847019,0.0,0.0012847018660977483
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,머신러닝,-0.0025878295,0.0,0.0025878294836729765
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,면접 시작 인사,-0.0016641897,0.0,0.0016641897382214665
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,상세 경험,-0.0016943091,0.0,0.0016943090595304966
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,수식,-0.002333797,0.0,0.002333797048777342
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,용어 질문,-0.0005445246,0.0,0.0005445245769806206
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,인공지능,-0.00296063,0.0,0.0029606299940496683
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,잠시 휴식,-0.0042908015,0.0,0.004290801472961903
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,좋아하는 아이돌,-0.0025206083,0.0,0.002520608250051737
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,핵심 아이디어,-0.0018708023,0.0,0.0018708022544160485
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,확률 예측에서 MSE Loss 미 사용 이유,-0.0010571224,0.0,0.0010571223683655262
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",BCE 가 좋은 task,-0.002643321,0.0,0.002643320942297578
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",BCE 가 좋은 이유,0.0012008221,0.0,0.0012008220655843616
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",LLM Fine-Tuning 의 PEFT,7.891993e-05,0.0,7.891993300290778e-05
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",LoRA,-0.00333082,0.0,0.003330820007249713
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",LoRA 와 QLoRA 의 차이,-0.0024654025,0.0,0.0024654024746268988
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",Loss Function 예시,-0.002449889,0.0,0.0024498889688402414
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",Loss Function 정의,0.9987988,1.0,0.0012012124061584473
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",MBTI,-0.001161246,0.0,0.001161245978437364
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",MSE Loss 설명,-0.0007267962,0.0,0.0007267962209880352
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",MSE Loss 용도,-0.001360166,0.0,0.0013601660029962659
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",Multi-Label 에서 CE + Softmax 적용 문제점,-0.0011464036,0.0,0.001146403606981039
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",PEFT 방법 5가지,-0.0009994076,0.0,0.000999407609924674
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",거대 언어 모델 정의,-0.0026899397,0.0,0.0026899396907538176
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",기본 경험,-0.0024264497,0.0,0.0024264496751129627
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",답변 실패,-0.0032291962,0.0,0.003229196183383465
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",딥러닝,-0.0007981444,0.0,0.0007981443777680397
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",마지막 할 말,0.0028304593,0.0,0.0028304592706263065
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",머신러닝,-0.004499733,0.0,0.004499732982367277
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",면접 시작 인사,-0.0025920656,0.0,0.002592065604403615
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",상세 경험,0.0041720984,0.0,0.004172098357230425
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",수식,0.0023411615,0.0,0.0023411614820361137
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",용어 질문,-7.883749e-05,0.0,7.883748912718147e-05
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",인공지능,-0.0052910666,0.0,0.005291066598147154
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",잠시 휴식,0.0015529258,0.0,0.0015529257943853736
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",좋아하는 아이돌,-0.0013138163,0.0,0.0013138162903487682
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",핵심 아이디어,-0.006262288,0.0,0.006262287963181734
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",확률 예측에서 MSE Loss 미 사용 이유,-0.0067840666,0.0,0.006784066557884216
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야,BCE 가 좋은 task,-0.00213608,0.0,0.0021360800601541996
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야,BCE 가 좋은 이유,-0.0051494692,0.0,0.005149469245225191
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야,LLM Fine-Tuning 의 PEFT,-0.0023915784,0.0,0.002391578396782279
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야,LoRA,-0.003003077,0.0,0.0030030771158635616
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야,LoRA 와 QLoRA 의 차이,-0.0022492327,0.0,0.0022492327261716127
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야,Loss Function 예시,-0.0016849614,0.0,0.001684961374849081
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야,Loss Function 정의,-0.00044693385,0.0,0.0004469338455237448
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야,MBTI,-0.0009672869,0.0,0.0009672868764027953
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야,MSE Loss 설명,-0.0033083528,0.0,0.003308352781459689
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야,MSE Loss 용도,-0.0025129009,0.0,0.002512900857254863
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야,Multi-Label 에서 CE + Softmax 적용 문제점,-0.0019586927,0.0,0.001958692679181695
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야,PEFT 방법 5가지,-0.003573149,0.0,0.003573148977011442
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야,거대 언어 모델 정의,-0.0005097036,0.0,0.0005097035900689662
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야,기본 경험,-0.0013066931,0.0,0.0013066930696368217
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야,답변 실패,0.9997417,1.0,0.00025832653045654297
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야,딥러닝,-0.0017253123,0.0,0.0017253123223781586
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야,마지막 할 말,-0.0016518676,0.0,0.0016518676420673728
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야,머신러닝,-0.0025055646,0.0,0.002505564596503973
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야,면접 시작 인사,-0.0013139661,0.0,0.0013139661168679595
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야,상세 경험,-0.002950785,0.0,0.002950784983113408
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야,수식,-0.00353917,0.0,0.00353916990570724
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야,용어 질문,-0.00035677975,0.0,0.0003567797539290041
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야,인공지능,-0.002544814,0.0,0.002544814022257924
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야,잠시 휴식,-0.0044339285,0.0,0.004433928523212671
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야,좋아하는 아이돌,-0.0017409046,0.0,0.0017409046413376927
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야,핵심 아이디어,-0.0028015778,0.0,0.0028015777934342623
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야,확률 예측에서 MSE Loss 미 사용 이유,-0.00077378296,0.0,0.0007737829582765698
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지,BCE 가 좋은 task,-0.0034207143,0.0,0.0034207142889499664
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지,BCE 가 좋은 이유,0.0010784619,0.0,0.0010784618789330125
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지,LLM Fine-Tuning 의 PEFT,-0.0012937536,0.0,0.0012937536230310798
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지,LoRA,-0.0034569183,0.0,0.003456918289884925
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지,LoRA 와 QLoRA 의 차이,-0.0026632987,0.0,0.0026632987428456545
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지,Loss Function 예시,-0.0016351319,0.0,0.0016351318918168545
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지,Loss Function 정의,0.99908453,1.0,0.0009154677391052246
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지,MBTI,-0.0013013848,0.0,0.0013013847637921572
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지,MSE Loss 설명,0.00031861462,0.0,0.00031861462048254907
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지,MSE Loss 용도,0.0014721304,0.0,0.0014721304178237915
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지,Multi-Label 에서 CE + Softmax 적용 문제점,-0.0034811352,0.0,0.0034811352379620075
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지,PEFT 방법 5가지,-0.0017819563,0.0,0.0017819562926888466
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지,거대 언어 모델 정의,-0.0031269756,0.0,0.003126975614577532
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지,기본 경험,-0.0017022333,0.0,0.0017022333340719342
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지,답변 실패,-0.003271385,0.0,0.003271385096013546
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지,딥러닝,-0.0007422945,0.0,0.0007422944763675332
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지,마지막 할 말,0.002216687,0.0,0.002216686960309744
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지,머신러닝,-0.005344731,0.0,0.00534473080188036
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지,면접 시작 인사,-0.0029593406,0.0,0.002959340577945113
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지,상세 경험,0.003549887,0.0,0.0035498871002346277
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지,수식,0.0027845618,0.0,0.0027845618315041065
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지,용어 질문,0.00019716864,0.0,0.00019716864335350692
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지,인공지능,-0.0010105429,0.0,0.0010105428518727422
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지,잠시 휴식,0.0016197981,0.0,0.001619798131287098
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지,좋아하는 아이돌,-0.002249669,0.0,0.00224966905079782
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지,핵심 아이디어,-0.0039200154,0.0,0.003920015413314104
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지,확률 예측에서 MSE Loss 미 사용 이유,-0.006266157,0.0,0.006266157142817974
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,BCE 가 좋은 task,-0.0033336275,0.0,0.0033336274791508913
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,BCE 가 좋은 이유,-0.0040872493,0.0,0.004087249282747507
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,LLM Fine-Tuning 의 PEFT,-0.0021581992,0.0,0.002158199204131961
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,LoRA,-0.0029922032,0.0,0.002992203226312995
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,LoRA 와 QLoRA 의 차이,-0.0021934116,0.0,0.0021934115793555975
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,Loss Function 예시,8.696446e-05,0.0,8.696445729583502e-05
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,Loss Function 정의,-0.0017924582,0.0,0.0017924582352861762
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,MBTI,-0.0018445282,0.0,0.001844528247602284
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,MSE Loss 설명,-0.0033656068,0.0,0.0033656067680567503
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,MSE Loss 용도,-0.0028661906,0.0,0.0028661906253546476
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,Multi-Label 에서 CE + Softmax 적용 문제점,-0.0022716874,0.0,0.0022716873791068792
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,PEFT 방법 5가지,-0.0028116717,0.0,0.0028116717003285885
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,거대 언어 모델 정의,-0.0006701888,0.0,0.0006701888050884008
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,기본 경험,-0.0019533513,0.0,0.0019533513113856316
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,답변 실패,0.9997733,1.0,0.00022667646408081055
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,딥러닝,-0.0024682158,0.0,0.0024682157672941685
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,마지막 할 말,-0.0024682714,0.0,0.0024682714138180017
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,머신러닝,-0.0022048464,0.0,0.0022048463579267263
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,면접 시작 인사,-0.0012461371,0.0,0.0012461370788514614
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,상세 경험,-0.0038384122,0.0,0.003838412230834365
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,수식,-0.003821039,0.0,0.003821039106696844
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,용어 질문,-0.0003365716,0.0,0.0003365715965628624
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,인공지능,-0.0024090474,0.0,0.002409047447144985
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,잠시 휴식,-0.0040555736,0.0,0.004055573605000973
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,좋아하는 아이돌,-0.0011959383,0.0,0.0011959383264183998
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,핵심 아이디어,-0.0024422223,0.0,0.002442222321406007
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,확률 예측에서 MSE Loss 미 사용 이유,-0.0011449005,0.0,0.0011449004523456097
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",BCE 가 좋은 task,-0.002795905,0.0,0.0027959051076322794
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",BCE 가 좋은 이유,-0.0049712565,0.0,0.0049712564796209335
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",LLM Fine-Tuning 의 PEFT,-0.00042781504,0.0,0.0004278150445315987
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",LoRA,-0.0018245421,0.0,0.0018245420651510358
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",LoRA 와 QLoRA 의 차이,-0.0042915116,0.0,0.004291511606425047
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",Loss Function 예시,0.9993285,1.0,0.0006715059280395508
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",Loss Function 정의,9.556202e-06,0.0,9.556201803206932e-06
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",MBTI,-0.003136128,0.0,0.0031361279543489218
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",MSE Loss 설명,-0.00464734,0.0,0.0046473401598632336
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",MSE Loss 용도,-0.0010724928,0.0,0.0010724927997216582
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",Multi-Label 에서 CE + Softmax 적용 문제점,-0.0012422872,0.0,0.0012422872241586447
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",PEFT 방법 5가지,-0.0028023513,0.0,0.0028023512568324804
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",거대 언어 모델 정의,0.0012112253,0.0,0.001211225287988782
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",기본 경험,-0.0018396147,0.0,0.0018396147061139345
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",답변 실패,-0.0014124647,0.0,0.0014124646550044417
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",딥러닝,-0.0010412136,0.0,0.0010412136325612664
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",마지막 할 말,-0.0022163214,0.0,0.0022163214161992073
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",머신러닝,-0.0026242626,0.0,0.0026242625899612904
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",면접 시작 인사,-0.0004420117,0.0,0.0004420116893015802
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",상세 경험,-0.0041924436,0.0,0.004192443564534187
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",수식,-0.002112349,0.0,0.0021123490296304226
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",용어 질문,0.0025028207,0.0,0.002502820687368512
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",인공지능,-0.0023970231,0.0,0.0023970231413841248
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",잠시 휴식,-0.0025943157,0.0,0.002594315679743886
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",좋아하는 아이돌,-0.0032446494,0.0,0.003244649386033416
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",핵심 아이디어,-0.0025837598,0.0,0.0025837598368525505
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",확률 예측에서 MSE Loss 미 사용 이유,-0.0030111875,0.0,0.0030111875385046005
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어",BCE 가 좋은 task,-0.002562099,0.0,0.0025620989035815
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어",BCE 가 좋은 이유,-0.0050054844,0.0,0.005005484446883202
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어",LLM Fine-Tuning 의 PEFT,-0.00024351719,0.0,0.0002435171918477863
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어",LoRA,-0.0025046752,0.0,0.0025046751834452152
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어",LoRA 와 QLoRA 의 차이,-0.004260276,0.0,0.004260275978595018
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어",Loss Function 예시,0.99918133,1.0,0.0008186697959899902
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어",Loss Function 정의,-7.976879e-05,0.0,7.97687898739241e-05
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어",MBTI,-0.0031077468,0.0,0.0031077468302100897
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어",MSE Loss 설명,-0.0050414186,0.0,0.005041418597102165
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어",MSE Loss 용도,-0.0017652379,0.0,0.0017652378883212805
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어",Multi-Label 에서 CE + Softmax 적용 문제점,-0.0007394719,0.0,0.0007394719286821783
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어",PEFT 방법 5가지,-0.0029916987,0.0,0.002991698682308197
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어",거대 언어 모델 정의,0.0013935248,0.0,0.001393524813465774
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어",기본 경험,-0.0022293194,0.0,0.0022293194197118282
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어",답변 실패,-0.0016430203,0.0,0.0016430203104391694
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어",딥러닝,-0.0021490385,0.0,0.0021490384824573994
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어",마지막 할 말,-0.0018740967,0.0,0.001874096691608429
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어",머신러닝,-0.0022580507,0.0,0.0022580507211387157
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어",면접 시작 인사,-0.0002499324,0.0,0.00024993240367621183
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어",상세 경험,-0.0008760785,0.0,0.0008760784985497594
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어",수식,-0.0025111877,0.0,0.0025111876893788576
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어",용어 질문,0.00249621,0.0,0.002496209926903248
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어",인공지능,-0.0019479209,0.0,0.0019479208858683705
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어",잠시 휴식,-0.0023328315,0.0,0.0023328315000981092
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어",좋아하는 아이돌,-0.002389279,0.0,0.0023892789613455534
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어",핵심 아이디어,-0.003260826,0.0,0.0032608259934931993
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어",확률 예측에서 MSE Loss 미 사용 이유,-0.002627167,0.0,0.0026271669194102287
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야?",BCE 가 좋은 task,-0.0027945638,0.0,0.0027945637702941895
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야?",BCE 가 좋은 이유,-0.00426759,0.0,0.004267590120434761
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야?",LLM Fine-Tuning 의 PEFT,-0.0017774069,0.0,0.0017774068983271718
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야?",LoRA,-0.0028923468,0.0,0.0028923468198627234
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야?",LoRA 와 QLoRA 의 차이,-0.0019025847,0.0,0.0019025846850126982
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야?",Loss Function 예시,-0.0016795164,0.0,0.0016795163974165916
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야?",Loss Function 정의,-0.002046322,0.0,0.0020463219843804836
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야?",MBTI,-0.0018819041,0.0,0.0018819040851667523
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야?",MSE Loss 설명,-0.002862926,0.0,0.0028629261068999767
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야?",MSE Loss 용도,-0.0032302903,0.0,0.0032302902545779943
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야?",Multi-Label 에서 CE + Softmax 적용 문제점,-0.0023728993,0.0,0.0023728993255645037
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야?",PEFT 방법 5가지,-0.0027507197,0.0,0.002750719664618373
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야?",거대 언어 모델 정의,-0.0007442327,0.0,0.0007442326750606298
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야?",기본 경험,-0.0017517363,0.0,0.0017517362721264362
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야?",답변 실패,0.9997549,1.0,0.00024509429931640625
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야?",딥러닝,-0.0023631193,0.0,0.002363119274377823
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야?",마지막 할 말,-0.0025202709,0.0,0.0025202708784490824
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야?",머신러닝,-0.0028437783,0.0,0.002843778347596526
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야?",면접 시작 인사,-0.0012623902,0.0,0.0012623901711776853
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야?",상세 경험,-0.0041385326,0.0,0.004138532560318708
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야?",수식,-0.0034334136,0.0,0.0034334135707467794
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야?",용어 질문,-0.00021838828,0.0,0.00021838827524334192
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야?",인공지능,-0.0019398257,0.0,0.001939825713634491
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야?",잠시 휴식,-0.004282342,0.0,0.004282341804355383
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야?",좋아하는 아이돌,-0.0009646199,0.0,0.0009646199177950621
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야?",핵심 아이디어,-0.002415576,0.0,0.0024155760183930397
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야?",확률 예측에서 MSE Loss 미 사용 이유,-0.00052572053,0.0,0.0005257205339148641
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어",BCE 가 좋은 task,-0.0029549447,0.0,0.002954944735392928
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어",BCE 가 좋은 이유,-0.004350632,0.0,0.0043506319634616375
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어",LLM Fine-Tuning 의 PEFT,-0.0004668902,0.0,0.00046689019654877484
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어",LoRA,-0.0024245284,0.0,0.002424528356641531
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어",LoRA 와 QLoRA 의 차이,-0.0041103326,0.0,0.004110332578420639
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어",Loss Function 예시,0.99906397,1.0,0.0009360313415527344
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어",Loss Function 정의,-0.001093983,0.0,0.0010939829517155886
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어",MBTI,-0.003811481,0.0,0.0038114809431135654
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어",MSE Loss 설명,-0.004765291,0.0,0.004765291232615709
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어",MSE Loss 용도,-0.0013927472,0.0,0.0013927471591159701
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어",Multi-Label 에서 CE + Softmax 적용 문제점,-0.0011257188,0.0,0.0011257188161835074
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어",PEFT 방법 5가지,-0.0028191556,0.0,0.002819155575707555
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어",거대 언어 모델 정의,0.0012789749,0.0,0.001278974930755794
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어",기본 경험,-0.0020919926,0.0,0.0020919926464557648
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어",답변 실패,-0.0018731671,0.0,0.0018731671152636409
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어",딥러닝,-0.0022689248,0.0,0.002268924843519926
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어",마지막 할 말,-0.0025151393,0.0,0.002515139291062951
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어",머신러닝,-0.002836177,0.0,0.0028361768927425146
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어",면접 시작 인사,-0.00043033078,0.0,0.00043033077963627875
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어",상세 경험,-0.0015608327,0.0,0.001560832723043859
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어",수식,-0.0026103244,0.0,0.002610324416309595
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어",용어 질문,0.002960085,0.0,0.0029600849375128746
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어",인공지능,-0.0023310445,0.0,0.002331044524908066
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어",잠시 휴식,-0.0027842137,0.0,0.002784213749691844
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어",좋아하는 아이돌,-0.0020022625,0.0,0.0020022625103592873
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어",핵심 아이디어,-0.0026696515,0.0,0.0026696515269577503
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어",확률 예측에서 MSE Loss 미 사용 이유,-0.003352651,0.0,0.0033526509068906307
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아",BCE 가 좋은 task,-0.0025546178,0.0,0.0025546178221702576
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아",BCE 가 좋은 이유,-0.0043305564,0.0,0.004330556374043226
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아",LLM Fine-Tuning 의 PEFT,-0.00094420725,0.0,0.0009442072478123009
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아",LoRA,-0.0027985624,0.0,0.002798562403768301
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아",LoRA 와 QLoRA 의 차이,-0.004629101,0.0,0.004629101138561964
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아",Loss Function 예시,0.99912184,1.0,0.0008781552314758301
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아",Loss Function 정의,-0.00027688246,0.0,0.0002768824633676559
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아",MBTI,-0.00425667,0.0,0.004256669897586107
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아",MSE Loss 설명,-0.00539359,0.0,0.005393589846789837
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아",MSE Loss 용도,-0.001215625,0.0,0.0012156249722465873
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아",Multi-Label 에서 CE + Softmax 적용 문제점,-0.00046200756,0.0,0.00046200756332837045
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아",PEFT 방법 5가지,-0.0025283506,0.0,0.002528350567445159
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아",거대 언어 모델 정의,0.0012219464,0.0,0.0012219464406371117
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아",기본 경험,-0.0015208732,0.0,0.0015208731638267636
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아",답변 실패,-0.0016958518,0.0,0.0016958517953753471
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아",딥러닝,-0.0020076851,0.0,0.002007685136049986
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아",마지막 할 말,-0.0023850948,0.0,0.0023850947618484497
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아",머신러닝,-0.002476704,0.0,0.0024767040740698576
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아",면접 시작 인사,-0.00057241716,0.0,0.000572417164221406
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아",상세 경험,-0.0023914892,0.0,0.0023914892226457596
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아",수식,-0.0021339084,0.0,0.00213390844874084
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아",용어 질문,0.0024056071,0.0,0.0024056071415543556
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아",인공지능,-0.0035428044,0.0,0.003542804392054677
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아",잠시 휴식,-0.0025805775,0.0,0.0025805775076150894
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아",좋아하는 아이돌,-0.0021009294,0.0,0.0021009293850511312
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아",핵심 아이디어,-0.0019734865,0.0,0.001973486505448818
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아",확률 예측에서 MSE Loss 미 사용 이유,-0.0031857695,0.0,0.0031857695430517197
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,BCE 가 좋은 task,-0.0005928963,0.0,0.0005928963073529303
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,BCE 가 좋은 이유,-0.004719683,0.0,0.004719682969152927
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,LLM Fine-Tuning 의 PEFT,-0.0034149035,0.0,0.003414903534576297
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,LoRA,-0.00637343,0.0,0.006373430136591196
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,LoRA 와 QLoRA 의 차이,-0.008609502,0.0,0.008609501644968987
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,Loss Function 예시,-0.006696688,0.0,0.006696688011288643
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,Loss Function 정의,-0.0017896029,0.0,0.001789602916687727
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,MBTI,-0.0039071315,0.0,0.0039071314968168736
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,MSE Loss 설명,0.9987632,1.0,0.0012367963790893555
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,MSE Loss 용도,-0.0027899763,0.0,0.0027899763081222773
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,Multi-Label 에서 CE + Softmax 적용 문제점,-0.00089365244,0.0,0.0008936524391174316
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,PEFT 방법 5가지,6.7271874e-05,0.0,6.727187428623438e-05
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,거대 언어 모델 정의,-0.0028382298,0.0,0.0028382297605276108
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,기본 경험,-0.003989349,0.0,0.003989349119365215
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,답변 실패,-0.0031384423,0.0,0.0031384422909468412
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,딥러닝,-0.0065307594,0.0,0.0065307593904435635
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,마지막 할 말,-0.0034879495,0.0,0.0034879494924098253
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,머신러닝,-0.002081851,0.0,0.0020818510092794895
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,면접 시작 인사,-0.0053906194,0.0,0.005390619393438101
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,상세 경험,-0.005848808,0.0,0.005848808214068413
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,수식,-0.006865879,0.0,0.0068658790551126
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,용어 질문,-0.00036428732,0.0,0.0003642873198259622
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,인공지능,0.0005239347,0.0,0.0005239347228780389
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,잠시 휴식,-0.00087169657,0.0,0.0008716965676285326
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,좋아하는 아이돌,-0.0003148273,0.0,0.00031482730992138386
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,핵심 아이디어,-0.0010939507,0.0,0.0010939507046714425
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,확률 예측에서 MSE Loss 미 사용 이유,-0.0015020318,0.0,0.0015020318096503615
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,BCE 가 좋은 task,-0.0028584742,0.0,0.002858474152162671
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,BCE 가 좋은 이유,-0.0048685954,0.0,0.004868595395237207
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,LLM Fine-Tuning 의 PEFT,-0.00250514,0.0,0.0025051399134099483
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,LoRA,-0.003182126,0.0,0.0031821259763091803
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,LoRA 와 QLoRA 의 차이,-0.0019034897,0.0,0.0019034896977245808
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,Loss Function 예시,-0.0006115363,0.0,0.0006115363212302327
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,Loss Function 정의,-0.0017859499,0.0,0.0017859499203041196
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,MBTI,-0.0018975956,0.0,0.001897595589980483
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,MSE Loss 설명,-0.0006902942,0.0,0.0006902941968291998
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,MSE Loss 용도,-0.0034487436,0.0,0.0034487436059862375
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,Multi-Label 에서 CE + Softmax 적용 문제점,-0.0025231198,0.0,0.002523119794204831
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,PEFT 방법 5가지,-0.0037581048,0.0,0.0037581047508865595
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,거대 언어 모델 정의,-9.12797e-05,0.0,9.127969678957015e-05
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,기본 경험,-0.001636688,0.0,0.0016366880154237151
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,답변 실패,0.9997299,1.0,0.0002701282501220703
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,딥러닝,-0.001841288,0.0,0.001841287943534553
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,마지막 할 말,-0.0029869745,0.0,0.00298697454854846
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,머신러닝,-0.0029164348,0.0,0.0029164347797632217
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,면접 시작 인사,-0.0020087545,0.0,0.002008754527196288
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,상세 경험,-0.0029891047,0.0,0.0029891047161072493
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,수식,-0.0037433757,0.0,0.003743375651538372
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,용어 질문,-0.0002480014,0.0,0.000248001393629238
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,인공지능,-0.0023700537,0.0,0.002370053669437766
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,잠시 휴식,-0.004226863,0.0,0.004226862918585539
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,좋아하는 아이돌,-0.0016194117,0.0,0.0016194117488339543
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,핵심 아이디어,-0.0027264287,0.0,0.0027264286763966084
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,확률 예측에서 MSE Loss 미 사용 이유,-0.00063308305,0.0,0.0006330830510705709
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",BCE 가 좋은 task,-0.004072144,0.0,0.004072144161909819
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",BCE 가 좋은 이유,-0.005234264,0.0,0.005234263837337494
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",LLM Fine-Tuning 의 PEFT,-0.004905578,0.0,0.004905578214675188
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",LoRA,0.0013751155,0.0,0.0013751154765486717
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",LoRA 와 QLoRA 의 차이,-0.00076057453,0.0,0.0007605745340697467
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",Loss Function 예시,-0.0010329538,0.0,0.0010329538490623236
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",Loss Function 정의,5.128761e-05,0.0,5.128761040396057e-05
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",MBTI,-0.0022511054,0.0,0.002251105383038521
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",MSE Loss 설명,-0.0013694713,0.0,0.0013694713125005364
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",MSE Loss 용도,0.9982651,1.0,0.001734912395477295
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",Multi-Label 에서 CE + Softmax 적용 문제점,-0.00062445883,0.0,0.0006244588294066489
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",PEFT 방법 5가지,-0.0013585954,0.0,0.0013585954438894987
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",거대 언어 모델 정의,-0.0049768025,0.0,0.004976802505552769
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",기본 경험,-0.0036389637,0.0,0.003638963680714369
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",답변 실패,-0.0045978148,0.0,0.004597814753651619
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",딥러닝,-0.005199954,0.0,0.00519995391368866
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",마지막 할 말,-0.008042913,0.0,0.008042912930250168
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",머신러닝,-1.9272646e-05,0.0,1.9272645658929832e-05
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",면접 시작 인사,-0.0074141975,0.0,0.0074141975492239
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",상세 경험,-0.0035681983,0.0,0.00356819829903543
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",수식,-0.0073402533,0.0,0.007340253330767155
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",용어 질문,-0.0010712366,0.0,0.0010712365619838238
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",인공지능,-0.00088961306,0.0,0.0008896130602806807
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",잠시 휴식,-0.0049124253,0.0,0.004912425298243761
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",좋아하는 아이돌,-0.0042006727,0.0,0.00420067273080349
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",핵심 아이디어,-0.0029601052,0.0,0.0029601051937788725
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",확률 예측에서 MSE Loss 미 사용 이유,0.0008939722,0.0,0.0008939721737988293
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,BCE 가 좋은 task,-0.0029818045,0.0,0.002981804544106126
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,BCE 가 좋은 이유,-0.004256361,0.0,0.004256361164152622
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,LLM Fine-Tuning 의 PEFT,-0.0023611877,0.0,0.0023611877113580704
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,LoRA,-0.0032344046,0.0,0.003234404604882002
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,LoRA 와 QLoRA 의 차이,-0.0015192439,0.0,0.0015192439313977957
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,Loss Function 예시,-0.0014821012,0.0,0.0014821011573076248
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,Loss Function 정의,-0.0016708699,0.0,0.0016708698822185397
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,MBTI,-0.00190379,0.0,0.0019037900492548943
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,MSE Loss 설명,-0.0033626512,0.0,0.003362651215866208
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,MSE Loss 용도,-0.001483992,0.0,0.001483991974964738
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,Multi-Label 에서 CE + Softmax 적용 문제점,-0.0024293922,0.0,0.0024293921887874603
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,PEFT 방법 5가지,-0.0037157799,0.0,0.0037157798651605844
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,거대 언어 모델 정의,0.000106673855,0.0,0.00010667385504348204
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,기본 경험,-0.0015078854,0.0,0.0015078854048624635
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,답변 실패,0.9997715,1.0,0.00022852420806884766
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,딥러닝,-0.0022134066,0.0,0.0022134066093713045
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,마지막 할 말,-0.0027469238,0.0,0.002746923826634884
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,머신러닝,-0.0022986208,0.0,0.0022986207623034716
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,면접 시작 인사,-0.0013000404,0.0,0.0013000403996556997
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,상세 경험,-0.0029525193,0.0,0.0029525193385779858
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,수식,-0.0032329438,0.0,0.0032329438254237175
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,용어 질문,-0.00039206765,0.0,0.0003920676535926759
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,인공지능,-0.002848517,0.0,0.0028485169168561697
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,잠시 휴식,-0.0046243705,0.0,0.004624370485544205
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,좋아하는 아이돌,-0.0010882367,0.0,0.001088236691430211
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,핵심 아이디어,-0.0030112122,0.0,0.003011212218552828
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,확률 예측에서 MSE Loss 미 사용 이유,-0.0005502355,0.0,0.0005502355052158237
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,BCE 가 좋은 task,-0.0026244107,0.0,0.0026244106702506542
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,BCE 가 좋은 이유,-0.004380832,0.0,0.004380831960588694
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,LLM Fine-Tuning 의 PEFT,-0.0023711827,0.0,0.0023711826652288437
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,LoRA,-0.0025600244,0.0,0.002560024382546544
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,LoRA 와 QLoRA 의 차이,-0.0023594021,0.0,0.002359402133151889
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,Loss Function 예시,-0.0012506337,0.0,0.0012506337370723486
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,Loss Function 정의,-0.0013002307,0.0,0.0013002307387068868
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,MBTI,-0.0018042196,0.0,0.0018042195588350296
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,MSE Loss 설명,-0.0030677526,0.0,0.00306775257922709
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,MSE Loss 용도,-0.0031121313,0.0,0.0031121312640607357
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,Multi-Label 에서 CE + Softmax 적용 문제점,-0.002966168,0.0,0.0029661681037396193
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,PEFT 방법 5가지,-0.0033925073,0.0,0.0033925073221325874
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,거대 언어 모델 정의,-0.001333387,0.0,0.001333386986516416
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,기본 경험,-0.0016889172,0.0,0.0016889171674847603
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,답변 실패,0.99970794,1.0,0.00029206275939941406
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,딥러닝,-0.0012567153,0.0,0.0012567152734845877
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,마지막 할 말,-0.002719437,0.0,0.0027194370049983263
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,머신러닝,-0.002134752,0.0,0.002134751994162798
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,면접 시작 인사,-0.0005817405,0.0,0.0005817405181005597
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,상세 경험,-0.003580359,0.0,0.0035803590435534716
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,수식,-0.0043088254,0.0,0.004308825358748436
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,용어 질문,6.730302e-05,0.0,6.73030226607807e-05
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,인공지능,-0.002382146,0.0,0.0023821459617465734
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,잠시 휴식,-0.004282168,0.0,0.004282168112695217
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,좋아하는 아이돌,-0.0010241827,0.0,0.001024182653054595
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,핵심 아이디어,-0.0028967035,0.0,0.0028967035468667746
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,확률 예측에서 MSE Loss 미 사용 이유,0.0013337152,0.0,0.0013337151613086462
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,BCE 가 좋은 task,-0.002788053,0.0,0.0027880528941750526
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,BCE 가 좋은 이유,-0.0021656584,0.0,0.0021656583994627
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,LLM Fine-Tuning 의 PEFT,-0.00115637,0.0,0.0011563700390979648
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,LoRA,-0.0029161552,0.0,0.0029161551501601934
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,LoRA 와 QLoRA 의 차이,-0.004141931,0.0,0.00414193095639348
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,Loss Function 예시,-0.0027394672,0.0,0.002739467192441225
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,Loss Function 정의,-0.0045467857,0.0,0.004546785727143288
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,MBTI,-0.003030382,0.0,0.0030303820967674255
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,MSE Loss 설명,-0.00086268235,0.0,0.0008626823546364903
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,MSE Loss 용도,0.00043529403,0.0,0.0004352940304670483
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,Multi-Label 에서 CE + Softmax 적용 문제점,-0.0066289366,0.0,0.006628936622291803
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,PEFT 방법 5가지,-0.0010291425,0.0,0.0010291425278410316
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,거대 언어 모델 정의,-0.0058952644,0.0,0.005895264446735382
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,기본 경험,-0.0057834373,0.0,0.00578343728557229
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,답변 실패,-0.0025011206,0.0,0.0025011205580085516
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,딥러닝,0.0032624966,0.0,0.003262496553361416
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,마지막 할 말,0.0010102073,0.0,0.001010207342915237
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,머신러닝,-0.0005579159,0.0,0.0005579158896580338
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,면접 시작 인사,-0.004512105,0.0,0.0045121051371097565
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,상세 경험,0.0015232846,0.0,0.0015232845908030868
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,수식,0.002919282,0.0,0.002919282065704465
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,용어 질문,0.0003495087,0.0,0.0003495086857583374
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,인공지능,0.0037588084,0.0,0.0037588083650916815
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,잠시 휴식,-0.0021065911,0.0,0.0021065911278128624
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,좋아하는 아이돌,-0.0003432136,0.0,0.00034321361454203725
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,핵심 아이디어,0.0013476153,0.0,0.001347615267150104
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,확률 예측에서 MSE Loss 미 사용 이유,0.9985532,1.0,0.0014467835426330566
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,BCE 가 좋은 task,0.005059913,0.0,0.005059912800788879
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,BCE 가 좋은 이유,-0.0050061415,0.0,0.005006141494959593
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,LLM Fine-Tuning 의 PEFT,-0.0027865148,0.0,0.002786514814943075
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,LoRA,-0.0011389813,0.0,0.0011389813153073192
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,LoRA 와 QLoRA 의 차이,-0.0018843918,0.0,0.0018843917641788721
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,Loss Function 예시,-0.0045593586,0.0,0.004559358581900597
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,Loss Function 정의,0.0016508889,0.0,0.0016508889384567738
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,MBTI,0.0011897137,0.0,0.0011897137155756354
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,MSE Loss 설명,-0.0049763834,0.0,0.004976383410394192
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,MSE Loss 용도,-0.0060458197,0.0,0.006045819725841284
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,Multi-Label 에서 CE + Softmax 적용 문제점,-0.00077136006,0.0,0.0007713600643910468
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,PEFT 방법 5가지,0.00017960108,0.0,0.0001796010765247047
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,거대 언어 모델 정의,-0.0065649804,0.0,0.006564980372786522
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,기본 경험,-0.0061029103,0.0,0.0061029102653265
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,답변 실패,-0.003347187,0.0,0.0033471870701760054
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,딥러닝,-0.0030329553,0.0,0.003032955341041088
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,마지막 할 말,-0.0040450064,0.0,0.004045006353408098
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,머신러닝,-0.0036058582,0.0,0.003605858189985156
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,면접 시작 인사,-0.00045908795,0.0,0.0004590879543684423
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,상세 경험,0.0037208162,0.0,0.0037208162248134613
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,수식,0.99807847,1.0,0.001921534538269043
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,용어 질문,-0.0045817997,0.0,0.004581799730658531
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,인공지능,0.0023739904,0.0,0.0023739903699606657
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,잠시 휴식,-0.001732558,0.0,0.001732558012008667
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,좋아하는 아이돌,-0.0011570663,0.0,0.0011570663191378117
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,핵심 아이디어,0.00074924686,0.0,0.0007492468575946987
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,확률 예측에서 MSE Loss 미 사용 이유,0.0027328539,0.0,0.0027328538708388805
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",BCE 가 좋은 task,-0.0069694826,0.0,0.006969482637941837
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",BCE 가 좋은 이유,-0.002373613,0.0,0.0023736129514873028
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",LLM Fine-Tuning 의 PEFT,-0.007479381,0.0,0.007479380816221237
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",LoRA,-0.0039580134,0.0,0.003958013374358416
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",LoRA 와 QLoRA 의 차이,0.002268937,0.0,0.002268936950713396
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",Loss Function 예시,-0.0046045776,0.0,0.004604577552527189
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",Loss Function 정의,-0.0029910135,0.0,0.0029910134617239237
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",MBTI,-0.0046902904,0.0,0.004690290428698063
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",MSE Loss 설명,0.0005539881,0.0,0.000553988094907254
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",MSE Loss 용도,-0.0018461518,0.0,0.0018461517756804824
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",Multi-Label 에서 CE + Softmax 적용 문제점,-0.004161158,0.0,0.004161158110946417
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",PEFT 방법 5가지,-0.0046463097,0.0,0.0046463096514344215
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",거대 언어 모델 정의,-0.004596058,0.0,0.004596057813614607
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",기본 경험,-0.0012472088,0.0,0.0012472087983042002
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",답변 실패,-0.0019516725,0.0,0.0019516724860295653
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",딥러닝,-0.0048007513,0.0,0.004800751339644194
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",마지막 할 말,-0.0041800016,0.0,0.004180001560598612
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",머신러닝,-0.0031124235,0.0,0.0031124234665185213
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",면접 시작 인사,-0.0023348685,0.0,0.002334868535399437
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",상세 경험,-0.0013817303,0.0,0.0013817303115502
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",수식,-0.007030805,0.0,0.007030805107206106
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",용어 질문,-0.0011084124,0.0,0.0011084123980253935
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",인공지능,-0.005956479,0.0,0.0059564788825809956
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",잠시 휴식,-0.0025386869,0.0,0.002538686851039529
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",좋아하는 아이돌,-0.0057080146,0.0,0.005708014592528343
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",핵심 아이디어,0.9985895,1.0,0.0014104843139648438
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",확률 예측에서 MSE Loss 미 사용 이유,-0.0031687913,0.0,0.003168791299685836
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,BCE 가 좋은 task,-0.008843978,0.0,0.008843977935612202
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,BCE 가 좋은 이유,-0.015100099,0.0,0.01510009914636612
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,LLM Fine-Tuning 의 PEFT,-0.014363204,0.0,0.014363204129040241
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,LoRA,-0.007402311,0.0,0.0074023110792040825
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,LoRA 와 QLoRA 의 차이,-0.0039865924,0.0,0.0039865924045443535
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,Loss Function 예시,0.010808213,0.0,0.01080821268260479
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,Loss Function 정의,-0.0061145434,0.0,0.006114543415606022
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,MBTI,-0.026685098,0.0,0.026685098186135292
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,MSE Loss 설명,-0.013924951,0.0,0.013924950733780861
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,MSE Loss 용도,0.001358832,0.0,0.001358831999823451
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,Multi-Label 에서 CE + Softmax 적용 문제점,0.069857545,0.0,0.06985754519701004
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,PEFT 방법 5가지,0.0010551082,0.0,0.0010551081504672766
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,거대 언어 모델 정의,-0.003197002,0.0,0.0031970019917935133
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,기본 경험,0.008830384,0.0,0.008830384351313114
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,답변 실패,0.043537106,1.0,0.9564628936350346
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,딥러닝,-0.04130568,0.0,0.04130567982792854
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,마지막 할 말,-0.008864131,0.0,0.008864130824804306
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,머신러닝,-0.0022370648,0.0,0.002237064763903618
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,면접 시작 인사,-0.0079693645,0.0,0.007969364523887634
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,상세 경험,0.0019377667,0.0,0.001937766675837338
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,수식,-0.006114623,0.0,0.0061146230436861515
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,용어 질문,-0.02370665,0.0,0.023706650361418724
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,인공지능,0.004594436,0.0,0.004594435915350914
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,잠시 휴식,-0.016679946,0.0,0.016679946333169937
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,좋아하는 아이돌,-0.0005829368,0.0,0.0005829368019476533
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,핵심 아이디어,0.9436074,0.0,0.9436073899269104
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,확률 예측에서 MSE Loss 미 사용 이유,-0.0050420472,0.0,0.005042047239840031
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",BCE 가 좋은 task,-0.0066070287,0.0,0.006607028655707836
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",BCE 가 좋은 이유,-0.0005089422,0.0,0.0005089421756565571
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",LLM Fine-Tuning 의 PEFT,-0.008191579,0.0,0.008191579021513462
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",LoRA,-0.004354929,0.0,0.004354929085820913
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",LoRA 와 QLoRA 의 차이,0.0007955849,0.0,0.0007955849287100136
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",Loss Function 예시,-0.0055079097,0.0,0.005507909692823887
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",Loss Function 정의,-0.003006816,0.0,0.0030068159103393555
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",MBTI,-0.005642874,0.0,0.005642874166369438
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",MSE Loss 설명,0.0033646761,0.0,0.0033646761439740658
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",MSE Loss 용도,-0.0006732905,0.0,0.0006732905167154968
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",Multi-Label 에서 CE + Softmax 적용 문제점,-0.004800504,0.0,0.004800504073500633
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",PEFT 방법 5가지,-0.0045063286,0.0,0.004506328608840704
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",거대 언어 모델 정의,-0.0050854734,0.0,0.0050854734145104885
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",기본 경험,-0.0012873382,0.0,0.001287338207475841
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",답변 실패,-0.0021438878,0.0,0.0021438878029584885
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",딥러닝,-0.0060706735,0.0,0.00607067346572876
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",마지막 할 말,-0.005924152,0.0,0.005924152210354805
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",머신러닝,-0.0028299405,0.0,0.0028299405239522457
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",면접 시작 인사,-0.0009540704,0.0,0.0009540704195387661
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",상세 경험,-6.931206e-05,0.0,6.931206007720903e-05
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",수식,-0.0072339983,0.0,0.0072339982725679874
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",용어 질문,-0.0007424338,0.0,0.00074243382550776
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",인공지능,-0.0059280223,0.0,0.00592802232131362
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",잠시 휴식,-0.0041428143,0.0,0.004142814315855503
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",좋아하는 아이돌,-0.00526973,0.0,0.005269729997962713
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",핵심 아이디어,0.99831396,1.0,0.0016860365867614746
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",확률 예측에서 MSE Loss 미 사용 이유,-0.0040195268,0.0,0.004019526764750481
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')]",BCE 가 좋은 task,0.003223425,0.0,0.0032234250102192163
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')]",BCE 가 좋은 이유,-0.005007188,0.0,0.005007187835872173
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')]",LLM Fine-Tuning 의 PEFT,-0.0025374887,0.0,0.002537488704547286
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')]",LoRA,-0.001244026,0.0,0.0012440260034054518
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')]",LoRA 와 QLoRA 의 차이,-0.00026513185,0.0,0.00026513185002841055
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')]",Loss Function 예시,-0.0036489014,0.0,0.0036489013582468033
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')]",Loss Function 정의,-0.00029173229,0.0,0.00029173228540457785
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')]",MBTI,0.0010808121,0.0,0.0010808120714500546
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')]",MSE Loss 설명,-0.005416174,0.0,0.005416173953562975
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')]",MSE Loss 용도,-0.006860949,0.0,0.006860949099063873
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')]",Multi-Label 에서 CE + Softmax 적용 문제점,-0.0016391783,0.0,0.001639178255572915
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')]",PEFT 방법 5가지,-0.0007825122,0.0,0.0007825121865607798
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')]",거대 언어 모델 정의,-0.0068747336,0.0,0.006874733604490757
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')]",기본 경험,-0.0058075543,0.0,0.0058075543493032455
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')]",답변 실패,-0.0031854212,0.0,0.0031854212284088135
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')]",딥러닝,-0.005575264,0.0,0.005575263872742653
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')]",마지막 할 말,-0.004250629,0.0,0.004250628873705864
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')]",머신러닝,-0.004922687,0.0,0.004922687076032162
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')]",면접 시작 인사,0.00073862216,0.0,0.0007386221550405025
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')]",상세 경험,0.005644215,0.0,0.005644214805215597
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')]",수식,0.9977082,1.0,0.0022917985916137695
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')]",용어 질문,-0.0064618145,0.0,0.006461814511567354
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')]",인공지능,0.0034267255,0.0,0.003426725510507822
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')]",잠시 휴식,-0.001355433,0.0,0.00135543302167207
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')]",좋아하는 아이돌,-0.000857404,0.0,0.0008574040257371962
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')]",핵심 아이디어,0.004074409,0.0,0.0040744091384112835
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')]",확률 예측에서 MSE Loss 미 사용 이유,0.0034186088,0.0,0.0034186088014394045
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다",BCE 가 좋은 task,-0.0072756335,0.0,0.00727563351392746
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다",BCE 가 좋은 이유,-0.0019421736,0.0,0.0019421735778450966
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다",LLM Fine-Tuning 의 PEFT,-0.007907207,0.0,0.007907207123935223
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다",LoRA,-0.005034552,0.0,0.005034551955759525
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다",LoRA 와 QLoRA 의 차이,0.0011614431,0.0,0.001161443069577217
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다",Loss Function 예시,-0.0054227947,0.0,0.005422794725745916
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다",Loss Function 정의,-0.003116628,0.0,0.003116627922281623
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다",MBTI,-0.0036192292,0.0,0.0036192291881889105
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다",MSE Loss 설명,0.00022692938,0.0,0.00022692937636747956
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다",MSE Loss 용도,-0.0012575795,0.0,0.0012575795408338308
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다",Multi-Label 에서 CE + Softmax 적용 문제점,-0.0051989406,0.0,0.005198940634727478
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다",PEFT 방법 5가지,-0.0036776194,0.0,0.0036776193883270025
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다",거대 언어 모델 정의,-0.0041311,0.0,0.00413110014051199
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다",기본 경험,-0.0015474461,0.0,0.0015474461251869798
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다",답변 실패,-0.0020882066,0.0,0.002088206587359309
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다",딥러닝,-0.0046305154,0.0,0.004630515351891518
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다",마지막 할 말,-0.004120642,0.0,0.004120641853660345
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다",머신러닝,-0.0029199198,0.0,0.002919919788837433
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다",면접 시작 인사,-0.0023295053,0.0,0.00232950528152287
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다",상세 경험,-0.0021789332,0.0,0.0021789332386106253
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다",수식,-0.007183199,0.0,0.007183198817074299
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다",용어 질문,-0.0012541555,0.0,0.001254155533388257
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다",인공지능,-0.0067885914,0.0,0.006788591388612986
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다",잠시 휴식,-0.0027404712,0.0,0.0027404711581766605
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다",좋아하는 아이돌,-0.006380659,0.0,0.006380659062415361
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다",핵심 아이디어,0.9985643,1.0,0.001435697078704834
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다",확률 예측에서 MSE Loss 미 사용 이유,-0.00402217,0.0,0.004022169858217239
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거,BCE 가 좋은 task,-0.0027450442,0.0,0.002745044184848666
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거,BCE 가 좋은 이유,-0.004189013,0.0,0.004189013037830591
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거,LLM Fine-Tuning 의 PEFT,-0.0024119648,0.0,0.002411964815109968
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거,LoRA,-0.0030434297,0.0,0.0030434296932071447
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거,LoRA 와 QLoRA 의 차이,-0.0019983817,0.0,0.0019983816891908646
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거,Loss Function 예시,-0.0014444097,0.0,0.0014444097178056836
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거,Loss Function 정의,-0.00144984,0.0,0.0014498400269076228
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거,MBTI,-0.0024759672,0.0,0.002475967165082693
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거,MSE Loss 설명,-0.0024227356,0.0,0.002422735560685396
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거,MSE Loss 용도,-0.002492492,0.0,0.002492492087185383
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거,Multi-Label 에서 CE + Softmax 적용 문제점,-0.0028814257,0.0,0.002881425665691495
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거,PEFT 방법 5가지,-0.0034290552,0.0,0.0034290552139282227
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거,거대 언어 모델 정의,-0.0003794143,0.0,0.00037941429764032364
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거,기본 경험,-0.0017298613,0.0,0.001729861251078546
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거,답변 실패,0.99978465,1.0,0.00021535158157348633
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거,딥러닝,-0.0018509697,0.0,0.001850969740189612
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거,마지막 할 말,-0.0021740857,0.0,0.0021740857046097517
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거,머신러닝,-0.0021753903,0.0,0.0021753902547061443
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거,면접 시작 인사,-0.0015670926,0.0,0.001567092607729137
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거,상세 경험,-0.0030196498,0.0,0.0030196497682482004
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거,수식,-0.0029129337,0.0,0.0029129337053745985
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거,용어 질문,-0.0007254834,0.0,0.0007254834054037929
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거,인공지능,-0.0024188564,0.0,0.002418856369331479
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거,잠시 휴식,-0.0042427457,0.0,0.004242745693773031
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거,좋아하는 아이돌,-0.0015520474,0.0,0.0015520474407821894
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거,핵심 아이디어,-0.0016370489,0.0,0.0016370489029213786
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거,확률 예측에서 MSE Loss 미 사용 이유,-0.0008522187,0.0,0.0008522187126800418
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지",BCE 가 좋은 task,-0.006284509,0.0,0.006284508854150772
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지",BCE 가 좋은 이유,-0.0021696074,0.0,0.002169607440009713
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지",LLM Fine-Tuning 의 PEFT,-0.008781265,0.0,0.008781264536082745
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지",LoRA,-0.005068585,0.0,0.005068584810942411
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지",LoRA 와 QLoRA 의 차이,0.0011797105,0.0,0.0011797104962170124
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지",Loss Function 예시,-0.003930361,0.0,0.00393036101013422
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지",Loss Function 정의,-0.0035257246,0.0,0.00352572463452816
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지",MBTI,-0.003384205,0.0,0.0033842050470411777
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지",MSE Loss 설명,0.00044202176,0.0,0.0004420217592269182
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지",MSE Loss 용도,-0.001317075,0.0,0.0013170749880373478
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지",Multi-Label 에서 CE + Softmax 적용 문제점,-0.0055497447,0.0,0.005549744702875614
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지",PEFT 방법 5가지,-0.0038113273,0.0,0.003811327274888754
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지",거대 언어 모델 정의,-0.0041442197,0.0,0.004144219681620598
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지",기본 경험,-0.0013142116,0.0,0.0013142116367816925
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지",답변 실패,-0.0012572833,0.0,0.0012572832638397813
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지",딥러닝,-0.0048721647,0.0,0.004872164689004421
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지",마지막 할 말,-0.003361832,0.0,0.00336183188483119
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지",머신러닝,-0.0034921903,0.0,0.003492190269753337
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지",면접 시작 인사,-0.0028339743,0.0,0.002833974314853549
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지",상세 경험,-0.002766431,0.0,0.002766431076452136
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지",수식,-0.0064902394,0.0,0.006490239407867193
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지",용어 질문,-0.001304934,0.0,0.0013049340341240168
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지",인공지능,-0.006376271,0.0,0.006376271136105061
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지",잠시 휴식,-0.0025576195,0.0,0.0025576194748282433
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지",좋아하는 아이돌,-0.0063123535,0.0,0.006312353536486626
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지",핵심 아이디어,0.9986467,1.0,0.0013533234596252441
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지",확률 예측에서 MSE Loss 미 사용 이유,-0.004292034,0.0,0.004292034078389406
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야?",BCE 가 좋은 task,0.0011024566,0.0,0.0011024565901607275
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야?",BCE 가 좋은 이유,-0.00019260238,0.0,0.00019260238332208246
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야?",LLM Fine-Tuning 의 PEFT,-0.0026271725,0.0,0.0026271725073456764
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야?",LoRA,-0.0009328518,0.0,0.0009328518062829971
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야?",LoRA 와 QLoRA 의 차이,-0.0020814063,0.0,0.0020814063027501106
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야?",Loss Function 예시,0.0024374134,0.0,0.00243741343729198
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야?",Loss Function 정의,0.002369255,0.0,0.0023692550603300333
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야?",MBTI,-0.0010304409,0.0,0.0010304409079253674
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야?",MSE Loss 설명,-0.00079944386,0.0,0.0007994438637979329
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야?",MSE Loss 용도,-0.005011969,0.0,0.005011968780308962
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야?",Multi-Label 에서 CE + Softmax 적용 문제점,-0.0019389434,0.0,0.0019389434019103646
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야?",PEFT 방법 5가지,0.00016903211,0.0,0.00016903210780583322
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야?",거대 언어 모델 정의,-0.003180127,0.0,0.003180126892402768
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야?",기본 경험,0.0005635468,0.0,0.0005635467823594809
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야?",답변 실패,0.0005113653,0.0,0.0005113653023727238
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야?",딥러닝,-0.000185705,0.0,0.000185704993782565
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야?",마지막 할 말,-0.0024440594,0.0,0.002444059355184436
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야?",머신러닝,0.00021431418,0.0,0.00021431417553685606
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야?",면접 시작 인사,-0.0067983316,0.0,0.006798331625759602
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야?",상세 경험,-0.00062976417,0.0,0.0006297641666606069
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야?",수식,-0.0049145236,0.0,0.00491452356800437
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야?",용어 질문,0.9976805,1.0,0.002319514751434326
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야?",인공지능,-0.0017307356,0.0,0.0017307356465607882
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야?",잠시 휴식,-3.459017e-05,0.0,3.459017170825973e-05
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야?",좋아하는 아이돌,-0.00081258954,0.0,0.0008125895401462913
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야?",핵심 아이디어,-0.0015774578,0.0,0.00157745776232332
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야?",확률 예측에서 MSE Loss 미 사용 이유,0.0038293712,0.0,0.0038293711841106415
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",BCE 가 좋은 task,-0.0026402227,0.0,0.002640222664922476
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",BCE 가 좋은 이유,-0.0043940786,0.0,0.004394078627228737
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",LLM Fine-Tuning 의 PEFT,-0.0019521904,0.0,0.0019521904177963734
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",LoRA,-0.0031736041,0.0,0.003173604141920805
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",LoRA 와 QLoRA 의 차이,-0.0019655894,0.0,0.0019655893556773663
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",Loss Function 예시,-0.0016106581,0.0,0.0016106581315398216
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",Loss Function 정의,-0.00124103,0.0,0.0012410300550982356
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",MBTI,-0.0012467451,0.0,0.0012467451160773635
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",MSE Loss 설명,-0.0028986186,0.0,0.0028986185789108276
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",MSE Loss 용도,-0.0024894662,0.0,0.002489466220140457
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",Multi-Label 에서 CE + Softmax 적용 문제점,-0.0023346662,0.0,0.0023346662055701017
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",PEFT 방법 5가지,-0.0038002168,0.0,0.003800216829404235
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",거대 언어 모델 정의,-0.00068968255,0.0,0.0006896825507283211
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",기본 경험,-0.0013864754,0.0,0.0013864754000678658
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",답변 실패,0.9997202,1.0,0.0002797842025756836
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",딥러닝,-0.001856547,0.0,0.0018565469654276967
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",마지막 할 말,-0.0023514738,0.0,0.0023514737840741873
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",머신러닝,-0.0030668168,0.0,0.003066816832870245
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",면접 시작 인사,-0.000750909,0.0,0.0007509089773520827
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",상세 경험,-0.0037865876,0.0,0.0037865876220166683
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",수식,-0.0036711995,0.0,0.0036711995489895344
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",용어 질문,4.9988852e-05,0.0,4.998885196982883e-05
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",인공지능,-0.0019351738,0.0,0.0019351737573742867
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",잠시 휴식,-0.0042014173,0.0,0.004201417323201895
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",좋아하는 아이돌,-0.00087857526,0.0,0.0008785752579569817
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",핵심 아이디어,-0.003013398,0.0,0.0030133980326354504
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",확률 예측에서 MSE Loss 미 사용 이유,-0.0016644793,0.0,0.0016644792631268501
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",BCE 가 좋은 task,-0.0019055445,1.0,1.0019055445445701
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",BCE 가 좋은 이유,-0.0047812928,0.0,0.004781292751431465
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",LLM Fine-Tuning 의 PEFT,-0.002016834,0.0,0.002016833983361721
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",LoRA,-0.0027402183,0.0,0.0027402183040976524
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",LoRA 와 QLoRA 의 차이,-0.002123953,0.0,0.002123953076079488
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",Loss Function 예시,-0.0034191487,0.0,0.003419148735702038
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",Loss Function 정의,-0.0026105635,0.0,0.0026105635333806276
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",MBTI,-0.0010047266,0.0,0.0010047266259789467
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",MSE Loss 설명,-0.0026042578,0.0,0.00260425778105855
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",MSE Loss 용도,-0.001941251,0.0,0.0019412509864196181
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",Multi-Label 에서 CE + Softmax 적용 문제점,-0.001593245,0.0,0.0015932449605315924
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",PEFT 방법 5가지,-0.0031048744,0.0,0.003104874398559332
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",거대 언어 모델 정의,-0.0011825771,0.0,0.0011825771071016788
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",기본 경험,-0.00076272036,0.0,0.0007627203594893217
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",답변 실패,0.99948555,0.0,0.9994855523109436
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",딥러닝,-0.001582759,0.0,0.001582758966833353
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",마지막 할 말,-0.0019720055,0.0,0.001972005469724536
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",머신러닝,-0.001348554,0.0,0.0013485540403053164
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",면접 시작 인사,-0.00051499566,0.0,0.0005149956559762359
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",상세 경험,-0.0033034426,0.0,0.0033034426160156727
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",수식,-0.0045031793,0.0,0.004503179341554642
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",용어 질문,-0.00043245257,0.0,0.00043245256529189646
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",인공지능,-0.0034269325,0.0,0.0034269324969500303
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",잠시 휴식,-0.0033476853,0.0,0.0033476853277534246
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",좋아하는 아이돌,-0.002773339,0.0,0.0027733389288187027
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",핵심 아이디어,-0.0029034384,0.0,0.0029034384060651064
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",확률 예측에서 MSE Loss 미 사용 이유,-0.00070730376,0.0,0.0007073037559166551
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",BCE 가 좋은 task,0.0034169138,0.0,0.0034169137943536043
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",BCE 가 좋은 이유,0.9971824,1.0,0.0028175711631774902
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",LLM Fine-Tuning 의 PEFT,-0.00013811728,0.0,0.00013811727694701403
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",LoRA,-0.00065860606,0.0,0.00065860606264323
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",LoRA 와 QLoRA 의 차이,-0.0035610134,0.0,0.003561013378202915
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",Loss Function 예시,-0.0031819902,0.0,0.00318199023604393
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",Loss Function 정의,0.0007748658,0.0,0.0007748657953925431
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",MBTI,-0.002072365,0.0,0.0020723650231957436
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",MSE Loss 설명,-0.006367807,0.0,0.006367806810885668
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",MSE Loss 용도,-0.0023435238,0.0,0.002343523781746626
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",Multi-Label 에서 CE + Softmax 적용 문제점,-0.0026006992,0.0,0.002600699197500944
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",PEFT 방법 5가지,-0.002644512,0.0,0.002644512103870511
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",거대 언어 모델 정의,-0.0018753516,0.0,0.0018753516487777233
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",기본 경험,3.299079e-05,0.0,3.2990788895403966e-05
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",답변 실패,-0.0044744816,0.0,0.00447448156774044
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",딥러닝,5.203069e-05,0.0,5.2030689403181896e-05
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",마지막 할 말,0.0012456835,0.0,0.0012456835247576237
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",머신러닝,-0.00056159927,0.0,0.0005615992704406381
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",면접 시작 인사,-0.0012372786,0.0,0.0012372785713523626
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",상세 경험,0.0011059676,0.0,0.001105967559851706
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",수식,-0.0050052656,0.0,0.005005265586078167
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",용어 질문,0.0031293395,0.0,0.0031293395441025496
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",인공지능,-0.0061313924,0.0,0.006131392437964678
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",잠시 휴식,-0.0028648518,0.0,0.002864851849153638
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",좋아하는 아이돌,-0.0032516776,0.0,0.0032516776118427515
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",핵심 아이디어,-0.00047531383,0.0,0.0004753138346131891
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",확률 예측에서 MSE Loss 미 사용 이유,0.00042279545,0.0,0.0004227954486850649
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,BCE 가 좋은 task,-0.003652944,0.0,0.0036529439967125654
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,BCE 가 좋은 이유,-0.0034933488,0.0,0.0034933488350361586
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,LLM Fine-Tuning 의 PEFT,-0.0023278412,0.0,0.002327841240912676
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,LoRA,-0.0023730285,0.0,0.0023730285465717316
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,LoRA 와 QLoRA 의 차이,-0.0020688288,0.0,0.0020688287913799286
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,Loss Function 예시,-0.0019878633,0.0,0.0019878633320331573
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,Loss Function 정의,-0.0032427746,0.0,0.003242774633690715
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,MBTI,-0.0016830935,0.0,0.0016830934910103679
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,MSE Loss 설명,-0.00024699036,0.0,0.00024699035566300154
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,MSE Loss 용도,0.00031723492,0.0,0.00031723492429591715
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,Multi-Label 에서 CE + Softmax 적용 문제점,0.99882257,1.0,0.0011774301528930664
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,PEFT 방법 5가지,-5.7330908e-05,0.0,5.7330908020958304e-05
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,거대 언어 모델 정의,-0.003688868,0.0,0.0036888679023832083
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,기본 경험,-0.0042516757,0.0,0.004251675680279732
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,답변 실패,-0.0036763255,0.0,0.003676325548440218
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,딥러닝,-0.0025410252,0.0,0.0025410251691937447
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,마지막 할 말,-0.0016073533,0.0,0.0016073533333837986
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,머신러닝,-0.005223272,0.0,0.005223271902650595
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,면접 시작 인사,-0.0024386984,0.0,0.0024386984296143055
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,상세 경험,-0.0011082274,0.0,0.0011082274140790105
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,수식,4.912055e-05,0.0,4.91205501020886e-05
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,용어 질문,0.0005563101,0.0,0.000556310114916414
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,인공지능,0.00016710254,0.0,0.000167102538398467
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,잠시 휴식,-0.0015035438,0.0,0.0015035438118502498
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,좋아하는 아이돌,0.0009875039,0.0,0.0009875039104372263
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,핵심 아이디어,-0.0032388764,0.0,0.003238876350224018
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,확률 예측에서 MSE Loss 미 사용 이유,-0.005262148,0.0,0.005262148100882769
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,BCE 가 좋은 task,-0.0021904784,0.0,0.002190478378906846
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,BCE 가 좋은 이유,-0.0038442253,0.0,0.003844225313514471
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,LLM Fine-Tuning 의 PEFT,-0.002033485,0.0,0.002033485099673271
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,LoRA,-0.0034618676,0.0,0.003461867570877075
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,LoRA 와 QLoRA 의 차이,-0.0021444682,0.0,0.0021444682497531176
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,Loss Function 예시,-0.0027688288,0.0,0.0027688287664204836
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,Loss Function 정의,-0.0016786003,0.0,0.0016786003252491355
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,MBTI,-0.0016180188,0.0,0.001618018839508295
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,MSE Loss 설명,-0.002627032,0.0,0.002627032110467553
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,MSE Loss 용도,-0.0030076574,0.0,0.0030076573602855206
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,Multi-Label 에서 CE + Softmax 적용 문제점,-0.0012521107,0.0,0.001252110698260367
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,PEFT 방법 5가지,-0.0031069356,0.0,0.0031069356482475996
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,거대 언어 모델 정의,-0.00072128285,0.0,0.0007212828495539725
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,기본 경험,-0.0021048656,0.0,0.0021048656199127436
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,답변 실패,0.99970376,1.0,0.0002962350845336914
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,딥러닝,-0.0036886681,0.0,0.0036886681336909533
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,마지막 할 말,-0.002420797,0.0,0.002420797012746334
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,머신러닝,-0.0022627648,0.0,0.0022627648431807756
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,면접 시작 인사,-0.00090086716,0.0,0.0009008671622723341
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,상세 경험,-0.0039314963,0.0,0.003931496292352676
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,수식,-0.004486012,0.0,0.004486011806875467
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,용어 질문,-1.8716539e-05,0.0,1.8716538761509582e-05
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,인공지능,-0.0028895377,0.0,0.0028895377181470394
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,잠시 휴식,-0.0046503716,0.0,0.004650371614843607
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,좋아하는 아이돌,-0.0010177579,0.0,0.0010177579242736101
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,핵심 아이디어,-0.002944702,0.0,0.002944702049717307
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,확률 예측에서 MSE Loss 미 사용 이유,-0.0008575894,0.0,0.0008575894171372056
Loss Function 관련 실무 경험 -> [기본 경험] 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,BCE 가 좋은 task,-0.003707981,0.0,0.0037079809699207544
Loss Function 관련 실무 경험 -> [기본 경험] 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,BCE 가 좋은 이유,-0.0015216278,0.0,0.0015216277679428458
Loss Function 관련 실무 경험 -> [기본 경험] 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,LLM Fine-Tuning 의 PEFT,-0.0016739639,0.0,0.0016739638522267342
Loss Function 관련 실무 경험 -> [기본 경험] 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,LoRA,5.3575634e-05,0.0,5.357563350116834e-05
Loss Function 관련 실무 경험 -> [기본 경험] 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,LoRA 와 QLoRA 의 차이,0.001118365,0.0,0.0011183649767190218
Loss Function 관련 실무 경험 -> [기본 경험] 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,Loss Function 예시,-0.0026850274,0.0,0.0026850274298340082
Loss Function 관련 실무 경험 -> [기본 경험] 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,Loss Function 정의,-0.0015143701,0.0,0.0015143700875341892
Loss Function 관련 실무 경험 -> [기본 경험] 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,MBTI,-0.0036902137,0.0,0.0036902136635035276
Loss Function 관련 실무 경험 -> [기본 경험] 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,MSE Loss 설명,-0.0047735036,0.0,0.004773503635078669
Loss Function 관련 실무 경험 -> [기본 경험] 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,MSE Loss 용도,-0.0019985153,0.0,0.001998515333980322
Loss Function 관련 실무 경험 -> [기본 경험] 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,Multi-Label 에서 CE + Softmax 적용 문제점,-0.001760236,0.0,0.0017602359876036644
Loss Function 관련 실무 경험 -> [기본 경험] 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,PEFT 방법 5가지,-0.0016587813,0.0,0.0016587813152000308
Loss Function 관련 실무 경험 -> [기본 경험] 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,거대 언어 모델 정의,-0.0056407037,0.0,0.005640703719109297
Loss Function 관련 실무 경험 -> [기본 경험] 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,기본 경험,0.9990304,1.0,0.0009695887565612793
Loss Function 관련 실무 경험 -> [기본 경험] 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,답변 실패,-0.0011722601,0.0,0.0011722601484507322
Loss Function 관련 실무 경험 -> [기본 경험] 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,딥러닝,-0.00097412267,0.0,0.0009741226676851511
Loss Function 관련 실무 경험 -> [기본 경험] 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,마지막 할 말,0.00079840235,0.0,0.0007984023541212082
Loss Function 관련 실무 경험 -> [기본 경험] 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,머신러닝,0.0014252384,0.0,0.001425238442607224
Loss Function 관련 실무 경험 -> [기본 경험] 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,면접 시작 인사,0.0012187675,0.0,0.0012187674874439836
Loss Function 관련 실무 경험 -> [기본 경험] 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,상세 경험,-0.003645185,0.0,0.0036451849155128
Loss Function 관련 실무 경험 -> [기본 경험] 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,수식,-0.00535086,0.0,0.005350859835743904
Loss Function 관련 실무 경험 -> [기본 경험] 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,용어 질문,-0.00033908503,0.0,0.0003390850324649364
Loss Function 관련 실무 경험 -> [기본 경험] 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,인공지능,-0.00049618236,0.0,0.0004961823578923941
Loss Function 관련 실무 경험 -> [기본 경험] 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,잠시 휴식,-0.002811527,0.0,0.0028115271124988794
Loss Function 관련 실무 경험 -> [기본 경험] 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,좋아하는 아이돌,0.00011501933,0.0,0.00011501932749524713
Loss Function 관련 실무 경험 -> [기본 경험] 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,핵심 아이디어,-0.0011410756,0.0,0.0011410756269469857
Loss Function 관련 실무 경험 -> [기본 경험] 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,확률 예측에서 MSE Loss 미 사용 이유,-0.0038201562,0.0,0.0038201562128961086
Loss Function 관련 실무 경험 -> [상세 경험] 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,BCE 가 좋은 task,-0.0051539564,0.0,0.0051539563573896885
Loss Function 관련 실무 경험 -> [상세 경험] 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,BCE 가 좋은 이유,-7.975695e-06,0.0,7.975694643391762e-06
Loss Function 관련 실무 경험 -> [상세 경험] 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,LLM Fine-Tuning 의 PEFT,-0.007372488,0.0,0.007372487802058458
Loss Function 관련 실무 경험 -> [상세 경험] 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,LoRA,-0.000653555,0.0,0.000653554976452142
Loss Function 관련 실무 경험 -> [상세 경험] 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,LoRA 와 QLoRA 의 차이,-0.0021397385,0.0,0.002139738528057933
Loss Function 관련 실무 경험 -> [상세 경험] 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,Loss Function 예시,-0.0028665713,0.0,0.0028665713034570217
Loss Function 관련 실무 경험 -> [상세 경험] 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,Loss Function 정의,0.0014929303,0.0,0.00149293034337461
Loss Function 관련 실무 경험 -> [상세 경험] 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,MBTI,-0.0022134504,0.0,0.0022134503815323114
Loss Function 관련 실무 경험 -> [상세 경험] 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,MSE Loss 설명,-0.0055621513,0.0,0.005562151316553354
Loss Function 관련 실무 경험 -> [상세 경험] 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,MSE Loss 용도,-0.0034203306,0.0,0.003420330584049225
Loss Function 관련 실무 경험 -> [상세 경험] 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,Multi-Label 에서 CE + Softmax 적용 문제점,-0.002900527,0.0,0.0029005270916968584
Loss Function 관련 실무 경험 -> [상세 경험] 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,PEFT 방법 5가지,-0.0047279466,0.0,0.0047279465943574905
Loss Function 관련 실무 경험 -> [상세 경험] 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,거대 언어 모델 정의,-0.000770784,0.0,0.0007707839831709862
Loss Function 관련 실무 경험 -> [상세 경험] 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,기본 경험,-0.0030488516,0.0,0.0030488516204059124
Loss Function 관련 실무 경험 -> [상세 경험] 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,답변 실패,-0.0024073836,0.0,0.0024073836393654346
Loss Function 관련 실무 경험 -> [상세 경험] 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,딥러닝,0.0008218764,0.0,0.0008218763978220522
Loss Function 관련 실무 경험 -> [상세 경험] 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,마지막 할 말,-0.0012600939,0.0,0.001260093878954649
Loss Function 관련 실무 경험 -> [상세 경험] 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,머신러닝,-0.002213837,0.0,0.0022138371132314205
Loss Function 관련 실무 경험 -> [상세 경험] 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,면접 시작 인사,0.000906682,0.0,0.0009066819911822677
Loss Function 관련 실무 경험 -> [상세 경험] 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,상세 경험,0.9985811,1.0,0.0014188885688781738
Loss Function 관련 실무 경험 -> [상세 경험] 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,수식,0.0022755202,0.0,0.0022755202371627092
Loss Function 관련 실무 경험 -> [상세 경험] 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,용어 질문,0.00027990676,0.0,0.0002799067588057369
Loss Function 관련 실무 경험 -> [상세 경험] 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,인공지능,-0.0037006258,0.0,0.0037006258498877287
Loss Function 관련 실무 경험 -> [상세 경험] 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,잠시 휴식,-0.0012045141,0.0,0.001204514061100781
Loss Function 관련 실무 경험 -> [상세 경험] 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,좋아하는 아이돌,-0.00010948915,0.0,0.00010948914859909564
Loss Function 관련 실무 경험 -> [상세 경험] 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,핵심 아이디어,-0.0027316234,0.0,0.00273162336088717
Loss Function 관련 실무 경험 -> [상세 경험] 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,확률 예측에서 MSE Loss 미 사용 이유,0.0024004087,0.0,0.0024004087317734957
Loss Function 관련 실무 경험 -> [상세 경험] 근데 기억 진짜 안나는데 어떡하지?,BCE 가 좋은 task,-0.002784215,0.0,0.0027842149138450623
Loss Function 관련 실무 경험 -> [상세 경험] 근데 기억 진짜 안나는데 어떡하지?,BCE 가 좋은 이유,-0.0045917784,0.0,0.004591778386384249
Loss Function 관련 실무 경험 -> [상세 경험] 근데 기억 진짜 안나는데 어떡하지?,LLM Fine-Tuning 의 PEFT,-0.0020268336,0.0,0.0020268335938453674
Loss Function 관련 실무 경험 -> [상세 경험] 근데 기억 진짜 안나는데 어떡하지?,LoRA,-0.0037958059,0.0,0.0037958058528602123
Loss Function 관련 실무 경험 -> [상세 경험] 근데 기억 진짜 안나는데 어떡하지?,LoRA 와 QLoRA 의 차이,-0.0023174852,0.0,0.0023174851667135954
Loss Function 관련 실무 경험 -> [상세 경험] 근데 기억 진짜 안나는데 어떡하지?,Loss Function 예시,-0.0019506995,0.0,0.0019506994867697358
Loss Function 관련 실무 경험 -> [상세 경험] 근데 기억 진짜 안나는데 어떡하지?,Loss Function 정의,-0.0018380044,0.0,0.0018380044493824244
Loss Function 관련 실무 경험 -> [상세 경험] 근데 기억 진짜 안나는데 어떡하지?,MBTI,-0.0017064976,0.0,0.0017064976273104548
Loss Function 관련 실무 경험 -> [상세 경험] 근데 기억 진짜 안나는데 어떡하지?,MSE Loss 설명,-0.0023882864,0.0,0.002388286404311657
Loss Function 관련 실무 경험 -> [상세 경험] 근데 기억 진짜 안나는데 어떡하지?,MSE Loss 용도,-0.0027839644,0.0,0.0027839643880724907
Loss Function 관련 실무 경험 -> [상세 경험] 근데 기억 진짜 안나는데 어떡하지?,Multi-Label 에서 CE + Softmax 적용 문제점,-0.0024060546,0.0,0.0024060546420514584
Loss Function 관련 실무 경험 -> [상세 경험] 근데 기억 진짜 안나는데 어떡하지?,PEFT 방법 5가지,-0.002610572,0.0,0.002610571915283799
Loss Function 관련 실무 경험 -> [상세 경험] 근데 기억 진짜 안나는데 어떡하지?,거대 언어 모델 정의,-0.00063067017,0.0,0.000630670168902725
Loss Function 관련 실무 경험 -> [상세 경험] 근데 기억 진짜 안나는데 어떡하지?,기본 경험,-0.0014668453,0.0,0.0014668452786281705
Loss Function 관련 실무 경험 -> [상세 경험] 근데 기억 진짜 안나는데 어떡하지?,답변 실패,0.99975663,1.0,0.00024336576461791992
Loss Function 관련 실무 경험 -> [상세 경험] 근데 기억 진짜 안나는데 어떡하지?,딥러닝,-0.0023372616,0.0,0.0023372615687549114
Loss Function 관련 실무 경험 -> [상세 경험] 근데 기억 진짜 안나는데 어떡하지?,마지막 할 말,-0.002349974,0.0,0.002349973889067769
Loss Function 관련 실무 경험 -> [상세 경험] 근데 기억 진짜 안나는데 어떡하지?,머신러닝,-0.0028895685,0.0,0.0028895684517920017
Loss Function 관련 실무 경험 -> [상세 경험] 근데 기억 진짜 안나는데 어떡하지?,면접 시작 인사,-0.0015305254,0.0,0.0015305253909900784
Loss Function 관련 실무 경험 -> [상세 경험] 근데 기억 진짜 안나는데 어떡하지?,상세 경험,-0.001726181,0.0,0.0017261810135096312
Loss Function 관련 실무 경험 -> [상세 경험] 근데 기억 진짜 안나는데 어떡하지?,수식,-0.0034837734,0.0,0.0034837734419852495
Loss Function 관련 실무 경험 -> [상세 경험] 근데 기억 진짜 안나는데 어떡하지?,용어 질문,-0.0002837834,0.0,0.00028378338902257383
Loss Function 관련 실무 경험 -> [상세 경험] 근데 기억 진짜 안나는데 어떡하지?,인공지능,-0.0023773971,0.0,0.002377397147938609
Loss Function 관련 실무 경험 -> [상세 경험] 근데 기억 진짜 안나는데 어떡하지?,잠시 휴식,-0.003999653,0.0,0.003999652806669474
Loss Function 관련 실무 경험 -> [상세 경험] 근데 기억 진짜 안나는데 어떡하지?,좋아하는 아이돌,-0.00078400463,0.0,0.0007840046309866011
Loss Function 관련 실무 경험 -> [상세 경험] 근데 기억 진짜 안나는데 어떡하지?,핵심 아이디어,-0.0026552207,0.0,0.0026552206836640835
Loss Function 관련 실무 경험 -> [상세 경험] 근데 기억 진짜 안나는데 어떡하지?,확률 예측에서 MSE Loss 미 사용 이유,-0.0010062094,0.0,0.0010062094079330564
Loss Function 관련 실무 경험 -> [기본 경험] 나 아직 실무 경험 없는데 ㅠㅠ,BCE 가 좋은 task,-0.0023891376,0.0,0.0023891376331448555
Loss Function 관련 실무 경험 -> [기본 경험] 나 아직 실무 경험 없는데 ㅠㅠ,BCE 가 좋은 이유,-0.0018978999,0.0,0.0018978998996317387
Loss Function 관련 실무 경험 -> [기본 경험] 나 아직 실무 경험 없는데 ㅠㅠ,LLM Fine-Tuning 의 PEFT,-0.0014425981,0.0,0.0014425980625674129
Loss Function 관련 실무 경험 -> [기본 경험] 나 아직 실무 경험 없는데 ㅠㅠ,LoRA,-0.00016910653,0.0,0.0001691065263003111
Loss Function 관련 실무 경험 -> [기본 경험] 나 아직 실무 경험 없는데 ㅠㅠ,LoRA 와 QLoRA 의 차이,0.0014647517,0.0,0.001464751665480435
Loss Function 관련 실무 경험 -> [기본 경험] 나 아직 실무 경험 없는데 ㅠㅠ,Loss Function 예시,-0.0012026565,0.0,0.0012026565382257104
Loss Function 관련 실무 경험 -> [기본 경험] 나 아직 실무 경험 없는데 ㅠㅠ,Loss Function 정의,-0.0024285393,0.0,0.002428539330139756
Loss Function 관련 실무 경험 -> [기본 경험] 나 아직 실무 경험 없는데 ㅠㅠ,MBTI,-0.0025899354,0.0,0.0025899354368448257
Loss Function 관련 실무 경험 -> [기본 경험] 나 아직 실무 경험 없는데 ㅠㅠ,MSE Loss 설명,-0.0041587683,0.0,0.0041587683372199535
Loss Function 관련 실무 경험 -> [기본 경험] 나 아직 실무 경험 없는데 ㅠㅠ,MSE Loss 용도,-0.00083706796,0.0,0.0008370679570361972
Loss Function 관련 실무 경험 -> [기본 경험] 나 아직 실무 경험 없는데 ㅠㅠ,Multi-Label 에서 CE + Softmax 적용 문제점,-0.0015703889,0.0,0.0015703889075666666
Loss Function 관련 실무 경험 -> [기본 경험] 나 아직 실무 경험 없는데 ㅠㅠ,PEFT 방법 5가지,-0.0011240068,0.0,0.0011240068124607205
Loss Function 관련 실무 경험 -> [기본 경험] 나 아직 실무 경험 없는데 ㅠㅠ,거대 언어 모델 정의,-0.0065999166,0.0,0.006599916610866785
Loss Function 관련 실무 경험 -> [기본 경험] 나 아직 실무 경험 없는데 ㅠㅠ,기본 경험,0.99903226,1.0,0.0009677410125732422
Loss Function 관련 실무 경험 -> [기본 경험] 나 아직 실무 경험 없는데 ㅠㅠ,답변 실패,-0.0011199904,0.0,0.0011199903674423695
Loss Function 관련 실무 경험 -> [기본 경험] 나 아직 실무 경험 없는데 ㅠㅠ,딥러닝,-0.00083432574,0.0,0.0008343257359229028
Loss Function 관련 실무 경험 -> [기본 경험] 나 아직 실무 경험 없는데 ㅠㅠ,마지막 할 말,0.0005956164,0.0,0.0005956164095550776
Loss Function 관련 실무 경험 -> [기본 경험] 나 아직 실무 경험 없는데 ㅠㅠ,머신러닝,1.4744922e-05,0.0,1.474492182751419e-05
Loss Function 관련 실무 경험 -> [기본 경험] 나 아직 실무 경험 없는데 ㅠㅠ,면접 시작 인사,-0.00031762975,0.0,0.0003176297468598932
Loss Function 관련 실무 경험 -> [기본 경험] 나 아직 실무 경험 없는데 ㅠㅠ,상세 경험,-0.0030039388,0.0,0.0030039388220757246
Loss Function 관련 실무 경험 -> [기본 경험] 나 아직 실무 경험 없는데 ㅠㅠ,수식,-0.0075768507,0.0,0.007576850708574057
Loss Function 관련 실무 경험 -> [기본 경험] 나 아직 실무 경험 없는데 ㅠㅠ,용어 질문,-0.00096592854,0.0,0.0009659285424277186
Loss Function 관련 실무 경험 -> [기본 경험] 나 아직 실무 경험 없는데 ㅠㅠ,인공지능,-0.00022367772,0.0,0.00022367772180587053
Loss Function 관련 실무 경험 -> [기본 경험] 나 아직 실무 경험 없는데 ㅠㅠ,잠시 휴식,-0.002373463,0.0,0.0023734630085527897
Loss Function 관련 실무 경험 -> [기본 경험] 나 아직 실무 경험 없는데 ㅠㅠ,좋아하는 아이돌,-3.3634453e-05,0.0,3.3634452847763896e-05
Loss Function 관련 실무 경험 -> [기본 경험] 나 아직 실무 경험 없는데 ㅠㅠ,핵심 아이디어,-0.0021584365,0.0,0.002158436458557844
Loss Function 관련 실무 경험 -> [기본 경험] 나 아직 실무 경험 없는데 ㅠㅠ,확률 예측에서 MSE Loss 미 사용 이유,-0.0053923996,0.0,0.005392399616539478
Loss Function 관련 실무 경험 -> [기본 경험] 대답하기 싫은데 힝,BCE 가 좋은 task,-0.0032962237,0.0,0.0032962237019091845
Loss Function 관련 실무 경험 -> [기본 경험] 대답하기 싫은데 힝,BCE 가 좋은 이유,-0.004206478,0.0,0.004206478130072355
Loss Function 관련 실무 경험 -> [기본 경험] 대답하기 싫은데 힝,LLM Fine-Tuning 의 PEFT,-0.0020032884,0.0,0.002003288362175226
Loss Function 관련 실무 경험 -> [기본 경험] 대답하기 싫은데 힝,LoRA,-0.0036563813,0.0,0.0036563812755048275
Loss Function 관련 실무 경험 -> [기본 경험] 대답하기 싫은데 힝,LoRA 와 QLoRA 의 차이,-0.00249267,0.0,0.0024926699697971344
Loss Function 관련 실무 경험 -> [기본 경험] 대답하기 싫은데 힝,Loss Function 예시,-0.0014315097,0.0,0.001431509735994041
Loss Function 관련 실무 경험 -> [기본 경험] 대답하기 싫은데 힝,Loss Function 정의,-0.0023409058,0.0,0.002340905833989382
Loss Function 관련 실무 경험 -> [기본 경험] 대답하기 싫은데 힝,MBTI,-0.0012885884,0.0,0.0012885883916169405
Loss Function 관련 실무 경험 -> [기본 경험] 대답하기 싫은데 힝,MSE Loss 설명,-0.0020790158,0.0,0.0020790158305317163
Loss Function 관련 실무 경험 -> [기본 경험] 대답하기 싫은데 힝,MSE Loss 용도,-0.0029362328,0.0,0.0029362328350543976
Loss Function 관련 실무 경험 -> [기본 경험] 대답하기 싫은데 힝,Multi-Label 에서 CE + Softmax 적용 문제점,-0.002270392,0.0,0.002270391909405589
Loss Function 관련 실무 경험 -> [기본 경험] 대답하기 싫은데 힝,PEFT 방법 5가지,-0.0030943542,0.0,0.0030943541787564754
Loss Function 관련 실무 경험 -> [기본 경험] 대답하기 싫은데 힝,거대 언어 모델 정의,-0.00029257173,0.0,0.0002925717271864414
Loss Function 관련 실무 경험 -> [기본 경험] 대답하기 싫은데 힝,기본 경험,-0.00047721434,0.0,0.00047721434384584427
Loss Function 관련 실무 경험 -> [기본 경험] 대답하기 싫은데 힝,답변 실패,0.9997205,1.0,0.00027948617935180664
Loss Function 관련 실무 경험 -> [기본 경험] 대답하기 싫은데 힝,딥러닝,-0.0030700348,0.0,0.003070034785196185
Loss Function 관련 실무 경험 -> [기본 경험] 대답하기 싫은데 힝,마지막 할 말,-0.0031149797,0.0,0.003114979714155197
Loss Function 관련 실무 경험 -> [기본 경험] 대답하기 싫은데 힝,머신러닝,-0.002768404,0.0,0.002768404083326459
Loss Function 관련 실무 경험 -> [기본 경험] 대답하기 싫은데 힝,면접 시작 인사,-0.00087755977,0.0,0.0008775597671046853
Loss Function 관련 실무 경험 -> [기본 경험] 대답하기 싫은데 힝,상세 경험,-0.0031171048,0.0,0.003117104759439826
Loss Function 관련 실무 경험 -> [기본 경험] 대답하기 싫은데 힝,수식,-0.0034804665,0.0,0.0034804665483534336
Loss Function 관련 실무 경험 -> [기본 경험] 대답하기 싫은데 힝,용어 질문,-0.00053470646,0.0,0.0005347064579837024
Loss Function 관련 실무 경험 -> [기본 경험] 대답하기 싫은데 힝,인공지능,-0.0021630218,0.0,0.0021630218252539635
Loss Function 관련 실무 경험 -> [기본 경험] 대답하기 싫은데 힝,잠시 휴식,-0.0044064247,0.0,0.004406424704939127
Loss Function 관련 실무 경험 -> [기본 경험] 대답하기 싫은데 힝,좋아하는 아이돌,-0.0017436015,0.0,0.0017436015186831355
Loss Function 관련 실무 경험 -> [기본 경험] 대답하기 싫은데 힝,핵심 아이디어,-0.0023207844,0.0,0.0023207843769341707
Loss Function 관련 실무 경험 -> [기본 경험] 대답하기 싫은데 힝,확률 예측에서 MSE Loss 미 사용 이유,-0.0006977608,0.0,0.0006977607845328748
Loss Function 관련 실무 경험 -> [기본 경험] 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ,BCE 가 좋은 task,-0.003589095,0.0,0.0035890950821340084
Loss Function 관련 실무 경험 -> [기본 경험] 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ,BCE 가 좋은 이유,-0.002160213,0.0,0.002160212956368923
Loss Function 관련 실무 경험 -> [기본 경험] 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ,LLM Fine-Tuning 의 PEFT,-0.0016946018,0.0,0.0016946018440648913
Loss Function 관련 실무 경험 -> [기본 경험] 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ,LoRA,-0.00019744648,0.0,0.00019744648307096213
Loss Function 관련 실무 경험 -> [기본 경험] 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ,LoRA 와 QLoRA 의 차이,0.0017116487,0.0,0.0017116486560553312
Loss Function 관련 실무 경험 -> [기본 경험] 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ,Loss Function 예시,-0.00177446,0.0,0.0017744599608704448
Loss Function 관련 실무 경험 -> [기본 경험] 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ,Loss Function 정의,-0.0020386416,0.0,0.0020386415999382734
Loss Function 관련 실무 경험 -> [기본 경험] 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ,MBTI,-0.0029050168,0.0,0.002905016764998436
Loss Function 관련 실무 경험 -> [기본 경험] 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ,MSE Loss 설명,-0.004295833,0.0,0.004295832943171263
Loss Function 관련 실무 경험 -> [기본 경험] 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ,MSE Loss 용도,-0.0014662436,0.0,0.0014662436442449689
Loss Function 관련 실무 경험 -> [기본 경험] 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ,Multi-Label 에서 CE + Softmax 적용 문제점,-0.0024028528,0.0,0.0024028527550399303
Loss Function 관련 실무 경험 -> [기본 경험] 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ,PEFT 방법 5가지,-0.0011373089,0.0,0.0011373088927939534
Loss Function 관련 실무 경험 -> [기본 경험] 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ,거대 언어 모델 정의,-0.0055990047,0.0,0.005599004682153463
Loss Function 관련 실무 경험 -> [기본 경험] 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ,기본 경험,0.9990331,1.0,0.0009669065475463867
Loss Function 관련 실무 경험 -> [기본 경험] 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ,답변 실패,-0.0010993925,0.0,0.0010993925388902426
Loss Function 관련 실무 경험 -> [기본 경험] 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ,딥러닝,-0.00069273595,0.0,0.0006927359499968588
Loss Function 관련 실무 경험 -> [기본 경험] 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ,마지막 할 말,0.0009463968,0.0,0.0009463967871852219
Loss Function 관련 실무 경험 -> [기본 경험] 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ,머신러닝,0.0005986348,0.0,0.0005986348260194063
Loss Function 관련 실무 경험 -> [기본 경험] 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ,면접 시작 인사,0.0003100462,0.0,0.00031004619086161256
Loss Function 관련 실무 경험 -> [기본 경험] 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ,상세 경험,-0.003381146,0.0,0.0033811458852142096
Loss Function 관련 실무 경험 -> [기본 경험] 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ,수식,-0.0062065953,0.0,0.006206595338881016
Loss Function 관련 실무 경험 -> [기본 경험] 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ,용어 질문,-0.00048409993,0.0,0.0004840999317821115
Loss Function 관련 실무 경험 -> [기본 경험] 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ,인공지능,-0.0008986065,0.0,0.0008986064931377769
Loss Function 관련 실무 경험 -> [기본 경험] 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ,잠시 휴식,-0.0029539093,0.0,0.0029539093375205994
Loss Function 관련 실무 경험 -> [기본 경험] 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ,좋아하는 아이돌,-9.283785e-05,0.0,9.283785038860515e-05
Loss Function 관련 실무 경험 -> [기본 경험] 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ,핵심 아이디어,-0.0008146367,0.0,0.0008146367035806179
Loss Function 관련 실무 경험 -> [기본 경험] 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ,확률 예측에서 MSE Loss 미 사용 이유,-0.004460224,0.0,0.004460223950445652
Loss Function 관련 실무 경험 -> [상세 경험] 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어,BCE 가 좋은 task,-0.00573788,0.0,0.005737879779189825
Loss Function 관련 실무 경험 -> [상세 경험] 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어,BCE 가 좋은 이유,0.0013669202,0.0,0.001366920187138021
Loss Function 관련 실무 경험 -> [상세 경험] 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어,LLM Fine-Tuning 의 PEFT,-0.007266856,0.0,0.007266855798661709
Loss Function 관련 실무 경험 -> [상세 경험] 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어,LoRA,-0.002480483,0.0,0.00248048291541636
Loss Function 관련 실무 경험 -> [상세 경험] 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어,LoRA 와 QLoRA 의 차이,-0.004099948,0.0,0.004099947866052389
Loss Function 관련 실무 경험 -> [상세 경험] 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어,Loss Function 예시,-0.0021237312,0.0,0.0021237311884760857
Loss Function 관련 실무 경험 -> [상세 경험] 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어,Loss Function 정의,0.0032079266,0.0,0.0032079266384243965
Loss Function 관련 실무 경험 -> [상세 경험] 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어,MBTI,-0.0019308957,0.0,0.0019308957271277905
Loss Function 관련 실무 경험 -> [상세 경험] 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어,MSE Loss 설명,-0.0054079713,0.0,0.005407971329987049
Loss Function 관련 실무 경험 -> [상세 경험] 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어,MSE Loss 용도,-0.003772127,0.0,0.003772126976400614
Loss Function 관련 실무 경험 -> [상세 경험] 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어,Multi-Label 에서 CE + Softmax 적용 문제점,-0.0011204012,0.0,0.0011204011971130967
Loss Function 관련 실무 경험 -> [상세 경험] 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어,PEFT 방법 5가지,-0.00760681,0.0,0.007606809958815575
Loss Function 관련 실무 경험 -> [상세 경험] 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어,거대 언어 모델 정의,8.881868e-05,0.0,8.881867688614875e-05
Loss Function 관련 실무 경험 -> [상세 경험] 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어,기본 경험,-0.0026445647,0.0,0.002644564723595977
Loss Function 관련 실무 경험 -> [상세 경험] 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어,답변 실패,-0.0029006263,0.0,0.002900626277551055
Loss Function 관련 실무 경험 -> [상세 경험] 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어,딥러닝,-0.00012321596,0.0,0.00012321595568209887
Loss Function 관련 실무 경험 -> [상세 경험] 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어,마지막 할 말,-0.0016340694,0.0,0.00163406936917454
Loss Function 관련 실무 경험 -> [상세 경험] 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어,머신러닝,-0.0026775906,0.0,0.00267759058624506
Loss Function 관련 실무 경험 -> [상세 경험] 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어,면접 시작 인사,0.0015923047,0.0,0.0015923046739771962
Loss Function 관련 실무 경험 -> [상세 경험] 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어,상세 경험,0.9982935,1.0,0.0017064809799194336
Loss Function 관련 실무 경험 -> [상세 경험] 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어,수식,-0.0007733364,0.0,0.0007733363891020417
Loss Function 관련 실무 경험 -> [상세 경험] 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어,용어 질문,0.00035284445,0.0,0.00035284445038996637
Loss Function 관련 실무 경험 -> [상세 경험] 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어,인공지능,-0.003086855,0.0,0.003086854936555028
Loss Function 관련 실무 경험 -> [상세 경험] 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어,잠시 휴식,0.0007857739,0.0,0.0007857739110477269
Loss Function 관련 실무 경험 -> [상세 경험] 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어,좋아하는 아이돌,-0.00043340275,0.0,0.0004334027471486479
Loss Function 관련 실무 경험 -> [상세 경험] 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어,핵심 아이디어,-0.0018247189,0.0,0.001824718900024891
Loss Function 관련 실무 경험 -> [상세 경험] 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어,확률 예측에서 MSE Loss 미 사용 이유,0.00013242224,0.0,0.00013242223940324038
Loss Function 관련 실무 경험 -> [상세 경험] 기억 진짜 안 나는데 가물가물하다,BCE 가 좋은 task,-0.0029629767,0.0,0.0029629766941070557
Loss Function 관련 실무 경험 -> [상세 경험] 기억 진짜 안 나는데 가물가물하다,BCE 가 좋은 이유,-0.004220129,0.0,0.004220128990709782
Loss Function 관련 실무 경험 -> [상세 경험] 기억 진짜 안 나는데 가물가물하다,LLM Fine-Tuning 의 PEFT,-0.0022079747,0.0,0.0022079746704548597
Loss Function 관련 실무 경험 -> [상세 경험] 기억 진짜 안 나는데 가물가물하다,LoRA,-0.0037264342,0.0,0.0037264341954141855
Loss Function 관련 실무 경험 -> [상세 경험] 기억 진짜 안 나는데 가물가물하다,LoRA 와 QLoRA 의 차이,-0.0023923186,0.0,0.0023923185653984547
Loss Function 관련 실무 경험 -> [상세 경험] 기억 진짜 안 나는데 가물가물하다,Loss Function 예시,-0.0016867025,0.0,0.0016867024824023247
Loss Function 관련 실무 경험 -> [상세 경험] 기억 진짜 안 나는데 가물가물하다,Loss Function 정의,-0.0018436624,0.0,0.0018436623504385352
Loss Function 관련 실무 경험 -> [상세 경험] 기억 진짜 안 나는데 가물가물하다,MBTI,-0.0019014054,0.0,0.0019014053978025913
Loss Function 관련 실무 경험 -> [상세 경험] 기억 진짜 안 나는데 가물가물하다,MSE Loss 설명,-0.002719048,0.0,0.0027190479449927807
Loss Function 관련 실무 경험 -> [상세 경험] 기억 진짜 안 나는데 가물가물하다,MSE Loss 용도,-0.0026463366,0.0,0.0026463365647941828
Loss Function 관련 실무 경험 -> [상세 경험] 기억 진짜 안 나는데 가물가물하다,Multi-Label 에서 CE + Softmax 적용 문제점,-0.0024679417,0.0,0.002467941725626588
Loss Function 관련 실무 경험 -> [상세 경험] 기억 진짜 안 나는데 가물가물하다,PEFT 방법 5가지,-0.0029659735,0.0,0.0029659734573215246
Loss Function 관련 실무 경험 -> [상세 경험] 기억 진짜 안 나는데 가물가물하다,거대 언어 모델 정의,-0.0005932099,0.0,0.0005932098720222712
Loss Function 관련 실무 경험 -> [상세 경험] 기억 진짜 안 나는데 가물가물하다,기본 경험,-0.0014547093,0.0,0.001454709330573678
Loss Function 관련 실무 경험 -> [상세 경험] 기억 진짜 안 나는데 가물가물하다,답변 실패,0.9997475,1.0,0.0002524852752685547
Loss Function 관련 실무 경험 -> [상세 경험] 기억 진짜 안 나는데 가물가물하다,딥러닝,-0.0024496182,0.0,0.002449618186801672
Loss Function 관련 실무 경험 -> [상세 경험] 기억 진짜 안 나는데 가물가물하다,마지막 할 말,-0.0021982356,0.0,0.002198235597461462
Loss Function 관련 실무 경험 -> [상세 경험] 기억 진짜 안 나는데 가물가물하다,머신러닝,-0.00230881,0.0,0.002308809896931052
Loss Function 관련 실무 경험 -> [상세 경험] 기억 진짜 안 나는데 가물가물하다,면접 시작 인사,-0.0015927926,0.0,0.001592792570590973
Loss Function 관련 실무 경험 -> [상세 경험] 기억 진짜 안 나는데 가물가물하다,상세 경험,-0.0016399779,0.0,0.0016399779124185443
Loss Function 관련 실무 경험 -> [상세 경험] 기억 진짜 안 나는데 가물가물하다,수식,-0.003356593,0.0,0.0033565929625183344
Loss Function 관련 실무 경험 -> [상세 경험] 기억 진짜 안 나는데 가물가물하다,용어 질문,-0.0003236096,0.0,0.00032360959448851645
Loss Function 관련 실무 경험 -> [상세 경험] 기억 진짜 안 나는데 가물가물하다,인공지능,-0.0027775886,0.0,0.002777588553726673
Loss Function 관련 실무 경험 -> [상세 경험] 기억 진짜 안 나는데 가물가물하다,잠시 휴식,-0.0036519042,0.0,0.0036519041750580072
Loss Function 관련 실무 경험 -> [상세 경험] 기억 진짜 안 나는데 가물가물하다,좋아하는 아이돌,-0.0010524595,0.0,0.0010524594690650702
Loss Function 관련 실무 경험 -> [상세 경험] 기억 진짜 안 나는데 가물가물하다,핵심 아이디어,-0.003074788,0.0,0.003074788022786379
Loss Function 관련 실무 경험 -> [상세 경험] 기억 진짜 안 나는데 가물가물하다,확률 예측에서 MSE Loss 미 사용 이유,-0.0012662332,0.0,0.0012662331573665142
Loss Function 관련 실무 경험 -> [기본 경험] 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ,BCE 가 좋은 task,-0.0028306448,0.0,0.0028306448366492987
Loss Function 관련 실무 경험 -> [기본 경험] 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ,BCE 가 좋은 이유,-0.001694073,0.0,0.0016940729692578316
Loss Function 관련 실무 경험 -> [기본 경험] 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ,LLM Fine-Tuning 의 PEFT,-0.0010841407,0.0,0.0010841407347470522
Loss Function 관련 실무 경험 -> [기본 경험] 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ,LoRA,-0.00017844752,0.0,0.0001784475171007216
Loss Function 관련 실무 경험 -> [기본 경험] 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ,LoRA 와 QLoRA 의 차이,0.001801975,0.0,0.0018019749550148845
Loss Function 관련 실무 경험 -> [기본 경험] 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ,Loss Function 예시,-0.0012027663,0.0,0.0012027663178741932
Loss Function 관련 실무 경험 -> [기본 경험] 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ,Loss Function 정의,-0.0023533634,0.0,0.002353363437578082
Loss Function 관련 실무 경험 -> [기본 경험] 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ,MBTI,-0.003342146,0.0,0.0033421460539102554
Loss Function 관련 실무 경험 -> [기본 경험] 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ,MSE Loss 설명,-0.00430123,0.0,0.0043012299574911594
Loss Function 관련 실무 경험 -> [기본 경험] 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ,MSE Loss 용도,-0.00048083244,0.0,0.00048083244473673403
Loss Function 관련 실무 경험 -> [기본 경험] 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ,Multi-Label 에서 CE + Softmax 적용 문제점,-0.0016567681,0.0,0.0016567681450396776
Loss Function 관련 실무 경험 -> [기본 경험] 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ,PEFT 방법 5가지,-0.0011442512,0.0,0.0011442512040957808
Loss Function 관련 실무 경험 -> [기본 경험] 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ,거대 언어 모델 정의,-0.0069996025,0.0,0.0069996025413274765
Loss Function 관련 실무 경험 -> [기본 경험] 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ,기본 경험,0.9990133,1.0,0.0009866952896118164
Loss Function 관련 실무 경험 -> [기본 경험] 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ,답변 실패,-0.0012057378,0.0,0.0012057378189638257
Loss Function 관련 실무 경험 -> [기본 경험] 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ,딥러닝,-0.0009696099,0.0,0.0009696098859421909
Loss Function 관련 실무 경험 -> [기본 경험] 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ,마지막 할 말,0.0009915337,0.0,0.0009915337432175875
Loss Function 관련 실무 경험 -> [기본 경험] 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ,머신러닝,0.0002137054,0.0,0.00021370539616327733
Loss Function 관련 실무 경험 -> [기본 경험] 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ,면접 시작 인사,0.00032844456,0.0,0.0003284445556346327
Loss Function 관련 실무 경험 -> [기본 경험] 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ,상세 경험,-0.0032031585,0.0,0.003203158499673009
Loss Function 관련 실무 경험 -> [기본 경험] 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ,수식,-0.007122331,0.0,0.007122330833226442
Loss Function 관련 실무 경험 -> [기본 경험] 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ,용어 질문,-0.000527356,0.0,0.0005273559945635498
Loss Function 관련 실무 경험 -> [기본 경험] 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ,인공지능,6.4683074e-05,0.0,6.468307401519269e-05
Loss Function 관련 실무 경험 -> [기본 경험] 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ,잠시 휴식,-0.0023747361,0.0,0.002374736126512289
Loss Function 관련 실무 경험 -> [기본 경험] 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ,좋아하는 아이돌,0.00030295085,0.0,0.0003029508516192436
Loss Function 관련 실무 경험 -> [기본 경험] 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ,핵심 아이디어,-0.001847057,0.0,0.0018470570212230086
Loss Function 관련 실무 경험 -> [기본 경험] 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ,확률 예측에서 MSE Loss 미 사용 이유,-0.00475777,0.0,0.004757769871503115
Loss Function 관련 실무 경험 -> [기본 경험] 이걸 왜 알려줘야 하지,BCE 가 좋은 task,-0.0042382833,0.0,0.004238283261656761
Loss Function 관련 실무 경험 -> [기본 경험] 이걸 왜 알려줘야 하지,BCE 가 좋은 이유,-0.004381234,0.0,0.00438123382627964
Loss Function 관련 실무 경험 -> [기본 경험] 이걸 왜 알려줘야 하지,LLM Fine-Tuning 의 PEFT,-0.0025844576,0.0,0.002584457630291581
Loss Function 관련 실무 경험 -> [기본 경험] 이걸 왜 알려줘야 하지,LoRA,-0.004199514,0.0,0.004199514165520668
Loss Function 관련 실무 경험 -> [기본 경험] 이걸 왜 알려줘야 하지,LoRA 와 QLoRA 의 차이,-0.0015933444,0.0,0.0015933443792164326
Loss Function 관련 실무 경험 -> [기본 경험] 이걸 왜 알려줘야 하지,Loss Function 예시,-0.0016946143,0.0,0.0016946143005043268
Loss Function 관련 실무 경험 -> [기본 경험] 이걸 왜 알려줘야 하지,Loss Function 정의,-0.0022633257,0.0,0.002263325732201338
Loss Function 관련 실무 경험 -> [기본 경험] 이걸 왜 알려줘야 하지,MBTI,-0.0011846465,0.0,0.0011846465058624744
Loss Function 관련 실무 경험 -> [기본 경험] 이걸 왜 알려줘야 하지,MSE Loss 설명,-0.0006470393,0.0,0.0006470393273048103
Loss Function 관련 실무 경험 -> [기본 경험] 이걸 왜 알려줘야 하지,MSE Loss 용도,-0.0035868662,0.0,0.00358686619438231
Loss Function 관련 실무 경험 -> [기본 경험] 이걸 왜 알려줘야 하지,Multi-Label 에서 CE + Softmax 적용 문제점,-0.0019995002,0.0,0.0019995002076029778
Loss Function 관련 실무 경험 -> [기본 경험] 이걸 왜 알려줘야 하지,PEFT 방법 5가지,-0.0038067605,0.0,0.003806760534644127
Loss Function 관련 실무 경험 -> [기본 경험] 이걸 왜 알려줘야 하지,거대 언어 모델 정의,-0.000494998,0.0,0.0004949980066157877
Loss Function 관련 실무 경험 -> [기본 경험] 이걸 왜 알려줘야 하지,기본 경험,0.0006356527,0.0,0.0006356526864692569
Loss Function 관련 실무 경험 -> [기본 경험] 이걸 왜 알려줘야 하지,답변 실패,0.99958175,1.0,0.000418245792388916
Loss Function 관련 실무 경험 -> [기본 경험] 이걸 왜 알려줘야 하지,딥러닝,-0.0029967416,0.0,0.002996741561219096
Loss Function 관련 실무 경험 -> [기본 경험] 이걸 왜 알려줘야 하지,마지막 할 말,-0.0027010352,0.0,0.0027010352350771427
Loss Function 관련 실무 경험 -> [기본 경험] 이걸 왜 알려줘야 하지,머신러닝,-0.0040184995,0.0,0.00401849951595068
Loss Function 관련 실무 경험 -> [기본 경험] 이걸 왜 알려줘야 하지,면접 시작 인사,-0.00025902962,0.0,0.00025902962079271674
Loss Function 관련 실무 경험 -> [기본 경험] 이걸 왜 알려줘야 하지,상세 경험,-0.0026784933,0.0,0.002678493270650506
Loss Function 관련 실무 경험 -> [기본 경험] 이걸 왜 알려줘야 하지,수식,-0.0030755266,0.0,0.003075526561588049
Loss Function 관련 실무 경험 -> [기본 경험] 이걸 왜 알려줘야 하지,용어 질문,-0.0004627448,0.0,0.00046274479245767
Loss Function 관련 실무 경험 -> [기본 경험] 이걸 왜 알려줘야 하지,인공지능,-0.002271898,0.0,0.002271898090839386
Loss Function 관련 실무 경험 -> [기본 경험] 이걸 왜 알려줘야 하지,잠시 휴식,-0.0044362047,0.0,0.0044362046755850315
Loss Function 관련 실무 경험 -> [기본 경험] 이걸 왜 알려줘야 하지,좋아하는 아이돌,-0.0014323218,0.0,0.0014323218492791057
Loss Function 관련 실무 경험 -> [기본 경험] 이걸 왜 알려줘야 하지,핵심 아이디어,-0.0015308734,0.0,0.0015308733563870192
Loss Function 관련 실무 경험 -> [기본 경험] 이걸 왜 알려줘야 하지,확률 예측에서 MSE Loss 미 사용 이유,-0.0011931257,0.0,0.001193125732243061
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,BCE 가 좋은 task,-0.0026792022,0.0,0.002679202239960432
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,BCE 가 좋은 이유,-0.0016215331,0.0,0.0016215330688282847
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,LLM Fine-Tuning 의 PEFT,-0.0024106915,0.0,0.002410691464319825
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,LoRA,-0.003723398,0.0,0.003723398083820939
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,LoRA 와 QLoRA 의 차이,-0.0035193826,0.0,0.0035193825606256723
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,Loss Function 예시,-0.004056169,0.0,0.004056169185787439
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,Loss Function 정의,0.0016446863,0.0,0.0016446863301098347
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,MBTI,0.9986815,1.0,0.001318514347076416
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,MSE Loss 설명,-0.0013205031,0.0,0.0013205030700191855
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,MSE Loss 용도,-0.003771442,0.0,0.003771441988646984
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,Multi-Label 에서 CE + Softmax 적용 문제점,-0.002450722,0.0,0.002450722036883235
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,PEFT 방법 5가지,0.0020900487,0.0,0.0020900487434118986
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,거대 언어 모델 정의,-0.0011190865,0.0,0.0011190865188837051
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,기본 경험,-0.002944257,0.0,0.0029442571103572845
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,답변 실패,-0.0022706005,0.0,0.002270600525662303
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,딥러닝,-0.001892438,0.0,0.0018924380419775844
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,마지막 할 말,-0.0032565873,0.0,0.0032565873116254807
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,머신러닝,-5.8602578e-05,0.0,5.860257806489244e-05
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,면접 시작 인사,-0.0035550483,0.0,0.003555048257112503
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,상세 경험,-0.0028126657,0.0,0.002812665654346347
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,수식,-0.0030606233,0.0,0.0030606233049184084
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,용어 질문,0.0008918334,0.0,0.0008918333915062249
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,인공지능,-0.0037380361,0.0,0.003738036146387458
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,잠시 휴식,-0.0011363591,0.0,0.0011363590601831675
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,좋아하는 아이돌,-0.00231057,0.0,0.0023105700965970755
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,핵심 아이디어,-0.0042982204,0.0,0.0042982203885912895
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,확률 예측에서 MSE Loss 미 사용 이유,-0.0010733827,0.0,0.0010733826784417033
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,BCE 가 좋은 task,0.0007405608,0.0,0.0007405608193948865
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,BCE 가 좋은 이유,-0.0003095393,0.0,0.00030953928944654763
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,LLM Fine-Tuning 의 PEFT,-0.0029622884,0.0,0.002962288446724415
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,LoRA,-0.0004708868,0.0,0.0004708867927547544
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,LoRA 와 QLoRA 의 차이,0.0044163154,0.0,0.004416315350681543
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,Loss Function 예시,-0.0029674026,0.0,0.002967402571812272
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,Loss Function 정의,-0.0018607114,0.0,0.0018607113743200898
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,MBTI,-0.0033278372,0.0,0.0033278372138738632
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,MSE Loss 설명,0.00044172266,0.0,0.00044172265916131437
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,MSE Loss 용도,-0.0022120734,0.0,0.0022120734211057425
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,Multi-Label 에서 CE + Softmax 적용 문제점,-0.0009818466,0.0,0.0009818465914577246
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,PEFT 방법 5가지,-0.0026423144,0.0,0.002642314415425062
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,거대 언어 모델 정의,-0.0015758674,0.0,0.0015758674126118422
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,기본 경험,-0.0011492043,0.0,0.0011492043267935514
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,답변 실패,-0.0008464557,0.0,0.0008464556885883212
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,딥러닝,-0.0013454292,0.0,0.0013454292202368379
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,마지막 할 말,-0.0022377141,0.0,0.0022377141285687685
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,머신러닝,-0.001898831,0.0,0.0018988309893757105
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,면접 시작 인사,-0.0051837997,0.0,0.005183799657970667
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,상세 경험,-0.00083668355,0.0,0.0008366835536435246
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,수식,-0.00026178593,0.0,0.00026178592815995216
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,용어 질문,-0.00037450317,0.0,0.0003745031717699021
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,인공지능,-0.0035789798,0.0,0.003578979754820466
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,잠시 휴식,0.00069476955,0.0,0.0006947695510461926
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,좋아하는 아이돌,0.9990102,1.0,0.0009897947311401367
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,핵심 아이디어,-0.008437429,0.0,0.008437428623437881
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,확률 예측에서 MSE Loss 미 사용 이유,0.0006938374,0.0,0.0006938374135643244
잠시 휴식 -> 재미있는 이야기 해줄래?,BCE 가 좋은 task,0.0009967524,0.0,0.0009967524092644453
잠시 휴식 -> 재미있는 이야기 해줄래?,BCE 가 좋은 이유,-0.0020021738,0.0,0.002002173801884055
잠시 휴식 -> 재미있는 이야기 해줄래?,LLM Fine-Tuning 의 PEFT,-0.0030326333,0.0,0.003032633336260915
잠시 휴식 -> 재미있는 이야기 해줄래?,LoRA,-0.004229203,0.0,0.00422920286655426
잠시 휴식 -> 재미있는 이야기 해줄래?,LoRA 와 QLoRA 의 차이,-0.0017582803,0.0,0.0017582803266122937
잠시 휴식 -> 재미있는 이야기 해줄래?,Loss Function 예시,-0.0036536357,0.0,0.003653635736554861
잠시 휴식 -> 재미있는 이야기 해줄래?,Loss Function 정의,0.0016767271,0.0,0.0016767270863056183
잠시 휴식 -> 재미있는 이야기 해줄래?,MBTI,-0.002534919,0.0,0.0025349189527332783
잠시 휴식 -> 재미있는 이야기 해줄래?,MSE Loss 설명,-0.0010424027,0.0,0.0010424026986584067
잠시 휴식 -> 재미있는 이야기 해줄래?,MSE Loss 용도,-0.0049126856,0.0,0.004912685602903366
잠시 휴식 -> 재미있는 이야기 해줄래?,Multi-Label 에서 CE + Softmax 적용 문제점,-0.0013864071,0.0,0.0013864070642739534
잠시 휴식 -> 재미있는 이야기 해줄래?,PEFT 방법 5가지,-0.0012512325,0.0,0.0012512324610725045
잠시 휴식 -> 재미있는 이야기 해줄래?,거대 언어 모델 정의,-0.0018951694,0.0,0.001895169378258288
잠시 휴식 -> 재미있는 이야기 해줄래?,기본 경험,-0.00337095,0.0,0.003370949998497963
잠시 휴식 -> 재미있는 이야기 해줄래?,답변 실패,-0.004187922,0.0,0.004187921993434429
잠시 휴식 -> 재미있는 이야기 해줄래?,딥러닝,-0.0021171873,0.0,0.00211718725040555
잠시 휴식 -> 재미있는 이야기 해줄래?,마지막 할 말,-0.0044240146,0.0,0.0044240145944058895
잠시 휴식 -> 재미있는 이야기 해줄래?,머신러닝,-0.005923578,0.0,0.0059235780499875546
잠시 휴식 -> 재미있는 이야기 해줄래?,면접 시작 인사,-0.0024143523,0.0,0.002414352260529995
잠시 휴식 -> 재미있는 이야기 해줄래?,상세 경험,-0.00026326257,0.0,0.00026326256920583546
잠시 휴식 -> 재미있는 이야기 해줄래?,수식,-0.000113978036,0.0,0.00011397803609725088
잠시 휴식 -> 재미있는 이야기 해줄래?,용어 질문,-0.00056761614,0.0,0.0005676161381416023
잠시 휴식 -> 재미있는 이야기 해줄래?,인공지능,-0.0029118778,0.0,0.002911877818405628
잠시 휴식 -> 재미있는 이야기 해줄래?,잠시 휴식,0.9993257,1.0,0.0006743073463439941
잠시 휴식 -> 재미있는 이야기 해줄래?,좋아하는 아이돌,0.0011650306,0.0,0.0011650306405499578
잠시 휴식 -> 재미있는 이야기 해줄래?,핵심 아이디어,-0.0017661955,0.0,0.0017661955207586288
잠시 휴식 -> 재미있는 이야기 해줄래?,확률 예측에서 MSE Loss 미 사용 이유,-0.0040770727,0.0,0.004077072720974684
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",BCE 가 좋은 task,-0.0008170879,0.0,0.0008170878863893449
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",BCE 가 좋은 이유,2.2081753e-05,0.0,2.208175283158198e-05
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",LLM Fine-Tuning 의 PEFT,0.99738586,1.0,0.002614140510559082
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",LoRA,-0.005468937,0.0,0.00546893710270524
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",LoRA 와 QLoRA 의 차이,-0.002355486,0.0,0.0023554859217256308
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",Loss Function 예시,-0.0019909635,0.0,0.0019909634720534086
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",Loss Function 정의,-0.0026477855,0.0,0.002647785469889641
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",MBTI,-0.0022839017,0.0,0.0022839016746729612
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",MSE Loss 설명,-0.0036422901,0.0,0.0036422901321202517
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",MSE Loss 용도,-0.0050838757,0.0,0.005083875730633736
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",Multi-Label 에서 CE + Softmax 적용 문제점,-0.0016705414,0.0,0.001670541358180344
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",PEFT 방법 5가지,0.0005590817,0.0,0.0005590816726908088
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",거대 언어 모델 정의,-0.004336259,0.0,0.004336258862167597
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",기본 경험,-0.0006496976,0.0,0.0006496976129710674
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",답변 실패,0.00021640073,0.0,0.0002164007310057059
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",딥러닝,0.00068423204,0.0,0.0006842320435680449
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",마지막 할 말,-0.005190078,0.0,0.005190078169107437
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",머신러닝,-0.0082143955,0.0,0.008214395493268967
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",면접 시작 인사,-0.0031975554,0.0,0.0031975554302334785
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",상세 경험,-0.005630306,0.0,0.005630305968225002
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",수식,-0.005282307,0.0,0.005282307043671608
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",용어 질문,-0.0046620984,0.0,0.004662098363041878
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",인공지능,0.0013735337,0.0,0.0013735337415710092
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",잠시 휴식,-0.004251119,0.0,0.004251119215041399
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",좋아하는 아이돌,-0.002576441,0.0,0.0025764410383999348
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",핵심 아이디어,-0.0097519485,0.0,0.009751948527991772
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",확률 예측에서 MSE Loss 미 사용 이유,-0.0010903244,0.0,0.0010903243673965335
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,BCE 가 좋은 task,-0.0024118582,0.0,0.0024118581786751747
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,BCE 가 좋은 이유,-0.004878988,0.0,0.0048789880238473415
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,LLM Fine-Tuning 의 PEFT,-0.0013239739,0.0,0.0013239738764241338
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,LoRA,-0.0031275244,0.0,0.003127524396404624
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,LoRA 와 QLoRA 의 차이,-0.002549951,0.0,0.0025499509647488594
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,Loss Function 예시,-0.0012263788,0.0,0.0012263788376003504
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,Loss Function 정의,-0.0019385434,0.0,0.0019385433988645673
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,MBTI,-0.0017825462,0.0,0.0017825461691245437
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,MSE Loss 설명,-0.0027863912,0.0,0.002786391181871295
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,MSE Loss 용도,-0.0034512316,0.0,0.0034512316342443228
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,Multi-Label 에서 CE + Softmax 적용 문제점,-0.002270574,0.0,0.0022705739829689264
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,PEFT 방법 5가지,-0.003013372,0.0,0.003013371955603361
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,거대 언어 모델 정의,-0.0007147398,0.0,0.0007147397845983505
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,기본 경험,-0.0016159692,0.0,0.00161596923135221
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,답변 실패,0.9997031,1.0,0.0002968907356262207
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,딥러닝,-0.0022678052,0.0,0.0022678051609545946
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,마지막 할 말,-0.0024074847,0.0,0.0024074846878647804
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,머신러닝,-0.002986975,0.0,0.0029869750142097473
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,면접 시작 인사,-0.0009113471,0.0,0.0009113471023738384
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,상세 경험,-0.0041008866,0.0,0.0041008866392076015
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,수식,-0.0038730116,0.0,0.003873011562973261
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,용어 질문,0.0003079403,0.0,0.0003079402958974242
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,인공지능,-0.0023416432,0.0,0.0023416432086378336
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,잠시 휴식,-0.0042321715,0.0,0.004232171457260847
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,좋아하는 아이돌,-0.00069574517,0.0,0.0006957451696507633
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,핵심 아이디어,-0.003429182,0.0,0.003429182106629014
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,확률 예측에서 MSE Loss 미 사용 이유,-0.0004799498,0.0,0.00047994981287047267
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야!,BCE 가 좋은 task,0.000795633,0.0,0.0007956330082379282
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야!,BCE 가 좋은 이유,-0.00046003,0.0,0.00046002998715266585
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야!,LLM Fine-Tuning 의 PEFT,0.997135,1.0,0.002865016460418701
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야!,LoRA,-0.0040543377,0.0,0.004054337739944458
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야!,LoRA 와 QLoRA 의 차이,-0.0015129391,0.0,0.0015129391103982925
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야!,Loss Function 예시,-0.0023645805,0.0,0.0023645805194973946
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야!,Loss Function 정의,-0.0041839248,0.0,0.0041839247569441795
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야!,MBTI,-0.0014842751,0.0,0.001484275097027421
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야!,MSE Loss 설명,-0.0039840187,0.0,0.003984018694609404
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야!,MSE Loss 용도,-0.0052449177,0.0,0.005244917701929808
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야!,Multi-Label 에서 CE + Softmax 적용 문제점,-0.00039091136,0.0,0.0003909113584086299
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야!,PEFT 방법 5가지,0.00028474355,0.0,0.00028474355349317193
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야!,거대 언어 모델 정의,-0.0027976376,0.0,0.002797637600451708
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야!,기본 경험,0.00038373662,0.0,0.000383736623916775
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야!,답변 실패,-0.0038942,0.0,0.0038942000828683376
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야!,딥러닝,0.0020106738,0.0,0.002010673750191927
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야!,마지막 할 말,-0.006872169,0.0,0.006872169207781553
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야!,머신러닝,-0.0060140556,0.0,0.006014055572450161
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야!,면접 시작 인사,-0.0035839153,0.0,0.0035839152988046408
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야!,상세 경험,-0.0051810206,0.0,0.005181020591408014
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야!,수식,-0.004969837,0.0,0.0049698371440172195
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야!,용어 질문,-0.004240282,0.0,0.004240281879901886
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야!,인공지능,-0.0006901307,0.0,0.0006901306915096939
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야!,잠시 휴식,-0.004027108,0.0,0.004027108196169138
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야!,좋아하는 아이돌,-0.0037167508,0.0,0.003716750768944621
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야!,핵심 아이디어,-0.010353607,0.0,0.01035360712558031
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야!,확률 예측에서 MSE Loss 미 사용 이유,-0.002496347,0.0,0.00249634706415236
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다,BCE 가 좋은 task,-0.0027465988,0.0,0.002746598795056343
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다,BCE 가 좋은 이유,-0.0051323553,0.0,0.005132355261594057
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다,LLM Fine-Tuning 의 PEFT,-0.0009871229,0.0,0.0009871228830888867
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다,LoRA,-0.0032270986,0.0,0.003227098612114787
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다,LoRA 와 QLoRA 의 차이,-0.0011083512,0.0,0.0011083511635661125
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다,Loss Function 예시,-0.002438462,0.0,0.002438462106510997
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다,Loss Function 정의,-0.0019724045,0.0,0.0019724045414477587
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다,MBTI,-0.0013463198,0.0,0.001346319797448814
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다,MSE Loss 설명,-0.0023367922,0.0,0.0023367921821773052
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다,MSE Loss 용도,-0.0027002846,0.0,0.0027002845890820026
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다,Multi-Label 에서 CE + Softmax 적용 문제점,-0.0020940288,0.0,0.002094028750434518
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다,PEFT 방법 5가지,-0.0034801543,0.0,0.0034801543224602938
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다,거대 언어 모델 정의,-0.00022679036,0.0,0.00022679036192130297
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다,기본 경험,-0.0011623552,0.0,0.0011623551836237311
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다,답변 실패,0.99960303,1.0,0.00039696693420410156
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다,딥러닝,-0.0023214344,0.0,0.0023214344400912523
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다,마지막 할 말,-0.0014126408,0.0,0.0014126407913863659
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다,머신러닝,-0.0024678747,0.0,0.0024678746704012156
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다,면접 시작 인사,-0.0006863729,0.0,0.0006863729213364422
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다,상세 경험,-0.0039705182,0.0,0.003970518242567778
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다,수식,-0.003932258,0.0,0.003932258114218712
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다,용어 질문,0.0001234913,0.0,0.00012349130702205002
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다,인공지능,-0.002869112,0.0,0.0028691119514405727
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다,잠시 휴식,-0.003988974,0.0,0.003988973796367645
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다,좋아하는 아이돌,-0.0011845136,0.0,0.001184513559564948
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다,핵심 아이디어,-0.003826607,0.0,0.0038266070187091827
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다,확률 예측에서 MSE Loss 미 사용 이유,-0.00088250893,0.0,0.0008825089316815138
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",BCE 가 좋은 task,-0.002422467,0.0,0.0024224671069532633
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",BCE 가 좋은 이유,-0.0040704254,0.0,0.004070425406098366
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",LLM Fine-Tuning 의 PEFT,0.0020523907,0.0,0.0020523907151073217
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",LoRA,-0.0036575627,0.0,0.0036575626581907272
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",LoRA 와 QLoRA 의 차이,-0.0026702723,0.0,0.0026702722534537315
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",Loss Function 예시,-0.00046057755,0.0,0.00046057754661887884
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",Loss Function 정의,-0.0020149704,0.0,0.0020149704068899155
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",MBTI,-0.0012464385,0.0,0.0012464384781196713
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",MSE Loss 설명,5.593176e-06,0.0,5.593175956164487e-06
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",MSE Loss 용도,-0.0034875455,0.0,0.003487545531243086
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",Multi-Label 에서 CE + Softmax 적용 문제점,-0.00054812705,0.0,0.0005481270491145551
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",PEFT 방법 5가지,0.9985349,1.0,0.0014650821685791016
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",거대 언어 모델 정의,-0.00093729404,0.0,0.0009372940403409302
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",기본 경험,0.000801059,0.0,0.0008010590099729598
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",답변 실패,-0.0033413288,0.0,0.0033413288183510303
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",딥러닝,-0.0032289147,0.0,0.0032289146911352873
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",마지막 할 말,0.0012111341,0.0,0.0012111341347917914
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",머신러닝,-0.0028690014,0.0,0.002869001356884837
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",면접 시작 인사,-0.0019634196,0.0,0.001963419606909156
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",상세 경험,-0.006085931,0.0,0.006085930857807398
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",수식,0.00020102452,0.0,0.0002010245225392282
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",용어 질문,-0.003046372,0.0,0.0030463719740509987
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",인공지능,-0.0041414783,0.0,0.004141478333622217
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",잠시 휴식,-0.0015138871,0.0,0.0015138870803639293
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",좋아하는 아이돌,-0.003463171,0.0,0.0034631709568202496
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",핵심 아이디어,-0.008641401,0.0,0.008641401305794716
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",확률 예측에서 MSE Loss 미 사용 이유,-0.0005683671,0.0,0.0005683670751750469
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,BCE 가 좋은 task,-0.0030893697,0.0,0.0030893697403371334
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,BCE 가 좋은 이유,-0.004114844,0.0,0.004114843904972076
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,LLM Fine-Tuning 의 PEFT,-0.002282368,0.0,0.002282368019223213
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,LoRA,-0.00337428,0.0,0.0033742799423635006
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,LoRA 와 QLoRA 의 차이,-0.0020431885,0.0,0.00204318854957819
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,Loss Function 예시,-0.002088383,0.0,0.002088383072987199
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,Loss Function 정의,-0.0018645526,0.0,0.0018645526142790914
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,MBTI,-0.0018138525,0.0,0.0018138524610549212
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,MSE Loss 설명,-0.0026658515,0.0,0.0026658514980226755
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,MSE Loss 용도,-0.0028650113,0.0,0.002865011338144541
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,Multi-Label 에서 CE + Softmax 적용 문제점,-0.0028432484,0.0,0.00284324842505157
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,PEFT 방법 5가지,-0.002077079,0.0,0.00207707891240716
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,거대 언어 모델 정의,-0.0007353711,0.0,0.0007353710825555027
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,기본 경험,-0.0014154251,0.0,0.0014154250966385007
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,답변 실패,0.9997454,1.0,0.00025457143783569336
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,딥러닝,-0.0022725414,0.0,0.0022725414019078016
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,마지막 할 말,-0.0015874077,0.0,0.0015874076634645462
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,머신러닝,-0.0030330734,0.0,0.0030330733861774206
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,면접 시작 인사,-0.0013552634,0.0,0.0013552634045481682
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,상세 경험,-0.0030254014,0.0,0.003025401383638382
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,수식,-0.0037314924,0.0,0.003731492441147566
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,용어 질문,-0.00029381973,0.0,0.0002938197285402566
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,인공지능,-0.0025225137,0.0,0.0025225137360394
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,잠시 휴식,-0.00434056,0.0,0.004340560175478458
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,좋아하는 아이돌,-0.0013235225,0.0,0.0013235225342214108
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,핵심 아이디어,-0.0023582075,0.0,0.002358207479119301
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,확률 예측에서 MSE Loss 미 사용 이유,-0.0007998764,0.0,0.0007998764049261808
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,BCE 가 좋은 task,-0.003254442,0.0,0.003254442010074854
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,BCE 가 좋은 이유,-0.0010602599,0.0,0.001060259877704084
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,LLM Fine-Tuning 의 PEFT,-0.000732881,0.0,0.0007328810170292854
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,LoRA,0.99761015,1.0,0.002389848232269287
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,LoRA 와 QLoRA 의 차이,0.0035866813,0.0,0.0035866813268512487
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,Loss Function 예시,-0.0011184096,0.0,0.0011184095637872815
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,Loss Function 정의,-0.0034507485,0.0,0.003450748510658741
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,MBTI,-0.003614669,0.0,0.003614668967202306
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,MSE Loss 설명,-0.0033641567,0.0,0.003364156698808074
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,MSE Loss 용도,-0.0019720613,0.0,0.001972061349079013
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,Multi-Label 에서 CE + Softmax 적용 문제점,-0.002102336,0.0,0.0021023359149694443
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,PEFT 방법 5가지,-0.0041998834,0.0,0.004199883434921503
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,거대 언어 모델 정의,-0.0034917851,0.0,0.003491785144433379
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,기본 경험,-3.9964005e-05,0.0,3.996400482719764e-05
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,답변 실패,-0.0037475969,0.0,0.0037475968711078167
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,딥러닝,-0.0035276937,0.0,0.003527693683281541
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,마지막 할 말,-0.0027102963,0.0,0.002710296306759119
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,머신러닝,-0.009991385,0.0,0.009991385042667389
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,면접 시작 인사,-0.0016491681,0.0,0.001649168087169528
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,상세 경험,-0.0018389184,0.0,0.0018389184260740876
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,수식,-0.003872848,0.0,0.003872848115861416
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,용어 질문,-0.0013062507,0.0,0.0013062506914138794
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,인공지능,-0.0023075233,0.0,0.002307523274794221
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,잠시 휴식,0.00023734302,0.0,0.00023734301794320345
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,좋아하는 아이돌,-0.0014687488,0.0,0.0014687487855553627
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,핵심 아이디어,-0.0033728466,0.0,0.0033728466369211674
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,확률 예측에서 MSE Loss 미 사용 이유,-0.002843817,0.0,0.0028438169974833727
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,BCE 가 좋은 task,-0.004200369,0.0,0.004200369119644165
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,BCE 가 좋은 이유,-0.0037136015,0.0,0.003713601501658559
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,LLM Fine-Tuning 의 PEFT,-0.0013980139,0.0,0.0013980139046907425
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,LoRA,-0.0020914085,0.0,0.002091408474370837
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,LoRA 와 QLoRA 의 차이,-0.0016964996,0.0,0.001696499646641314
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,Loss Function 예시,-0.0009245177,0.0,0.0009245176916010678
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,Loss Function 정의,-0.0020746873,0.0,0.0020746872760355473
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,MBTI,-0.0030014873,0.0,0.003001487348228693
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,MSE Loss 설명,-0.0020058448,0.0,0.0020058448426425457
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,MSE Loss 용도,-0.0020572941,0.0,0.0020572941284626722
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,Multi-Label 에서 CE + Softmax 적용 문제점,-0.0033353479,0.0,0.0033353478647768497
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,PEFT 방법 5가지,-0.003971296,0.0,0.0039712958969175816
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,거대 언어 모델 정의,-0.0006340349,0.0,0.0006340349209494889
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,기본 경험,-0.0010619127,0.0,0.0010619127424433827
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,답변 실패,0.99963474,1.0,0.00036525726318359375
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,딥러닝,-0.0013656423,0.0,0.0013656422961503267
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,마지막 할 말,-0.002256094,0.0,0.0022560940124094486
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,머신러닝,-0.00089031586,0.0,0.0008903158595785499
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,면접 시작 인사,-0.002068834,0.0,0.002068833913654089
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,상세 경험,-0.0032362628,0.0,0.0032362628262490034
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,수식,-0.003173399,0.0,0.003173399018123746
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,용어 질문,-0.0007029404,0.0,0.0007029403932392597
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,인공지능,-0.002653287,0.0,0.002653287025168538
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,잠시 휴식,-0.0035469371,0.0,0.003546937135979533
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,좋아하는 아이돌,-0.001177821,0.0,0.0011778209591284394
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,핵심 아이디어,-0.0024185102,0.0,0.0024185101501643658
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,확률 예측에서 MSE Loss 미 사용 이유,-0.0018571195,0.0,0.0018571194959804416
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야!,BCE 가 좋은 task,-0.0023257758,0.0,0.0023257758002728224
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야!,BCE 가 좋은 이유,-0.0018610426,0.0,0.0018610425759106874
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야!,LLM Fine-Tuning 의 PEFT,-0.0028745867,0.0,0.00287458673119545
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야!,LoRA,0.9976701,1.0,0.002329885959625244
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야!,LoRA 와 QLoRA 의 차이,-0.0017885749,0.0,0.0017885748529806733
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야!,Loss Function 예시,-0.0013819181,0.0,0.0013819180894643068
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야!,Loss Function 정의,-0.002928865,0.0,0.002928864909335971
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야!,MBTI,-0.0027586843,0.0,0.0027586843352764845
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야!,MSE Loss 설명,-0.0026481124,0.0,0.002648112364113331
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야!,MSE Loss 용도,-0.0008441561,0.0,0.0008441560785286129
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야!,Multi-Label 에서 CE + Softmax 적용 문제점,-0.001508945,0.0,0.0015089450171217322
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야!,PEFT 방법 5가지,-0.004693237,0.0,0.004693237133324146
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야!,거대 언어 모델 정의,-0.0011014242,0.0,0.0011014242190867662
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야!,기본 경험,-8.747753e-05,0.0,8.747752872295678e-05
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야!,답변 실패,-0.004074513,0.0,0.004074512980878353
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야!,딥러닝,-0.0015824821,0.0,0.0015824821311980486
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야!,마지막 할 말,-0.003776388,0.0,0.0037763880100101233
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야!,머신러닝,-0.007661253,0.0,0.007661253213882446
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야!,면접 시작 인사,-0.0019112001,0.0,0.0019112001173198223
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야!,상세 경험,-0.0009779215,0.0,0.0009779215324670076
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야!,수식,-0.003340038,0.0,0.0033400380052626133
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야!,용어 질문,-0.0012639234,0.0,0.001263923360966146
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야!,인공지능,-0.0003101079,0.0,0.00031010789098218083
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야!,잠시 휴식,-0.0018088942,0.0,0.0018088942160829902
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야!,좋아하는 아이돌,-0.0011420902,0.0,0.0011420901864767075
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야!,핵심 아이디어,-0.0061984397,0.0,0.006198439747095108
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야!,확률 예측에서 MSE Loss 미 사용 이유,-0.0027936487,0.0,0.0027936487458646297
LoRA -> 무슨 OOM 없앤다는 것 같은데,BCE 가 좋은 task,-0.0030365773,0.0,0.0030365772545337677
LoRA -> 무슨 OOM 없앤다는 것 같은데,BCE 가 좋은 이유,-0.003952668,0.0,0.00395266804844141
LoRA -> 무슨 OOM 없앤다는 것 같은데,LLM Fine-Tuning 의 PEFT,-0.0022495934,0.0,0.0022495933808386326
LoRA -> 무슨 OOM 없앤다는 것 같은데,LoRA,-0.0025385718,0.0,0.002538571832701564
LoRA -> 무슨 OOM 없앤다는 것 같은데,LoRA 와 QLoRA 의 차이,-0.0023859409,0.0,0.002385940868407488
LoRA -> 무슨 OOM 없앤다는 것 같은데,Loss Function 예시,-0.0020267214,0.0,0.0020267213694751263
LoRA -> 무슨 OOM 없앤다는 것 같은데,Loss Function 정의,-0.0015762446,0.0,0.0015762445982545614
LoRA -> 무슨 OOM 없앤다는 것 같은데,MBTI,-0.0021471742,0.0,0.002147174207493663
LoRA -> 무슨 OOM 없앤다는 것 같은데,MSE Loss 설명,-0.002451033,0.0,0.0024510330986231565
LoRA -> 무슨 OOM 없앤다는 것 같은데,MSE Loss 용도,-0.0027523404,0.0,0.0027523403987288475
LoRA -> 무슨 OOM 없앤다는 것 같은데,Multi-Label 에서 CE + Softmax 적용 문제점,-0.0023974127,0.0,0.0023974126670509577
LoRA -> 무슨 OOM 없앤다는 것 같은데,PEFT 방법 5가지,-0.0028877347,0.0,0.002887734677642584
LoRA -> 무슨 OOM 없앤다는 것 같은데,거대 언어 모델 정의,-0.0005443224,0.0,0.0005443224217742682
LoRA -> 무슨 OOM 없앤다는 것 같은데,기본 경험,-0.0016751105,0.0,0.0016751105431467295
LoRA -> 무슨 OOM 없앤다는 것 같은데,답변 실패,0.99980694,1.0,0.00019305944442749023
LoRA -> 무슨 OOM 없앤다는 것 같은데,딥러닝,-0.0020195781,0.0,0.0020195781253278255
LoRA -> 무슨 OOM 없앤다는 것 같은데,마지막 할 말,-0.0022939045,0.0,0.0022939045447856188
LoRA -> 무슨 OOM 없앤다는 것 같은데,머신러닝,-0.0019156483,0.0,0.0019156483467668295
LoRA -> 무슨 OOM 없앤다는 것 같은데,면접 시작 인사,-0.0012873948,0.0,0.001287394785322249
LoRA -> 무슨 OOM 없앤다는 것 같은데,상세 경험,-0.0031454382,0.0,0.003145438153296709
LoRA -> 무슨 OOM 없앤다는 것 같은데,수식,-0.0030560743,0.0,0.003056074259802699
LoRA -> 무슨 OOM 없앤다는 것 같은데,용어 질문,-0.0004359647,0.0,0.00043596469913609326
LoRA -> 무슨 OOM 없앤다는 것 같은데,인공지능,-0.0019901025,0.0,0.0019901024643331766
LoRA -> 무슨 OOM 없앤다는 것 같은데,잠시 휴식,-0.0044475296,0.0,0.004447529558092356
LoRA -> 무슨 OOM 없앤다는 것 같은데,좋아하는 아이돌,-0.0011667182,0.0,0.001166718197055161
LoRA -> 무슨 OOM 없앤다는 것 같은데,핵심 아이디어,-0.0026465296,0.0,0.002646529581397772
LoRA -> 무슨 OOM 없앤다는 것 같은데,확률 예측에서 MSE Loss 미 사용 이유,-0.0008248169,0.0,0.0008248168742284179
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,BCE 가 좋은 task,-0.0023417654,0.0,0.002341765444725752
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,BCE 가 좋은 이유,-0.0029597185,0.0,0.0029597184620797634
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,LLM Fine-Tuning 의 PEFT,0.00039828857,0.0,0.0003982885682489723
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,LoRA,-0.0018363821,0.0,0.0018363820854574442
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,LoRA 와 QLoRA 의 차이,0.998718,1.0,0.0012819766998291016
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,Loss Function 예시,-0.004826029,0.0,0.00482602883130312
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,Loss Function 정의,-0.0036392377,0.0,0.0036392377223819494
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,MBTI,-0.004234324,0.0,0.004234324209392071
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,MSE Loss 설명,-0.008368603,0.0,0.008368602953851223
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,MSE Loss 용도,-0.0014374399,0.0,0.001437439932487905
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,Multi-Label 에서 CE + Softmax 적용 문제점,-0.0009432982,0.0,0.0009432982187718153
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,PEFT 방법 5가지,-0.005628887,0.0,0.005628887098282576
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,거대 언어 모델 정의,-0.0054734196,0.0,0.0054734195582568645
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,기본 경험,0.0015463269,0.0,0.0015463269082829356
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,답변 실패,-0.0015799075,0.0,0.001579907489940524
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,딥러닝,-0.003918627,0.0,0.003918626811355352
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,마지막 할 말,-0.0011922135,0.0,0.0011922135017812252
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,머신러닝,0.00097648555,0.0,0.0009764855494722724
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,면접 시작 인사,-0.003498692,0.0,0.003498692065477371
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,상세 경험,-0.0030292184,0.0,0.0030292184092104435
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,수식,-0.00082917215,0.0,0.0008291721460409462
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,용어 질문,-0.0034117447,0.0,0.0034117447212338448
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,인공지능,-0.0047019287,0.0,0.004701928701251745
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,잠시 휴식,-0.0013549845,0.0,0.0013549844734370708
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,좋아하는 아이돌,0.0023779504,0.0,0.0023779503535479307
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,핵심 아이디어,0.001593576,0.0,0.0015935760457068682
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,확률 예측에서 MSE Loss 미 사용 이유,-0.0019236603,0.0,0.0019236602820456028
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,BCE 가 좋은 task,-0.0035334984,0.0,0.0035334983840584755
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,BCE 가 좋은 이유,-0.00424018,0.0,0.004240179900079966
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,LLM Fine-Tuning 의 PEFT,-0.0026385498,0.0,0.0026385497767478228
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,LoRA,-0.0031213914,0.0,0.0031213914044201374
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,LoRA 와 QLoRA 의 차이,-0.0002859932,0.0,0.00028599321376532316
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,Loss Function 예시,-0.0016008157,0.0,0.0016008156817406416
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,Loss Function 정의,-0.0014078876,0.0,0.0014078875537961721
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,MBTI,-0.0020390698,0.0,0.002039069775491953
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,MSE Loss 설명,-0.0024839593,0.0,0.002483959309756756
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,MSE Loss 용도,-0.0023091773,0.0,0.002309177303686738
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,Multi-Label 에서 CE + Softmax 적용 문제점,-0.0031029987,0.0,0.0031029987148940563
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,PEFT 방법 5가지,-0.0032302723,0.0,0.003230272326618433
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,거대 언어 모델 정의,-0.00072811387,0.0,0.0007281138678081334
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,기본 경험,-0.0015296178,0.0,0.0015296178171411157
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,답변 실패,0.99974567,1.0,0.0002543330192565918
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,딥러닝,-0.0023416912,0.0,0.0023416911717504263
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,마지막 할 말,-0.002549376,0.0,0.002549376105889678
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,머신러닝,-0.0018966073,0.0,0.0018966073403134942
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,면접 시작 인사,-0.001481815,0.0,0.0014818150084465742
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,상세 경험,-0.0029133528,0.0,0.0029133528005331755
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,수식,-0.0035151956,0.0,0.003515195567160845
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,용어 질문,-0.00050291186,0.0,0.0005029118619859219
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,인공지능,-0.0022435456,0.0,0.0022435456048697233
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,잠시 휴식,-0.004035702,0.0,0.004035701975226402
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,좋아하는 아이돌,-0.0015047627,0.0,0.0015047626802697778
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,핵심 아이디어,-0.0024117683,0.0,0.0024117683060467243
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,확률 예측에서 MSE Loss 미 사용 이유,-0.0006634273,0.0,0.0006634272867813706
마지막 할 말 -> 로라야 정말 고마워!,BCE 가 좋은 task,-0.0022469407,0.0,0.002246940741315484
마지막 할 말 -> 로라야 정말 고마워!,BCE 가 좋은 이유,-0.0015951578,0.0,0.0015951577806845307
마지막 할 말 -> 로라야 정말 고마워!,LLM Fine-Tuning 의 PEFT,-0.0050649946,0.0,0.005064994562417269
마지막 할 말 -> 로라야 정말 고마워!,LoRA,-0.0016269562,0.0,0.0016269561601802707
마지막 할 말 -> 로라야 정말 고마워!,LoRA 와 QLoRA 의 차이,-0.00080841273,0.0,0.0008084127330221236
마지막 할 말 -> 로라야 정말 고마워!,Loss Function 예시,-0.0020647107,0.0,0.0020647107157856226
마지막 할 말 -> 로라야 정말 고마워!,Loss Function 정의,0.0019063982,0.0,0.001906398218125105
마지막 할 말 -> 로라야 정말 고마워!,MBTI,-0.004327248,0.0,0.004327247850596905
마지막 할 말 -> 로라야 정말 고마워!,MSE Loss 설명,-0.002638111,0.0,0.0026381108909845352
마지막 할 말 -> 로라야 정말 고마워!,MSE Loss 용도,-0.007110577,0.0,0.007110577076673508
마지막 할 말 -> 로라야 정말 고마워!,Multi-Label 에서 CE + Softmax 적용 문제점,-0.0005605263,0.0,0.0005605263286270201
마지막 할 말 -> 로라야 정말 고마워!,PEFT 방법 5가지,0.0014630867,0.0,0.001463086693547666
마지막 할 말 -> 로라야 정말 고마워!,거대 언어 모델 정의,-0.0057839053,0.0,0.005783905275166035
마지막 할 말 -> 로라야 정말 고마워!,기본 경험,-0.0002321857,0.0,0.0002321857027709484
마지막 할 말 -> 로라야 정말 고마워!,답변 실패,-0.0027531215,0.0,0.0027531215455383062
마지막 할 말 -> 로라야 정말 고마워!,딥러닝,-0.0009763652,0.0,0.0009763651760295033
마지막 할 말 -> 로라야 정말 고마워!,마지막 할 말,0.9992304,1.0,0.0007696151733398438
마지막 할 말 -> 로라야 정말 고마워!,머신러닝,-0.0015103717,0.0,0.0015103716868907213
마지막 할 말 -> 로라야 정말 고마워!,면접 시작 인사,0.0013800119,0.0,0.0013800119049847126
마지막 할 말 -> 로라야 정말 고마워!,상세 경험,-6.694727e-05,0.0,6.694727198919281e-05
마지막 할 말 -> 로라야 정말 고마워!,수식,-0.003023269,0.0,0.003023268887773156
마지막 할 말 -> 로라야 정말 고마워!,용어 질문,-0.0038356597,0.0,0.003835659706965089
마지막 할 말 -> 로라야 정말 고마워!,인공지능,-0.0042322497,0.0,0.004232249688357115
마지막 할 말 -> 로라야 정말 고마워!,잠시 휴식,-0.0062075243,0.0,0.006207524333149195
마지막 할 말 -> 로라야 정말 고마워!,좋아하는 아이돌,-0.0015609378,0.0,0.0015609378460794687
마지막 할 말 -> 로라야 정말 고마워!,핵심 아이디어,-0.0055314535,0.0,0.005531453527510166
마지막 할 말 -> 로라야 정말 고마워!,확률 예측에서 MSE Loss 미 사용 이유,-7.831762e-05,0.0,7.831762195564806e-05
마지막 할 말 -> 로라야 사랑해,BCE 가 좋은 task,-0.0023975044,0.0,0.0023975044023245573
마지막 할 말 -> 로라야 사랑해,BCE 가 좋은 이유,-0.0019494571,0.0,0.0019494571024551988
마지막 할 말 -> 로라야 사랑해,LLM Fine-Tuning 의 PEFT,-0.0049602273,0.0,0.00496022729203105
마지막 할 말 -> 로라야 사랑해,LoRA,-0.0017100514,0.0,0.0017100514378398657
마지막 할 말 -> 로라야 사랑해,LoRA 와 QLoRA 의 차이,-0.0011224538,0.0,0.0011224538320675492
마지막 할 말 -> 로라야 사랑해,Loss Function 예시,-0.0020586636,0.0,0.0020586636383086443
마지막 할 말 -> 로라야 사랑해,Loss Function 정의,0.00175587,0.0,0.001755869947373867
마지막 할 말 -> 로라야 사랑해,MBTI,-0.0047725914,0.0,0.004772591404616833
마지막 할 말 -> 로라야 사랑해,MSE Loss 설명,-0.0021098163,0.0,0.002109816297888756
마지막 할 말 -> 로라야 사랑해,MSE Loss 용도,-0.007111728,0.0,0.007111728191375732
마지막 할 말 -> 로라야 사랑해,Multi-Label 에서 CE + Softmax 적용 문제점,-0.0001676529,0.0,0.00016765290638431907
마지막 할 말 -> 로라야 사랑해,PEFT 방법 5가지,0.0016626639,0.0,0.001662663882598281
마지막 할 말 -> 로라야 사랑해,거대 언어 모델 정의,-0.005187993,0.0,0.005187992937862873
마지막 할 말 -> 로라야 사랑해,기본 경험,-0.00011125059,0.0,0.00011125059245387092
마지막 할 말 -> 로라야 사랑해,답변 실패,-0.002775428,0.0,0.002775427885353565
마지막 할 말 -> 로라야 사랑해,딥러닝,-0.0004919272,0.0,0.0004919272032566369
마지막 할 말 -> 로라야 사랑해,마지막 할 말,0.999316,1.0,0.0006840229034423828
마지막 할 말 -> 로라야 사랑해,머신러닝,-0.0018788446,0.0,0.0018788445740938187
마지막 할 말 -> 로라야 사랑해,면접 시작 인사,0.00078727066,0.0,0.0007872706628404558
마지막 할 말 -> 로라야 사랑해,상세 경험,-0.00044711283,0.0,0.00044711283408105373
마지막 할 말 -> 로라야 사랑해,수식,-0.002999744,0.0,0.002999743912369013
마지막 할 말 -> 로라야 사랑해,용어 질문,-0.0036493035,0.0,0.0036493034567683935
마지막 할 말 -> 로라야 사랑해,인공지능,-0.00414019,0.0,0.004140189848840237
마지막 할 말 -> 로라야 사랑해,잠시 휴식,-0.005806152,0.0,0.005806151777505875
마지막 할 말 -> 로라야 사랑해,좋아하는 아이돌,-0.0011149093,0.0,0.001114909304305911
마지막 할 말 -> 로라야 사랑해,핵심 아이디어,-0.005421773,0.0,0.005421773064881563
마지막 할 말 -> 로라야 사랑해,확률 예측에서 MSE Loss 미 사용 이유,0.0004099122,0.0,0.0004099122015759349
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,BCE 가 좋은 task,-0.0033002808,0.0,0.003300280775874853
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,BCE 가 좋은 이유,-0.0014781101,0.0,0.001478110090829432
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,LLM Fine-Tuning 의 PEFT,-0.0041420143,0.0,0.004142014309763908
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,LoRA,-0.0020774733,0.0,0.0020774733275175095
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,LoRA 와 QLoRA 의 차이,-0.00053537096,0.0,0.0005353709566406906
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,Loss Function 예시,-0.0020074577,0.0,0.002007457660511136
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,Loss Function 정의,0.001697566,0.0,0.0016975660109892488
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,MBTI,-0.004145235,0.0,0.004145234823226929
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,MSE Loss 설명,-0.0023422074,0.0,0.002342207357287407
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,MSE Loss 용도,-0.006659419,0.0,0.006659418810158968
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,Multi-Label 에서 CE + Softmax 적용 문제점,-0.0013099134,0.0,0.0013099133502691984
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,PEFT 방법 5가지,0.0019072995,0.0,0.001907299505546689
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,거대 언어 모델 정의,-0.0056901807,0.0,0.005690180696547031
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,기본 경험,-0.00010134398,0.0,0.00010134398326044902
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,답변 실패,-0.0022507561,0.0,0.00225075613707304
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,딥러닝,-0.0014251908,0.0,0.0014251908287405968
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,마지막 할 말,0.99890786,1.0,0.0010921359062194824
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,머신러닝,-0.0019055456,0.0,0.0019055455923080444
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,면접 시작 인사,0.0015954991,0.0,0.0015954991104081273
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,상세 경험,-0.00012766373,0.0,0.00012766373401973397
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,수식,-0.0027480791,0.0,0.0027480791322886944
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,용어 질문,-0.0049505765,0.0,0.004950576461851597
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,인공지능,-0.004824201,0.0,0.004824201110750437
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,잠시 휴식,-0.005451103,0.0,0.0054511032067239285
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,좋아하는 아이돌,-0.0016026582,0.0,0.0016026581870391965
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,핵심 아이디어,-0.006269446,0.0,0.006269446108490229
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,확률 예측에서 MSE Loss 미 사용 이유,0.00029063024,0.0,0.00029063023976050317
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워,BCE 가 좋은 task,-0.0022945064,0.0,0.002294506411999464
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워,BCE 가 좋은 이유,-0.0020559975,0.0,0.002055997494608164
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워,LLM Fine-Tuning 의 PEFT,-0.00482046,0.0,0.004820459987968206
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워,LoRA,-0.0013258907,0.0,0.0013258906546980143
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워,LoRA 와 QLoRA 의 차이,-0.0012518134,0.0,0.001251813373528421
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워,Loss Function 예시,-0.0024719883,0.0,0.002471988322213292
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워,Loss Function 정의,0.002613736,0.0,0.0026137360837310553
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워,MBTI,-0.004342902,0.0,0.004342901986092329
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워,MSE Loss 설명,-0.0018417127,0.0,0.0018417127430438995
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워,MSE Loss 용도,-0.0075079715,0.0,0.007507971487939358
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워,Multi-Label 에서 CE + Softmax 적용 문제점,-0.0004217647,0.0,0.00042176470742560923
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워,PEFT 방법 5가지,0.0008828879,0.0,0.0008828879217617214
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워,거대 언어 모델 정의,-0.006216618,0.0,0.0062166182324290276
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워,기본 경험,-0.00033289977,0.0,0.0003328997700009495
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워,답변 실패,-0.0027474079,0.0,0.0027474078815430403
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워,딥러닝,-0.0005885865,0.0,0.0005885864957235754
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워,마지막 할 말,0.99895114,1.0,0.0010488629341125488
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워,머신러닝,-0.0025428925,0.0,0.0025428924709558487
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워,면접 시작 인사,0.002276882,0.0,0.0022768820635974407
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워,상세 경험,0.0010600094,0.0,0.0010600093519315124
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워,수식,-0.0029664636,0.0,0.002966463565826416
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워,용어 질문,-0.0048016612,0.0,0.004801661241799593
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워,인공지능,-0.004894617,0.0,0.004894617013633251
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워,잠시 휴식,-0.005880841,0.0,0.005880841054022312
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워,좋아하는 아이돌,-0.0020668537,0.0,0.002066853689029813
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워,핵심 아이디어,-0.0057411967,0.0,0.005741196684539318
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워,확률 예측에서 MSE Loss 미 사용 이유,-0.00030599476,0.0,0.0003059947630390525
