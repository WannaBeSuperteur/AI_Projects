input_part,output_answer,predicted_score,ground_truth_score,absolute_error
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,BCE 가 좋은 task,-0.0010291823,0.0,0.0010291823418810964
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,"BCE 가 좋은 task, 이유",-0.005247608,0.0,0.0052476078271865845
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,LLM Fine-Tuning 의 PEFT,0.0045753396,0.0,0.004575339611619711
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,LoRA,-0.010096222,0.0,0.010096222162246704
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,LoRA 와 QLoRA 의 차이,0.0028756573,0.0,0.0028756572864949703
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,Loss Function 예시,-0.017726757,0.0,0.017726756632328033
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,Loss Function 정의,-0.0027645908,0.0,0.002764590783044696
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,MBTI,-0.0029662738,0.0,0.002966273808851838
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,MSE Loss 설명,0.0030298072,0.0,0.003029807237908244
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,MSE Loss 용도,0.0060914303,0.0,0.006091430317610502
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,Multi-Label 에서 CE + Softmax 적용 문제점,-0.00016491931,0.0,0.00016491931455675513
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,PEFT 방법 5가지,0.017685609,0.0,0.017685608938336372
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,거대 언어 모델 정의,-0.0060529597,0.0,0.006052959710359573
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,기본 경험,-0.0089804055,0.0,0.008980405516922474
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,답변 실패,-0.0075995317,0.0,0.007599531672894955
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,딥러닝,0.0037579245,0.0,0.0037579245399683714
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,마지막 할 말,-0.011451246,0.0,0.011451246216893196
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,머신러닝,-0.008210727,0.0,0.008210727013647556
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,면접 시작 인사,0.9922149,1.0,0.00778508186340332
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,상세 경험,-0.0037718883,0.0,0.0037718883249908686
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,수식,0.013956896,0.0,0.013956896029412746
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,인공지능,-0.00615497,0.0,0.006154969800263643
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,잠시 휴식,-0.03018187,0.0,0.030181869864463806
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,좋아하는 아이돌,-0.00043158547,0.0,0.000431585474871099
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,핵심 아이디어,-0.015045761,0.0,0.01504576113075018
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,확률 예측에서 MSE Loss 미 사용 이유,0.010625955,0.0,0.01062595471739769
면접 시작 인사 -> 로라야 안녕 정말 반가워,BCE 가 좋은 task,0.0034744286,0.0,0.003474428551271558
면접 시작 인사 -> 로라야 안녕 정말 반가워,"BCE 가 좋은 task, 이유",-0.0038424183,0.0,0.0038424183148890734
면접 시작 인사 -> 로라야 안녕 정말 반가워,LLM Fine-Tuning 의 PEFT,0.002828495,0.0,0.0028284951113164425
면접 시작 인사 -> 로라야 안녕 정말 반가워,LoRA,0.0007860766,0.0,0.0007860765908844769
면접 시작 인사 -> 로라야 안녕 정말 반가워,LoRA 와 QLoRA 의 차이,0.0007625241,0.0,0.0007625240832567215
면접 시작 인사 -> 로라야 안녕 정말 반가워,Loss Function 예시,-0.019960303,0.0,0.019960302859544754
면접 시작 인사 -> 로라야 안녕 정말 반가워,Loss Function 정의,-0.0010787641,0.0,0.0010787640931084752
면접 시작 인사 -> 로라야 안녕 정말 반가워,MBTI,0.0077297823,0.0,0.00772978225722909
면접 시작 인사 -> 로라야 안녕 정말 반가워,MSE Loss 설명,0.002424296,0.0,0.0024242959916591644
면접 시작 인사 -> 로라야 안녕 정말 반가워,MSE Loss 용도,-0.0029296598,0.0,0.002929659793153405
면접 시작 인사 -> 로라야 안녕 정말 반가워,Multi-Label 에서 CE + Softmax 적용 문제점,0.0021453712,0.0,0.0021453711669892073
면접 시작 인사 -> 로라야 안녕 정말 반가워,PEFT 방법 5가지,0.0016944243,0.0,0.0016944243106991053
면접 시작 인사 -> 로라야 안녕 정말 반가워,거대 언어 모델 정의,0.0028889936,0.0,0.0028889935929328203
면접 시작 인사 -> 로라야 안녕 정말 반가워,기본 경험,-0.003789533,0.0,0.0037895329296588898
면접 시작 인사 -> 로라야 안녕 정말 반가워,답변 실패,-0.0033249627,0.0,0.0033249626867473125
면접 시작 인사 -> 로라야 안녕 정말 반가워,딥러닝,0.0066360435,0.0,0.006636043544858694
면접 시작 인사 -> 로라야 안녕 정말 반가워,마지막 할 말,-0.005288807,0.0,0.005288807209581137
면접 시작 인사 -> 로라야 안녕 정말 반가워,머신러닝,-0.009648713,0.0,0.009648713283240795
면접 시작 인사 -> 로라야 안녕 정말 반가워,면접 시작 인사,0.9927462,1.0,0.007253825664520264
면접 시작 인사 -> 로라야 안녕 정말 반가워,상세 경험,-0.010714099,0.0,0.010714098811149597
면접 시작 인사 -> 로라야 안녕 정말 반가워,수식,0.0055566467,0.0,0.0055566467344760895
면접 시작 인사 -> 로라야 안녕 정말 반가워,인공지능,-0.007026679,0.0,0.007026678882539272
면접 시작 인사 -> 로라야 안녕 정말 반가워,잠시 휴식,-0.032428596,0.0,0.032428596168756485
면접 시작 인사 -> 로라야 안녕 정말 반가워,좋아하는 아이돌,-0.0027427045,0.0,0.0027427044697105885
면접 시작 인사 -> 로라야 안녕 정말 반가워,핵심 아이디어,-0.022867115,0.0,0.02286711521446705
면접 시작 인사 -> 로라야 안녕 정말 반가워,확률 예측에서 MSE Loss 미 사용 이유,0.013049012,0.0,0.013049012050032616
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,BCE 가 좋은 task,0.00577314,0.0,0.005773140117526054
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,"BCE 가 좋은 task, 이유",-0.005333422,0.0,0.005333422217518091
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,LLM Fine-Tuning 의 PEFT,0.006161082,0.0,0.006161082070320845
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,LoRA,0.0025723963,0.0,0.0025723963044583797
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,LoRA 와 QLoRA 의 차이,0.003326182,0.0,0.003326182020828128
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,Loss Function 예시,-0.012854411,0.0,0.012854411266744137
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,Loss Function 정의,-0.0059765032,0.0,0.005976503249257803
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,MBTI,-0.0053261374,0.0,0.005326137412339449
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,MSE Loss 설명,0.0047269217,0.0,0.004726921673864126
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,MSE Loss 용도,0.0031496098,0.0,0.0031496097799390554
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,Multi-Label 에서 CE + Softmax 적용 문제점,-0.00015556328,0.0,0.00015556327707599849
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,PEFT 방법 5가지,0.0047588046,0.0,0.0047588045708835125
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,거대 언어 모델 정의,-0.0038490293,0.0,0.0038490293081849813
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,기본 경험,0.0031001517,0.0,0.003100151661783457
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,답변 실패,-0.005903414,0.0,0.005903413984924555
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,딥러닝,0.0119507285,0.0,0.011950728483498096
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,마지막 할 말,-0.011890085,0.0,0.01189008541405201
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,머신러닝,-0.018097235,0.0,0.01809723488986492
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,면접 시작 인사,0.990202,1.0,0.009797990322113037
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,상세 경험,-0.011169717,0.0,0.011169716715812683
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,수식,0.008648198,0.0,0.00864819809794426
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,인공지능,-0.009327145,0.0,0.009327145293354988
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,잠시 휴식,-0.040615156,0.0,0.040615156292915344
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,좋아하는 아이돌,-0.0041132285,0.0,0.004113228525966406
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,핵심 아이디어,-0.020173077,0.0,0.020173076540231705
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,확률 예측에서 MSE Loss 미 사용 이유,0.010179481,0.0,0.010179480537772179
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,BCE 가 좋은 task,0.0052393763,0.0,0.0052393763326108456
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,"BCE 가 좋은 task, 이유",8.173659e-05,0.0,8.173658716259524e-05
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,LLM Fine-Tuning 의 PEFT,0.0108860945,0.0,0.010886094532907009
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,LoRA,0.00328151,0.0,0.003281509969383478
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,LoRA 와 QLoRA 의 차이,0.0042285733,0.0,0.00422857329249382
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,Loss Function 예시,-0.015067791,0.0,0.015067790634930134
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,Loss Function 정의,-0.0061092833,0.0,0.006109283305704594
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,MBTI,-0.011874901,0.0,0.011874901130795479
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,MSE Loss 설명,0.005231927,0.0,0.005231927148997784
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,MSE Loss 용도,-0.0031710444,0.0,0.0031710444018244743
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,Multi-Label 에서 CE + Softmax 적용 문제점,-0.0019519979,0.0,0.0019519978668540716
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,PEFT 방법 5가지,0.0021408864,0.0,0.0021408863831311464
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,거대 언어 모델 정의,-0.0051037027,0.0,0.005103702656924725
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,기본 경험,-0.006336232,0.0,0.006336232181638479
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,답변 실패,-0.01416419,0.0,0.01416418980807066
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,딥러닝,0.008820747,0.0,0.008820747025310993
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,마지막 할 말,0.0055141174,0.0,0.005514117423444986
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,머신러닝,-0.008838906,0.0,0.008838905952870846
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,면접 시작 인사,0.9866068,1.0,0.013393223285675049
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,상세 경험,-0.0026270137,0.0,0.0026270137168467045
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,수식,0.005371972,0.0,0.005371971987187862
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,인공지능,-0.016121065,0.0,0.016121065244078636
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,잠시 휴식,-0.038771994,0.0,0.03877199441194534
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,좋아하는 아이돌,-0.0071601844,0.0,0.0071601844392716885
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,핵심 아이디어,-0.013864405,0.0,0.013864405453205109
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,확률 예측에서 MSE Loss 미 사용 이유,0.017705508,0.0,0.017705507576465607
면접 시작 인사 -> 파이팅! 시작하자,BCE 가 좋은 task,0.0031093787,0.0,0.0031093787401914597
면접 시작 인사 -> 파이팅! 시작하자,"BCE 가 좋은 task, 이유",-0.0051804883,0.0,0.0051804883405566216
면접 시작 인사 -> 파이팅! 시작하자,LLM Fine-Tuning 의 PEFT,0.0076315724,0.0,0.007631572429090738
면접 시작 인사 -> 파이팅! 시작하자,LoRA,-0.008689211,0.0,0.008689210750162601
면접 시작 인사 -> 파이팅! 시작하자,LoRA 와 QLoRA 의 차이,-0.0011892167,0.0,0.0011892167385667562
면접 시작 인사 -> 파이팅! 시작하자,Loss Function 예시,-0.013337153,0.0,0.013337153010070324
면접 시작 인사 -> 파이팅! 시작하자,Loss Function 정의,-0.00091413927,0.0,0.0009141392656601965
면접 시작 인사 -> 파이팅! 시작하자,MBTI,0.0110530695,0.0,0.011053069494664669
면접 시작 인사 -> 파이팅! 시작하자,MSE Loss 설명,-0.0012534399,0.0,0.0012534399284049869
면접 시작 인사 -> 파이팅! 시작하자,MSE Loss 용도,-0.0025869193,0.0,0.0025869193486869335
면접 시작 인사 -> 파이팅! 시작하자,Multi-Label 에서 CE + Softmax 적용 문제점,0.0035253728,0.0,0.003525372827425599
면접 시작 인사 -> 파이팅! 시작하자,PEFT 방법 5가지,0.0146051,0.0,0.014605100266635418
면접 시작 인사 -> 파이팅! 시작하자,거대 언어 모델 정의,-0.008544863,0.0,0.0085448632016778
면접 시작 인사 -> 파이팅! 시작하자,기본 경험,-0.012777871,0.0,0.012777871452271938
면접 시작 인사 -> 파이팅! 시작하자,답변 실패,-0.015746323,0.0,0.01574632339179516
면접 시작 인사 -> 파이팅! 시작하자,딥러닝,0.00028144615,0.0,0.0002814461477100849
면접 시작 인사 -> 파이팅! 시작하자,마지막 할 말,-0.008440094,0.0,0.008440094068646431
면접 시작 인사 -> 파이팅! 시작하자,머신러닝,-0.0032192778,0.0,0.0032192778307944536
면접 시작 인사 -> 파이팅! 시작하자,면접 시작 인사,0.9927663,1.0,0.007233679294586182
면접 시작 인사 -> 파이팅! 시작하자,상세 경험,-0.0041735824,0.0,0.004173582419753075
면접 시작 인사 -> 파이팅! 시작하자,수식,0.008206313,0.0,0.008206312544643879
면접 시작 인사 -> 파이팅! 시작하자,인공지능,-0.012024987,0.0,0.012024986557662487
면접 시작 인사 -> 파이팅! 시작하자,잠시 휴식,-0.0327233,0.0,0.03272330015897751
면접 시작 인사 -> 파이팅! 시작하자,좋아하는 아이돌,0.0039485325,0.0,0.00394853251054883
면접 시작 인사 -> 파이팅! 시작하자,핵심 아이디어,-0.013783964,0.0,0.013783964328467846
면접 시작 인사 -> 파이팅! 시작하자,확률 예측에서 MSE Loss 미 사용 이유,0.015520696,0.0,0.015520695596933365
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",BCE 가 좋은 task,-0.0036542537,0.0,0.0036542536690831184
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데","BCE 가 좋은 task, 이유",-0.0074030044,0.0,0.007403004448860884
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",LLM Fine-Tuning 의 PEFT,-0.00064396206,0.0,0.000643962062895298
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",LoRA,0.0087053245,0.0,0.008705324493348598
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",LoRA 와 QLoRA 의 차이,0.0049306876,0.0,0.004930687602609396
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",Loss Function 예시,0.000841232,0.0,0.0008412320166826248
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",Loss Function 정의,-0.011229914,0.0,0.01122991368174553
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",MBTI,-0.0020467988,0.0,0.0020467988215386868
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",MSE Loss 설명,-0.0090079885,0.0,0.00900798849761486
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",MSE Loss 용도,0.00040091554,0.0,0.000400915538193658
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",Multi-Label 에서 CE + Softmax 적용 문제점,-0.0045377295,0.0,0.004537729546427727
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",PEFT 방법 5가지,-0.011552231,0.0,0.011552231386303902
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",거대 언어 모델 정의,-0.006650966,0.0,0.006650966126471758
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",기본 경험,0.05631472,0.0,0.05631472170352936
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",답변 실패,0.9767541,1.0,0.02324587106704712
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",딥러닝,-0.007949665,0.0,0.007949665188789368
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",마지막 할 말,-0.010840983,0.0,0.010840983130037785
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",머신러닝,0.0044205915,0.0,0.00442059151828289
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",면접 시작 인사,-0.0072206683,0.0,0.007220668252557516
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",상세 경험,0.007890194,0.0,0.007890193723142147
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",수식,-0.01158822,0.0,0.011588219553232193
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",인공지능,0.012254884,0.0,0.012254883535206318
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",잠시 휴식,-0.015600844,0.0,0.015600844286382198
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",좋아하는 아이돌,0.017704833,0.0,0.017704833298921585
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",핵심 아이디어,-0.0061849565,0.0,0.0061849565245211124
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",확률 예측에서 MSE Loss 미 사용 이유,-0.0045508025,0.0,0.004550802521407604
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",BCE 가 좋은 task,-0.0010407179,0.0,0.0010407179361209273
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!","BCE 가 좋은 task, 이유",0.0008656365,0.0,0.0008656365098431706
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",LLM Fine-Tuning 의 PEFT,0.00969253,0.0,0.009692530147731304
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",LoRA,-0.00034444025,0.0,0.0003444402536842972
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",LoRA 와 QLoRA 의 차이,0.010664493,0.0,0.010664492845535278
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",Loss Function 예시,0.0059915795,0.0,0.005991579499095678
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",Loss Function 정의,0.01592375,0.0,0.015923749655485153
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",MBTI,0.014384405,0.0,0.014384404756128788
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",MSE Loss 설명,-0.000908259,0.0,0.0009082590113393962
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",MSE Loss 용도,-0.0061099357,0.0,0.006109935697168112
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",Multi-Label 에서 CE + Softmax 적용 문제점,0.0026245795,0.0,0.0026245794724673033
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",PEFT 방법 5가지,-0.003782103,0.0,0.003782103070989251
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",거대 언어 모델 정의,-0.009794269,0.0,0.009794268757104874
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",기본 경험,-0.0028786708,0.0,0.0028786708135157824
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",답변 실패,-0.0062801824,0.0,0.006280182395130396
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",딥러닝,-0.026366014,0.0,0.026366014033555984
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",마지막 할 말,-0.0033437095,0.0,0.003343709511682391
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",머신러닝,-0.074431024,0.0,0.07443102449178696
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",면접 시작 인사,-0.005605306,0.0,0.005605306010693312
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",상세 경험,0.0022777983,0.0,0.0022777982521802187
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",수식,-0.016352369,0.0,0.016352368518710136
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",인공지능,0.97467196,1.0,0.02532804012298584
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",잠시 휴식,-0.01210807,0.0,0.012108069844543934
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",좋아하는 아이돌,-0.017537085,0.0,0.017537085339426994
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",핵심 아이디어,0.0009368503,0.0,0.0009368503233417869
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",확률 예측에서 MSE Loss 미 사용 이유,-0.002838838,0.0,0.0028388379141688347
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",BCE 가 좋은 task,-0.0039032877,0.0,0.003903287695720792
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지","BCE 가 좋은 task, 이유",0.0020925777,0.0,0.002092577749863267
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",LLM Fine-Tuning 의 PEFT,0.072725415,0.0,0.07272541522979736
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",LoRA,0.008471787,0.0,0.008471786975860596
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",LoRA 와 QLoRA 의 차이,0.014048474,0.0,0.014048473909497261
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",Loss Function 예시,-0.010403422,0.0,0.010403421707451344
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",Loss Function 정의,-0.0033040142,0.0,0.003304014215245843
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",MBTI,0.008188525,0.0,0.008188525214791298
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",MSE Loss 설명,-0.016474403,0.0,0.0164744034409523
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",MSE Loss 용도,-0.011664469,0.0,0.011664468795061111
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",Multi-Label 에서 CE + Softmax 적용 문제점,0.008172724,0.0,0.008172724395990372
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",PEFT 방법 5가지,-0.01916545,0.0,0.01916545070707798
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",거대 언어 모델 정의,0.015317835,0.0,0.015317834913730621
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",기본 경험,0.0017895821,0.0,0.00178958207834512
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",답변 실패,-0.0023944632,0.0,0.0023944631684571505
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",딥러닝,-0.074469306,0.0,0.07446930557489395
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",마지막 할 말,-0.036799725,0.0,0.03679972514510155
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",머신러닝,0.9588838,1.0,0.041116178035736084
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",면접 시작 인사,-0.007023929,0.0,0.00702392915263772
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",상세 경험,0.022520326,0.0,0.02252032607793808
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",수식,-0.0069283694,0.0,0.006928369402885437
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",인공지능,-0.09539441,0.0,0.09539441019296646
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",잠시 휴식,0.00054755254,0.0,0.0005475525395013392
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",좋아하는 아이돌,-0.016853621,0.0,0.01685362122952938
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",핵심 아이디어,-0.006389005,0.0,0.006389005109667778
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",확률 예측에서 MSE Loss 미 사용 이유,-0.014582435,0.0,0.014582434669137001
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",BCE 가 좋은 task,0.0030957304,0.0,0.0030957304406911135
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?","BCE 가 좋은 task, 이유",0.0025091076,0.0,0.002509107580408454
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",LLM Fine-Tuning 의 PEFT,-0.014622242,0.0,0.01462224218994379
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",LoRA,-0.018542454,0.0,0.01854245364665985
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",LoRA 와 QLoRA 의 차이,0.015003521,0.0,0.015003520995378494
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",Loss Function 예시,-0.007594196,0.0,0.007594196125864983
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",Loss Function 정의,-0.007576197,0.0,0.0075761969201266766
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",MBTI,-0.00994526,0.0,0.009945260360836983
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",MSE Loss 설명,-0.011159466,0.0,0.01115946564823389
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",MSE Loss 용도,-0.006879998,0.0,0.006879997905343771
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",Multi-Label 에서 CE + Softmax 적용 문제점,-0.0030336278,0.0,0.0030336277559399605
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",PEFT 방법 5가지,-0.006850083,0.0,0.006850082892924547
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",거대 언어 모델 정의,-0.000264145,0.0,0.00026414499734528363
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",기본 경험,-0.005103657,0.0,0.005103657022118568
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",답변 실패,-0.01349558,0.0,0.013495580293238163
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",딥러닝,0.9800085,1.0,0.019991517066955566
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",마지막 할 말,0.0012135167,0.0,0.0012135166907683015
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",머신러닝,-0.04254856,0.0,0.04254855960607529
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",면접 시작 인사,0.00023501912,0.0,0.00023501912073697895
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",상세 경험,-0.008886442,0.0,0.008886441588401794
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",수식,0.012170944,0.0,0.0121709443628788
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",인공지능,-0.047332417,0.0,0.04733241721987724
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",잠시 휴식,-0.024109282,0.0,0.024109281599521637
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",좋아하는 아이돌,-0.010771795,0.0,0.0107717951759696
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",핵심 아이디어,0.03950921,0.0,0.039509210735559464
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",확률 예측에서 MSE Loss 미 사용 이유,-0.0026742711,0.0,0.0026742711197584867
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",BCE 가 좋은 task,0.001954891,0.0,0.0019548910204321146
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야","BCE 가 좋은 task, 이유",-7.979821e-05,0.0,7.979820657055825e-05
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",LLM Fine-Tuning 의 PEFT,-0.00781979,0.0,0.00781979039311409
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",LoRA,-0.030672023,0.0,0.030672023072838783
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",LoRA 와 QLoRA 의 차이,0.020154567,0.0,0.020154567435383797
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",Loss Function 예시,-0.008954453,0.0,0.008954453282058239
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",Loss Function 정의,-0.0059940405,0.0,0.0059940405189991
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",MBTI,-0.0020993673,0.0,0.0020993673242628574
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",MSE Loss 설명,-0.018974537,0.0,0.018974537029862404
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",MSE Loss 용도,-0.005508346,0.0,0.005508346017450094
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",Multi-Label 에서 CE + Softmax 적용 문제점,0.0036138673,0.0,0.0036138673312962055
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",PEFT 방법 5가지,-0.010964204,0.0,0.010964203625917435
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",거대 언어 모델 정의,0.0027903402,0.0,0.0027903402224183083
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",기본 경험,-0.004472081,0.0,0.004472081083804369
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",답변 실패,-0.009754584,0.0,0.009754584170877934
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",딥러닝,0.97977525,1.0,0.02022475004196167
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",마지막 할 말,0.0031321286,0.0,0.0031321286223828793
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",머신러닝,-0.033733424,0.0,0.03373342379927635
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",면접 시작 인사,0.0034299574,0.0,0.0034299574326723814
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",상세 경험,-0.01545959,0.0,0.015459589660167694
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",수식,0.0072966604,0.0,0.007296660449355841
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",인공지능,-0.030940806,0.0,0.03094080649316311
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",잠시 휴식,-0.026242599,0.0,0.02624259889125824
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",좋아하는 아이돌,-0.0055004414,0.0,0.005500441417098045
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",핵심 아이디어,0.026154993,0.0,0.02615499310195446
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",확률 예측에서 MSE Loss 미 사용 이유,0.0017371472,0.0,0.0017371472204104066
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",BCE 가 좋은 task,0.00066301273,0.0,0.0006630127318203449
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?","BCE 가 좋은 task, 이유",-0.013660323,0.0,0.01366032287478447
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",LLM Fine-Tuning 의 PEFT,0.09110349,0.0,0.09110348671674728
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",LoRA,0.0038603584,0.0,0.0038603583816438913
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",LoRA 와 QLoRA 의 차이,0.011164486,0.0,0.011164486408233643
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",Loss Function 예시,-0.021409487,0.0,0.02140948735177517
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",Loss Function 정의,-0.0062152697,0.0,0.006215269677340984
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",MBTI,0.0077011166,0.0,0.007701116614043713
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",MSE Loss 설명,0.00046996615,0.0,0.00046996615128591657
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",MSE Loss 용도,-0.01442876,0.0,0.014428759925067425
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",Multi-Label 에서 CE + Softmax 적용 문제점,0.0023082746,0.0,0.002308274619281292
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",PEFT 방법 5가지,-0.009795767,0.0,0.00979576725512743
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",거대 언어 모델 정의,0.01068174,0.0,0.010681740008294582
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",기본 경험,-0.011593262,0.0,0.011593261733651161
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",답변 실패,-0.010490888,1.0,1.010490887798369
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",딥러닝,-0.050226696,0.0,0.05022669583559036
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",마지막 할 말,-0.027677963,0.0,0.02767796255648136
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",머신러닝,0.9438018,0.0,0.9438018202781677
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",면접 시작 인사,-0.0022940494,0.0,0.0022940493654459715
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",상세 경험,-8.906959e-05,0.0,8.906958828447387e-05
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",수식,-0.005151629,0.0,0.005151628982275724
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",인공지능,-0.0984664,0.0,0.09846639633178711
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",잠시 휴식,0.0295821,0.0,0.029582099989056587
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",좋아하는 아이돌,-0.009678655,0.0,0.009678655304014683
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",핵심 아이디어,0.024956264,0.0,0.024956263601779938
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",확률 예측에서 MSE Loss 미 사용 이유,-0.039527785,0.0,0.039527785032987595
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,BCE 가 좋은 task,-0.009360475,0.0,0.009360475465655327
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,"BCE 가 좋은 task, 이유",-0.00432054,0.0,0.004320539999753237
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,LLM Fine-Tuning 의 PEFT,0.00046751453,0.0,0.0004675145319197327
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,LoRA,-0.016855702,0.0,0.016855701804161072
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,LoRA 와 QLoRA 의 차이,0.015185417,0.0,0.015185416676104069
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,Loss Function 예시,0.002627342,0.0,0.0026273420080542564
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,Loss Function 정의,-0.008047393,0.0,0.008047392591834068
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,MBTI,0.00067481096,0.0,0.0006748109590262175
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,MSE Loss 설명,0.0044018184,0.0,0.004401818383485079
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,MSE Loss 용도,-0.011360365,0.0,0.011360364966094494
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,Multi-Label 에서 CE + Softmax 적용 문제점,-0.010359356,0.0,0.010359356179833412
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,PEFT 방법 5가지,-0.016730685,0.0,0.01673068478703499
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,거대 언어 모델 정의,0.9896709,1.0,0.010329127311706543
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,기본 경험,-0.011651405,0.0,0.01165140513330698
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,답변 실패,0.002198417,0.0,0.0021984169725328684
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,딥러닝,-0.010153548,0.0,0.01015354786068201
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,마지막 할 말,-0.0043831468,0.0,0.0043831467628479
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,머신러닝,0.002776615,0.0,0.0027766150888055563
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,면접 시작 인사,-0.011318739,0.0,0.01131873857229948
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,상세 경험,-0.006945493,0.0,0.0069454931654036045
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,수식,-0.020913303,0.0,0.020913302898406982
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,인공지능,-0.020781837,0.0,0.02078183740377426
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,잠시 휴식,-0.008199875,0.0,0.008199875243008137
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,좋아하는 아이돌,0.0010358691,0.0,0.0010358691215515137
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,핵심 아이디어,-0.023180723,0.0,0.023180723190307617
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,확률 예측에서 MSE Loss 미 사용 이유,0.009449529,0.0,0.009449529461562634
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,BCE 가 좋은 task,-0.007773911,0.0,0.007773911114782095
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,"BCE 가 좋은 task, 이유",0.007994181,0.0,0.007994180545210838
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,LLM Fine-Tuning 의 PEFT,0.005327664,0.0,0.005327663850039244
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,LoRA,-0.0028534923,0.0,0.0028534922748804092
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,LoRA 와 QLoRA 의 차이,-0.0025817768,0.0,0.0025817768182605505
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,Loss Function 예시,-0.0011786898,0.0,0.0011786897666752338
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,Loss Function 정의,-0.0054091723,0.0,0.005409172270447016
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,MBTI,-0.00084620493,0.0,0.000846204929985106
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,MSE Loss 설명,0.0018884522,0.0,0.0018884522141888738
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,MSE Loss 용도,-0.009604782,0.0,0.009604781866073608
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,Multi-Label 에서 CE + Softmax 적용 문제점,-0.014720659,0.0,0.014720658771693707
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,PEFT 방법 5가지,-0.00913,0.0,0.00913000013679266
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,거대 언어 모델 정의,0.014910718,0.0,0.01491071842610836
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,기본 경험,-0.0013775628,0.0,0.0013775627594441175
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,답변 실패,0.99405754,1.0,0.0059424638748168945
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,딥러닝,0.004501262,0.0,0.004501262214034796
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,마지막 할 말,-0.0032378407,0.0,0.0032378407195210457
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,머신러닝,-0.0025943064,0.0,0.00259430636651814
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,면접 시작 인사,-0.011835074,0.0,0.011835074052214622
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,상세 경험,-0.010503322,0.0,0.010503321886062622
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,수식,-0.008845273,0.0,0.008845273405313492
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,인공지능,-0.008061756,0.0,0.008061756379902363
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,잠시 휴식,-0.0097498,0.0,0.009749799966812134
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,좋아하는 아이돌,-0.0019292054,0.0,0.0019292053766548634
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,핵심 아이디어,0.001313935,0.0,0.0013139350339770317
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,확률 예측에서 MSE Loss 미 사용 이유,-0.002309681,0.0,0.0023096809163689613
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,BCE 가 좋은 task,0.008493071,0.0,0.008493071421980858
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,"BCE 가 좋은 task, 이유",-0.007752604,0.0,0.007752603851258755
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,LLM Fine-Tuning 의 PEFT,0.015956538,0.0,0.015956537798047066
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,LoRA,-0.027344665,0.0,0.027344664558768272
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,LoRA 와 QLoRA 의 차이,-0.0013069166,0.0,0.0013069165870547295
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,Loss Function 예시,-0.0129216565,0.0,0.012921656481921673
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,Loss Function 정의,0.98100793,0.0,0.9810079336166382
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,MBTI,0.015893865,0.0,0.01589386537671089
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,MSE Loss 설명,-0.025092399,0.0,0.02509239874780178
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,MSE Loss 용도,-0.02823066,0.0,0.028230659663677216
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,Multi-Label 에서 CE + Softmax 적용 문제점,0.018374514,0.0,0.01837451383471489
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,PEFT 방법 5가지,0.01719791,0.0,0.01719791069626808
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,거대 언어 모델 정의,-0.014215717,0.0,0.01421571709215641
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,기본 경험,0.0007710655,0.0,0.0007710654754191637
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,답변 실패,-0.0061481423,1.0,1.0061481422744691
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,딥러닝,-0.026300842,0.0,0.026300841942429543
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,마지막 할 말,-0.00354825,0.0,0.0035482500679790974
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,머신러닝,0.0094885975,0.0,0.009488597512245178
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,면접 시작 인사,-0.008445213,0.0,0.008445212617516518
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,상세 경험,0.01915622,0.0,0.01915621943771839
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,수식,0.018019829,0.0,0.01801982894539833
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,인공지능,0.0031714542,0.0,0.003171454183757305
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,잠시 휴식,-0.0013384325,0.0,0.0013384325429797173
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,좋아하는 아이돌,0.0068638483,0.0,0.006863848306238651
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,핵심 아이디어,0.03652607,0.0,0.036526069045066833
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,확률 예측에서 MSE Loss 미 사용 이유,-0.0011116883,0.0,0.0011116883251816034
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",BCE 가 좋은 task,-0.008194789,0.0,0.008194789290428162
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!","BCE 가 좋은 task, 이유",0.0012419606,0.0,0.0012419605627655983
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",LLM Fine-Tuning 의 PEFT,0.0135696735,0.0,0.013569673523306847
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",LoRA,-0.014427463,0.0,0.014427462592720985
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",LoRA 와 QLoRA 의 차이,0.005495319,0.0,0.00549531914293766
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",Loss Function 예시,-0.017936688,0.0,0.017936687916517258
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",Loss Function 정의,0.9785066,1.0,0.021493375301361084
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",MBTI,0.013725461,0.0,0.013725461438298225
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",MSE Loss 설명,-0.01995632,0.0,0.019956320524215698
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",MSE Loss 용도,-0.014123687,0.0,0.014123686589300632
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",Multi-Label 에서 CE + Softmax 적용 문제점,0.005647717,0.0,0.005647717043757439
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",PEFT 방법 5가지,0.012109639,0.0,0.012109639123082161
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",거대 언어 모델 정의,-0.012961053,0.0,0.012961053289473057
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",기본 경험,0.004123345,0.0,0.0041233450174331665
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",답변 실패,-0.007842512,0.0,0.007842511869966984
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",딥러닝,-0.03225368,0.0,0.032253678888082504
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",마지막 할 말,-0.008340183,0.0,0.008340182714164257
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",머신러닝,-0.012533674,0.0,0.012533674016594887
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",면접 시작 인사,-0.00024386012,0.0,0.000243860122282058
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",상세 경험,0.035092544,0.0,0.035092543810606
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",수식,0.06426642,0.0,0.06426642090082169
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",인공지능,-0.0045935386,0.0,0.004593538586050272
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",잠시 휴식,-0.010757759,0.0,0.01075775921344757
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",좋아하는 아이돌,1.5163806e-05,0.0,1.5163805983320344e-05
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",핵심 아이디어,-0.012438243,0.0,0.012438243255019188
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",확률 예측에서 MSE Loss 미 사용 이유,-0.0039044549,0.0,0.0039044548757374287
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,BCE 가 좋은 task,-0.00029680625,0.0,0.0002968062472064048
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,"BCE 가 좋은 task, 이유",0.0016880165,0.0,0.0016880164621397853
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,LLM Fine-Tuning 의 PEFT,-0.003487565,0.0,0.003487565089017153
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,LoRA,-0.0030084953,0.0,0.003008495317772031
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,LoRA 와 QLoRA 의 차이,-0.004295221,0.0,0.00429522106423974
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,Loss Function 예시,0.02203717,0.0,0.022037170827388763
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,Loss Function 정의,-0.0027260582,0.0,0.002726058242842555
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,MBTI,-0.0008337219,0.0,0.0008337218896485865
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,MSE Loss 설명,-0.0053979764,0.0,0.005397976376116276
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,MSE Loss 용도,-0.00082590146,0.0,0.0008259014575742185
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,Multi-Label 에서 CE + Softmax 적용 문제점,-0.0047098757,0.0,0.004709875676780939
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,PEFT 방법 5가지,-0.0025480678,0.0,0.002548067830502987
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,거대 언어 모델 정의,-0.009156457,0.0,0.009156457148492336
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,기본 경험,-0.005703858,0.0,0.005703858099877834
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,답변 실패,0.9930223,1.0,0.006977677345275879
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,딥러닝,-0.00977178,0.0,0.009771780110895634
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,마지막 할 말,-0.0015992157,0.0,0.0015992156695574522
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,머신러닝,-0.0029995206,0.0,0.0029995206277817488
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,면접 시작 인사,-0.009094282,0.0,0.009094282053411007
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,상세 경험,-0.015921487,0.0,0.015921486541628838
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,수식,-0.0094930595,0.0,0.009493059478700161
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,인공지능,-0.001032185,0.0,0.0010321850422769785
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,잠시 휴식,-0.008288907,0.0,0.008288906887173653
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,좋아하는 아이돌,-0.00096354587,0.0,0.0009635458700358868
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,핵심 아이디어,-0.000500151,0.0,0.0005001510144211352
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,확률 예측에서 MSE Loss 미 사용 이유,-0.0009468732,0.0,0.0009468732168897986
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",BCE 가 좋은 task,0.011095217,0.0,0.011095217429101467
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지","BCE 가 좋은 task, 이유",-0.0056669638,0.0,0.005666963756084442
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",LLM Fine-Tuning 의 PEFT,-0.003933403,0.0,0.003933403175324202
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",LoRA,0.006155322,0.0,0.006155321840196848
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",LoRA 와 QLoRA 의 차이,-0.005673379,0.0,0.005673379171639681
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",Loss Function 예시,0.9755913,1.0,0.024408698081970215
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",Loss Function 정의,-0.027792681,0.0,0.027792681008577347
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",MBTI,-0.019066302,0.0,0.019066302105784416
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",MSE Loss 설명,-0.006623951,0.0,0.006623950786888599
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",MSE Loss 용도,-0.0076755555,0.0,0.007675555534660816
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",Multi-Label 에서 CE + Softmax 적용 문제점,0.0043758596,0.0,0.004375859629362822
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",PEFT 방법 5가지,0.0051963422,0.0,0.005196342244744301
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",거대 언어 모델 정의,-0.00426194,0.0,0.004261939786374569
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",기본 경험,0.010652445,0.0,0.010652445256710052
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",답변 실패,-0.00068747334,0.0,0.0006874733371660113
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",딥러닝,0.0038483099,0.0,0.003848309861496091
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",마지막 할 말,-0.003926039,0.0,0.003926039207726717
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",머신러닝,-0.027989496,0.0,0.027989495545625687
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",면접 시작 인사,-0.012843409,0.0,0.01284340862184763
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",상세 경험,-0.00911149,0.0,0.009111490100622177
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",수식,0.011696181,0.0,0.011696181260049343
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",인공지능,0.013000132,0.0,0.013000131584703922
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",잠시 휴식,0.0020286923,0.0,0.0020286922808736563
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",좋아하는 아이돌,-0.012273498,0.0,0.012273497879505157
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",핵심 아이디어,0.0036797572,0.0,0.0036797572392970324
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",확률 예측에서 MSE Loss 미 사용 이유,-0.008792883,0.0,0.008792882785201073
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,BCE 가 좋은 task,-0.0011663613,0.0,0.0011663612676784396
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,"BCE 가 좋은 task, 이유",-0.0143118985,0.0,0.01431189849972725
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,LLM Fine-Tuning 의 PEFT,0.0027614993,0.0,0.0027614992577582598
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,LoRA,-0.0007673703,0.0,0.000767370278481394
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,LoRA 와 QLoRA 의 차이,-0.0033514397,0.0,0.0033514397218823433
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,Loss Function 예시,0.004044039,0.0,0.00404403917491436
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,Loss Function 정의,-0.013729621,0.0,0.013729620724916458
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,MBTI,-0.012501974,0.0,0.012501973658800125
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,MSE Loss 설명,0.9853126,1.0,0.014687418937683105
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,MSE Loss 용도,-0.024186375,0.0,0.024186374619603157
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,Multi-Label 에서 CE + Softmax 적용 문제점,0.009670178,0.0,0.009670178405940533
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,PEFT 방법 5가지,0.0017252171,0.0,0.0017252170946449041
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,거대 언어 모델 정의,0.0048543746,0.0,0.004854374565184116
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,기본 경험,-0.003937825,0.0,0.003937825094908476
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,답변 실패,0.0027817811,0.0,0.0027817811351269484
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,딥러닝,-0.009954981,0.0,0.009954980574548244
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,마지막 할 말,-0.0044768425,0.0,0.004476842470467091
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,머신러닝,-0.011089793,0.0,0.011089793406426907
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,면접 시작 인사,0.0021503647,0.0,0.002150364685803652
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,상세 경험,-0.010214742,0.0,0.01021474227309227
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,수식,-0.011376732,0.0,0.011376732029020786
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,인공지능,-0.0035371345,0.0,0.003537134500220418
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,잠시 휴식,0.010900692,0.0,0.010900692082941532
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,좋아하는 아이돌,-0.011124333,0.0,0.01112433336675167
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,핵심 아이디어,0.0006351827,0.0,0.0006351827178150415
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,확률 예측에서 MSE Loss 미 사용 이유,-0.0114213815,0.0,0.011421381495893002
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,BCE 가 좋은 task,-0.005792136,0.0,0.005792135838419199
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,"BCE 가 좋은 task, 이유",-0.0006584058,0.0,0.0006584058282896876
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,LLM Fine-Tuning 의 PEFT,-0.005804516,0.0,0.0058045159094035625
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,LoRA,-0.0036242853,0.0,0.003624285338446498
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,LoRA 와 QLoRA 의 차이,-0.0034588233,0.0,0.003458823310211301
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,Loss Function 예시,0.0033952072,0.0,0.0033952072262763977
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,Loss Function 정의,-0.01697923,0.0,0.01697923056781292
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,MBTI,-0.023038136,0.0,0.023038135841488838
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,MSE Loss 설명,0.97502846,0.0,0.9750284552574158
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,MSE Loss 용도,-0.029499615,0.0,0.029499614611268044
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,Multi-Label 에서 CE + Softmax 적용 문제점,-0.0004405581,0.0,0.0004405580984894186
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,PEFT 방법 5가지,-0.002492261,0.0,0.0024922608863562346
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,거대 언어 모델 정의,0.01118974,0.0,0.011189740151166916
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,기본 경험,-0.0046181795,0.0,0.004618179518729448
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,답변 실패,-0.003236863,1.0,1.0032368630636483
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,딥러닝,-0.011625175,0.0,0.01162517536431551
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,마지막 할 말,-0.01359301,0.0,0.013593009673058987
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,머신러닝,0.00038214258,0.0,0.0003821425780188292
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,면접 시작 인사,-2.7357604e-05,0.0,2.735760426730849e-05
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,상세 경험,0.019686472,0.0,0.019686471670866013
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,수식,-0.016969176,0.0,0.01696917600929737
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,인공지능,-0.014674936,0.0,0.014674936421215534
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,잠시 휴식,0.0057303966,0.0,0.00573039660230279
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,좋아하는 아이돌,-0.025392143,0.0,0.025392143055796623
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,핵심 아이디어,-0.0075786035,0.0,0.007578603457659483
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,확률 예측에서 MSE Loss 미 사용 이유,-0.018319307,0.0,0.018319306895136833
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",BCE 가 좋은 task,-0.012160896,0.0,0.012160896323621273
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지","BCE 가 좋은 task, 이유",0.0014770954,0.0,0.0014770954148843884
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",LLM Fine-Tuning 의 PEFT,-0.008097219,0.0,0.008097219280898571
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",LoRA,-0.013468258,0.0,0.013468258082866669
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",LoRA 와 QLoRA 의 차이,0.0012103677,0.0,0.001210367656312883
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",Loss Function 예시,-0.0042396323,0.0,0.004239632282406092
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",Loss Function 정의,-0.025274735,0.0,0.02527473494410515
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",MBTI,-0.01748741,0.0,0.017487410455942154
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",MSE Loss 설명,-0.026656682,0.0,0.026656681671738625
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",MSE Loss 용도,0.9887439,1.0,0.011256098747253418
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",Multi-Label 에서 CE + Softmax 적용 문제점,0.016676094,0.0,0.016676094383001328
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",PEFT 방법 5가지,0.0043721087,0.0,0.004372108727693558
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",거대 언어 모델 정의,-0.0064855083,0.0,0.006485508289188147
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",기본 경험,0.0029125046,0.0,0.0029125045984983444
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",답변 실패,-0.007951343,0.0,0.007951343432068825
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",딥러닝,0.010680917,0.0,0.010680916719138622
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",마지막 할 말,-0.005636187,0.0,0.005636186804622412
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",머신러닝,0.014868348,0.0,0.014868347905576229
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",면접 시작 인사,-0.0008946019,0.0,0.0008946019224822521
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",상세 경험,0.006670759,0.0,0.006670759059488773
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",수식,0.0010726526,0.0,0.0010726526379585266
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",인공지능,-0.0078156395,0.0,0.007815639488399029
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",잠시 휴식,-0.0003468817,0.0,0.0003468816867098212
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",좋아하는 아이돌,-0.007567797,0.0,0.007567796856164932
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",핵심 아이디어,-0.055794287,0.0,0.05579428747296333
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",확률 예측에서 MSE Loss 미 사용 이유,-0.006351013,0.0,0.0063510132022202015
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,BCE 가 좋은 task,-0.0024168605,0.0,0.002416860545054078
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,"BCE 가 좋은 task, 이유",0.0017706569,0.0,0.0017706569051370025
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,LLM Fine-Tuning 의 PEFT,0.0065676016,0.0,0.006567601580172777
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,LoRA,0.003315982,0.0,0.0033159819431602955
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,LoRA 와 QLoRA 의 차이,-0.0032860958,0.0,0.0032860958017408848
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,Loss Function 예시,0.00046231833,0.0,0.00046231833402998745
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,Loss Function 정의,0.00021432592,0.0,0.00021432591893244535
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,MBTI,-0.0012846718,0.0,0.001284671830944717
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,MSE Loss 설명,-0.0012374567,0.0,0.0012374566867947578
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,MSE Loss 용도,0.004693901,0.0,0.004693901166319847
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,Multi-Label 에서 CE + Softmax 적용 문제점,-0.007827243,0.0,0.007827242836356163
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,PEFT 방법 5가지,-0.008922973,0.0,0.008922972716391087
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,거대 언어 모델 정의,-0.0005448955,0.0,0.0005448954761959612
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,기본 경험,-0.0073286165,0.0,0.007328616455197334
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,답변 실패,0.99475086,1.0,0.005249142646789551
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,딥러닝,-0.0031562534,0.0,0.003156253369525075
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,마지막 할 말,0.0011534708,0.0,0.0011534708319231868
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,머신러닝,-0.0060698977,0.0,0.006069897674024105
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,면접 시작 인사,-0.008084539,0.0,0.008084539324045181
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,상세 경험,-0.0102417,0.0,0.01024170033633709
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,수식,-0.010632481,0.0,0.010632481426000595
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,인공지능,-0.0031888627,0.0,0.0031888626981526613
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,잠시 휴식,-0.0049948096,0.0,0.004994809627532959
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,좋아하는 아이돌,-0.008188273,0.0,0.008188272826373577
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,핵심 아이디어,-0.0036317876,0.0,0.003631787607446313
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,확률 예측에서 MSE Loss 미 사용 이유,0.0005095225,0.0,0.0005095225060358644
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,BCE 가 좋은 task,-0.018138772,0.0,0.018138771876692772
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,"BCE 가 좋은 task, 이유",0.0087962495,0.0,0.008796249516308308
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,LLM Fine-Tuning 의 PEFT,0.003200085,0.0,0.003200084902346134
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,LoRA,0.019799747,0.0,0.019799746572971344
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,LoRA 와 QLoRA 의 차이,-0.0018120605,0.0,0.0018120604800060391
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,Loss Function 예시,0.007916497,0.0,0.007916497066617012
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,Loss Function 정의,-0.011189498,0.0,0.011189498007297516
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,MBTI,-0.0067017484,0.0,0.006701748352497816
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,MSE Loss 설명,-0.009119444,0.0,0.009119443595409393
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,MSE Loss 용도,-0.0074234325,0.0,0.007423432543873787
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,Multi-Label 에서 CE + Softmax 적용 문제점,0.054582078,0.0,0.054582078009843826
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,PEFT 방법 5가지,-0.016993385,0.0,0.016993384808301926
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,거대 언어 모델 정의,-0.007679453,0.0,0.007679453119635582
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,기본 경험,-0.010756141,0.0,0.010756140574812889
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,답변 실패,0.9629278,1.0,0.037072181701660156
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,딥러닝,0.003067678,0.0,0.0030676780734211206
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,마지막 할 말,0.012720748,0.0,0.012720747850835323
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,머신러닝,-0.018171374,0.0,0.01817137375473976
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,면접 시작 인사,-0.006993613,0.0,0.006993613205850124
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,상세 경험,0.0051347157,0.0,0.00513471569865942
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,수식,-0.018681144,0.0,0.01868114434182644
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,인공지능,0.0065621585,0.0,0.006562158465385437
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,잠시 휴식,-0.0013317962,0.0,0.001331796171143651
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,좋아하는 아이돌,-0.020680439,0.0,0.020680438727140427
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,핵심 아이디어,0.0017863666,0.0,0.0017863665707409382
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,확률 예측에서 MSE Loss 미 사용 이유,0.03093426,0.0,0.030934259295463562
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,BCE 가 좋은 task,0.00086994935,0.0,0.0008699493482708931
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,"BCE 가 좋은 task, 이유",-0.01759238,0.0,0.017592379823327065
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,LLM Fine-Tuning 의 PEFT,-0.0112508945,0.0,0.011250894516706467
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,LoRA,-0.012319494,0.0,0.012319494038820267
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,LoRA 와 QLoRA 의 차이,-0.018612552,0.0,0.01861255243420601
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,Loss Function 예시,-0.008107124,0.0,0.008107123896479607
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,Loss Function 정의,-0.01935867,0.0,0.019358670338988304
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,MBTI,-0.009414536,0.0,0.009414535947144032
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,MSE Loss 설명,0.008545713,0.0,0.008545712567865849
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,MSE Loss 용도,-0.003789739,0.0,0.0037897389847785234
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,Multi-Label 에서 CE + Softmax 적용 문제점,-0.0042335163,0.0,0.004233516287058592
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,PEFT 방법 5가지,-0.012148792,0.0,0.012148791924118996
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,거대 언어 모델 정의,-0.0029917064,0.0,0.0029917063657194376
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,기본 경험,-0.0019207264,0.0,0.0019207263831049204
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,답변 실패,-0.00061807723,0.0,0.0006180772325024009
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,딥러닝,-0.0036309855,0.0,0.0036309855058789253
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,마지막 할 말,-0.0032017091,0.0,0.0032017091289162636
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,머신러닝,-0.018167557,0.0,0.018167557194828987
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,면접 시작 인사,0.009408859,0.0,0.009408858604729176
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,상세 경험,-0.023545638,0.0,0.023545637726783752
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,수식,-0.0222095,0.0,0.022209500893950462
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,인공지능,-0.0037538416,0.0,0.003753841621801257
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,잠시 휴식,0.0033664855,0.0,0.0033664854709059
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,좋아하는 아이돌,-0.015819948,0.0,0.01581994816660881
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,핵심 아이디어,-0.0009626588,0.0,0.0009626587852835655
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,확률 예측에서 MSE Loss 미 사용 이유,0.98755574,1.0,0.012444257736206055
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,BCE 가 좋은 task,-0.004367994,0.0,0.0043679941445589066
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,"BCE 가 좋은 task, 이유",-0.008053914,0.0,0.008053913712501526
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,LLM Fine-Tuning 의 PEFT,0.003321553,0.0,0.0033215531148016453
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,LoRA,0.007826766,0.0,0.00782676599919796
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,LoRA 와 QLoRA 의 차이,-0.028956456,0.0,0.0289564561098814
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,Loss Function 예시,-0.014556974,0.0,0.014556974172592163
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,Loss Function 정의,-0.008391908,0.0,0.0083919083699584
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,MBTI,0.013845118,0.0,0.013845117762684822
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,MSE Loss 설명,-0.04621998,0.0,0.04621997848153114
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,MSE Loss 용도,0.015070318,0.0,0.01507031824439764
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,Multi-Label 에서 CE + Softmax 적용 문제점,-0.007393404,0.0,0.00739340391010046
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,PEFT 방법 5가지,0.00051217544,0.0,0.0005121754365973175
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,거대 언어 모델 정의,-0.0211703,0.0,0.021170299500226974
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,기본 경험,-0.021110956,0.0,0.021110955625772476
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,답변 실패,0.0090662595,0.0,0.009066259488463402
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,딥러닝,0.014537565,0.0,0.014537565410137177
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,마지막 할 말,-0.0029268777,0.0,0.002926877699792385
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,머신러닝,-0.011104008,0.0,0.011104008182883263
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,면접 시작 인사,-0.00092148443,0.0,0.0009214844321832061
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,상세 경험,-0.012842917,0.0,0.012842916883528233
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,수식,0.9839783,1.0,0.016021728515625
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,인공지능,-0.008849425,0.0,0.008849425241351128
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,잠시 휴식,0.0036533705,0.0,0.0036533705424517393
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,좋아하는 아이돌,0.0014005733,0.0,0.0014005732955411077
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,핵심 아이디어,-0.004403259,0.0,0.004403259139508009
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,확률 예측에서 MSE Loss 미 사용 이유,-0.022159848,0.0,0.022159848362207413
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",BCE 가 좋은 task,-0.0012740116,0.0,0.0012740115635097027
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거","BCE 가 좋은 task, 이유",-0.02862881,0.0,0.02862880937755108
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",LLM Fine-Tuning 의 PEFT,-0.013476354,0.0,0.013476354070007801
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",LoRA,0.008873524,0.0,0.008873524144291878
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",LoRA 와 QLoRA 의 차이,-0.009949929,0.0,0.00994992908090353
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",Loss Function 예시,0.003347241,0.0,0.003347241086885333
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",Loss Function 정의,0.0056520547,0.0,0.00565205467864871
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",MBTI,-0.047092598,0.0,0.04709259793162346
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",MSE Loss 설명,-0.018158909,0.0,0.018158908933401108
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",MSE Loss 용도,-0.051989313,0.0,0.05198931321501732
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",Multi-Label 에서 CE + Softmax 적용 문제점,-0.019259993,0.0,0.019259992986917496
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",PEFT 방법 5가지,-0.013954159,0.0,0.013954158872365952
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",거대 언어 모델 정의,-0.02546783,0.0,0.025467829778790474
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",기본 경험,-0.006985487,0.0,0.006985486950725317
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",답변 실패,0.0025851992,0.0,0.0025851991958916187
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",딥러닝,0.0076410733,0.0,0.007641073316335678
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",마지막 할 말,-0.0043542306,0.0,0.004354230593889952
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",머신러닝,-0.013407126,0.0,0.013407126069068909
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",면접 시작 인사,-0.01759764,0.0,0.017597639933228493
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",상세 경험,-0.008182457,0.0,0.008182456716895103
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",수식,-0.032632075,0.0,0.03263207525014877
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",인공지능,-0.0062120105,0.0,0.0062120105139911175
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",잠시 휴식,0.00010433253,0.0,0.00010433253191877156
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",좋아하는 아이돌,-0.0030457196,0.0,0.0030457195825874805
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",핵심 아이디어,0.9768215,1.0,0.023178517818450928
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",확률 예측에서 MSE Loss 미 사용 이유,-0.034744438,0.0,0.03474443778395653
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,BCE 가 좋은 task,-0.0097899595,0.0,0.009789959527552128
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,"BCE 가 좋은 task, 이유",0.038785238,0.0,0.038785237818956375
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,LLM Fine-Tuning 의 PEFT,-0.061045174,0.0,0.061045173555612564
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,LoRA,-0.01770141,0.0,0.0177014097571373
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,LoRA 와 QLoRA 의 차이,-0.012862031,0.0,0.01286203134804964
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,Loss Function 예시,-0.018320624,0.0,0.01832062378525734
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,Loss Function 정의,0.030857226,0.0,0.030857225880026817
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,MBTI,0.0246927,0.0,0.024692699313163757
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,MSE Loss 설명,-0.010859367,0.0,0.010859367437660694
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,MSE Loss 용도,-0.0020243323,0.0,0.002024332294240594
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,Multi-Label 에서 CE + Softmax 적용 문제점,0.5029792,0.0,0.5029792189598083
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,PEFT 방법 5가지,-0.00084368547,0.0,0.0008436854695901275
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,거대 언어 모델 정의,0.08331202,0.0,0.0833120197057724
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,기본 경험,0.0051116445,0.0,0.005111644510179758
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,답변 실패,0.054395255,1.0,0.945604745298624
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,딥러닝,0.05048419,0.0,0.05048419162631035
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,마지막 할 말,-0.041924678,0.0,0.04192467778921127
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,머신러닝,-0.062902465,0.0,0.06290246546268463
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,면접 시작 인사,-0.040889986,0.0,0.040889985859394073
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,상세 경험,-0.0043461937,0.0,0.0043461937457323074
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,수식,0.031162726,0.0,0.031162725761532784
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,인공지능,-0.03175752,0.0,0.03175751864910126
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,잠시 휴식,-0.03333707,0.0,0.0333370715379715
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,좋아하는 아이돌,-0.009358006,0.0,0.009358005598187447
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,핵심 아이디어,0.52521086,0.0,0.5252108573913574
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,확률 예측에서 MSE Loss 미 사용 이유,-0.152933,0.0,0.1529330015182495
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",BCE 가 좋은 task,-0.017094087,0.0,0.017094086855649948
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고","BCE 가 좋은 task, 이유",-0.026714765,0.0,0.026714764535427094
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",LLM Fine-Tuning 의 PEFT,-0.02421227,0.0,0.024212269112467766
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",LoRA,0.007188738,0.0,0.007188737858086824
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",LoRA 와 QLoRA 의 차이,-0.0041660466,0.0,0.0041660466231405735
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",Loss Function 예시,-0.003986359,0.0,0.003986359108239412
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",Loss Function 정의,0.0050671804,0.0,0.005067180376499891
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",MBTI,-0.050225634,0.0,0.0502256341278553
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",MSE Loss 설명,-0.0039056314,0.0,0.0039056313689798117
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",MSE Loss 용도,-0.02730047,0.0,0.02730046957731247
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",Multi-Label 에서 CE + Softmax 적용 문제점,-0.020027496,0.0,0.02002749592065811
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",PEFT 방법 5가지,-0.005445114,0.0,0.005445113871246576
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",거대 언어 모델 정의,-0.017680004,0.0,0.017680004239082336
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",기본 경험,-0.0062003713,0.0,0.0062003713101148605
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",답변 실패,-0.00066969625,0.0,0.0006696962518617511
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",딥러닝,0.041373312,0.0,0.04137331247329712
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",마지막 할 말,-0.014949659,0.0,0.014949658885598183
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",머신러닝,-0.024617594,0.0,0.024617593735456467
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",면접 시작 인사,-0.014613046,0.0,0.014613046310842037
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",상세 경험,0.004595212,0.0,0.004595212172716856
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",수식,-0.02868138,0.0,0.028681380674242973
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",인공지능,-0.0124099925,0.0,0.012409992516040802
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",잠시 휴식,-0.011745194,0.0,0.011745193973183632
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",좋아하는 아이돌,-0.001974527,0.0,0.0019745270255953074
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",핵심 아이디어,0.9695306,1.0,0.030469417572021484
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",확률 예측에서 MSE Loss 미 사용 이유,-0.038543705,0.0,0.0385437048971653
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",BCE 가 좋은 task,0.03500452,0.0,0.035004518926143646
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!","BCE 가 좋은 task, 이유",-0.00023941691,0.0,0.0002394169132458046
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",LLM Fine-Tuning 의 PEFT,-0.007891404,0.0,0.007891404442489147
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",LoRA,-0.0020199355,0.0,0.0020199355203658342
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",LoRA 와 QLoRA 의 차이,-0.008457407,0.0,0.008457407355308533
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",Loss Function 예시,-0.016972193,0.0,0.016972193494439125
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",Loss Function 정의,-0.00026870097,0.0,0.00026870096917264163
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",MBTI,-0.0006832805,0.0,0.0006832805229350924
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",MSE Loss 설명,-0.001775568,0.0,0.0017755680019035935
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",MSE Loss 용도,-0.00064481824,0.0,0.0006448182393796742
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",Multi-Label 에서 CE + Softmax 적용 문제점,-0.011667019,0.0,0.011667018756270409
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",PEFT 방법 5가지,-0.020503586,0.0,0.020503586158156395
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",거대 언어 모델 정의,0.0052901004,0.0,0.00529010035097599
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",기본 경험,0.023464046,0.0,0.02346404641866684
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",답변 실패,0.95942,1.0,0.04057997465133667
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",딥러닝,-0.003229024,0.0,0.003229023888707161
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",마지막 할 말,-0.010803813,0.0,0.010803813114762306
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",머신러닝,0.0076069846,0.0,0.007606984581798315
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",면접 시작 인사,0.0014293307,0.0,0.0014293306740000844
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",상세 경험,-0.017107433,0.0,0.017107432708144188
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",수식,-0.009021355,0.0,0.009021354839205742
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",인공지능,-0.007734864,0.0,0.007734864018857479
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",잠시 휴식,-0.01557312,0.0,0.01557311974465847
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",좋아하는 아이돌,-0.017627265,0.0,0.01762726530432701
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",핵심 아이디어,-0.00040115343,0.0,0.00040115343290381134
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",확률 예측에서 MSE Loss 미 사용 이유,-0.01424438,0.0,0.01424438040703535
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",BCE 가 좋은 task,0.0024955014,1.0,0.9975044985767454
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어","BCE 가 좋은 task, 이유",0.007688134,0.0,0.007688133977353573
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",LLM Fine-Tuning 의 PEFT,-0.0011282699,0.0,0.001128269941546023
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",LoRA,-0.002071575,0.0,0.002071575028821826
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",LoRA 와 QLoRA 의 차이,0.00036388094,0.0,0.0003638809430412948
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",Loss Function 예시,-0.0015916205,0.0,0.0015916205011308193
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",Loss Function 정의,-0.00052056834,0.0,0.0005205683410167694
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",MBTI,5.5309738e-05,0.0,5.5309737945208326e-05
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",MSE Loss 설명,-0.0013633019,0.0,0.001363301882520318
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",MSE Loss 용도,-0.0033886496,0.0,0.0033886495511978865
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",Multi-Label 에서 CE + Softmax 적용 문제점,-0.007296305,0.0,0.007296305149793625
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",PEFT 방법 5가지,-0.009185669,0.0,0.009185669012367725
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",거대 언어 모델 정의,0.0030921053,0.0,0.0030921052675694227
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",기본 경험,-0.0027958157,0.0,0.0027958157006651163
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",답변 실패,0.99453986,0.0,0.9945398569107056
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",딥러닝,-0.0034417098,0.0,0.003441709792241454
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",마지막 할 말,-0.001251556,0.0,0.0012515559792518616
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",머신러닝,0.0019662986,0.0,0.001966298557817936
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",면접 시작 인사,-0.0060257656,0.0,0.006025765556842089
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",상세 경험,-0.011520207,0.0,0.011520206928253174
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",수식,-0.011272275,0.0,0.011272274889051914
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",인공지능,-0.0018424877,0.0,0.0018424877198413014
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",잠시 휴식,-0.011410605,0.0,0.011410605162382126
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",좋아하는 아이돌,-0.007515112,0.0,0.007515111938118935
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",핵심 아이디어,0.002982601,0.0,0.0029826010577380657
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",확률 예측에서 MSE Loss 미 사용 이유,0.0029122315,0.0,0.0029122314881533384
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",BCE 가 좋은 task,0.02496387,0.0,0.024963870644569397
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아","BCE 가 좋은 task, 이유",0.9742699,1.0,0.02573007345199585
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",LLM Fine-Tuning 의 PEFT,-0.0030191527,0.0,0.0030191526748239994
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",LoRA,0.008441543,0.0,0.008441543206572533
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",LoRA 와 QLoRA 의 차이,0.017857345,0.0,0.017857344821095467
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",Loss Function 예시,0.0046848017,0.0,0.004684801679104567
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",Loss Function 정의,-0.00237102,0.0,0.0023710199166089296
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",MBTI,0.00023862359,0.0,0.00023862358648329973
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",MSE Loss 설명,-0.011192572,0.0,0.011192572303116322
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",MSE Loss 용도,-0.0099073965,0.0,0.009907396510243416
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",Multi-Label 에서 CE + Softmax 적용 문제점,0.0072509623,0.0,0.007250962313264608
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",PEFT 방법 5가지,-0.00040735677,0.0,0.00040735676884651184
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",거대 언어 모델 정의,-0.008632239,0.0,0.008632238954305649
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",기본 경험,0.007845332,0.0,0.00784533191472292
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",답변 실패,0.002853216,0.0,0.002853215904906392
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",딥러닝,-0.013297706,0.0,0.01329770591109991
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",마지막 할 말,-0.008238319,0.0,0.008238319307565689
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",머신러닝,-0.0073144976,0.0,0.007314497604966164
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",면접 시작 인사,-0.00935216,0.0,0.009352159686386585
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",상세 경험,-0.023054242,0.0,0.02305424213409424
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",수식,-0.012260529,0.0,0.012260529212653637
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",인공지능,0.004481056,0.0,0.004481055773794651
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",잠시 휴식,-0.0017516898,0.0,0.0017516898224130273
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",좋아하는 아이돌,-0.01319556,0.0,0.013195560313761234
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",핵심 아이디어,-0.01382049,0.0,0.01382048986852169
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",확률 예측에서 MSE Loss 미 사용 이유,-0.0055461605,0.0,0.0055461605079472065
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,BCE 가 좋은 task,0.0009922661,0.0,0.0009922661120072007
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,"BCE 가 좋은 task, 이유",-0.011195972,0.0,0.011195971630513668
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,LLM Fine-Tuning 의 PEFT,-0.019495638,0.0,0.019495638087391853
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,LoRA,0.00935905,0.0,0.00935904961079359
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,LoRA 와 QLoRA 의 차이,-0.00070031505,0.0,0.0007003150531090796
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,Loss Function 예시,0.0038436202,0.0,0.0038436201866716146
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,Loss Function 정의,0.012524943,0.0,0.012524942867457867
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,MBTI,-0.0063930126,0.0,0.006393012590706348
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,MSE Loss 설명,0.01099224,0.0,0.010992240160703659
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,MSE Loss 용도,0.008631047,0.0,0.008631046861410141
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,Multi-Label 에서 CE + Softmax 적용 문제점,0.9851303,1.0,0.01486968994140625
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,PEFT 방법 5가지,0.00541225,0.0,0.005412249825894833
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,거대 언어 모델 정의,-0.007946327,0.0,0.007946327328681946
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,기본 경험,0.0123781,0.0,0.01237810030579567
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,답변 실패,-0.00576243,0.0,0.005762429907917976
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,딥러닝,0.009248123,0.0,0.009248122572898865
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,마지막 할 말,-0.004050187,0.0,0.004050186835229397
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,머신러닝,0.010699072,0.0,0.010699071921408176
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,면접 시작 인사,0.0030251506,0.0,0.0030251506250351667
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,상세 경험,0.016044939,0.0,0.016044938936829567
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,수식,-0.016636595,0.0,0.016636595129966736
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,인공지능,-0.0030310752,0.0,0.003031075233593583
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,잠시 휴식,0.002641375,0.0,0.002641374943777919
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,좋아하는 아이돌,-0.005790201,0.0,0.005790201015770435
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,핵심 아이디어,-0.010759462,0.0,0.010759461671113968
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,확률 예측에서 MSE Loss 미 사용 이유,-0.0006750647,0.0,0.0006750646862201393
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,BCE 가 좋은 task,0.000376787,0.0,0.0003767870075535029
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,"BCE 가 좋은 task, 이유",0.0017741582,0.0,0.0017741582123562694
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,LLM Fine-Tuning 의 PEFT,0.0018421625,0.0,0.001842162455432117
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,LoRA,0.000937201,0.0,0.0009372010244987905
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,LoRA 와 QLoRA 의 차이,-0.0007079455,0.0,0.000707945495378226
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,Loss Function 예시,0.0014451976,0.0,0.0014451976167038083
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,Loss Function 정의,-0.0066839103,0.0,0.0066839102655649185
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,MBTI,0.0044485736,0.0,0.0044485735706985
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,MSE Loss 설명,-0.006591862,0.0,0.006591862067580223
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,MSE Loss 용도,-0.0048552714,0.0,0.004855271428823471
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,Multi-Label 에서 CE + Softmax 적용 문제점,0.0039034213,0.0,0.003903421340510249
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,PEFT 방법 5가지,-0.0036628505,0.0,0.0036628504749387503
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,거대 언어 모델 정의,-0.0039394293,0.0,0.003939429298043251
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,기본 경험,-0.0019244722,0.0,0.0019244721625000238
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,답변 실패,0.9951557,1.0,0.004844307899475098
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,딥러닝,-0.0013593467,0.0,0.001359346671961248
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,마지막 할 말,-0.0046446156,0.0,0.004644615575671196
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,머신러닝,-0.00015743221,0.0,0.0001574322086526081
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,면접 시작 인사,-0.007482858,0.0,0.007482857909053564
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,상세 경험,-0.011297962,0.0,0.011297961696982384
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,수식,-0.012150915,0.0,0.012150915339589119
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,인공지능,-0.002930894,0.0,0.0029308940283954144
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,잠시 휴식,-0.005439835,0.0,0.005439835134893656
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,좋아하는 아이돌,0.0008337937,0.0,0.0008337937179021537
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,핵심 아이디어,0.0046003317,0.0,0.004600331652909517
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,확률 예측에서 MSE Loss 미 사용 이유,-0.003956059,0.0,0.003956058993935585
Loss Function 관련 실무 경험 -> 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,BCE 가 좋은 task,0.017718649,0.0,0.01771864853799343
Loss Function 관련 실무 경험 -> 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,"BCE 가 좋은 task, 이유",-0.0047633913,0.0,0.004763391334563494
Loss Function 관련 실무 경험 -> 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,LLM Fine-Tuning 의 PEFT,-0.03162436,0.0,0.031624358147382736
Loss Function 관련 실무 경험 -> 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,LoRA,-0.017921468,0.0,0.01792146824300289
Loss Function 관련 실무 경험 -> 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,LoRA 와 QLoRA 의 차이,-0.0003662841,0.0,0.00036628410452976823
Loss Function 관련 실무 경험 -> 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,Loss Function 예시,0.016810972,0.0,0.01681097224354744
Loss Function 관련 실무 경험 -> 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,Loss Function 정의,0.02628746,0.0,0.026287460699677467
Loss Function 관련 실무 경험 -> 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,MBTI,-0.046463188,0.0,0.04646318778395653
Loss Function 관련 실무 경험 -> 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,MSE Loss 설명,-0.0057105618,0.0,0.0057105617597699165
Loss Function 관련 실무 경험 -> 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,MSE Loss 용도,0.010159353,0.0,0.010159352794289589
Loss Function 관련 실무 경험 -> 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,Multi-Label 에서 CE + Softmax 적용 문제점,-0.004389263,0.0,0.0043892632238566875
Loss Function 관련 실무 경험 -> 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,PEFT 방법 5가지,-0.034350686,0.0,0.03435068577528
Loss Function 관련 실무 경험 -> 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,거대 언어 모델 정의,0.038041223,0.0,0.03804122284054756
Loss Function 관련 실무 경험 -> 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,기본 경험,0.104561485,1.0,0.895438514649868
Loss Function 관련 실무 경험 -> 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,답변 실패,-0.021312343,0.0,0.021312342956662178
Loss Function 관련 실무 경험 -> 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,딥러닝,0.009423156,0.0,0.009423156268894672
Loss Function 관련 실무 경험 -> 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,마지막 할 말,-0.00304587,0.0,0.003045869991183281
Loss Function 관련 실무 경험 -> 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,머신러닝,-0.005734834,0.0,0.005734833888709545
Loss Function 관련 실무 경험 -> 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,면접 시작 인사,0.0043933927,0.0,0.004393392708152533
Loss Function 관련 실무 경험 -> 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,상세 경험,0.90454906,0.0,0.9045490622520447
Loss Function 관련 실무 경험 -> 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,수식,-0.02639107,0.0,0.02639107033610344
Loss Function 관련 실무 경험 -> 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,인공지능,-0.0046984605,0.0,0.004698460455983877
Loss Function 관련 실무 경험 -> 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,잠시 휴식,-0.014635631,0.0,0.014635630883276463
Loss Function 관련 실무 경험 -> 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,좋아하는 아이돌,-0.018188043,0.0,0.01818804256618023
Loss Function 관련 실무 경험 -> 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,핵심 아이디어,-0.008928421,0.0,0.008928420953452587
Loss Function 관련 실무 경험 -> 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,확률 예측에서 MSE Loss 미 사용 이유,-0.0098135155,0.0,0.009813515469431877
Loss Function 관련 실무 경험 -> 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,BCE 가 좋은 task,0.012183023,0.0,0.012183022685348988
Loss Function 관련 실무 경험 -> 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,"BCE 가 좋은 task, 이유",-0.0045407168,0.0,0.004540716763585806
Loss Function 관련 실무 경험 -> 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,LLM Fine-Tuning 의 PEFT,-0.007424315,0.0,0.007424314972013235
Loss Function 관련 실무 경험 -> 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,LoRA,-0.010662265,0.0,0.010662265121936798
Loss Function 관련 실무 경험 -> 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,LoRA 와 QLoRA 의 차이,0.0036248951,0.0,0.0036248951219022274
Loss Function 관련 실무 경험 -> 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,Loss Function 예시,-0.0060995673,0.0,0.006099567282944918
Loss Function 관련 실무 경험 -> 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,Loss Function 정의,-0.011572556,0.0,0.011572555638849735
Loss Function 관련 실무 경험 -> 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,MBTI,-0.0006624606,0.0,0.0006624605739489198
Loss Function 관련 실무 경험 -> 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,MSE Loss 설명,-0.0037613364,0.0,0.003761336440220475
Loss Function 관련 실무 경험 -> 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,MSE Loss 용도,-0.018992994,0.0,0.018992993980646133
Loss Function 관련 실무 경험 -> 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,Multi-Label 에서 CE + Softmax 적용 문제점,-0.010407895,0.0,0.010407894849777222
Loss Function 관련 실무 경험 -> 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,PEFT 방법 5가지,-0.0005017435,0.0,0.0005017435178160667
Loss Function 관련 실무 경험 -> 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,거대 언어 모델 정의,-0.003731519,0.0,0.0037315189838409424
Loss Function 관련 실무 경험 -> 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,기본 경험,0.9540255,0.0,0.9540255069732666
Loss Function 관련 실무 경험 -> 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,답변 실패,0.011316755,0.0,0.01131675485521555
Loss Function 관련 실무 경험 -> 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,딥러닝,-0.0006039826,0.0,0.0006039825966581702
Loss Function 관련 실무 경험 -> 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,마지막 할 말,-0.013543374,0.0,0.01354337390512228
Loss Function 관련 실무 경험 -> 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,머신러닝,-0.014437966,0.0,0.014437966048717499
Loss Function 관련 실무 경험 -> 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,면접 시작 인사,-0.011255654,0.0,0.011255653575062752
Loss Function 관련 실무 경험 -> 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,상세 경험,0.09093229,1.0,0.9090677127242088
Loss Function 관련 실무 경험 -> 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,수식,0.011092166,0.0,0.011092166416347027
Loss Function 관련 실무 경험 -> 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,인공지능,-0.0019887213,0.0,0.001988721312955022
Loss Function 관련 실무 경험 -> 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,잠시 휴식,-0.019363219,0.0,0.019363218918442726
Loss Function 관련 실무 경험 -> 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,좋아하는 아이돌,-0.0068547577,0.0,0.00685475766658783
Loss Function 관련 실무 경험 -> 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,핵심 아이디어,-0.019434204,0.0,0.019434204325079918
Loss Function 관련 실무 경험 -> 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,확률 예측에서 MSE Loss 미 사용 이유,-0.02020938,0.0,0.020209379494190216
Loss Function 관련 실무 경험 -> 근데 기억 진짜 안나는데 어떡하지?,BCE 가 좋은 task,-0.004094154,0.0,0.004094154108315706
Loss Function 관련 실무 경험 -> 근데 기억 진짜 안나는데 어떡하지?,"BCE 가 좋은 task, 이유",0.002638734,0.0,0.002638733945786953
Loss Function 관련 실무 경험 -> 근데 기억 진짜 안나는데 어떡하지?,LLM Fine-Tuning 의 PEFT,0.0035158526,0.0,0.003515852615237236
Loss Function 관련 실무 경험 -> 근데 기억 진짜 안나는데 어떡하지?,LoRA,0.0017771441,0.0,0.0017771441489458084
Loss Function 관련 실무 경험 -> 근데 기억 진짜 안나는데 어떡하지?,LoRA 와 QLoRA 의 차이,-0.0010525297,0.0,0.0010525296675041318
Loss Function 관련 실무 경험 -> 근데 기억 진짜 안나는데 어떡하지?,Loss Function 예시,-0.0009378428,0.0,0.0009378428221680224
Loss Function 관련 실무 경험 -> 근데 기억 진짜 안나는데 어떡하지?,Loss Function 정의,-0.0057439376,0.0,0.0057439375668764114
Loss Function 관련 실무 경험 -> 근데 기억 진짜 안나는데 어떡하지?,MBTI,0.0037595571,0.0,0.0037595571484416723
Loss Function 관련 실무 경험 -> 근데 기억 진짜 안나는데 어떡하지?,MSE Loss 설명,-0.0042569158,0.0,0.004256915766745806
Loss Function 관련 실무 경험 -> 근데 기억 진짜 안나는데 어떡하지?,MSE Loss 용도,-0.002998597,0.0,0.002998596988618374
Loss Function 관련 실무 경험 -> 근데 기억 진짜 안나는데 어떡하지?,Multi-Label 에서 CE + Softmax 적용 문제점,-0.008749058,0.0,0.008749057538807392
Loss Function 관련 실무 경험 -> 근데 기억 진짜 안나는데 어떡하지?,PEFT 방법 5가지,-0.0066351225,0.0,0.006635122466832399
Loss Function 관련 실무 경험 -> 근데 기억 진짜 안나는데 어떡하지?,거대 언어 모델 정의,-2.8884982e-05,0.0,2.888498238462489e-05
Loss Function 관련 실무 경험 -> 근데 기억 진짜 안나는데 어떡하지?,기본 경험,0.0014307689,0.0,0.0014307688688859344
Loss Function 관련 실무 경험 -> 근데 기억 진짜 안나는데 어떡하지?,답변 실패,0.99665433,1.0,0.003345668315887451
Loss Function 관련 실무 경험 -> 근데 기억 진짜 안나는데 어떡하지?,딥러닝,-0.009052084,0.0,0.00905208382755518
Loss Function 관련 실무 경험 -> 근데 기억 진짜 안나는데 어떡하지?,마지막 할 말,-0.0053401915,0.0,0.005340191535651684
Loss Function 관련 실무 경험 -> 근데 기억 진짜 안나는데 어떡하지?,머신러닝,-0.003052982,0.0,0.003052982036024332
Loss Function 관련 실무 경험 -> 근데 기억 진짜 안나는데 어떡하지?,면접 시작 인사,-0.0050846487,0.0,0.0050846487283706665
Loss Function 관련 실무 경험 -> 근데 기억 진짜 안나는데 어떡하지?,상세 경험,-0.002875844,0.0,0.0028758440166711807
Loss Function 관련 실무 경험 -> 근데 기억 진짜 안나는데 어떡하지?,수식,-0.009354992,0.0,0.009354991838335991
Loss Function 관련 실무 경험 -> 근데 기억 진짜 안나는데 어떡하지?,인공지능,-0.0002902285,0.0,0.0002902284904848784
Loss Function 관련 실무 경험 -> 근데 기억 진짜 안나는데 어떡하지?,잠시 휴식,-0.008637564,0.0,0.0086375642567873
Loss Function 관련 실무 경험 -> 근데 기억 진짜 안나는데 어떡하지?,좋아하는 아이돌,-0.0008441048,0.0,0.0008441047975793481
Loss Function 관련 실무 경험 -> 근데 기억 진짜 안나는데 어떡하지?,핵심 아이디어,0.0044980748,0.0,0.004498074762523174
Loss Function 관련 실무 경험 -> 근데 기억 진짜 안나는데 어떡하지?,확률 예측에서 MSE Loss 미 사용 이유,-0.0035804997,0.0,0.0035804996732622385
Loss Function 관련 실무 경험 -> 나 아직 실무 경험 없는데 ㅠㅠ,BCE 가 좋은 task,-0.004101039,0.0,0.004101038910448551
Loss Function 관련 실무 경험 -> 나 아직 실무 경험 없는데 ㅠㅠ,"BCE 가 좋은 task, 이유",0.0025461696,0.0,0.002546169562265277
Loss Function 관련 실무 경험 -> 나 아직 실무 경험 없는데 ㅠㅠ,LLM Fine-Tuning 의 PEFT,-0.0010841702,0.0,0.0010841701878234744
Loss Function 관련 실무 경험 -> 나 아직 실무 경험 없는데 ㅠㅠ,LoRA,-0.0006857715,0.0,0.0006857715197838843
Loss Function 관련 실무 경험 -> 나 아직 실무 경험 없는데 ㅠㅠ,LoRA 와 QLoRA 의 차이,-0.0037579064,0.0,0.0037579063791781664
Loss Function 관련 실무 경험 -> 나 아직 실무 경험 없는데 ㅠㅠ,Loss Function 예시,0.0034639917,0.0,0.0034639916848391294
Loss Function 관련 실무 경험 -> 나 아직 실무 경험 없는데 ㅠㅠ,Loss Function 정의,0.00165579,0.0,0.0016557900235056877
Loss Function 관련 실무 경험 -> 나 아직 실무 경험 없는데 ㅠㅠ,MBTI,-0.00621525,0.0,0.006215250119566917
Loss Function 관련 실무 경험 -> 나 아직 실무 경험 없는데 ㅠㅠ,MSE Loss 설명,-0.0055628596,0.0,0.005562859587371349
Loss Function 관련 실무 경험 -> 나 아직 실무 경험 없는데 ㅠㅠ,MSE Loss 용도,-0.00010014112,0.0,0.00010014112194767222
Loss Function 관련 실무 경험 -> 나 아직 실무 경험 없는데 ㅠㅠ,Multi-Label 에서 CE + Softmax 적용 문제점,0.005303059,0.0,0.00530305877327919
Loss Function 관련 실무 경험 -> 나 아직 실무 경험 없는데 ㅠㅠ,PEFT 방법 5가지,0.0015448317,0.0,0.0015448316698893905
Loss Function 관련 실무 경험 -> 나 아직 실무 경험 없는데 ㅠㅠ,거대 언어 모델 정의,-0.009982772,0.0,0.009982772171497345
Loss Function 관련 실무 경험 -> 나 아직 실무 경험 없는데 ㅠㅠ,기본 경험,0.98173857,1.0,0.018261432647705078
Loss Function 관련 실무 경험 -> 나 아직 실무 경험 없는데 ㅠㅠ,답변 실패,-0.0004206819,0.0,0.00042068189941346645
Loss Function 관련 실무 경험 -> 나 아직 실무 경험 없는데 ㅠㅠ,딥러닝,-0.0052295844,0.0,0.005229584407061338
Loss Function 관련 실무 경험 -> 나 아직 실무 경험 없는데 ㅠㅠ,마지막 할 말,-0.015972015,0.0,0.015972014516592026
Loss Function 관련 실무 경험 -> 나 아직 실무 경험 없는데 ㅠㅠ,머신러닝,-0.0134387445,0.0,0.013438744470477104
Loss Function 관련 실무 경험 -> 나 아직 실무 경험 없는데 ㅠㅠ,면접 시작 인사,-0.011856368,0.0,0.01185636781156063
Loss Function 관련 실무 경험 -> 나 아직 실무 경험 없는데 ㅠㅠ,상세 경험,0.045593526,0.0,0.04559352621436119
Loss Function 관련 실무 경험 -> 나 아직 실무 경험 없는데 ㅠㅠ,수식,-0.009474468,0.0,0.009474468417465687
Loss Function 관련 실무 경험 -> 나 아직 실무 경험 없는데 ㅠㅠ,인공지능,-0.0053089415,0.0,0.005308941472321749
Loss Function 관련 실무 경험 -> 나 아직 실무 경험 없는데 ㅠㅠ,잠시 휴식,-0.018851746,0.0,0.01885174587368965
Loss Function 관련 실무 경험 -> 나 아직 실무 경험 없는데 ㅠㅠ,좋아하는 아이돌,-0.014919039,0.0,0.014919038861989975
Loss Function 관련 실무 경험 -> 나 아직 실무 경험 없는데 ㅠㅠ,핵심 아이디어,-0.0144123295,0.0,0.014412329532206059
Loss Function 관련 실무 경험 -> 나 아직 실무 경험 없는데 ㅠㅠ,확률 예측에서 MSE Loss 미 사용 이유,-0.003649355,0.0,0.003649354912340641
Loss Function 관련 실무 경험 -> 대답하기 싫은데 힝,BCE 가 좋은 task,-6.9259e-06,0.0,6.925899924681289e-06
Loss Function 관련 실무 경험 -> 대답하기 싫은데 힝,"BCE 가 좋은 task, 이유",0.00013347552,0.0,0.0001334755215793848
Loss Function 관련 실무 경험 -> 대답하기 싫은데 힝,LLM Fine-Tuning 의 PEFT,0.002412682,0.0,0.002412681933492422
Loss Function 관련 실무 경험 -> 대답하기 싫은데 힝,LoRA,0.00039651396,0.0,0.00039651396218687296
Loss Function 관련 실무 경험 -> 대답하기 싫은데 힝,LoRA 와 QLoRA 의 차이,-0.0027467154,0.0,0.0027467154432088137
Loss Function 관련 실무 경험 -> 대답하기 싫은데 힝,Loss Function 예시,-0.0020333447,0.0,0.002033344702795148
Loss Function 관련 실무 경험 -> 대답하기 싫은데 힝,Loss Function 정의,-0.0035686428,0.0,0.003568642772734165
Loss Function 관련 실무 경험 -> 대답하기 싫은데 힝,MBTI,0.0028160883,0.0,0.0028160882648080587
Loss Function 관련 실무 경험 -> 대답하기 싫은데 힝,MSE Loss 설명,-0.0024187933,0.0,0.002418793272227049
Loss Function 관련 실무 경험 -> 대답하기 싫은데 힝,MSE Loss 용도,-0.0013890606,0.0,0.0013890606351196766
Loss Function 관련 실무 경험 -> 대답하기 싫은데 힝,Multi-Label 에서 CE + Softmax 적용 문제점,-0.007742798,0.0,0.007742797955870628
Loss Function 관련 실무 경험 -> 대답하기 싫은데 힝,PEFT 방법 5가지,-0.007355143,0.0,0.00735514285042882
Loss Function 관련 실무 경험 -> 대답하기 싫은데 힝,거대 언어 모델 정의,-0.0009462924,0.0,0.0009462924208492041
Loss Function 관련 실무 경험 -> 대답하기 싫은데 힝,기본 경험,0.0022793578,0.0,0.0022793577518314123
Loss Function 관련 실무 경험 -> 대답하기 싫은데 힝,답변 실패,0.99686676,1.0,0.003133237361907959
Loss Function 관련 실무 경험 -> 대답하기 싫은데 힝,딥러닝,-0.0054245964,0.0,0.00542459636926651
Loss Function 관련 실무 경험 -> 대답하기 싫은데 힝,마지막 할 말,-0.0076191598,0.0,0.007619159761816263
Loss Function 관련 실무 경험 -> 대답하기 싫은데 힝,머신러닝,-0.0017468855,0.0,0.0017468854784965515
Loss Function 관련 실무 경험 -> 대답하기 싫은데 힝,면접 시작 인사,-0.004370733,0.0,0.004370733164250851
Loss Function 관련 실무 경험 -> 대답하기 싫은데 힝,상세 경험,-0.0044465344,0.0,0.004446534439921379
Loss Function 관련 실무 경험 -> 대답하기 싫은데 힝,수식,-0.007948504,0.0,0.007948503829538822
Loss Function 관련 실무 경험 -> 대답하기 싫은데 힝,인공지능,-2.322763e-05,0.0,2.3227630663313903e-05
Loss Function 관련 실무 경험 -> 대답하기 싫은데 힝,잠시 휴식,-0.0057310215,0.0,0.005731021519750357
Loss Function 관련 실무 경험 -> 대답하기 싫은데 힝,좋아하는 아이돌,-0.002706374,0.0,0.002706374041736126
Loss Function 관련 실무 경험 -> 대답하기 싫은데 힝,핵심 아이디어,-0.00035314832,0.0,0.0003531483234837651
Loss Function 관련 실무 경험 -> 대답하기 싫은데 힝,확률 예측에서 MSE Loss 미 사용 이유,-0.00041293603,0.0,0.0004129360313527286
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,BCE 가 좋은 task,0.007536299,0.0,0.0075362990610301495
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,"BCE 가 좋은 task, 이유",-0.0013481744,0.0,0.0013481744099408388
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,LLM Fine-Tuning 의 PEFT,-0.03309163,0.0,0.03309163078665733
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,LoRA,-0.025459105,0.0,0.025459105148911476
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,LoRA 와 QLoRA 의 차이,-0.0006276382,0.0,0.0006276381900534034
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,Loss Function 예시,-0.012281482,0.0,0.012281482107937336
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,Loss Function 정의,0.0012125795,0.0,0.0012125795474275947
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,MBTI,0.9829954,1.0,0.01700460910797119
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,MSE Loss 설명,-0.014721325,0.0,0.014721324667334557
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,MSE Loss 용도,-0.027704671,0.0,0.027704671025276184
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,Multi-Label 에서 CE + Softmax 적용 문제점,-0.012782398,0.0,0.01278239767998457
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,PEFT 방법 5가지,-0.007985798,0.0,0.007985797710716724
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,거대 언어 모델 정의,-0.02000657,0.0,0.020006569102406502
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,기본 경험,-0.000515887,0.0,0.0005158869898878038
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,답변 실패,-0.009873381,0.0,0.00987338088452816
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,딥러닝,-0.019076541,0.0,0.01907654106616974
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,마지막 할 말,-0.018845616,0.0,0.018845615908503532
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,머신러닝,0.008423522,0.0,0.008423522114753723
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,면접 시작 인사,-0.0074617905,0.0,0.007461790461093187
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,상세 경험,0.008527076,0.0,0.008527075871825218
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,수식,-0.0063199797,0.0,0.0063199796713888645
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,인공지능,0.0021154678,0.0,0.002115467796102166
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,잠시 휴식,-0.004266288,0.0,0.004266288131475449
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,좋아하는 아이돌,-0.015686655,0.0,0.015686655417084694
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,핵심 아이디어,-0.023851832,0.0,0.023851832374930382
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,확률 예측에서 MSE Loss 미 사용 이유,-0.0009653733,0.0,0.0009653732995502651
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,BCE 가 좋은 task,-0.0044004275,0.0,0.004400427453219891
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,"BCE 가 좋은 task, 이유",-0.010502812,0.0,0.010502812452614307
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,LLM Fine-Tuning 의 PEFT,0.008846511,0.0,0.008846511133015156
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,LoRA,0.0045619984,0.0,0.004561998415738344
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,LoRA 와 QLoRA 의 차이,0.010180468,0.0,0.010180467739701271
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,Loss Function 예시,-0.0075520216,0.0,0.007552021648734808
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,Loss Function 정의,0.008839464,0.0,0.00883946381509304
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,MBTI,0.0045285043,0.0,0.004528504330664873
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,MSE Loss 설명,-0.010610164,0.0,0.010610164143145084
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,MSE Loss 용도,-0.004206778,0.0,0.0042067780159413815
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,Multi-Label 에서 CE + Softmax 적용 문제점,-0.000432708,0.0,0.0004327080096118152
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,PEFT 방법 5가지,-0.0027960932,0.0,0.0027960932347923517
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,거대 언어 모델 정의,0.0070619723,0.0,0.0070619722828269005
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,기본 경험,-0.02456835,0.0,0.0245683491230011
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,답변 실패,-0.008941534,0.0,0.008941533975303173
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,딥러닝,-0.0006224219,0.0,0.000622421910520643
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,마지막 할 말,-0.022886112,0.0,0.022886112332344055
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,머신러닝,-0.0073546483,0.0,0.007354648318141699
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,면접 시작 인사,0.008891626,0.0,0.008891626261174679
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,상세 경험,-0.0029120503,0.0,0.0029120503459125757
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,수식,0.00030682766,0.0,0.0003068276564590633
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,인공지능,-0.010465034,0.0,0.010465034283697605
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,잠시 휴식,-0.0023879623,0.0,0.0023879623040556908
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,좋아하는 아이돌,0.9882087,1.0,0.01179128885269165
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,핵심 아이디어,-0.016718922,0.0,0.016718922182917595
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,확률 예측에서 MSE Loss 미 사용 이유,-0.02261704,0.0,0.0226170402020216
잠시 휴식 -> 재미있는 이야기 해줄래?,BCE 가 좋은 task,0.005679318,0.0,0.005679318215698004
잠시 휴식 -> 재미있는 이야기 해줄래?,"BCE 가 좋은 task, 이유",-0.0021689113,0.0,0.002168911276385188
잠시 휴식 -> 재미있는 이야기 해줄래?,LLM Fine-Tuning 의 PEFT,0.005002143,0.0,0.005002142861485481
잠시 휴식 -> 재미있는 이야기 해줄래?,LoRA,0.0055978117,0.0,0.005597811657935381
잠시 휴식 -> 재미있는 이야기 해줄래?,LoRA 와 QLoRA 의 차이,-0.0052768877,0.0,0.0052768876776099205
잠시 휴식 -> 재미있는 이야기 해줄래?,Loss Function 예시,-0.00037080309,0.0,0.00037080308538861573
잠시 휴식 -> 재미있는 이야기 해줄래?,Loss Function 정의,-0.0025376435,0.0,0.002537643536925316
잠시 휴식 -> 재미있는 이야기 해줄래?,MBTI,-0.007947945,0.0,0.007947945035994053
잠시 휴식 -> 재미있는 이야기 해줄래?,MSE Loss 설명,0.010450693,0.0,0.010450692847371101
잠시 휴식 -> 재미있는 이야기 해줄래?,MSE Loss 용도,0.00037397267,0.0,0.00037397266714833677
잠시 휴식 -> 재미있는 이야기 해줄래?,Multi-Label 에서 CE + Softmax 적용 문제점,0.0017639662,0.0,0.001763966167345643
잠시 휴식 -> 재미있는 이야기 해줄래?,PEFT 방법 5가지,-0.0021559545,0.0,0.002155954483896494
잠시 휴식 -> 재미있는 이야기 해줄래?,거대 언어 모델 정의,-0.0071232757,0.0,0.00712327565997839
잠시 휴식 -> 재미있는 이야기 해줄래?,기본 경험,-0.014042601,0.0,0.014042600989341736
잠시 휴식 -> 재미있는 이야기 해줄래?,답변 실패,-0.010620983,0.0,0.010620983317494392
잠시 휴식 -> 재미있는 이야기 해줄래?,딥러닝,-0.01587753,0.0,0.015877529978752136
잠시 휴식 -> 재미있는 이야기 해줄래?,마지막 할 말,-0.011731031,0.0,0.011731031350791454
잠시 휴식 -> 재미있는 이야기 해줄래?,머신러닝,-0.0061678793,0.0,0.006167879328131676
잠시 휴식 -> 재미있는 이야기 해줄래?,면접 시작 인사,-0.030012358,0.0,0.030012357980012894
잠시 휴식 -> 재미있는 이야기 해줄래?,상세 경험,0.0044507734,0.0,0.0044507733546197414
잠시 휴식 -> 재미있는 이야기 해줄래?,수식,-0.00059764396,0.0,0.0005976439570076764
잠시 휴식 -> 재미있는 이야기 해줄래?,인공지능,-0.007707669,0.0,0.00770766893401742
잠시 휴식 -> 재미있는 이야기 해줄래?,잠시 휴식,0.9944986,1.0,0.005501389503479004
잠시 휴식 -> 재미있는 이야기 해줄래?,좋아하는 아이돌,-0.009059069,0.0,0.009059068746864796
잠시 휴식 -> 재미있는 이야기 해줄래?,핵심 아이디어,0.007286176,0.0,0.007286176085472107
잠시 휴식 -> 재미있는 이야기 해줄래?,확률 예측에서 MSE Loss 미 사용 이유,0.0011828187,0.0,0.0011828186688944697
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",BCE 가 좋은 task,-0.026418855,0.0,0.026418855413794518
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야","BCE 가 좋은 task, 이유",0.00890147,0.0,0.008901470340788364
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",LLM Fine-Tuning 의 PEFT,0.9623774,1.0,0.03762257099151611
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",LoRA,0.016264923,0.0,0.01626492291688919
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",LoRA 와 QLoRA 의 차이,0.009109523,0.0,0.00910952314734459
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",Loss Function 예시,-0.0035511844,0.0,0.003551184432581067
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",Loss Function 정의,-0.002498155,0.0,0.0024981549941003323
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",MBTI,-0.012797353,0.0,0.012797352857887745
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",MSE Loss 설명,-0.0006856354,0.0,0.0006856353720650077
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",MSE Loss 용도,0.002460414,0.0,0.0024604140780866146
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",Multi-Label 에서 CE + Softmax 적용 문제점,-0.012289871,0.0,0.012289871461689472
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",PEFT 방법 5가지,0.010502114,0.0,0.010502113960683346
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",거대 언어 모델 정의,-0.003783321,0.0,0.0037833210080862045
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",기본 경험,-0.014752908,0.0,0.014752907678484917
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",답변 실패,0.008682059,0.0,0.00868205912411213
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",딥러닝,0.0032160962,0.0,0.0032160962000489235
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",마지막 할 말,-0.0014319116,0.0,0.0014319116016849875
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",머신러닝,0.0030181687,0.0,0.003018168732523918
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",면접 시작 인사,0.009213156,0.0,0.009213156066834927
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",상세 경험,-0.023731874,0.0,0.02373187430202961
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",수식,0.00860601,0.0,0.008606010116636753
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",인공지능,-0.009647153,0.0,0.009647153317928314
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",잠시 휴식,-0.0012355451,0.0,0.0012355451472103596
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",좋아하는 아이돌,0.021629473,0.0,0.021629473194479942
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",핵심 아이디어,0.00061913225,0.0,0.0006191322463564575
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",확률 예측에서 MSE Loss 미 사용 이유,-0.0069670407,0.0,0.0069670407101511955
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,BCE 가 좋은 task,-0.007239429,0.0,0.00723942881450057
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,"BCE 가 좋은 task, 이유",1.8998317e-07,0.0,1.8998316875240562e-07
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,LLM Fine-Tuning 의 PEFT,0.012817443,0.0,0.01281744334846735
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,LoRA,-0.001217006,0.0,0.0012170060072094202
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,LoRA 와 QLoRA 의 차이,-0.003018363,0.0,0.0030183629132807255
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,Loss Function 예시,0.0033842484,0.0,0.0033842483535408974
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,Loss Function 정의,-7.835089e-05,0.0,7.835088763386011e-05
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,MBTI,-0.0011907356,0.0,0.0011907356092706323
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,MSE Loss 설명,-0.005505755,0.0,0.005505755078047514
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,MSE Loss 용도,-0.005712156,0.0,0.005712156184017658
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,Multi-Label 에서 CE + Softmax 적용 문제점,-0.004207351,0.0,0.00420735077932477
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,PEFT 방법 5가지,-0.00719053,0.0,0.007190530188381672
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,거대 언어 모델 정의,-0.0031792975,0.0,0.003179297549650073
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,기본 경험,-0.00014912165,0.0,0.00014912165352143347
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,답변 실패,0.9946568,1.0,0.005343198776245117
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,딥러닝,-0.00349577,0.0,0.003495770040899515
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,마지막 할 말,0.0011264266,0.0,0.0011264266213402152
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,머신러닝,0.0017914256,0.0,0.0017914256313815713
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,면접 시작 인사,-0.0042618713,0.0,0.004261871334165335
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,상세 경험,-0.018517854,0.0,0.018517853692173958
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,수식,-0.0062504313,0.0,0.006250431295484304
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,인공지능,0.0002793376,0.0,0.000279337604297325
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,잠시 휴식,-0.010870866,0.0,0.010870865546166897
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,좋아하는 아이돌,-0.005032517,0.0,0.0050325170159339905
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,핵심 아이디어,0.0030657612,0.0,0.0030657611787319183
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,확률 예측에서 MSE Loss 미 사용 이유,-0.0059186337,0.0,0.005918633658438921
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",BCE 가 좋은 task,-0.0034033656,0.0,0.0034033656120300293
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?","BCE 가 좋은 task, 이유",-0.0067235455,0.0,0.006723545491695404
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",LLM Fine-Tuning 의 PEFT,0.0047031203,0.0,0.004703120328485966
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",LoRA,-0.020191316,0.0,0.020191315561532974
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",LoRA 와 QLoRA 의 차이,0.014681607,0.0,0.01468160655349493
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",Loss Function 예시,0.002158137,0.0,0.0021581370383501053
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",Loss Function 정의,0.0041776337,0.0,0.004177633672952652
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",MBTI,-0.0037163424,0.0,0.003716342383995652
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",MSE Loss 설명,0.0045869946,0.0,0.004586994647979736
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",MSE Loss 용도,0.012078201,0.0,0.01207820139825344
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",Multi-Label 에서 CE + Softmax 적용 문제점,0.012736655,0.0,0.012736654840409756
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",PEFT 방법 5가지,0.98801416,1.0,0.011985838413238525
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",거대 언어 모델 정의,-0.015795674,0.0,0.015795674175024033
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",기본 경험,0.0036454927,0.0,0.0036454927176237106
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",답변 실패,-0.010494585,0.0,0.010494585148990154
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",딥러닝,-0.008376003,0.0,0.008376003243029118
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",마지막 할 말,-0.032258015,0.0,0.032258015125989914
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",머신러닝,-0.028524755,0.0,0.02852475456893444
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",면접 시작 인사,0.004237981,0.0,0.0042379810474812984
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",상세 경험,-0.008750189,0.0,0.00875018909573555
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",수식,-0.0025909455,0.0,0.0025909454561769962
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",인공지능,0.006758101,0.0,0.0067581008188426495
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",잠시 휴식,0.0009122998,0.0,0.0009122997871600091
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",좋아하는 아이돌,-0.0046717576,0.0,0.004671757575124502
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",핵심 아이디어,-0.013637335,0.0,0.013637335039675236
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",확률 예측에서 MSE Loss 미 사용 이유,-0.0065096226,0.0,0.006509622558951378
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,BCE 가 좋은 task,-0.043445643,0.0,0.0434456430375576
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,"BCE 가 좋은 task, 이유",-0.033181377,0.0,0.03318137675523758
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,LLM Fine-Tuning 의 PEFT,-0.009174863,0.0,0.009174862876534462
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,LoRA,0.011462813,0.0,0.01146281324326992
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,LoRA 와 QLoRA 의 차이,-0.030798761,0.0,0.030798761174082756
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,Loss Function 예시,-0.0087659275,0.0,0.008765927515923977
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,Loss Function 정의,-0.0024130223,0.0,0.002413022331893444
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,MBTI,0.07844677,0.0,0.07844676822423935
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,MSE Loss 설명,0.011324721,0.0,0.01132472138851881
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,MSE Loss 용도,-0.01531679,0.0,0.015316789969801903
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,Multi-Label 에서 CE + Softmax 적용 문제점,0.0036062854,0.0,0.003606285434216261
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,PEFT 방법 5가지,0.28317958,0.0,0.2831795811653137
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,거대 언어 모델 정의,-0.02025695,0.0,0.020256949588656425
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,기본 경험,-0.015176742,0.0,0.0151767423376441
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,답변 실패,0.87196785,1.0,0.1280321478843689
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,딥러닝,-0.013726408,0.0,0.013726407662034035
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,마지막 할 말,-0.016119562,0.0,0.016119562089443207
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,머신러닝,-0.020497486,0.0,0.020497485995292664
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,면접 시작 인사,0.0076557244,0.0,0.0076557244174182415
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,상세 경험,-0.0012580508,0.0,0.0012580507900565863
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,수식,-0.030748982,0.0,0.03074898198246956
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,인공지능,-0.018713301,0.0,0.018713301047682762
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,잠시 휴식,0.012776702,0.0,0.012776701711118221
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,좋아하는 아이돌,0.008130973,0.0,0.00813097320497036
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,핵심 아이디어,-0.018708875,0.0,0.01870887540280819
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,확률 예측에서 MSE Loss 미 사용 이유,-0.030235644,0.0,0.030235644429922104
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,BCE 가 좋은 task,-0.0014982687,0.0,0.0014982686843723059
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,"BCE 가 좋은 task, 이유",-0.0100931255,0.0,0.010093125514686108
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,LLM Fine-Tuning 의 PEFT,0.010970684,0.0,0.01097068376839161
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,LoRA,0.9836362,1.0,0.016363799571990967
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,LoRA 와 QLoRA 의 차이,-0.0047216048,0.0,0.0047216047532856464
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,Loss Function 예시,0.0048889215,0.0,0.00488892151042819
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,Loss Function 정의,-0.032014813,0.0,0.032014813274145126
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,MBTI,-0.0019473513,0.0,0.0019473512656986713
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,MSE Loss 설명,-0.009633326,0.0,0.009633325971662998
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,MSE Loss 용도,-0.024830973,0.0,0.024830972775816917
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,Multi-Label 에서 CE + Softmax 적용 문제점,0.009089195,0.0,0.009089195169508457
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,PEFT 방법 5가지,-0.011296302,0.0,0.011296302080154419
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,거대 언어 모델 정의,-0.018693531,0.0,0.018693530932068825
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,기본 경험,0.0074173943,0.0,0.0074173943139612675
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,답변 실패,-0.0027910399,0.0,0.002791039878502488
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,딥러닝,-0.02428583,0.0,0.024285830557346344
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,마지막 할 말,-0.004786504,0.0,0.004786503966897726
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,머신러닝,-0.008613028,0.0,0.00861302763223648
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,면접 시작 인사,-0.011971082,0.0,0.011971081607043743
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,상세 경험,-0.009276431,0.0,0.009276431053876877
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,수식,0.004602897,0.0,0.004602896980941296
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,인공지능,-0.014191612,0.0,0.014191611669957638
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,잠시 휴식,-0.0037510262,0.0,0.0037510262336581945
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,좋아하는 아이돌,-0.01568891,0.0,0.015688909217715263
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,핵심 아이디어,-0.009857897,0.0,0.009857896715402603
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,확률 예측에서 MSE Loss 미 사용 이유,-0.0067717936,0.0,0.0067717935889959335
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,BCE 가 좋은 task,-0.009832547,0.0,0.009832547046244144
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,"BCE 가 좋은 task, 이유",-0.011748665,0.0,0.011748665012419224
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,LLM Fine-Tuning 의 PEFT,0.002991915,0.0,0.0029919149819761515
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,LoRA,0.033487193,0.0,0.033487193286418915
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,LoRA 와 QLoRA 의 차이,-0.0043664724,0.0,0.004366472363471985
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,Loss Function 예시,0.005029367,0.0,0.005029366817325354
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,Loss Function 정의,-0.0056252726,0.0,0.005625272635370493
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,MBTI,-0.0012274801,0.0,0.0012274801265448332
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,MSE Loss 설명,-0.007388704,0.0,0.007388703990727663
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,MSE Loss 용도,0.006200045,0.0,0.006200044881552458
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,Multi-Label 에서 CE + Softmax 적용 문제점,-0.0007939696,0.0,0.000793969607912004
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,PEFT 방법 5가지,-0.011867839,0.0,0.01186783891171217
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,거대 언어 모델 정의,-0.0034257406,0.0,0.003425740636885166
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,기본 경험,-0.009352917,0.0,0.009352916851639748
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,답변 실패,0.98306787,1.0,0.016932129859924316
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,딥러닝,-0.0020772214,0.0,0.002077221404761076
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,마지막 할 말,-0.001452446,0.0,0.0014524459838867188
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,머신러닝,-0.0063548456,0.0,0.006354845594614744
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,면접 시작 인사,-0.013232635,0.0,0.013232635334134102
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,상세 경험,0.0030621553,0.0,0.003062155330553651
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,수식,-0.011733246,0.0,0.01173324603587389
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,인공지능,-2.6543383e-05,0.0,2.654338277352508e-05
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,잠시 휴식,-0.003265421,0.0,0.0032654209062457085
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,좋아하는 아이돌,0.0039139064,0.0,0.003913906402885914
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,핵심 아이디어,0.013401498,0.0,0.013401498086750507
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,확률 예측에서 MSE Loss 미 사용 이유,0.003377061,0.0,0.003377061104401946
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,BCE 가 좋은 task,-0.0011601873,0.0,0.00116018729750067
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,"BCE 가 좋은 task, 이유",0.0244717,0.0,0.024471700191497803
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,LLM Fine-Tuning 의 PEFT,0.010946095,0.0,0.010946094989776611
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,LoRA,-0.022723367,0.0,0.0227233674377203
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,LoRA 와 QLoRA 의 차이,0.98798555,1.0,0.012014448642730713
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,Loss Function 예시,0.0025231831,0.0,0.002523183124139905
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,Loss Function 정의,-0.0009341967,0.0,0.0009341966942884028
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,MBTI,-0.00046883724,0.0,0.0004688372428063303
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,MSE Loss 설명,-0.0001633867,0.0,0.00016338670684490353
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,MSE Loss 용도,0.0038855104,0.0,0.003885510377585888
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,Multi-Label 에서 CE + Softmax 적용 문제점,0.0012333294,0.0,0.0012333294143900275
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,PEFT 방법 5가지,0.0011508465,0.0,0.0011508464813232422
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,거대 언어 모델 정의,0.021401204,0.0,0.02140120416879654
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,기본 경험,-0.0033822926,0.0,0.003382292576134205
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,답변 실패,-0.005959989,0.0,0.005959989037364721
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,딥러닝,0.030527221,0.0,0.03052722103893757
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,마지막 할 말,-0.0021832485,0.0,0.002183248521760106
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,머신러닝,-0.0009998708,0.0,0.0009998708264902234
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,면접 시작 인사,0.00084679615,0.0,0.0008467961451970041
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,상세 경험,2.8059452e-05,0.0,2.805945223371964e-05
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,수식,-0.015371128,0.0,0.015371127985417843
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,인공지능,0.0019703377,0.0,0.0019703377038240433
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,잠시 휴식,-0.0057350565,0.0,0.005735056474804878
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,좋아하는 아이돌,-0.008398933,0.0,0.008398933336138725
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,핵심 아이디어,-0.016009225,0.0,0.016009224578738213
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,확률 예측에서 MSE Loss 미 사용 이유,-0.011802557,0.0,0.011802556924521923
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,BCE 가 좋은 task,-0.00872857,0.0,0.008728570304811
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,"BCE 가 좋은 task, 이유",0.0020729117,0.0,0.002072911709547043
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,LLM Fine-Tuning 의 PEFT,0.007700402,0.0,0.007700401823967695
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,LoRA,0.009177903,0.0,0.009177902713418007
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,LoRA 와 QLoRA 의 차이,0.021430116,0.0,0.021430116146802902
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,Loss Function 예시,-0.0016086743,0.0,0.0016086742980405688
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,Loss Function 정의,-0.008475128,0.0,0.008475127629935741
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,MBTI,0.0008993265,0.0,0.0008993265219032764
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,MSE Loss 설명,-0.009582133,0.0,0.009582133032381535
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,MSE Loss 용도,0.00047685512,0.0,0.0004768551152665168
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,Multi-Label 에서 CE + Softmax 적용 문제점,-0.0090948595,0.0,0.009094859473407269
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,PEFT 방법 5가지,-0.0009301705,0.0,0.0009301705285906792
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,거대 언어 모델 정의,-0.01306918,0.0,0.013069179840385914
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,기본 경험,0.0017120887,0.0,0.001712088705971837
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,답변 실패,0.98792195,1.0,0.012078046798706055
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,딥러닝,-0.008345894,0.0,0.0083458935841918
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,마지막 할 말,-0.0023309218,0.0,0.00233092182315886
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,머신러닝,0.007257512,0.0,0.007257511839270592
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,면접 시작 인사,-0.011857869,0.0,0.011857869103550911
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,상세 경험,-0.0014387222,0.0,0.0014387222472578287
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,수식,-0.025890443,0.0,0.025890443474054337
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,인공지능,-0.003772124,0.0,0.0037721239496022463
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,잠시 휴식,-0.008086018,0.0,0.00808601826429367
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,좋아하는 아이돌,-0.0032522632,0.0,0.003252263180911541
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,핵심 아이디어,0.013592582,0.0,0.013592582195997238
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,확률 예측에서 MSE Loss 미 사용 이유,-0.006571372,0.0,0.006571372039616108
마지막 할 말 -> 로라야 정말 고마워!,BCE 가 좋은 task,-0.0023250803,0.0,0.0023250803351402283
마지막 할 말 -> 로라야 정말 고마워!,"BCE 가 좋은 task, 이유",-6.767666e-05,0.0,6.767665763618425e-05
마지막 할 말 -> 로라야 정말 고마워!,LLM Fine-Tuning 의 PEFT,-0.0020564562,0.0,0.002056456170976162
마지막 할 말 -> 로라야 정말 고마워!,LoRA,-0.0034539078,0.0,0.0034539077896624804
마지막 할 말 -> 로라야 정말 고마워!,LoRA 와 QLoRA 의 차이,-0.0019995638,0.0,0.0019995637703686953
마지막 할 말 -> 로라야 정말 고마워!,Loss Function 예시,0.010655662,0.0,0.010655662044882774
마지막 할 말 -> 로라야 정말 고마워!,Loss Function 정의,0.009457802,0.0,0.009457802399992943
마지막 할 말 -> 로라야 정말 고마워!,MBTI,-0.009896124,0.0,0.00989612378180027
마지막 할 말 -> 로라야 정말 고마워!,MSE Loss 설명,-0.010753181,0.0,0.010753180831670761
마지막 할 말 -> 로라야 정말 고마워!,MSE Loss 용도,-0.00069968234,0.0,0.0006996823358349502
마지막 할 말 -> 로라야 정말 고마워!,Multi-Label 에서 CE + Softmax 적용 문제점,-0.015937323,0.0,0.0159373227506876
마지막 할 말 -> 로라야 정말 고마워!,PEFT 방법 5가지,-0.025602702,0.0,0.02560270205140114
마지막 할 말 -> 로라야 정말 고마워!,거대 언어 모델 정의,-0.00058447366,0.0,0.0005844736588187516
마지막 할 말 -> 로라야 정말 고마워!,기본 경험,-0.01681573,0.0,0.016815729439258575
마지막 할 말 -> 로라야 정말 고마워!,답변 실패,-0.0023410707,0.0,0.0023410706780850887
마지막 할 말 -> 로라야 정말 고마워!,딥러닝,0.017029522,0.0,0.017029521986842155
마지막 할 말 -> 로라야 정말 고마워!,마지막 할 말,0.9925688,1.0,0.007431209087371826
마지막 할 말 -> 로라야 정말 고마워!,머신러닝,-0.027950801,0.0,0.027950800955295563
마지막 할 말 -> 로라야 정말 고마워!,면접 시작 인사,-0.0005962268,0.0,0.0005962267750874162
마지막 할 말 -> 로라야 정말 고마워!,상세 경험,-0.02477978,0.0,0.024779779836535454
마지막 할 말 -> 로라야 정말 고마워!,수식,0.004599256,0.0,0.004599255975335836
마지막 할 말 -> 로라야 정말 고마워!,인공지능,-0.0022352668,0.0,0.0022352668456733227
마지막 할 말 -> 로라야 정말 고마워!,잠시 휴식,-0.016366525,0.0,0.01636652462184429
마지막 할 말 -> 로라야 정말 고마워!,좋아하는 아이돌,-0.0029303478,0.0,0.0029303478077054024
마지막 할 말 -> 로라야 정말 고마워!,핵심 아이디어,-0.012532947,0.0,0.012532946653664112
마지막 할 말 -> 로라야 정말 고마워!,확률 예측에서 MSE Loss 미 사용 이유,-0.00077171216,0.0,0.0007717121625319123
마지막 할 말 -> 로라야 사랑해,BCE 가 좋은 task,-0.0075708316,0.0,0.007570831570774317
마지막 할 말 -> 로라야 사랑해,"BCE 가 좋은 task, 이유",0.0023839679,0.0,0.002383967861533165
마지막 할 말 -> 로라야 사랑해,LLM Fine-Tuning 의 PEFT,-0.004263099,0.0,0.004263098817318678
마지막 할 말 -> 로라야 사랑해,LoRA,-0.0054860543,0.0,0.0054860543459653854
마지막 할 말 -> 로라야 사랑해,LoRA 와 QLoRA 의 차이,0.0009972389,0.0,0.00099723890889436
마지막 할 말 -> 로라야 사랑해,Loss Function 예시,0.009479029,0.0,0.009479029104113579
마지막 할 말 -> 로라야 사랑해,Loss Function 정의,0.008691733,0.0,0.00869173277169466
마지막 할 말 -> 로라야 사랑해,MBTI,-0.008992386,0.0,0.008992386050522327
마지막 할 말 -> 로라야 사랑해,MSE Loss 설명,-0.012851914,0.0,0.012851914390921593
마지막 할 말 -> 로라야 사랑해,MSE Loss 용도,0.0010576877,0.0,0.0010576876811683178
마지막 할 말 -> 로라야 사랑해,Multi-Label 에서 CE + Softmax 적용 문제점,-0.017024584,0.0,0.017024584114551544
마지막 할 말 -> 로라야 사랑해,PEFT 방법 5가지,-0.030606743,0.0,0.030606742948293686
마지막 할 말 -> 로라야 사랑해,거대 언어 모델 정의,-0.0017080888,0.0,0.0017080887919291854
마지막 할 말 -> 로라야 사랑해,기본 경험,-0.013778004,0.0,0.013778003863990307
마지막 할 말 -> 로라야 사랑해,답변 실패,-0.00046866896,0.0,0.0004686689644586295
마지막 할 말 -> 로라야 사랑해,딥러닝,0.017738027,0.0,0.01773802749812603
마지막 할 말 -> 로라야 사랑해,마지막 할 말,0.9927038,1.0,0.007296204566955566
마지막 할 말 -> 로라야 사랑해,머신러닝,-0.031130187,0.0,0.031130187213420868
마지막 할 말 -> 로라야 사랑해,면접 시작 인사,-0.005666218,0.0,0.0056662182323634624
마지막 할 말 -> 로라야 사랑해,상세 경험,-0.028163927,0.0,0.028163926675915718
마지막 할 말 -> 로라야 사랑해,수식,0.006379795,0.0,0.006379794795066118
마지막 할 말 -> 로라야 사랑해,인공지능,-0.0028708826,0.0,0.0028708826284855604
마지막 할 말 -> 로라야 사랑해,잠시 휴식,-0.010471495,0.0,0.010471494868397713
마지막 할 말 -> 로라야 사랑해,좋아하는 아이돌,0.009434037,0.0,0.009434036910533905
마지막 할 말 -> 로라야 사랑해,핵심 아이디어,-0.01348126,0.0,0.013481260277330875
마지막 할 말 -> 로라야 사랑해,확률 예측에서 MSE Loss 미 사용 이유,-0.0028573591,0.0,0.0028573591262102127
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,BCE 가 좋은 task,-0.003908164,0.0,0.003908163867890835
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,"BCE 가 좋은 task, 이유",0.0043760007,0.0,0.004376000724732876
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,LLM Fine-Tuning 의 PEFT,-0.0031018215,0.0,0.0031018215231597424
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,LoRA,-0.007213807,0.0,0.007213807199150324
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,LoRA 와 QLoRA 의 차이,0.00010426073,0.0,0.0001042607327690348
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,Loss Function 예시,0.010234219,0.0,0.010234219022095203
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,Loss Function 정의,0.008072337,0.0,0.00807233713567257
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,MBTI,-0.008170687,0.0,0.008170686662197113
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,MSE Loss 설명,-0.0068677096,0.0,0.006867709569633007
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,MSE Loss 용도,-0.0019659046,0.0,0.0019659046083688736
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,Multi-Label 에서 CE + Softmax 적용 문제점,-0.01655584,0.0,0.016555840149521828
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,PEFT 방법 5가지,-0.031240499,0.0,0.031240498647093773
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,거대 언어 모델 정의,-0.0025390212,0.0,0.002539021195843816
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,기본 경험,-0.017547566,0.0,0.017547566443681717
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,답변 실패,-0.0029292468,0.0,0.0029292467515915632
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,딥러닝,0.013667195,0.0,0.013667195104062557
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,마지막 할 말,0.9905873,1.0,0.009412705898284912
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,머신러닝,-0.02771717,0.0,0.027717169374227524
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,면접 시작 인사,-0.0074004848,0.0,0.0074004847556352615
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,상세 경험,-0.027569426,0.0,0.027569426223635674
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,수식,0.0013391337,0.0,0.0013391337124630809
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,인공지능,-0.0017892154,0.0,0.0017892153700813651
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,잠시 휴식,-0.009597,0.0,0.009596999734640121
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,좋아하는 아이돌,9.5899595e-05,0.0,9.589959518052638e-05
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,핵심 아이디어,-0.011211785,0.0,0.011211784556508064
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,확률 예측에서 MSE Loss 미 사용 이유,-0.0030794344,0.0,0.0030794343911111355
