input_part,output_answer,predicted_score,ground_truth_score,absolute_error
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,BCE 가 좋은 task,0.000851235,0.0,0.0008512350032106042
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,BCE 가 좋은 이유,0.0065909126,0.0,0.006590912584215403
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,LLM Fine-Tuning 의 PEFT,0.0013505954,0.0,0.0013505953829735518
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,LoRA,0.0050635166,0.0,0.005063516553491354
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,LoRA 와 QLoRA 의 차이,-0.0047827344,0.0,0.00478273443877697
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,Loss Function 예시,0.030897925,0.0,0.030897924676537514
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,Loss Function 정의,0.007879086,0.0,0.007879085838794708
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,MBTI,0.001641739,0.0,0.0016417390434071422
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,MSE Loss 설명,0.011067958,0.0,0.011067957617342472
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,MSE Loss 용도,0.023663474,0.0,0.02366347424685955
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,Multi-Label 에서 CE + Softmax 적용 문제점,0.01657126,0.0,0.01657125912606716
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,PEFT 방법 5가지,-0.008727529,0.0,0.00872752908617258
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,거대 언어 모델 정의,-0.004632004,0.0,0.0046320040710270405
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,기본 경험,0.015943907,0.0,0.01594390720129013
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,답변 실패,-0.019682515,0.0,0.019682515412569046
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,딥러닝,0.0063282046,0.0,0.006328204646706581
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,마지막 할 말,0.0015834784,0.0,0.0015834784135222435
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,머신러닝,-0.0031609116,0.0,0.003160911612212658
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,면접 시작 인사,0.9716714,1.0,0.02832859754562378
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,상세 경험,-0.0043478394,0.0,0.004347839392721653
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,수식,0.010054633,0.0,0.010054633021354675
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,용어 질문,0.005665315,0.0,0.0056653148494660854
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,인공지능,-0.020498024,0.0,0.02049802429974079
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,잠시 휴식,-0.02739355,0.0,0.02739354968070984
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,좋아하는 아이돌,-0.0049047545,0.0,0.004904754459857941
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,핵심 아이디어,-0.012382781,0.0,0.012382781133055687
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,확률 예측에서 MSE Loss 미 사용 이유,0.0043713334,0.0,0.00437133340165019
면접 시작 인사 -> 로라야 안녕 정말 반가워,BCE 가 좋은 task,0.004781883,0.0,0.004781883209943771
면접 시작 인사 -> 로라야 안녕 정말 반가워,BCE 가 좋은 이유,0.0025198413,0.0,0.002519841305911541
면접 시작 인사 -> 로라야 안녕 정말 반가워,LLM Fine-Tuning 의 PEFT,-0.010855946,0.0,0.010855945758521557
면접 시작 인사 -> 로라야 안녕 정말 반가워,LoRA,0.031443674,0.0,0.031443674117326736
면접 시작 인사 -> 로라야 안녕 정말 반가워,LoRA 와 QLoRA 의 차이,-0.0033438173,0.0,0.003343817312270403
면접 시작 인사 -> 로라야 안녕 정말 반가워,Loss Function 예시,0.017060649,0.0,0.017060648649930954
면접 시작 인사 -> 로라야 안녕 정말 반가워,Loss Function 정의,0.0070262845,0.0,0.007026284467428923
면접 시작 인사 -> 로라야 안녕 정말 반가워,MBTI,0.0015717512,0.0,0.0015717511996626854
면접 시작 인사 -> 로라야 안녕 정말 반가워,MSE Loss 설명,0.00916413,0.0,0.009164130315184593
면접 시작 인사 -> 로라야 안녕 정말 반가워,MSE Loss 용도,0.01982567,0.0,0.01982567086815834
면접 시작 인사 -> 로라야 안녕 정말 반가워,Multi-Label 에서 CE + Softmax 적용 문제점,0.008374333,0.0,0.008374333381652832
면접 시작 인사 -> 로라야 안녕 정말 반가워,PEFT 방법 5가지,-0.023125583,0.0,0.023125583305954933
면접 시작 인사 -> 로라야 안녕 정말 반가워,거대 언어 모델 정의,0.0122615555,0.0,0.012261555530130863
면접 시작 인사 -> 로라야 안녕 정말 반가워,기본 경험,0.007165976,0.0,0.007165975868701935
면접 시작 인사 -> 로라야 안녕 정말 반가워,답변 실패,-0.017277839,0.0,0.01727783866226673
면접 시작 인사 -> 로라야 안녕 정말 반가워,딥러닝,0.0043276083,0.0,0.004327608272433281
면접 시작 인사 -> 로라야 안녕 정말 반가워,마지막 할 말,-0.0038679927,0.0,0.003867992665618658
면접 시작 인사 -> 로라야 안녕 정말 반가워,머신러닝,0.009707677,0.0,0.00970767717808485
면접 시작 인사 -> 로라야 안녕 정말 반가워,면접 시작 인사,0.97531015,1.0,0.024689853191375732
면접 시작 인사 -> 로라야 안녕 정말 반가워,상세 경험,-0.00048564764,0.0,0.00048564764438197017
면접 시작 인사 -> 로라야 안녕 정말 반가워,수식,-0.006132718,0.0,0.006132718175649643
면접 시작 인사 -> 로라야 안녕 정말 반가워,용어 질문,0.0031404516,0.0,0.003140451619401574
면접 시작 인사 -> 로라야 안녕 정말 반가워,인공지능,-0.019499157,0.0,0.01949915662407875
면접 시작 인사 -> 로라야 안녕 정말 반가워,잠시 휴식,-0.047637112,0.0,0.04763711243867874
면접 시작 인사 -> 로라야 안녕 정말 반가워,좋아하는 아이돌,-0.009447033,0.0,0.00944703258574009
면접 시작 인사 -> 로라야 안녕 정말 반가워,핵심 아이디어,-0.014819404,0.0,0.014819404110312462
면접 시작 인사 -> 로라야 안녕 정말 반가워,확률 예측에서 MSE Loss 미 사용 이유,0.00223673,0.0,0.0022367299534380436
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,BCE 가 좋은 task,-0.0034648562,0.0,0.0034648561850190163
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,BCE 가 좋은 이유,0.000977963,0.0,0.000977962976321578
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,LLM Fine-Tuning 의 PEFT,-0.006564947,0.0,0.006564946845173836
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,LoRA,0.017786128,0.0,0.01778612844645977
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,LoRA 와 QLoRA 의 차이,0.0018203908,0.0,0.0018203908111900091
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,Loss Function 예시,0.021550816,0.0,0.021550815552473068
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,Loss Function 정의,0.0015789755,0.0,0.0015789754688739777
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,MBTI,-0.00836585,0.0,0.00836584996432066
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,MSE Loss 설명,0.009847614,0.0,0.009847613982856274
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,MSE Loss 용도,0.020037817,0.0,0.02003781683743
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,Multi-Label 에서 CE + Softmax 적용 문제점,0.01395218,0.0,0.013952179811894894
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,PEFT 방법 5가지,-0.024202324,0.0,0.02420232445001602
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,거대 언어 모델 정의,-0.005318194,0.0,0.0053181941621005535
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,기본 경험,0.009095673,0.0,0.009095673449337482
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,답변 실패,0.0054828743,0.0,0.005482874345034361
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,딥러닝,0.00096602686,0.0,0.0009660268551670015
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,마지막 할 말,-0.004913183,0.0,0.004913182929158211
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,머신러닝,0.0016167201,0.0,0.0016167201101779938
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,면접 시작 인사,0.9686409,1.0,0.031359076499938965
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,상세 경험,-0.0074681174,0.0,0.007468117401003838
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,수식,0.0020537614,0.0,0.002053761389106512
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,용어 질문,0.018127665,0.0,0.018127664923667908
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,인공지능,-0.028016886,0.0,0.028016885742545128
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,잠시 휴식,-0.046997,0.0,0.04699699953198433
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,좋아하는 아이돌,-0.007810536,0.0,0.007810535840690136
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,핵심 아이디어,-0.0108453445,0.0,0.010845344513654709
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,확률 예측에서 MSE Loss 미 사용 이유,0.0029665867,0.0,0.002966586733236909
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,BCE 가 좋은 task,0.010377993,0.0,0.010377992875874043
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,BCE 가 좋은 이유,-0.0020023494,0.0,0.00200234935618937
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,LLM Fine-Tuning 의 PEFT,0.011379299,0.0,0.011379298754036427
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,LoRA,0.036912654,0.0,0.03691265359520912
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,LoRA 와 QLoRA 의 차이,-0.00028276065,0.0,0.00028276065131649375
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,Loss Function 예시,0.012831689,0.0,0.012831688858568668
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,Loss Function 정의,-0.003524932,0.0,0.0035249320790171623
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,MBTI,-0.015116918,0.0,0.0151169179007411
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,MSE Loss 설명,0.011062641,0.0,0.011062640696763992
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,MSE Loss 용도,0.034261055,0.0,0.034261055290699005
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,Multi-Label 에서 CE + Softmax 적용 문제점,0.004868066,0.0,0.0048680659383535385
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,PEFT 방법 5가지,-0.026738537,0.0,0.026738537475466728
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,거대 언어 모델 정의,-0.005603773,0.0,0.005603773053735495
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,기본 경험,-0.004330498,0.0,0.004330498166382313
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,답변 실패,-0.030802753,0.0,0.030802752822637558
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,딥러닝,0.011974084,0.0,0.011974084191024303
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,마지막 할 말,0.019196127,0.0,0.019196126610040665
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,머신러닝,0.0042388164,0.0,0.0042388164438307285
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,면접 시작 인사,0.9598206,1.0,0.04017937183380127
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,상세 경험,-0.008090314,0.0,0.008090314455330372
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,수식,0.008172673,0.0,0.008172673173248768
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,용어 질문,0.0052062757,0.0,0.0052062757313251495
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,인공지능,-0.026775964,0.0,0.026775963604450226
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,잠시 휴식,-0.04155891,0.0,0.04155891016125679
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,좋아하는 아이돌,-0.01156299,0.0,0.01156299002468586
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,핵심 아이디어,-0.006358034,0.0,0.00635803397744894
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,확률 예측에서 MSE Loss 미 사용 이유,0.0073013613,0.0,0.007301361300051212
면접 시작 인사 -> 파이팅! 시작하자,BCE 가 좋은 task,-0.0046633016,0.0,0.004663301631808281
면접 시작 인사 -> 파이팅! 시작하자,BCE 가 좋은 이유,0.008510517,0.0,0.008510516956448555
면접 시작 인사 -> 파이팅! 시작하자,LLM Fine-Tuning 의 PEFT,-0.009061147,0.0,0.009061147458851337
면접 시작 인사 -> 파이팅! 시작하자,LoRA,0.013356552,0.0,0.013356552459299564
면접 시작 인사 -> 파이팅! 시작하자,LoRA 와 QLoRA 의 차이,-0.004091986,0.0,0.004091985989362001
면접 시작 인사 -> 파이팅! 시작하자,Loss Function 예시,0.033635695,0.0,0.0336356945335865
면접 시작 인사 -> 파이팅! 시작하자,Loss Function 정의,0.0071206377,0.0,0.007120637688785791
면접 시작 인사 -> 파이팅! 시작하자,MBTI,0.0077207102,0.0,0.00772071024402976
면접 시작 인사 -> 파이팅! 시작하자,MSE Loss 설명,-0.0020774105,0.0,0.002077410463243723
면접 시작 인사 -> 파이팅! 시작하자,MSE Loss 용도,0.030088892,0.0,0.030088892206549644
면접 시작 인사 -> 파이팅! 시작하자,Multi-Label 에서 CE + Softmax 적용 문제점,0.011639928,0.0,0.011639928445219994
면접 시작 인사 -> 파이팅! 시작하자,PEFT 방법 5가지,-0.010306874,0.0,0.01030687429010868
면접 시작 인사 -> 파이팅! 시작하자,거대 언어 모델 정의,0.0059478693,0.0,0.005947869271039963
면접 시작 인사 -> 파이팅! 시작하자,기본 경험,0.02142761,0.0,0.021427610889077187
면접 시작 인사 -> 파이팅! 시작하자,답변 실패,-0.030532543,0.0,0.03053254261612892
면접 시작 인사 -> 파이팅! 시작하자,딥러닝,0.0036802178,0.0,0.0036802177783101797
면접 시작 인사 -> 파이팅! 시작하자,마지막 할 말,-0.0016074228,0.0,0.0016074228333309293
면접 시작 인사 -> 파이팅! 시작하자,머신러닝,0.006609099,0.0,0.006609098985791206
면접 시작 인사 -> 파이팅! 시작하자,면접 시작 인사,0.9776471,1.0,0.022352874279022217
면접 시작 인사 -> 파이팅! 시작하자,상세 경험,-0.0074763685,0.0,0.007476368453353643
면접 시작 인사 -> 파이팅! 시작하자,수식,0.0020087734,0.0,0.002008773386478424
면접 시작 인사 -> 파이팅! 시작하자,용어 질문,-0.0012055835,0.0,0.0012055834522470832
면접 시작 인사 -> 파이팅! 시작하자,인공지능,-0.0304051,0.0,0.03040510043501854
면접 시작 인사 -> 파이팅! 시작하자,잠시 휴식,-0.03875374,0.0,0.03875374048948288
면접 시작 인사 -> 파이팅! 시작하자,좋아하는 아이돌,-0.0059669814,0.0,0.005966981407254934
면접 시작 인사 -> 파이팅! 시작하자,핵심 아이디어,0.015065648,0.0,0.015065647661685944
면접 시작 인사 -> 파이팅! 시작하자,확률 예측에서 MSE Loss 미 사용 이유,0.0048968815,0.0,0.004896881524473429
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",BCE 가 좋은 task,0.0032869412,0.0,0.003286941209807992
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",BCE 가 좋은 이유,0.023078978,0.0,0.023078978061676025
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",LLM Fine-Tuning 의 PEFT,-0.012592455,0.0,0.012592455372214317
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",LoRA,0.021725798,0.0,0.021725798025727272
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",LoRA 와 QLoRA 의 차이,-0.014728709,0.0,0.014728709124028683
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",Loss Function 예시,-0.01708108,0.0,0.01708108000457287
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",Loss Function 정의,-0.014941992,0.0,0.014941992238163948
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",MBTI,-0.000874247,0.0,0.0008742469944991171
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",MSE Loss 설명,0.009944438,0.0,0.009944438003003597
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",MSE Loss 용도,-0.006363709,0.0,0.00636370899155736
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",Multi-Label 에서 CE + Softmax 적용 문제점,-0.024735432,0.0,0.024735432118177414
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",PEFT 방법 5가지,0.011816705,0.0,0.011816704645752907
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",거대 언어 모델 정의,-0.004969347,0.0,0.0049693468026816845
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",기본 경험,-0.026188398,0.0,0.026188397780060768
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",답변 실패,0.96708685,1.0,0.032913148403167725
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",딥러닝,0.012886991,0.0,0.012886990793049335
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",마지막 할 말,-0.007999249,0.0,0.007999248802661896
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",머신러닝,0.0026798411,0.0,0.0026798411272466183
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",면접 시작 인사,-0.02412235,0.0,0.02412234991788864
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",상세 경험,-0.0032057818,0.0,0.003205781802535057
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",수식,-0.017595688,0.0,0.0175956878811121
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",용어 질문,-0.0063563,0.0,0.006356299854815006
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",인공지능,0.022835204,0.0,0.022835204377770424
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",잠시 휴식,-0.013614437,0.0,0.013614436611533165
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",좋아하는 아이돌,0.018799957,0.0,0.018799956887960434
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",핵심 아이디어,-0.004820055,0.0,0.004820054862648249
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",확률 예측에서 MSE Loss 미 사용 이유,-0.006560483,0.0,0.006560483016073704
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",BCE 가 좋은 task,0.009236837,0.0,0.009236836805939674
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",BCE 가 좋은 이유,-0.021572936,0.0,0.021572936326265335
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",LLM Fine-Tuning 의 PEFT,0.015778909,0.0,0.015778908506035805
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",LoRA,0.030743463,0.0,0.030743462964892387
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",LoRA 와 QLoRA 의 차이,0.00066236494,0.0,0.000662364938762039
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",Loss Function 예시,0.003633814,0.0,0.0036338139325380325
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",Loss Function 정의,-0.051633824,0.0,0.05163382366299629
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",MBTI,-0.010966219,0.0,0.010966219007968903
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",MSE Loss 설명,0.019805064,0.0,0.0198050644248724
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",MSE Loss 용도,-0.00012268555,0.0,0.00012268555292394012
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",Multi-Label 에서 CE + Softmax 적용 문제점,-0.013371135,0.0,0.013371135108172894
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",PEFT 방법 5가지,-0.00178727,0.0,0.0017872699536383152
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",거대 언어 모델 정의,0.017116804,0.0,0.01711680367588997
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",기본 경험,-0.017551072,0.0,0.01755107194185257
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",답변 실패,0.013945452,0.0,0.013945451937615871
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",딥러닝,-0.0022079137,0.0,0.0022079136688262224
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",마지막 할 말,0.013346552,0.0,0.013346551917493343
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",머신러닝,-0.0804192,0.0,0.08041919767856598
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",면접 시작 인사,-0.013456524,0.0,0.013456524349749088
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",상세 경험,0.028942384,0.0,0.02894238382577896
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",수식,-0.0042375405,0.0,0.004237540531903505
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",용어 질문,0.04316064,0.0,0.04316063970327377
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",인공지능,0.9087105,1.0,0.09128952026367188
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",잠시 휴식,0.0013916398,0.0,0.0013916398165747523
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",좋아하는 아이돌,0.0058574174,0.0,0.005857417359948158
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",핵심 아이디어,0.019836543,0.0,0.0198365431278944
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",확률 예측에서 MSE Loss 미 사용 이유,-0.0131596085,0.0,0.013159608468413353
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",BCE 가 좋은 task,-0.010476103,0.0,0.01047610305249691
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",BCE 가 좋은 이유,-0.039462756,0.0,0.03946275636553764
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",LLM Fine-Tuning 의 PEFT,-0.039478477,0.0,0.03947847709059715
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",LoRA,-0.0222616,0.0,0.022261599078774452
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",LoRA 와 QLoRA 의 차이,-0.0043477737,0.0,0.004347773734480143
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",Loss Function 예시,0.030655663,0.0,0.030655663460493088
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",Loss Function 정의,-0.029131282,0.0,0.02913128212094307
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",MBTI,0.02059999,0.0,0.020599989220499992
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",MSE Loss 설명,-0.02553252,0.0,0.025532519444823265
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",MSE Loss 용도,0.003926214,0.0,0.003926213830709457
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",Multi-Label 에서 CE + Softmax 적용 문제점,-0.046976577,0.0,0.04697657749056816
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",PEFT 방법 5가지,0.029542629,0.0,0.029542628675699234
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",거대 언어 모델 정의,0.018767796,0.0,0.018767796456813812
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",기본 경험,-0.039053258,0.0,0.039053257554769516
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",답변 실패,-0.00519221,0.0,0.005192209966480732
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",딥러닝,-0.009246468,0.0,0.009246467612683773
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",마지막 할 말,-0.013824397,0.0,0.013824396766722202
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",머신러닝,0.8559792,1.0,0.14402079582214355
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",면접 시작 인사,0.0012958321,0.0,0.0012958321021869779
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",상세 경험,-0.043742586,0.0,0.043742585927248
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",수식,0.003599029,0.0,0.0035990290343761444
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",용어 질문,-0.02972493,0.0,0.029724929481744766
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",인공지능,-0.037252944,0.0,0.037252943962812424
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",잠시 휴식,-0.000431905,0.0,0.0004319050058256835
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",좋아하는 아이돌,-0.017900065,0.0,0.01790006458759308
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",핵심 아이디어,0.0076319957,0.0,0.007631995715200901
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",확률 예측에서 MSE Loss 미 사용 이유,0.0047131693,0.0,0.004713169299066067
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",BCE 가 좋은 task,0.0027228785,0.0,0.002722878474742174
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",BCE 가 좋은 이유,-0.0015830579,0.0,0.0015830579213798046
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",LLM Fine-Tuning 의 PEFT,0.037447944,0.0,0.03744794428348541
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",LoRA,0.014032225,0.0,0.014032225124537945
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",LoRA 와 QLoRA 의 차이,-0.013847154,0.0,0.013847153633832932
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",Loss Function 예시,0.006793154,0.0,0.006793153937906027
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",Loss Function 정의,-0.074115574,0.0,0.0741155743598938
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",MBTI,-0.004721693,0.0,0.004721693228930235
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",MSE Loss 설명,0.05165519,0.0,0.05165518820285797
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",MSE Loss 용도,0.020041494,0.0,0.020041493698954582
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",Multi-Label 에서 CE + Softmax 적용 문제점,0.0050828713,0.0,0.005082871299237013
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",PEFT 방법 5가지,-0.022353785,0.0,0.02235378511250019
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",거대 언어 모델 정의,0.024678648,0.0,0.02467864751815796
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",기본 경험,0.04602701,0.0,0.046027008444070816
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",답변 실패,-0.011980839,0.0,0.01198083907365799
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",딥러닝,0.899685,1.0,0.10031497478485107
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",마지막 할 말,0.0019010721,0.0,0.0019010721007362008
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",머신러닝,0.009965222,0.0,0.009965222328901291
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",면접 시작 인사,-0.0021866849,0.0,0.0021866848692297935
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",상세 경험,-0.008630023,0.0,0.008630023337900639
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",수식,0.024065383,0.0,0.024065382778644562
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",용어 질문,0.03047122,0.0,0.03047122061252594
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",인공지능,-0.052558113,0.0,0.052558112889528275
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",잠시 휴식,-0.017947815,0.0,0.017947815358638763
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",좋아하는 아이돌,-0.018428236,0.0,0.01842823624610901
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",핵심 아이디어,-0.0249017,0.0,0.024901699274778366
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",확률 예측에서 MSE Loss 미 사용 이유,-0.010789186,0.0,0.010789185762405396
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",BCE 가 좋은 task,0.002385634,0.0,0.002385633997619152
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",BCE 가 좋은 이유,-0.012877966,0.0,0.012877966277301311
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",LLM Fine-Tuning 의 PEFT,0.024145156,0.0,0.024145156145095825
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",LoRA,0.0041895243,0.0,0.004189524333924055
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",LoRA 와 QLoRA 의 차이,0.0060290615,0.0,0.006029061507433653
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",Loss Function 예시,0.01107528,0.0,0.011075279675424099
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",Loss Function 정의,-0.058929134,0.0,0.05892913416028023
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",MBTI,0.0025797288,0.0,0.002579728839918971
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",MSE Loss 설명,0.022823382,0.0,0.022823382169008255
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",MSE Loss 용도,0.013845596,0.0,0.013845596462488174
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",Multi-Label 에서 CE + Softmax 적용 문제점,-0.0034443378,0.0,0.0034443377517163754
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",PEFT 방법 5가지,-0.021438245,0.0,0.021438244730234146
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",거대 언어 모델 정의,0.02637617,0.0,0.02637616917490959
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",기본 경험,0.061603535,0.0,0.06160353496670723
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",답변 실패,0.06975853,0.0,0.06975852698087692
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",딥러닝,0.89150923,1.0,0.10849076509475708
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",마지막 할 말,-0.004313411,0.0,0.004313411191105843
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",머신러닝,0.033636138,0.0,0.03363613784313202
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",면접 시작 인사,0.002333849,0.0,0.0023338489700108767
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",상세 경험,-0.039720327,0.0,0.0397203266620636
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",수식,0.013175328,0.0,0.013175328262150288
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",용어 질문,0.048696328,0.0,0.048696327954530716
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",인공지능,-0.024777869,0.0,0.024777868762612343
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",잠시 휴식,-0.02339414,0.0,0.023394139483571053
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",좋아하는 아이돌,-0.0066127945,0.0,0.006612794473767281
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",핵심 아이디어,-0.036743924,0.0,0.036743924021720886
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",확률 예측에서 MSE Loss 미 사용 이유,-0.028248439,0.0,0.028248438611626625
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",BCE 가 좋은 task,-0.020171577,0.0,0.020171577110886574
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",BCE 가 좋은 이유,-0.047861997,0.0,0.04786199703812599
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",LLM Fine-Tuning 의 PEFT,-0.025407244,0.0,0.02540724352002144
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",LoRA,-0.024159115,0.0,0.024159114807844162
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",LoRA 와 QLoRA 의 차이,-0.008872289,0.0,0.008872289210557938
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",Loss Function 예시,0.04263224,0.0,0.04263224080204964
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",Loss Function 정의,-0.022822926,0.0,0.022822925820946693
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",MBTI,0.02693131,0.0,0.026931310072541237
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",MSE Loss 설명,-0.020118056,0.0,0.02011805586516857
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",MSE Loss 용도,-0.009105221,0.0,0.00910522136837244
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",Multi-Label 에서 CE + Softmax 적용 문제점,-0.047400605,0.0,0.04740060493350029
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",PEFT 방법 5가지,0.026360627,0.0,0.02636062726378441
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",거대 언어 모델 정의,-0.015263543,0.0,0.015263543464243412
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",기본 경험,-0.042782605,0.0,0.042782604694366455
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",답변 실패,0.06905453,1.0,0.9309454709291458
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",딥러닝,-0.0047282013,0.0,0.004728201311081648
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",마지막 할 말,-0.0034999263,0.0,0.0034999263007193804
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",머신러닝,0.8044745,0.0,0.8044744729995728
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",면접 시작 인사,-0.006546767,0.0,0.006546766962856054
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",상세 경험,-0.045030583,0.0,0.04503058269619942
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",수식,0.012907962,0.0,0.012907962314784527
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",용어 질문,-0.012275136,0.0,0.012275136075913906
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",인공지능,0.0249591,0.0,0.024959100410342216
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",잠시 휴식,0.028179614,0.0,0.02817961387336254
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",좋아하는 아이돌,-0.016372152,0.0,0.01637215167284012
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",핵심 아이디어,-0.00050488155,0.0,0.0005048815510235727
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",확률 예측에서 MSE Loss 미 사용 이유,-0.0013645373,0.0,0.0013645372819155455
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아",BCE 가 좋은 task,-0.012700534,0.0,0.012700534425675869
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아",BCE 가 좋은 이유,0.011543013,0.0,0.011543013155460358
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아",LLM Fine-Tuning 의 PEFT,-0.015031514,0.0,0.015031513758003712
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아",LoRA,0.007641546,0.0,0.0076415459625422955
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아",LoRA 와 QLoRA 의 차이,0.002337304,0.0,0.0023373039439320564
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아",Loss Function 예시,-0.004528149,0.0,0.004528149031102657
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아",Loss Function 정의,-0.020239078,0.0,0.020239077508449554
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아",MBTI,-0.0025433942,0.0,0.0025433942209929228
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아",MSE Loss 설명,0.0125399055,0.0,0.012539905495941639
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아",MSE Loss 용도,-0.016353989,0.0,0.016353989019989967
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아",Multi-Label 에서 CE + Softmax 적용 문제점,-0.019622957,0.0,0.019622957333922386
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아",PEFT 방법 5가지,0.0008918916,0.0,0.0008918915991671383
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아",거대 언어 모델 정의,-0.006676361,0.0,0.0066763609647750854
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아",기본 경험,-0.012206088,0.0,0.012206087820231915
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아",답변 실패,0.979037,1.0,0.020963013172149658
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아",딥러닝,0.0116822645,0.0,0.011682264506816864
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아",마지막 할 말,-0.0123231225,0.0,0.012323122471570969
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아",머신러닝,0.0020662488,0.0,0.0020662487950176
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아",면접 시작 인사,-0.011539646,0.0,0.011539646424353123
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아",상세 경험,-0.0072551873,0.0,0.0072551872581243515
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아",수식,-0.02853196,0.0,0.028531959280371666
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아",용어 질문,0.0016219137,0.0,0.0016219137469306588
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아",인공지능,0.015700402,0.0,0.01570040173828602
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아",잠시 휴식,-0.0037999875,0.0,0.003799987491220236
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아",좋아하는 아이돌,0.004724088,0.0,0.004724088124930859
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아",핵심 아이디어,0.00054849737,0.0,0.0005484973662532866
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아",확률 예측에서 MSE Loss 미 사용 이유,-0.0025275487,0.0,0.002527548698708415
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지?",BCE 가 좋은 task,0.0030507415,0.0,0.003050741506740451
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지?",BCE 가 좋은 이유,-0.0128770135,0.0,0.01287701353430748
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지?",LLM Fine-Tuning 의 PEFT,0.016128426,0.0,0.016128426417708397
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지?",LoRA,0.026789416,0.0,0.026789415627717972
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지?",LoRA 와 QLoRA 의 차이,0.00651071,0.0,0.0065107098780572414
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지?",Loss Function 예시,0.014899807,0.0,0.014899807050824165
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지?",Loss Function 정의,-0.06732405,0.0,0.06732404977083206
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지?",MBTI,-0.018714808,0.0,0.01871480792760849
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지?",MSE Loss 설명,0.03189548,0.0,0.031895481050014496
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지?",MSE Loss 용도,0.0049702153,0.0,0.004970215260982513
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지?",Multi-Label 에서 CE + Softmax 적용 문제점,-0.012623451,0.0,0.012623450718820095
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지?",PEFT 방법 5가지,-0.0062398263,0.0,0.006239826325327158
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지?",거대 언어 모델 정의,0.011367272,0.0,0.011367271654307842
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지?",기본 경험,-0.029403396,0.0,0.02940339595079422
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지?",답변 실패,-0.008231351,0.0,0.008231351152062416
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지?",딥러닝,0.011427447,0.0,0.011427447199821472
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지?",마지막 할 말,0.026036898,0.0,0.02603689767420292
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지?",머신러닝,-0.07768459,0.0,0.07768458873033524
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지?",면접 시작 인사,-0.024539536,0.0,0.024539535865187645
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지?",상세 경험,0.018435286,0.0,0.018435286357998848
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지?",수식,0.005324849,0.0,0.005324848927557468
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지?",용어 질문,0.050922096,0.0,0.05092209577560425
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지?",인공지능,0.89996654,1.0,0.1000334620475769
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지?",잠시 휴식,0.0007365924,0.0,0.000736592395696789
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지?",좋아하는 아이돌,0.010547957,0.0,0.010547957383096218
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지?",핵심 아이디어,0.022105949,0.0,0.022105948999524117
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지?",확률 예측에서 MSE Loss 미 사용 이유,-0.0005433087,0.0,0.0005433086771517992
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야!",BCE 가 좋은 task,-0.013565815,0.0,0.013565815053880215
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야!",BCE 가 좋은 이유,-0.03760459,0.0,0.03760458901524544
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야!",LLM Fine-Tuning 의 PEFT,-0.033723526,0.0,0.03372352570295334
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야!",LoRA,-0.008782449,0.0,0.008782449178397655
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야!",LoRA 와 QLoRA 의 차이,-0.008979845,0.0,0.008979844860732555
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야!",Loss Function 예시,0.021071479,0.0,0.021071478724479675
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야!",Loss Function 정의,-0.050062947,0.0,0.05006294697523117
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야!",MBTI,0.026242515,0.0,0.026242515072226524
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야!",MSE Loss 설명,-0.019651674,0.0,0.01965167373418808
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야!",MSE Loss 용도,-0.015162731,0.0,0.01516273058950901
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야!",Multi-Label 에서 CE + Softmax 적용 문제점,-0.0516551,0.0,0.05165509879589081
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야!",PEFT 방법 5가지,0.019719562,0.0,0.0197195615619421
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야!",거대 언어 모델 정의,0.011569001,0.0,0.011569000780582428
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야!",기본 경험,-0.054851435,0.0,0.054851435124874115
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야!",답변 실패,-0.021341855,0.0,0.021341854706406593
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야!",딥러닝,0.008862908,0.0,0.008862907998263836
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야!",마지막 할 말,-0.012088154,0.0,0.012088154442608356
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야!",머신러닝,0.8488031,1.0,0.1511968970298767
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야!",면접 시작 인사,-0.004925795,0.0,0.0049257948994636536
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야!",상세 경험,-0.035382345,0.0,0.03538234531879425
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야!",수식,0.014976186,0.0,0.014976185746490955
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야!",용어 질문,-0.025111482,0.0,0.025111481547355652
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야!",인공지능,-0.047438797,0.0,0.04743879660964012
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야!",잠시 휴식,-0.0038810768,0.0,0.003881076816469431
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야!",좋아하는 아이돌,-0.015695347,0.0,0.015695346519351006
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야!",핵심 아이디어,0.011568938,0.0,0.011568938381969929
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야!",확률 예측에서 MSE Loss 미 사용 이유,0.009762471,0.0,0.009762470610439777
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지",BCE 가 좋은 task,-0.0071506854,0.0,0.007150685414671898
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지",BCE 가 좋은 이유,-0.0016324193,0.0,0.001632419298402965
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지",LLM Fine-Tuning 의 PEFT,0.047243338,0.0,0.04724333807826042
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지",LoRA,0.005908009,0.0,0.005908009130507708
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지",LoRA 와 QLoRA 의 차이,-0.011455409,0.0,0.011455409228801727
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지",Loss Function 예시,0.009081158,0.0,0.009081157855689526
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지",Loss Function 정의,-0.059725758,0.0,0.05972575768828392
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지",MBTI,0.00904984,0.0,0.009049840271472931
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지",MSE Loss 설명,0.035990518,0.0,0.03599051758646965
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지",MSE Loss 용도,0.016148612,0.0,0.016148611903190613
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지",Multi-Label 에서 CE + Softmax 적용 문제점,0.00039955456,0.0,0.00039955455577000976
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지",PEFT 방법 5가지,-0.014728493,0.0,0.014728493057191372
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지",거대 언어 모델 정의,0.013710498,0.0,0.013710497878491879
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지",기본 경험,0.036203925,0.0,0.03620392456650734
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지",답변 실패,-0.015043681,0.0,0.015043680556118488
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지",딥러닝,0.90750444,1.0,0.09249556064605713
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지",마지막 할 말,-0.0027655817,0.0,0.0027655817102640867
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지",머신러닝,0.026327234,0.0,0.026327233761548996
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지",면접 시작 인사,-0.011401054,0.0,0.011401054449379444
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지",상세 경험,-0.021944748,0.0,0.021944748237729073
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지",수식,0.031840093,0.0,0.031840093433856964
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지",용어 질문,0.012305318,0.0,0.012305318377912045
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지",인공지능,-0.025707481,0.0,0.025707481428980827
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지",잠시 휴식,-0.0034302177,0.0,0.0034302177373319864
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지",좋아하는 아이돌,-0.008869413,0.0,0.008869413286447525
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지",핵심 아이디어,-0.039309178,0.0,0.03930917754769325
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지",확률 예측에서 MSE Loss 미 사용 이유,-0.0021025059,0.0,0.0021025058813393116
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야.",BCE 가 좋은 task,-0.004267894,0.0,0.004267894197255373
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야.",BCE 가 좋은 이유,-0.0026727514,0.0,0.002672751434147358
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야.",LLM Fine-Tuning 의 PEFT,0.056440853,0.0,0.05644085258245468
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야.",LoRA,0.010406568,0.0,0.010406567715108395
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야.",LoRA 와 QLoRA 의 차이,-0.018718574,0.0,0.018718574196100235
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야.",Loss Function 예시,0.008338162,0.0,0.008338161744177341
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야.",Loss Function 정의,-0.06884899,0.0,0.06884898990392685
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야.",MBTI,0.00509912,0.0,0.005099120084196329
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야.",MSE Loss 설명,0.060339723,0.0,0.06033972278237343
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야.",MSE Loss 용도,0.01617964,0.0,0.016179639846086502
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야.",Multi-Label 에서 CE + Softmax 적용 문제점,0.003622058,0.0,0.003622058080509305
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야.",PEFT 방법 5가지,-0.022817079,0.0,0.022817078977823257
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야.",거대 언어 모델 정의,0.0077824816,0.0,0.007782481610774994
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야.",기본 경험,0.044344153,0.0,0.04434415325522423
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야.",답변 실패,-0.01790835,0.0,0.017908349633216858
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야.",딥러닝,0.9038906,1.0,0.09610939025878906
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야.",마지막 할 말,-0.008443571,0.0,0.00844357069581747
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야.",머신러닝,0.024459377,0.0,0.02445937693119049
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야.",면접 시작 인사,-0.0055305655,0.0,0.00553056551143527
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야.",상세 경험,-0.018897183,0.0,0.01889718323945999
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야.",수식,0.027784081,0.0,0.027784081175923347
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야.",용어 질문,0.023904577,0.0,0.023904576897621155
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야.",인공지능,-0.02997468,0.0,0.02997468039393425
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야.",잠시 휴식,-0.014557263,0.0,0.014557262882590294
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야.",좋아하는 아이돌,-0.011538187,0.0,0.0115381870418787
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야.",핵심 아이디어,-0.047026567,0.0,0.04702656716108322
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야.",확률 예측에서 MSE Loss 미 사용 이유,-0.00980012,0.0,0.009800120256841183
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로",BCE 가 좋은 task,-0.008203197,0.0,0.00820319727063179
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로",BCE 가 좋은 이유,0.010442758,0.0,0.010442757979035378
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로",LLM Fine-Tuning 의 PEFT,-0.021189887,0.0,0.021189887076616287
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로",LoRA,0.0032435271,0.0,0.003243527142331004
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로",LoRA 와 QLoRA 의 차이,-0.008844132,0.0,0.008844131603837013
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로",Loss Function 예시,-0.0058478224,0.0,0.005847822409123182
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로",Loss Function 정의,-0.024511829,0.0,0.024511829018592834
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로",MBTI,-0.0028751506,0.0,0.0028751506470143795
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로",MSE Loss 설명,0.022019101,0.0,0.022019101306796074
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로",MSE Loss 용도,-0.004753541,0.0,0.004753541201353073
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로",Multi-Label 에서 CE + Softmax 적용 문제점,-0.022433434,0.0,0.022433433681726456
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로",PEFT 방법 5가지,-0.008665484,0.0,0.008665484376251698
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로",거대 언어 모델 정의,0.0049600285,0.0,0.004960028454661369
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로",기본 경험,-0.009775054,0.0,0.009775053709745407
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로",답변 실패,0.96987414,1.0,0.030125856399536133
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로",딥러닝,0.008968815,0.0,0.008968815207481384
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로",마지막 할 말,-0.0016453997,0.0,0.0016453997232019901
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로",머신러닝,0.030936133,0.0,0.03093613311648369
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로",면접 시작 인사,-0.014380579,0.0,0.014380578882992268
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로",상세 경험,-0.01647605,0.0,0.01647605001926422
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로",수식,-0.008591263,0.0,0.008591262623667717
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로",용어 질문,0.008267361,0.0,0.008267360739409924
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로",인공지능,0.0071262377,0.0,0.007126237731426954
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로",잠시 휴식,-0.0022290826,0.0,0.0022290826309472322
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로",좋아하는 아이돌,-0.0113769155,0.0,0.011376915499567986
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로",핵심 아이디어,-0.0052075926,0.0,0.005207592621445656
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로",확률 예측에서 MSE Loss 미 사용 이유,-0.0069673713,0.0,0.006967371329665184
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,BCE 가 좋은 task,-0.017658805,0.0,0.01765880547463894
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,BCE 가 좋은 이유,-0.017451825,0.0,0.017451824620366096
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,LLM Fine-Tuning 의 PEFT,0.009068103,0.0,0.009068102575838566
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,LoRA,-0.0026240933,0.0,0.002624093322083354
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,LoRA 와 QLoRA 의 차이,-0.0035965045,0.0,0.0035965044517070055
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,Loss Function 예시,-0.009918633,0.0,0.009918632917106152
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,Loss Function 정의,0.010444031,0.0,0.010444031096994877
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,MBTI,-0.013400087,0.0,0.013400087133049965
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,MSE Loss 설명,-0.02843505,0.0,0.028435049578547478
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,MSE Loss 용도,-0.02676503,0.0,0.02676502987742424
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,Multi-Label 에서 CE + Softmax 적용 문제점,0.0020154028,0.0,0.0020154027733951807
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,PEFT 방법 5가지,-0.010783818,0.0,0.010783817619085312
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,거대 언어 모델 정의,0.9569522,1.0,0.04304778575897217
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,기본 경험,0.01747624,0.0,0.017476240172982216
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,답변 실패,-0.0112849735,0.0,0.011284973472356796
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,딥러닝,-0.012220276,0.0,0.012220275588333607
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,마지막 할 말,0.0009475977,0.0,0.0009475977276451886
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,머신러닝,0.0012656397,0.0,0.0012656396720558405
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,면접 시작 인사,-0.0025269247,0.0,0.0025269247125834227
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,상세 경험,-0.009988015,0.0,0.009988014586269855
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,수식,-0.024186214,0.0,0.024186214432120323
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,용어 질문,-0.07956486,0.0,0.07956486195325851
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,인공지능,-0.011850504,0.0,0.011850504204630852
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,잠시 휴식,-0.015998157,0.0,0.015998156741261482
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,좋아하는 아이돌,-0.007787023,0.0,0.007787023205310106
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,핵심 아이디어,0.028606603,0.0,0.028606602922081947
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,확률 예측에서 MSE Loss 미 사용 이유,-0.018920626,0.0,0.018920626491308212
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,BCE 가 좋은 task,-0.0042811953,0.0,0.004281195346266031
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,BCE 가 좋은 이유,-0.023564586,0.0,0.02356458641588688
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,LLM Fine-Tuning 의 PEFT,0.06611944,0.0,0.06611943989992142
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,LoRA,0.013038481,0.0,0.013038480654358864
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,LoRA 와 QLoRA 의 차이,-0.04118571,0.0,0.041185710579156876
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,Loss Function 예시,-0.036052395,0.0,0.0360523946583271
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,Loss Function 정의,0.009583089,0.0,0.00958308856934309
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,MBTI,-0.0010450914,0.0,0.0010450914269313216
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,MSE Loss 설명,0.010449031,0.0,0.010449031367897987
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,MSE Loss 용도,-0.07917473,0.0,0.07917472720146179
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,Multi-Label 에서 CE + Softmax 적용 문제점,-0.01305707,0.0,0.013057069852948189
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,PEFT 방법 5가지,-0.010529203,0.0,0.010529203340411186
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,거대 언어 모델 정의,0.8128096,0.0,0.8128095865249634
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,기본 경험,-0.0048363633,0.0,0.00483636325225234
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,답변 실패,0.21338528,1.0,0.786614716053009
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,딥러닝,0.061511613,0.0,0.06151161342859268
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,마지막 할 말,-0.00083045504,0.0,0.0008304550428874791
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,머신러닝,-0.0033351597,0.0,0.0033351597376167774
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,면접 시작 인사,-0.0131191425,0.0,0.01311914250254631
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,상세 경험,-0.017020352,0.0,0.01702035218477249
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,수식,-0.039681077,0.0,0.039681077003479004
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,용어 질문,-0.024841513,0.0,0.024841513484716415
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,인공지능,-0.03723937,0.0,0.03723936900496483
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,잠시 휴식,-0.043507617,0.0,0.043507616966962814
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,좋아하는 아이돌,-0.0150899505,0.0,0.015089950524270535
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,핵심 아이디어,0.028762469,0.0,0.028762469068169594
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,확률 예측에서 MSE Loss 미 사용 이유,-0.07080384,0.0,0.07080384343862534
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델,BCE 가 좋은 task,0.002426218,0.0,0.002426218008622527
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델,BCE 가 좋은 이유,-0.037236948,0.0,0.03723694756627083
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델,LLM Fine-Tuning 의 PEFT,-0.013204855,0.0,0.013204854913055897
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델,LoRA,0.0312722,0.0,0.031272199004888535
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델,LoRA 와 QLoRA 의 차이,-0.006964733,0.0,0.006964732892811298
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델,Loss Function 예시,-0.03792277,0.0,0.03792276978492737
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델,Loss Function 정의,0.013322789,0.0,0.01332278922200203
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델,MBTI,-0.0039452906,0.0,0.0039452905766665936
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델,MSE Loss 설명,-0.021614885,0.0,0.021614884957671165
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델,MSE Loss 용도,-0.035469577,0.0,0.035469576716423035
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델,Multi-Label 에서 CE + Softmax 적용 문제점,0.006772349,0.0,0.006772349122911692
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델,PEFT 방법 5가지,-0.03138398,0.0,0.03138398006558418
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델,거대 언어 모델 정의,0.9224411,1.0,0.07755887508392334
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델,기본 경험,0.012790553,0.0,0.012790553271770477
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델,답변 실패,-0.022649376,0.0,0.022649375721812248
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델,딥러닝,-0.008948763,0.0,0.008948762901127338
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델,마지막 할 말,-0.013659389,0.0,0.01365938875824213
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델,머신러닝,-0.0023261542,0.0,0.00232615415006876
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델,면접 시작 인사,-0.004982753,0.0,0.004982753191143274
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델,상세 경험,-0.010707504,0.0,0.010707504115998745
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델,수식,-0.01172259,0.0,0.01172258984297514
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델,용어 질문,-0.06757201,0.0,0.06757201254367828
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델,인공지능,0.0005311452,0.0,0.0005311451968736947
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델,잠시 휴식,-0.021489281,0.0,0.021489281207323074
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델,좋아하는 아이돌,-0.00969465,0.0,0.009694649837911129
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델,핵심 아이디어,0.0398772,0.0,0.03987719863653183
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델,확률 예측에서 MSE Loss 미 사용 이유,-0.016666457,0.0,0.016666457056999207
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지.,BCE 가 좋은 task,0.00079944025,0.0,0.0007994402549229562
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지.,BCE 가 좋은 이유,0.000101136626,0.0,0.00010113662574440241
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지.,LLM Fine-Tuning 의 PEFT,0.014682257,0.0,0.014682256616652012
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지.,LoRA,-0.0016031015,0.0,0.0016031014965847135
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지.,LoRA 와 QLoRA 의 차이,-0.0015396397,0.0,0.001539639662951231
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지.,Loss Function 예시,-0.013335714,0.0,0.013335714116692543
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지.,Loss Function 정의,-0.022951124,0.0,0.022951124235987663
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지.,MBTI,-0.008939298,0.0,0.008939297869801521
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지.,MSE Loss 설명,0.0032531745,0.0,0.0032531744800508022
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지.,MSE Loss 용도,-0.020821016,0.0,0.020821016281843185
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지.,Multi-Label 에서 CE + Softmax 적용 문제점,-0.03809721,0.0,0.038097210228443146
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지.,PEFT 방법 5가지,-0.02915349,0.0,0.02915349043905735
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지.,거대 언어 모델 정의,0.15482122,0.0,0.1548212170600891
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지.,기본 경험,-0.03868412,0.0,0.038684118539094925
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지.,답변 실패,0.9360345,1.0,0.06396549940109253
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지.,딥러닝,0.0021703036,0.0,0.0021703036036342382
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지.,마지막 할 말,-0.007964386,0.0,0.00796438567340374
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지.,머신러닝,-0.009150377,0.0,0.009150376543402672
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지.,면접 시작 인사,-0.0029244085,0.0,0.002924408530816436
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지.,상세 경험,-0.010164362,0.0,0.010164362378418446
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지.,수식,-0.020151164,0.0,0.02015116438269615
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지.,용어 질문,-0.0071245716,0.0,0.007124571595340967
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지.,인공지능,-0.015207805,0.0,0.01520780473947525
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지.,잠시 휴식,-0.013469053,0.0,0.01346905343234539
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지.,좋아하는 아이돌,-0.013310794,0.0,0.013310793787240982
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지.,핵심 아이디어,-0.010253638,0.0,0.01025363802909851
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지.,확률 예측에서 MSE Loss 미 사용 이유,-0.02423326,0.0,0.02423325926065445
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,BCE 가 좋은 task,0.019424103,0.0,0.01942410320043564
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,BCE 가 좋은 이유,-0.0050114896,0.0,0.005011489614844322
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,LLM Fine-Tuning 의 PEFT,-0.008009189,0.0,0.008009188808500767
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,LoRA,-0.008084713,0.0,0.00808471255004406
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,LoRA 와 QLoRA 의 차이,-0.000547492,0.0,0.0005474920035339892
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,Loss Function 예시,-0.023193879,0.0,0.023193879052996635
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,Loss Function 정의,0.9136591,0.0,0.9136590957641602
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,MBTI,-0.019907055,0.0,0.01990705542266369
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,MSE Loss 설명,0.048502415,0.0,0.04850241541862488
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,MSE Loss 용도,0.005697561,0.0,0.005697560962289572
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,Multi-Label 에서 CE + Softmax 적용 문제점,-0.010303707,0.0,0.010303706862032413
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,PEFT 방법 5가지,0.03161694,0.0,0.0316169410943985
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,거대 언어 모델 정의,-0.0073426054,0.0,0.007342605385929346
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,기본 경험,0.03667842,0.0,0.03667841851711273
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,답변 실패,-0.0068729604,1.0,1.0068729603663087
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,딥러닝,0.023908328,0.0,0.023908328264951706
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,마지막 할 말,0.029218998,0.0,0.029218997806310654
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,머신러닝,-0.00895424,0.0,0.008954240009188652
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,면접 시작 인사,0.003642083,0.0,0.0036420829128473997
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,상세 경험,0.01001567,0.0,0.010015670210123062
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,수식,-0.014067482,0.0,0.014067482203245163
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,용어 질문,-0.05874764,0.0,0.05874764174222946
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,인공지능,-0.010077766,0.0,0.01007776614278555
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,잠시 휴식,-0.0010648259,0.0,0.00106482591945678
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,좋아하는 아이돌,-0.014636683,0.0,0.014636683277785778
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,핵심 아이디어,0.0322494,0.0,0.03224939852952957
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,확률 예측에서 MSE Loss 미 사용 이유,-0.026511578,0.0,0.026511577889323235
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",BCE 가 좋은 task,0.0055212826,0.0,0.0055212825536727905
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",BCE 가 좋은 이유,0.0286174,0.0,0.02861740067601204
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",LLM Fine-Tuning 의 PEFT,-0.007046567,0.0,0.0070465668104588985
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",LoRA,0.02852903,0.0,0.028529029339551926
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",LoRA 와 QLoRA 의 차이,-0.0052735405,0.0,0.0052735405042767525
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",Loss Function 예시,-0.0465849,0.0,0.046584900468587875
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",Loss Function 정의,0.9264287,1.0,0.07357132434844971
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",MBTI,-0.0033090059,0.0,0.0033090058714151382
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",MSE Loss 설명,-0.018249668,0.0,0.018249668180942535
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",MSE Loss 용도,-0.0042796987,0.0,0.004279698710888624
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",Multi-Label 에서 CE + Softmax 적용 문제점,0.009152453,0.0,0.009152453392744064
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",PEFT 방법 5가지,0.0585476,0.0,0.058547601103782654
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",거대 언어 모델 정의,-0.052224547,0.0,0.052224546670913696
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",기본 경험,0.00334678,0.0,0.0033467800822108984
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",답변 실패,0.0056805722,0.0,0.0056805722415447235
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",딥러닝,-0.021809727,0.0,0.02180972695350647
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",마지막 할 말,-0.006739287,0.0,0.006739287171512842
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",머신러닝,-0.013598723,0.0,0.013598723337054253
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",면접 시작 인사,-0.007792625,0.0,0.007792625110596418
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",상세 경험,-0.012342278,0.0,0.01234227791428566
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",수식,0.025077827,0.0,0.025077827274799347
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",용어 질문,-0.068030626,0.0,0.06803062558174133
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",인공지능,0.0002629275,0.0,0.0002629274968057871
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",잠시 휴식,0.009743011,0.0,0.009743010625243187
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",좋아하는 아이돌,-0.015499325,0.0,0.015499324537813663
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",핵심 아이디어,0.045507967,0.0,0.045507967472076416
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",확률 예측에서 MSE Loss 미 사용 이유,-0.008715788,0.0,0.008715787902474403
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야,BCE 가 좋은 task,0.00884856,0.0,0.00884856004267931
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야,BCE 가 좋은 이유,-0.00041906215,0.0,0.0004190621548332274
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야,LLM Fine-Tuning 의 PEFT,-0.016578795,0.0,0.01657879538834095
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야,LoRA,0.022369098,0.0,0.02236909791827202
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야,LoRA 와 QLoRA 의 차이,-0.005857015,0.0,0.005857015028595924
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야,Loss Function 예시,0.024832444,0.0,0.02483244426548481
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야,Loss Function 정의,0.025861181,0.0,0.025861181318759918
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야,MBTI,-0.006224453,0.0,0.00622445298358798
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야,MSE Loss 설명,-0.0020105098,0.0,0.0020105098374187946
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야,MSE Loss 용도,-0.018305132,0.0,0.018305132165551186
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야,Multi-Label 에서 CE + Softmax 적용 문제점,-0.023341086,0.0,0.023341085761785507
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야,PEFT 방법 5가지,-0.019709531,0.0,0.019709531217813492
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야,거대 언어 모델 정의,-0.006318035,0.0,0.006318035069853067
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야,기본 경험,-0.0131090665,0.0,0.013109066523611546
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야,답변 실패,0.9584335,1.0,0.04156649112701416
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야,딥러닝,0.012549604,0.0,0.012549604289233685
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야,마지막 할 말,-0.0023973063,0.0,0.002397306263446808
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야,머신러닝,-0.010683419,0.0,0.010683419182896614
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야,면접 시작 인사,-0.01780049,0.0,0.01780048944056034
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야,상세 경험,-0.0003641697,0.0,0.0003641697112470865
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야,수식,-0.021531917,0.0,0.02153191715478897
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야,용어 질문,-0.015322096,0.0,0.015322095714509487
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야,인공지능,-0.019436369,0.0,0.019436368718743324
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야,잠시 휴식,-0.011697591,0.0,0.011697591282427311
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야,좋아하는 아이돌,0.0011257455,0.0,0.0011257454752922058
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야,핵심 아이디어,-0.00044622447,0.0,0.0004462244687601924
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야,확률 예측에서 MSE Loss 미 사용 이유,-0.011437691,0.0,0.011437690816819668
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지,BCE 가 좋은 task,0.01248906,0.0,0.012489059939980507
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지,BCE 가 좋은 이유,0.035252333,0.0,0.03525233268737793
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지,LLM Fine-Tuning 의 PEFT,-0.012686087,0.0,0.012686086818575859
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지,LoRA,-0.007016979,0.0,0.007016979157924652
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지,LoRA 와 QLoRA 의 차이,-0.0027438907,0.0,0.002743890741840005
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지,Loss Function 예시,-0.0488994,0.0,0.04889940097928047
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지,Loss Function 정의,0.9406872,1.0,0.05931282043457031
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지,MBTI,0.00648622,0.0,0.006486219819635153
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지,MSE Loss 설명,0.0063284696,0.0,0.006328469607979059
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지,MSE Loss 용도,-0.00719746,0.0,0.007197460159659386
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지,Multi-Label 에서 CE + Softmax 적용 문제점,0.009983079,0.0,0.009983078576624393
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지,PEFT 방법 5가지,0.05002144,0.0,0.05002143979072571
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지,거대 언어 모델 정의,-0.02636239,0.0,0.02636238932609558
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지,기본 경험,0.010670021,0.0,0.010670021176338196
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지,답변 실패,-0.007720451,0.0,0.00772045087069273
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지,딥러닝,-0.018704273,0.0,0.01870427280664444
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지,마지막 할 말,-0.0048304982,0.0,0.004830498248338699
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지,머신러닝,0.016466191,0.0,0.016466191038489342
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지,면접 시작 인사,-0.00028142123,0.0,0.00028142123483121395
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지,상세 경험,-0.036131136,0.0,0.03613113611936569
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지,수식,0.007874809,0.0,0.007874809205532074
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지,용어 질문,-0.049963422,0.0,0.04996342211961746
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지,인공지능,0.000103805,0.0,0.00010380500316387042
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지,잠시 휴식,0.0047647976,0.0,0.004764797631651163
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지,좋아하는 아이돌,-0.023134785,0.0,0.023134784772992134
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지,핵심 아이디어,0.05633974,0.0,0.05633974075317383
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지,확률 예측에서 MSE Loss 미 사용 이유,0.014494134,0.0,0.014494134113192558
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,BCE 가 좋은 task,0.01142411,0.0,0.011424110271036625
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,BCE 가 좋은 이유,-0.0017496699,0.0,0.0017496699001640081
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,LLM Fine-Tuning 의 PEFT,-0.02791722,0.0,0.027917219325900078
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,LoRA,0.017790811,0.0,0.017790811136364937
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,LoRA 와 QLoRA 의 차이,0.0040743626,0.0,0.004074362572282553
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,Loss Function 예시,0.13208057,0.0,0.1320805698633194
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,Loss Function 정의,-0.023911659,0.0,0.02391165867447853
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,MBTI,0.01171808,0.0,0.011718080379068851
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,MSE Loss 설명,-0.017690422,0.0,0.017690422013401985
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,MSE Loss 용도,-0.0057086716,0.0,0.005708671640604734
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,Multi-Label 에서 CE + Softmax 적용 문제점,-0.016070673,0.0,0.016070673242211342
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,PEFT 방법 5가지,-0.005615823,0.0,0.005615822970867157
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,거대 언어 모델 정의,-0.0038569488,0.0,0.003856948809698224
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,기본 경험,-0.030034907,0.0,0.030034907162189484
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,답변 실패,0.9450743,1.0,0.05492568016052246
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,딥러닝,-0.013576424,0.0,0.01357642374932766
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,마지막 할 말,-0.010072108,0.0,0.01007210835814476
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,머신러닝,-0.010972055,0.0,0.010972054675221443
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,면접 시작 인사,-0.026108434,0.0,0.026108434423804283
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,상세 경험,-0.010909558,0.0,0.010909558273851871
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,수식,-0.010221337,0.0,0.010221336968243122
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,용어 질문,0.024128955,0.0,0.024128954857587814
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,인공지능,-0.024487102,0.0,0.024487102404236794
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,잠시 휴식,-0.005122241,0.0,0.005122241098433733
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,좋아하는 아이돌,0.008737118,0.0,0.008737117983400822
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,핵심 아이디어,-0.013503554,0.0,0.01350355427712202
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,확률 예측에서 MSE Loss 미 사용 이유,-0.008041308,0.0,0.008041308261454105
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",BCE 가 좋은 task,-0.02135112,0.0,0.021351119503378868
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",BCE 가 좋은 이유,-0.05415409,0.0,0.05415409058332443
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",LLM Fine-Tuning 의 PEFT,-0.03532449,0.0,0.03532449156045914
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",LoRA,-0.02041369,0.0,0.02041368931531906
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",LoRA 와 QLoRA 의 차이,0.014830473,0.0,0.014830472879111767
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",Loss Function 예시,0.8863241,1.0,0.11367589235305786
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",Loss Function 정의,-0.091057785,0.0,0.09105778485536575
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",MBTI,0.0012527318,0.0,0.0012527317740023136
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",MSE Loss 설명,-0.04593652,0.0,0.045936521142721176
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",MSE Loss 용도,-0.045175575,0.0,0.04517557471990585
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",Multi-Label 에서 CE + Softmax 적용 문제점,-0.021095905,0.0,0.02109590545296669
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",PEFT 방법 5가지,0.016805425,0.0,0.01680542528629303
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",거대 언어 모델 정의,-0.0037628093,0.0,0.0037628093268722296
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",기본 경험,-0.030849192,0.0,0.030849192291498184
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",답변 실패,0.23662545,0.0,0.23662544786930084
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",딥러닝,0.0033443074,0.0,0.0033443074207752943
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",마지막 할 말,-0.005613088,0.0,0.005613088142126799
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",머신러닝,-0.0018352745,0.0,0.0018352745100855827
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",면접 시작 인사,0.00044943125,0.0,0.00044943124521523714
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",상세 경험,-0.024779897,0.0,0.024779897183179855
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",수식,0.022065647,0.0,0.022065646946430206
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",용어 질문,0.07431944,0.0,0.07431943714618683
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",인공지능,-0.011158066,0.0,0.011158065870404243
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",잠시 휴식,0.02289169,0.0,0.022891689091920853
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",좋아하는 아이돌,0.016552322,0.0,0.01655232161283493
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",핵심 아이디어,0.011171992,0.0,0.011171991936862469
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",확률 예측에서 MSE Loss 미 사용 이유,-0.012004134,0.0,0.012004134245216846
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어",BCE 가 좋은 task,-0.047419365,0.0,0.047419365495443344
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어",BCE 가 좋은 이유,-0.04772804,0.0,0.0477280393242836
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어",LLM Fine-Tuning 의 PEFT,-0.05440116,0.0,0.05440115928649902
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어",LoRA,0.018284418,0.0,0.018284417688846588
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어",LoRA 와 QLoRA 의 차이,-0.011245045,0.0,0.011245044879615307
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어",Loss Function 예시,0.93166405,1.0,0.06833595037460327
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어",Loss Function 정의,-0.0689927,0.0,0.06899269670248032
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어",MBTI,-0.007861743,0.0,0.007861742749810219
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어",MSE Loss 설명,-0.030562213,0.0,0.03056221269071102
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어",MSE Loss 용도,-0.035267524,0.0,0.03526752442121506
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어",Multi-Label 에서 CE + Softmax 적용 문제점,-0.0021241521,0.0,0.002124152146279812
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어",PEFT 방법 5가지,-0.0042680185,0.0,0.004268018528819084
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어",거대 언어 모델 정의,-0.0043272465,0.0,0.004327246453613043
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어",기본 경험,-0.024013733,0.0,0.024013733491301537
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어",답변 실패,-0.041019473,0.0,0.04101947322487831
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어",딥러닝,0.007608597,0.0,0.007608597166836262
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어",마지막 할 말,-0.001205992,0.0,0.001205991953611374
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어",머신러닝,-0.0007726692,0.0,0.0007726692128926516
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어",면접 시작 인사,-0.0050907247,0.0,0.005090724676847458
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어",상세 경험,-0.029933572,0.0,0.029933571815490723
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어",수식,0.003321512,0.0,0.0033215119037777185
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어",용어 질문,0.0009772787,0.0,0.0009772786870598793
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어",인공지능,-0.023784027,0.0,0.023784026503562927
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어",잠시 휴식,0.015608694,0.0,0.015608694404363632
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어",좋아하는 아이돌,0.011733191,0.0,0.011733191087841988
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어",핵심 아이디어,0.04350108,0.0,0.043501079082489014
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어",확률 예측에서 MSE Loss 미 사용 이유,0.011478629,0.0,0.01147862896323204
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야?",BCE 가 좋은 task,0.0059702727,0.0,0.005970272701233625
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야?",BCE 가 좋은 이유,0.00517045,0.0,0.005170450080186129
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야?",LLM Fine-Tuning 의 PEFT,-0.0055608614,0.0,0.005560861434787512
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야?",LoRA,0.0031370511,0.0,0.0031370511278510094
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야?",LoRA 와 QLoRA 의 차이,0.0011127305,0.0,0.0011127304751425982
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야?",Loss Function 예시,0.043265454,0.0,0.0432654544711113
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야?",Loss Function 정의,-0.005963684,0.0,0.005963684059679508
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야?",MBTI,0.010144454,0.0,0.010144454427063465
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야?",MSE Loss 설명,-0.021364374,0.0,0.021364374086260796
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야?",MSE Loss 용도,-0.011682346,0.0,0.01168234646320343
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야?",Multi-Label 에서 CE + Softmax 적용 문제점,-0.010820615,0.0,0.010820615105330944
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야?",PEFT 방법 5가지,0.007065058,0.0,0.007065058220177889
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야?",거대 언어 모델 정의,-0.0024258264,0.0,0.0024258263874799013
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야?",기본 경험,-0.013369248,0.0,0.013369248248636723
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야?",답변 실패,0.96734834,1.0,0.032651662826538086
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야?",딥러닝,-0.004876931,0.0,0.004876931197941303
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야?",마지막 할 말,-0.010281048,0.0,0.010281047783792019
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야?",머신러닝,-0.017703032,0.0,0.01770303212106228
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야?",면접 시작 인사,-0.023257624,0.0,0.023257624357938766
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야?",상세 경험,-0.01660284,0.0,0.016602840274572372
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야?",수식,-0.02235689,0.0,0.02235689014196396
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야?",용어 질문,0.026483826,0.0,0.026483826339244843
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야?",인공지능,-0.0049045878,0.0,0.0049045877531170845
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야?",잠시 휴식,0.0006640601,0.0,0.0006640601204708219
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야?",좋아하는 아이돌,-0.0012549501,0.0,0.0012549500679597259
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야?",핵심 아이디어,-0.011405027,0.0,0.01140502654016018
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야?",확률 예측에서 MSE Loss 미 사용 이유,0.0023874363,0.0,0.0023874363396316767
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어",BCE 가 좋은 task,-0.039372772,0.0,0.039372771978378296
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어",BCE 가 좋은 이유,-0.06265304,0.0,0.06265304237604141
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어",LLM Fine-Tuning 의 PEFT,-0.0014087154,0.0,0.0014087153831496835
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어",LoRA,-0.008044858,0.0,0.00804485846310854
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어",LoRA 와 QLoRA 의 차이,0.0046660616,0.0,0.004666061606258154
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어",Loss Function 예시,0.9347202,1.0,0.06527978181838989
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어",Loss Function 정의,-0.07482317,0.0,0.07482317090034485
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어",MBTI,-0.023036199,0.0,0.023036198690533638
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어",MSE Loss 설명,-0.026591662,0.0,0.02659166231751442
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어",MSE Loss 용도,-4.88133e-05,0.0,4.881330096395686e-05
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어",Multi-Label 에서 CE + Softmax 적용 문제점,-0.012902208,0.0,0.012902207672595978
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어",PEFT 방법 5가지,-0.020543953,0.0,0.020543953403830528
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어",거대 언어 모델 정의,-0.022320196,0.0,0.02232019603252411
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어",기본 경험,-0.015856253,0.0,0.01585625298321247
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어",답변 실패,-0.022905096,0.0,0.022905096411705017
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어",딥러닝,-0.0027685184,0.0,0.002768518403172493
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어",마지막 할 말,0.007845361,0.0,0.007845360785722733
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어",머신러닝,-0.0009741975,0.0,0.0009741975227370858
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어",면접 시작 인사,-0.004038811,0.0,0.004038811195641756
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어",상세 경험,-0.04835063,0.0,0.04835062846541405
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어",수식,0.004619373,0.0,0.004619373008608818
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어",용어 질문,0.022190556,0.0,0.022190555930137634
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어",인공지능,-0.0124336695,0.0,0.012433669529855251
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어",잠시 휴식,0.026100576,0.0,0.026100575923919678
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어",좋아하는 아이돌,0.018424725,0.0,0.01842472516000271
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어",핵심 아이디어,0.004136965,0.0,0.004136965144425631
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어",확률 예측에서 MSE Loss 미 사용 이유,0.006164131,0.0,0.006164131220430136
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아",BCE 가 좋은 task,-0.035108358,0.0,0.035108357667922974
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아",BCE 가 좋은 이유,-0.046193425,0.0,0.04619342461228371
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아",LLM Fine-Tuning 의 PEFT,-0.041006777,0.0,0.04100677743554115
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아",LoRA,-0.021921871,0.0,0.021921871230006218
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아",LoRA 와 QLoRA 의 차이,0.022399023,0.0,0.022399023175239563
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아",Loss Function 예시,0.9383503,1.0,0.06164968013763428
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아",Loss Function 정의,-0.06974131,0.0,0.06974130868911743
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아",MBTI,0.0111953365,0.0,0.01119533646851778
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아",MSE Loss 설명,-0.06353649,0.0,0.06353648751974106
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아",MSE Loss 용도,-0.058700293,0.0,0.05870029330253601
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아",Multi-Label 에서 CE + Softmax 적용 문제점,-0.0075231697,0.0,0.007523169741034508
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아",PEFT 방법 5가지,-0.011279854,0.0,0.011279853992164135
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아",거대 언어 모델 정의,-0.007510651,0.0,0.0075106509029865265
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아",기본 경험,-0.027508194,0.0,0.027508193626999855
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아",답변 실패,-0.010650187,0.0,0.01065018679946661
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아",딥러닝,0.012610289,0.0,0.012610289268195629
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아",마지막 할 말,0.015159003,0.0,0.0151590034365654
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아",머신러닝,-0.007936766,0.0,0.007936766371130943
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아",면접 시작 인사,-0.012531018,0.0,0.012531017884612083
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아",상세 경험,-0.01747069,0.0,0.017470689490437508
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아",수식,0.004868554,0.0,0.004868553951382637
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아",용어 질문,0.018330479,0.0,0.01833047904074192
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아",인공지능,-0.006682885,0.0,0.006682884879410267
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아",잠시 휴식,0.023637304,0.0,0.023637304082512856
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아",좋아하는 아이돌,0.004853299,0.0,0.0048532988876104355
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아",핵심 아이디어,0.01549083,0.0,0.015490829944610596
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아",확률 예측에서 MSE Loss 미 사용 이유,-0.006972957,0.0,0.00697295693680644
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,BCE 가 좋은 task,-0.020155752,0.0,0.020155752077698708
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,BCE 가 좋은 이유,-0.032621473,0.0,0.03262147307395935
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,LLM Fine-Tuning 의 PEFT,-0.025113005,0.0,0.025113005191087723
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,LoRA,0.0066498956,0.0,0.006649895571172237
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,LoRA 와 QLoRA 의 차이,-0.0077538243,0.0,0.007753824349492788
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,Loss Function 예시,0.022887323,0.0,0.022887323051691055
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,Loss Function 정의,-0.04102827,0.0,0.04102826863527298
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,MBTI,0.024275297,0.0,0.024275297299027443
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,MSE Loss 설명,0.9217714,1.0,0.07822859287261963
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,MSE Loss 용도,-0.0026729302,0.0,0.002672930248081684
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,Multi-Label 에서 CE + Softmax 적용 문제점,-0.0025126257,0.0,0.002512625651434064
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,PEFT 방법 5가지,0.011411249,0.0,0.011411248706281185
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,거대 언어 모델 정의,0.0063168816,0.0,0.006316881626844406
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,기본 경험,-0.0018006283,0.0,0.0018006282625719905
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,답변 실패,-0.019044038,0.0,0.01904403790831566
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,딥러닝,0.05329774,0.0,0.0532977394759655
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,마지막 할 말,-0.0077557676,0.0,0.0077557675540447235
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,머신러닝,-0.043361243,0.0,0.04336124286055565
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,면접 시작 인사,-0.01480185,0.0,0.014801849611103535
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,상세 경험,-0.059851956,0.0,0.059851955622434616
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,수식,-0.0403199,0.0,0.04031990095973015
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,용어 질문,0.093904085,0.0,0.09390408545732498
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,인공지능,0.025489667,0.0,0.025489667430520058
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,잠시 휴식,-0.018579403,0.0,0.018579402938485146
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,좋아하는 아이돌,-0.005291865,0.0,0.005291865207254887
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,핵심 아이디어,0.016469844,0.0,0.016469843685626984
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,확률 예측에서 MSE Loss 미 사용 이유,-0.057464775,0.0,0.05746477469801903
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,BCE 가 좋은 task,0.0024838161,0.0,0.0024838161189109087
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,BCE 가 좋은 이유,-0.09525506,0.0,0.0952550619840622
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,LLM Fine-Tuning 의 PEFT,0.03648019,0.0,0.03648018836975098
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,LoRA,-0.06730721,0.0,0.06730721145868301
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,LoRA 와 QLoRA 의 차이,0.014735713,0.0,0.014735712669789791
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,Loss Function 예시,-0.042776197,0.0,0.0427761971950531
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,Loss Function 정의,0.074898206,0.0,0.07489820569753647
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,MBTI,-0.023037774,0.0,0.023037774488329887
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,MSE Loss 설명,0.6509709,0.0,0.6509708762168884
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,MSE Loss 용도,-0.011241565,0.0,0.011241565458476543
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,Multi-Label 에서 CE + Softmax 적용 문제점,0.007301934,0.0,0.007301934063434601
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,PEFT 방법 5가지,-0.0056552,0.0,0.005655200220644474
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,거대 언어 모델 정의,0.028567646,0.0,0.028567645698785782
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,기본 경험,-0.008781276,0.0,0.00878127571195364
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,답변 실패,0.19754766,1.0,0.802452340722084
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,딥러닝,0.017927164,0.0,0.01792716421186924
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,마지막 할 말,-0.050087195,0.0,0.05008719488978386
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,머신러닝,-0.06247407,0.0,0.06247406825423241
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,면접 시작 인사,-0.020493254,0.0,0.02049325406551361
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,상세 경험,-0.044535,0.0,0.04453499987721443
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,수식,-0.012676821,0.0,0.01267682109028101
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,용어 질문,0.0002853091,0.0,0.0002853090991266072
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,인공지능,-0.018831663,0.0,0.018831662833690643
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,잠시 휴식,-0.06515717,0.0,0.06515716761350632
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,좋아하는 아이돌,-0.028904585,0.0,0.028904585167765617
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,핵심 아이디어,0.026665961,0.0,0.026665961369872093
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,확률 예측에서 MSE Loss 미 사용 이유,0.011306027,0.0,0.011306026950478554
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",BCE 가 좋은 task,-0.014577408,0.0,0.014577408321201801
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",BCE 가 좋은 이유,-0.010622912,0.0,0.010622912086546421
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",LLM Fine-Tuning 의 PEFT,-0.003741377,0.0,0.0037413770332932472
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",LoRA,0.0039019138,0.0,0.0039019137620925903
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",LoRA 와 QLoRA 의 차이,-0.031535164,0.0,0.03153516352176666
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",Loss Function 예시,-0.036991585,0.0,0.03699158504605293
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",Loss Function 정의,0.008488924,0.0,0.008488924242556095
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",MBTI,-0.025522007,0.0,0.025522006675601006
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",MSE Loss 설명,-0.04018751,0.0,0.04018751159310341
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",MSE Loss 용도,0.93553954,1.0,0.06446045637130737
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",Multi-Label 에서 CE + Softmax 적용 문제점,-0.021409243,0.0,0.02140924334526062
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",PEFT 방법 5가지,-0.0075381584,0.0,0.007538158446550369
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",거대 언어 모델 정의,-0.021801945,0.0,0.021801944822072983
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",기본 경험,0.026769692,0.0,0.026769692078232765
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",답변 실패,-0.004249523,0.0,0.004249522928148508
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",딥러닝,-0.012234032,0.0,0.012234032154083252
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",마지막 할 말,-0.015489791,0.0,0.015489790588617325
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",머신러닝,0.012761042,0.0,0.012761042453348637
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",면접 시작 인사,0.00618637,0.0,0.006186369806528091
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",상세 경험,-0.0142032355,0.0,0.014203235507011414
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",수식,-0.04182017,0.0,0.04182016849517822
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",용어 질문,-0.05243819,0.0,0.05243818834424019
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",인공지능,0.0022762585,0.0,0.0022762585431337357
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",잠시 휴식,-0.027270472,0.0,0.027270471677184105
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",좋아하는 아이돌,0.0012080966,0.0,0.001208096626214683
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",핵심 아이디어,0.030106554,0.0,0.030106553807854652
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",확률 예측에서 MSE Loss 미 사용 이유,0.02404856,0.0,0.024048559367656708
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,BCE 가 좋은 task,-0.0009796976,0.0,0.0009796975646167994
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,BCE 가 좋은 이유,-0.0023841609,0.0,0.002384160878136754
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,LLM Fine-Tuning 의 PEFT,-0.0025071495,0.0,0.002507149474695325
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,LoRA,0.004100255,0.0,0.004100255202502012
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,LoRA 와 QLoRA 의 차이,0.009319355,0.0,0.00931935478001833
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,Loss Function 예시,-0.009896704,0.0,0.009896703995764256
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,Loss Function 정의,-0.016985038,0.0,0.016985038295388222
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,MBTI,-0.022062415,0.0,0.02206241525709629
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,MSE Loss 설명,-0.009156893,0.0,0.009156893007457256
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,MSE Loss 용도,0.0666316,0.0,0.06663160026073456
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,Multi-Label 에서 CE + Softmax 적용 문제점,-0.028105566,0.0,0.028105566278100014
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,PEFT 방법 5가지,-0.024782907,0.0,0.024782907217741013
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,거대 언어 모델 정의,0.015323439,0.0,0.015323438681662083
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,기본 경험,-0.026056752,0.0,0.026056751608848572
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,답변 실패,0.9505176,1.0,0.04948240518569946
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,딥러닝,-0.006445181,0.0,0.006445181090384722
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,마지막 할 말,0.003994223,0.0,0.003994223196059465
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,머신러닝,-0.028602276,0.0,0.028602275997400284
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,면접 시작 인사,-0.018300507,0.0,0.018300507217645645
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,상세 경험,0.00748291,0.0,0.0074829100631177425
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,수식,-0.016363662,0.0,0.016363661736249924
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,용어 질문,-0.018876329,0.0,0.0188763290643692
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,인공지능,-0.019612482,0.0,0.01961248181760311
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,잠시 휴식,-0.004374881,0.0,0.004374880809336901
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,좋아하는 아이돌,-0.0063750115,0.0,0.006375011522322893
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,핵심 아이디어,-0.0044658403,0.0,0.004465840291231871
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,확률 예측에서 MSE Loss 미 사용 이유,-0.0058117737,0.0,0.005811773706227541
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,BCE 가 좋은 task,-0.017900968,0.0,0.017900967970490456
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,BCE 가 좋은 이유,0.004682586,0.0,0.004682586062699556
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,LLM Fine-Tuning 의 PEFT,0.0057196924,0.0,0.005719692446291447
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,LoRA,0.01236594,0.0,0.012365940026938915
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,LoRA 와 QLoRA 의 차이,-0.005668522,0.0,0.005668521858751774
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,Loss Function 예시,-0.0040680235,0.0,0.0040680235251784325
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,Loss Function 정의,-0.015771722,0.0,0.01577172242105007
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,MBTI,-0.019620186,0.0,0.01962018571794033
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,MSE Loss 설명,-0.013451342,0.0,0.013451341539621353
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,MSE Loss 용도,0.017099498,0.0,0.017099497839808464
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,Multi-Label 에서 CE + Softmax 적용 문제점,-0.013863763,0.0,0.013863762840628624
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,PEFT 방법 5가지,-0.009860557,0.0,0.009860556572675705
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,거대 언어 모델 정의,-0.004199803,0.0,0.004199802875518799
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,기본 경험,-0.013602631,0.0,0.01360263116657734
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,답변 실패,0.94675356,1.0,0.05324643850326538
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,딥러닝,-0.0011174572,0.0,0.0011174571700394154
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,마지막 할 말,0.0016441506,0.0,0.0016441505867987871
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,머신러닝,-0.0036144378,0.0,0.0036144377663731575
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,면접 시작 인사,-0.030006647,0.0,0.03000664710998535
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,상세 경험,-0.031504996,0.0,0.03150499612092972
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,수식,-0.0047165886,0.0,0.0047165886498987675
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,용어 질문,0.002123834,0.0,0.0021238340996205807
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,인공지능,-0.009365031,0.0,0.009365030564367771
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,잠시 휴식,0.0092489775,0.0,0.009248977527022362
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,좋아하는 아이돌,0.00094766996,0.0,0.0009476699633523822
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,핵심 아이디어,0.004731194,0.0,0.004731194116175175
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,확률 예측에서 MSE Loss 미 사용 이유,0.09823838,0.0,0.09823837876319885
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,BCE 가 좋은 task,0.013951276,0.0,0.013951276428997517
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,BCE 가 좋은 이유,-0.071830764,0.0,0.07183076441287994
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,LLM Fine-Tuning 의 PEFT,0.018002516,0.0,0.01800251565873623
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,LoRA,-0.027335431,0.0,0.027335431426763535
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,LoRA 와 QLoRA 의 차이,-0.0077188704,0.0,0.0077188704162836075
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,Loss Function 예시,0.0025368081,0.0,0.0025368081405758858
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,Loss Function 정의,-0.00078623445,0.0,0.0007862344500608742
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,MBTI,-0.025125276,0.0,0.025125276297330856
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,MSE Loss 설명,-0.0122374715,0.0,0.012237471528351307
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,MSE Loss 용도,0.0012276872,0.0,0.0012276872294023633
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,Multi-Label 에서 CE + Softmax 적용 문제점,0.008124625,0.0,0.00812462531030178
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,PEFT 방법 5가지,-0.019559665,0.0,0.019559664651751518
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,거대 언어 모델 정의,-0.03715382,0.0,0.03715382143855095
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,기본 경험,0.0036106808,0.0,0.0036106808111071587
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,답변 실패,-0.014454527,0.0,0.014454526826739311
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,딥러닝,-0.042599022,0.0,0.04259902238845825
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,마지막 할 말,0.02068259,0.0,0.02068259008228779
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,머신러닝,0.011120202,0.0,0.011120202019810677
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,면접 시작 인사,0.0011381969,0.0,0.0011381969088688493
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,상세 경험,-0.018029563,0.0,0.01802956312894821
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,수식,0.010882312,0.0,0.010882312431931496
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,용어 질문,-0.004174591,0.0,0.004174591042101383
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,인공지능,-0.01839711,0.0,0.01839710958302021
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,잠시 휴식,-0.0011116398,0.0,0.0011116397799924016
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,좋아하는 아이돌,-0.0283529,0.0,0.028352899476885796
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,핵심 아이디어,0.045335393,0.0,0.04533539339900017
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,확률 예측에서 MSE Loss 미 사용 이유,0.94177693,1.0,0.058223068714141846
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,BCE 가 좋은 task,0.015778385,0.0,0.01577838510274887
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,BCE 가 좋은 이유,-0.0030662226,0.0,0.0030662226490676403
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,LLM Fine-Tuning 의 PEFT,0.025211565,0.0,0.02521156519651413
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,LoRA,-0.00062951527,0.0,0.0006295152707025409
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,LoRA 와 QLoRA 의 차이,-0.016166177,0.0,0.01616617664694786
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,Loss Function 예시,0.000757951,0.0,0.0007579509983770549
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,Loss Function 정의,0.015241049,0.0,0.015241049230098724
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,MBTI,0.0043251147,0.0,0.004325114656239748
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,MSE Loss 설명,-0.04380167,0.0,0.04380166903138161
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,MSE Loss 용도,-0.06034799,0.0,0.060347989201545715
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,Multi-Label 에서 CE + Softmax 적용 문제점,-0.017664732,0.0,0.017664732411503792
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,PEFT 방법 5가지,0.008098688,0.0,0.00809868797659874
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,거대 언어 모델 정의,-0.012419231,0.0,0.012419231235980988
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,기본 경험,-0.02271276,0.0,0.02271275967359543
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,답변 실패,0.21084112,0.0,0.2108411192893982
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,딥러닝,0.03847112,0.0,0.038471121340990067
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,마지막 할 말,-0.029693093,0.0,0.02969309315085411
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,머신러닝,-0.008970631,0.0,0.008970631286501884
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,면접 시작 인사,-0.02766108,0.0,0.027661079540848732
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,상세 경험,-0.009265773,0.0,0.009265772998332977
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,수식,0.8480083,1.0,0.15199172496795654
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,용어 질문,-0.02674857,0.0,0.026748569682240486
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,인공지능,-0.0098304385,0.0,0.009830438531935215
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,잠시 휴식,0.010093873,0.0,0.010093873366713524
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,좋아하는 아이돌,-0.0008775713,0.0,0.0008775712922215462
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,핵심 아이디어,-0.018967487,0.0,0.018967486917972565
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,확률 예측에서 MSE Loss 미 사용 이유,0.007457149,0.0,0.007457149215042591
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",BCE 가 좋은 task,-0.008024478,0.0,0.008024478331208229
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",BCE 가 좋은 이유,-0.0486732,0.0,0.048673201352357864
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",LLM Fine-Tuning 의 PEFT,0.0314124,0.0,0.03141240030527115
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",LoRA,-0.028035035,0.0,0.028035035356879234
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",LoRA 와 QLoRA 의 차이,0.023441637,0.0,0.023441636934876442
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",Loss Function 예시,-0.0088356985,0.0,0.00883569847792387
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",Loss Function 정의,0.020865427,0.0,0.0208654273301363
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",MBTI,0.0036645432,0.0,0.003664543153718114
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",MSE Loss 설명,-0.047002025,0.0,0.047002024948596954
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",MSE Loss 용도,-0.012087581,0.0,0.012087580747902393
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",Multi-Label 에서 CE + Softmax 적용 문제점,-0.031914588,0.0,0.03191458806395531
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",PEFT 방법 5가지,0.027611705,0.0,0.027611704543232918
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",거대 언어 모델 정의,0.010773694,0.0,0.010773694142699242
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",기본 경험,-0.0145112015,0.0,0.014511201530694962
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",답변 실패,0.014168462,0.0,0.01416846178472042
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",딥러닝,-0.07068848,0.0,0.07068847864866257
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",마지막 할 말,0.010173893,0.0,0.010173892602324486
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",머신러닝,0.0315556,0.0,0.031555600464344025
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",면접 시작 인사,-0.029585637,0.0,0.029585637152194977
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",상세 경험,-0.034094438,0.0,0.03409443795681
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",수식,0.049952786,0.0,0.04995278641581535
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",용어 질문,-0.017811015,0.0,0.017811015248298645
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",인공지능,0.012622969,0.0,0.012622969225049019
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",잠시 휴식,0.0006474915,0.0,0.0006474914844147861
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",좋아하는 아이돌,0.0016934004,0.0,0.0016934004379436374
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",핵심 아이디어,0.9215961,1.0,0.07840389013290405
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",확률 예측에서 MSE Loss 미 사용 이유,-0.037385534,0.0,0.03738553449511528
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,BCE 가 좋은 task,-0.04952982,0.0,0.049529820680618286
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,BCE 가 좋은 이유,0.060744558,0.0,0.06074455752968788
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,LLM Fine-Tuning 의 PEFT,-0.017478503,0.0,0.01747850328683853
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,LoRA,0.0066756466,0.0,0.006675646640360355
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,LoRA 와 QLoRA 의 차이,-0.019146692,0.0,0.019146692007780075
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,Loss Function 예시,0.039389085,0.0,0.03938908502459526
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,Loss Function 정의,0.035187636,0.0,0.03518763557076454
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,MBTI,0.045344543,0.0,0.04534454271197319
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,MSE Loss 설명,-0.07000294,0.0,0.07000294327735901
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,MSE Loss 용도,-0.019655604,0.0,0.019655603915452957
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,Multi-Label 에서 CE + Softmax 적용 문제점,0.025313167,0.0,0.02531316690146923
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,PEFT 방법 5가지,0.019705629,0.0,0.019705628976225853
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,거대 언어 모델 정의,-0.00031226472,0.0,0.00031226471764966846
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,기본 경험,-0.007872431,0.0,0.007872430607676506
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,답변 실패,0.5337877,1.0,0.46621227264404297
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,딥러닝,-0.039918836,0.0,0.03991883620619774
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,마지막 할 말,-0.012482856,0.0,0.012482856400310993
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,머신러닝,0.0033659209,0.0,0.0033659208565950394
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,면접 시작 인사,0.022890856,0.0,0.022890856489539146
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,상세 경험,-0.075235404,0.0,0.07523540407419205
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,수식,0.08788158,0.0,0.08788157999515533
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,용어 질문,-0.006428923,0.0,0.006428922992199659
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,인공지능,0.011882761,0.0,0.011882760562002659
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,잠시 휴식,-0.019357441,0.0,0.01935744099318981
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,좋아하는 아이돌,-0.007233659,0.0,0.00723365880548954
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,핵심 아이디어,0.5166836,0.0,0.5166835784912109
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,확률 예측에서 MSE Loss 미 사용 이유,-0.14501698,0.0,0.14501698315143585
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",BCE 가 좋은 task,-0.010251864,0.0,0.010251863859593868
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",BCE 가 좋은 이유,-0.042243004,0.0,0.042243003845214844
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",LLM Fine-Tuning 의 PEFT,0.019788763,0.0,0.01978876255452633
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",LoRA,-0.022659061,0.0,0.02265906147658825
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",LoRA 와 QLoRA 의 차이,0.019473584,0.0,0.01947358436882496
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",Loss Function 예시,-0.012415165,0.0,0.012415165081620216
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",Loss Function 정의,0.02506216,0.0,0.025062160566449165
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",MBTI,-0.007977872,0.0,0.007977872155606747
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",MSE Loss 설명,-0.027745971,0.0,0.027745971456170082
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",MSE Loss 용도,-0.008640812,0.0,0.008640811778604984
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",Multi-Label 에서 CE + Softmax 적용 문제점,-0.0070152394,0.0,0.00701523944735527
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",PEFT 방법 5가지,0.017798562,0.0,0.017798561602830887
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",거대 언어 모델 정의,0.001903124,0.0,0.0019031240371987224
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",기본 경험,-0.029690212,0.0,0.02969021163880825
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",답변 실패,0.02159085,0.0,0.02159084938466549
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",딥러닝,-0.05561698,0.0,0.05561697855591774
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",마지막 할 말,0.0068923268,0.0,0.006892326753586531
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",머신러닝,0.018290987,0.0,0.018290987238287926
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",면접 시작 인사,-0.029057907,0.0,0.029057906940579414
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",상세 경험,-0.041889876,0.0,0.04188987612724304
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",수식,0.03429663,0.0,0.034296631813049316
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",용어 질문,-0.013815352,0.0,0.013815351761877537
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",인공지능,0.018667147,0.0,0.018667146563529968
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",잠시 휴식,0.009486096,0.0,0.009486095979809761
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",좋아하는 아이돌,-0.017011164,0.0,0.017011163756251335
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",핵심 아이디어,0.91556925,1.0,0.0844307541847229
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",확률 예측에서 MSE Loss 미 사용 이유,-0.03826056,0.0,0.0382605604827404
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')]",BCE 가 좋은 task,0.012492797,0.0,0.012492797337472439
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')]",BCE 가 좋은 이유,-0.08728035,0.0,0.08728034794330597
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')]",LLM Fine-Tuning 의 PEFT,0.009979006,0.0,0.0099790059030056
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')]",LoRA,-0.0039035848,0.0,0.003903584787622094
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')]",LoRA 와 QLoRA 의 차이,0.0114208935,0.0,0.011420893482863903
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')]",Loss Function 예시,0.0014801694,0.0,0.0014801693614572287
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')]",Loss Function 정의,-0.059618615,0.0,0.05961861461400986
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')]",MBTI,-0.038956817,0.0,0.038956817239522934
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')]",MSE Loss 설명,-0.003594106,0.0,0.003594106063246727
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')]",MSE Loss 용도,-0.013355047,0.0,0.013355047442018986
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')]",Multi-Label 에서 CE + Softmax 적용 문제점,-0.022652525,0.0,0.022652525454759598
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')]",PEFT 방법 5가지,-0.0012286694,0.0,0.0012286694254726171
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')]",거대 언어 모델 정의,-0.034112062,0.0,0.03411206230521202
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')]",기본 경험,-0.016087538,0.0,0.01608753763139248
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')]",답변 실패,0.44946092,0.0,0.4494609236717224
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')]",딥러닝,-0.027693901,0.0,0.02769390121102333
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')]",마지막 할 말,0.0035683603,0.0,0.003568360349163413
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')]",머신러닝,-0.0242571,0.0,0.024257099255919456
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')]",면접 시작 인사,-0.09908791,0.0,0.0990879088640213
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')]",상세 경험,0.020652581,0.0,0.02065258100628853
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')]",수식,0.29113206,1.0,0.7088679373264313
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')]",용어 질문,-0.037509717,0.0,0.03750971704721451
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')]",인공지능,-0.023445036,0.0,0.02344503626227379
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')]",잠시 휴식,-0.017298736,0.0,0.017298735678195953
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')]",좋아하는 아이돌,-0.013220237,0.0,0.013220236636698246
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')]",핵심 아이디어,0.42809927,0.0,0.42809927463531494
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')]",확률 예측에서 MSE Loss 미 사용 이유,-0.06957572,0.0,0.0695757195353508
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다",BCE 가 좋은 task,-0.007370881,0.0,0.007370880804955959
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다",BCE 가 좋은 이유,-0.047254305,0.0,0.047254305332899094
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다",LLM Fine-Tuning 의 PEFT,0.028599545,0.0,0.02859954535961151
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다",LoRA,-0.03919859,0.0,0.03919858857989311
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다",LoRA 와 QLoRA 의 차이,0.01331482,0.0,0.013314819894731045
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다",Loss Function 예시,0.00020187235,0.0,0.0002018723462242633
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다",Loss Function 정의,0.015063819,0.0,0.015063818544149399
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다",MBTI,3.6888043e-05,0.0,3.68880428140983e-05
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다",MSE Loss 설명,-0.021268146,0.0,0.021268146112561226
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다",MSE Loss 용도,-0.01656315,0.0,0.01656314916908741
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다",Multi-Label 에서 CE + Softmax 적용 문제점,-0.009750846,0.0,0.009750845842063427
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다",PEFT 방법 5가지,0.015974035,0.0,0.01597403548657894
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다",거대 언어 모델 정의,0.027523283,0.0,0.027523282915353775
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다",기본 경험,-0.014790505,0.0,0.014790505170822144
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다",답변 실패,0.0073769377,0.0,0.007376937661319971
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다",딥러닝,-0.069601394,0.0,0.0696013942360878
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다",마지막 할 말,0.0086307265,0.0,0.008630726486444473
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다",머신러닝,0.022170559,0.0,0.02217055857181549
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다",면접 시작 인사,-0.024862994,0.0,0.024862993508577347
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다",상세 경험,-0.028254692,0.0,0.028254691511392593
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다",수식,0.04171387,0.0,0.04171387106180191
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다",용어 질문,-0.020275103,0.0,0.02027510292828083
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다",인공지능,0.0010593433,0.0,0.0010593433398753405
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다",잠시 휴식,-0.0061409613,0.0,0.0061409613117575645
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다",좋아하는 아이돌,-0.014121834,0.0,0.014121834188699722
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다",핵심 아이디어,0.9201651,1.0,0.07983487844467163
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다",확률 예측에서 MSE Loss 미 사용 이유,-0.033675242,0.0,0.033675242215394974
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거,BCE 가 좋은 task,0.001690403,0.0,0.0016904029762372375
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거,BCE 가 좋은 이유,0.008093436,0.0,0.008093436248600483
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거,LLM Fine-Tuning 의 PEFT,-0.018415688,0.0,0.01841568760573864
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거,LoRA,0.053394094,0.0,0.05339409410953522
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거,LoRA 와 QLoRA 의 차이,0.01601932,0.0,0.016019320115447044
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거,Loss Function 예시,0.013956382,0.0,0.013956381939351559
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거,Loss Function 정의,-0.043311127,0.0,0.04331112653017044
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거,MBTI,0.0018045462,0.0,0.001804546220228076
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거,MSE Loss 설명,0.004983488,0.0,0.004983488004654646
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거,MSE Loss 용도,0.0004371948,0.0,0.00043719480163417757
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거,Multi-Label 에서 CE + Softmax 적용 문제점,-0.018983783,0.0,0.018983783200383186
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거,PEFT 방법 5가지,-0.018653404,0.0,0.018653403967618942
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거,거대 언어 모델 정의,-0.025416564,0.0,0.02541656419634819
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거,기본 경험,-0.015107412,0.0,0.015107411891222
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거,답변 실패,0.93345946,1.0,0.06654053926467896
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거,딥러닝,-0.011492728,0.0,0.011492728255689144
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거,마지막 할 말,0.00045484473,0.0,0.00045484473230317235
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거,머신러닝,-0.03027964,0.0,0.03027964010834694
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거,면접 시작 인사,-0.01201668,0.0,0.012016680091619492
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거,상세 경험,-0.010727409,0.0,0.010727409273386002
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거,수식,-0.0013363062,0.0,0.0013363062171265483
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거,용어 질문,-0.0023391352,0.0,0.002339135156944394
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거,인공지능,0.015212165,0.0,0.0152121651917696
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거,잠시 휴식,-0.010878077,0.0,0.010878076776862144
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거,좋아하는 아이돌,0.012062471,0.0,0.012062471359968185
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거,핵심 아이디어,0.07360355,0.0,0.0736035481095314
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거,확률 예측에서 MSE Loss 미 사용 이유,-0.013498281,0.0,0.013498281128704548
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지",BCE 가 좋은 task,-0.020973373,0.0,0.02097337320446968
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지",BCE 가 좋은 이유,-0.021873614,0.0,0.021873613819479942
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지",LLM Fine-Tuning 의 PEFT,0.03423656,0.0,0.03423656150698662
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지",LoRA,-0.0026476402,0.0,0.002647640183568001
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지",LoRA 와 QLoRA 의 차이,0.010533538,0.0,0.010533537715673447
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지",Loss Function 예시,-0.014740474,0.0,0.014740473590791225
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지",Loss Function 정의,0.010333801,0.0,0.010333800688385963
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지",MBTI,-0.0024378665,0.0,0.0024378665257245302
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지",MSE Loss 설명,-0.017723536,0.0,0.017723536118865013
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지",MSE Loss 용도,-0.01580786,0.0,0.0158078595995903
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지",Multi-Label 에서 CE + Softmax 적용 문제점,-0.0044527575,0.0,0.00445275753736496
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지",PEFT 방법 5가지,0.028921861,0.0,0.028921861201524734
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지",거대 언어 모델 정의,0.018526591,0.0,0.018526591360569
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지",기본 경험,-0.008031544,0.0,0.008031544275581837
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지",답변 실패,0.021303663,0.0,0.021303663030266762
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지",딥러닝,-0.051443074,0.0,0.05144307389855385
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지",마지막 할 말,-0.0018256811,0.0,0.0018256810726597905
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지",머신러닝,0.0030293248,0.0,0.0030293248128145933
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지",면접 시작 인사,-0.011771623,0.0,0.01177162304520607
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지",상세 경험,-0.05197113,0.0,0.051971130073070526
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지",수식,0.053883135,0.0,0.053883135318756104
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지",용어 질문,-0.017744897,0.0,0.017744896933436394
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지",인공지능,0.0016262227,0.0,0.001626222743652761
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지",잠시 휴식,-0.010598355,0.0,0.01059835497289896
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지",좋아하는 아이돌,-0.016065331,0.0,0.016065331175923347
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지",핵심 아이디어,0.9130855,1.0,0.08691447973251343
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지",확률 예측에서 MSE Loss 미 사용 이유,-0.06893902,0.0,0.06893902271986008
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야?",BCE 가 좋은 task,0.240941,0.0,0.24094100296497345
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야?",BCE 가 좋은 이유,-0.06912927,0.0,0.06912927329540253
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야?",LLM Fine-Tuning 의 PEFT,-0.01804197,0.0,0.01804197020828724
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야?",LoRA,0.012067574,0.0,0.012067574076354504
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야?",LoRA 와 QLoRA 의 차이,0.0067738066,0.0,0.006773806642740965
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야?",Loss Function 예시,0.0066350517,0.0,0.006635051686316729
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야?",Loss Function 정의,-0.027294675,0.0,0.027294674888253212
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야?",MBTI,-0.015390529,0.0,0.015390529297292233
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야?",MSE Loss 설명,0.012491781,0.0,0.012491781264543533
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야?",MSE Loss 용도,-0.018318104,0.0,0.01831810362637043
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야?",Multi-Label 에서 CE + Softmax 적용 문제점,-0.028584318,0.0,0.028584318235516548
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야?",PEFT 방법 5가지,-0.008685145,0.0,0.00868514459580183
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야?",거대 언어 모델 정의,0.06261649,0.0,0.0626164898276329
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야?",기본 경험,-0.04918325,0.0,0.04918324947357178
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야?",답변 실패,0.63829035,0.0,0.6382903456687927
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야?",딥러닝,0.019796222,0.0,0.019796222448349
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야?",마지막 할 말,-0.026941134,0.0,0.02694113366305828
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야?",머신러닝,-0.014861198,0.0,0.014861198142170906
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야?",면접 시작 인사,-0.006573517,0.0,0.006573516875505447
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야?",상세 경험,-0.02578444,0.0,0.025784440338611603
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야?",수식,0.02645803,0.0,0.026458030566573143
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야?",용어 질문,0.24920405,1.0,0.7507959455251694
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야?",인공지능,-0.0006423493,0.0,0.0006423493032343686
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야?",잠시 휴식,0.020990273,0.0,0.020990272983908653
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야?",좋아하는 아이돌,-0.027611148,0.0,0.027611147612333298
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야?",핵심 아이디어,-0.024515526,0.0,0.024515526369214058
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야?",확률 예측에서 MSE Loss 미 사용 이유,0.010599393,0.0,0.010599393397569656
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",BCE 가 좋은 task,0.14996532,0.0,0.1499653160572052
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",BCE 가 좋은 이유,-0.042116046,0.0,0.04211604595184326
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",LLM Fine-Tuning 의 PEFT,-0.021028288,0.0,0.021028287708759308
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",LoRA,-0.009603624,0.0,0.009603624232113361
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",LoRA 와 QLoRA 의 차이,0.00031499358,0.0,0.0003149935801047832
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",Loss Function 예시,-0.028325535,0.0,0.028325535356998444
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",Loss Function 정의,-0.009742906,0.0,0.00974290631711483
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",MBTI,-0.016950574,0.0,0.016950573772192
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",MSE Loss 설명,0.02207312,0.0,0.02207311987876892
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",MSE Loss 용도,-0.0120370425,0.0,0.012037042528390884
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",Multi-Label 에서 CE + Softmax 적용 문제점,-0.025838256,0.0,0.025838255882263184
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",PEFT 방법 5가지,-0.03385477,0.0,0.03385477140545845
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",거대 언어 모델 정의,-0.021576075,0.0,0.02157607488334179
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",기본 경험,-0.030333702,0.0,0.03033370152115822
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",답변 실패,0.86402154,1.0,0.13597846031188965
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",딥러닝,-0.006825148,0.0,0.00682514812797308
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",마지막 할 말,-0.009151921,0.0,0.009151920676231384
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",머신러닝,-0.018703531,0.0,0.018703531473875046
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",면접 시작 인사,0.0049730893,0.0,0.004973089322447777
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",상세 경험,-0.004175594,0.0,0.004175594076514244
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",수식,-0.0063608317,0.0,0.006360831670463085
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",용어 질문,0.038044408,0.0,0.03804440796375275
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",인공지능,-0.012890494,0.0,0.012890494428575039
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",잠시 휴식,-0.0042262934,0.0,0.0042262934148311615
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",좋아하는 아이돌,-0.029543364,0.0,0.02954336442053318
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",핵심 아이디어,-0.014641118,0.0,0.014641118235886097
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",확률 예측에서 MSE Loss 미 사용 이유,0.013454886,0.0,0.01345488615334034
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",BCE 가 좋은 task,0.05404846,1.0,0.9459515400230885
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",BCE 가 좋은 이유,0.0018554579,0.0,0.0018554579000920057
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",LLM Fine-Tuning 의 PEFT,-0.010565086,0.0,0.010565086267888546
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",LoRA,-0.014315482,0.0,0.01431548222899437
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",LoRA 와 QLoRA 의 차이,0.015648915,0.0,0.015648914501070976
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",Loss Function 예시,0.0014918877,0.0,0.001491887727752328
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",Loss Function 정의,0.012958866,0.0,0.012958865612745285
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",MBTI,-0.012261021,0.0,0.012261020950973034
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",MSE Loss 설명,0.001475417,0.0,0.0014754170551896095
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",MSE Loss 용도,-0.009203256,0.0,0.009203256107866764
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",Multi-Label 에서 CE + Softmax 적용 문제점,-0.023559524,0.0,0.02355952374637127
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",PEFT 방법 5가지,-0.015734434,0.0,0.015734434127807617
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",거대 언어 모델 정의,0.006634925,0.0,0.006634925026446581
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",기본 경험,-0.04078129,0.0,0.04078128933906555
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",답변 실패,0.9357278,0.0,0.9357277750968933
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",딥러닝,-0.011037399,0.0,0.011037399061024189
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",마지막 할 말,-0.009425386,0.0,0.009425385855138302
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",머신러닝,-0.004589888,0.0,0.004589887801557779
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",면접 시작 인사,-0.022159798,0.0,0.022159798070788383
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",상세 경험,-0.010503341,0.0,0.010503341443836689
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",수식,0.0014395359,0.0,0.0014395358739420772
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",용어 질문,0.05114416,0.0,0.05114416033029556
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",인공지능,-0.0036265904,0.0,0.0036265903618186712
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",잠시 휴식,0.0013356504,0.0,0.0013356504496186972
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",좋아하는 아이돌,-0.007854056,0.0,0.007854055613279343
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",핵심 아이디어,-0.0031768798,0.0,0.0031768798362463713
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",확률 예측에서 MSE Loss 미 사용 이유,0.007839302,0.0,0.007839301601052284
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",BCE 가 좋은 task,0.35891035,0.0,0.35891035199165344
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",BCE 가 좋은 이유,0.5858506,1.0,0.4141494035720825
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",LLM Fine-Tuning 의 PEFT,0.014292449,0.0,0.01429244875907898
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",LoRA,0.016310655,0.0,0.01631065458059311
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",LoRA 와 QLoRA 의 차이,-0.02869586,0.0,0.028695860877633095
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",Loss Function 예시,-0.10430392,0.0,0.10430391877889633
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",Loss Function 정의,0.018624986,0.0,0.018624985590577126
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",MBTI,-0.05944599,0.0,0.05944598838686943
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",MSE Loss 설명,-0.024472985,0.0,0.024472985416650772
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",MSE Loss 용도,-0.037575554,0.0,0.037575554102659225
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",Multi-Label 에서 CE + Softmax 적용 문제점,-0.0411901,0.0,0.041190098971128464
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",PEFT 방법 5가지,0.070304796,0.0,0.07030479609966278
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",거대 언어 모델 정의,-0.03086541,0.0,0.030865410342812538
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",기본 경험,0.0057638176,0.0,0.0057638175785541534
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",답변 실패,0.19390258,0.0,0.19390258193016052
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",딥러닝,-0.036767166,0.0,0.03676716610789299
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",마지막 할 말,-0.0069917943,0.0,0.0069917943328619
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",머신러닝,-0.00806086,0.0,0.008060860447585583
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",면접 시작 인사,-0.036822807,0.0,0.03682280704379082
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",상세 경험,-0.07342832,0.0,0.07342831790447235
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",수식,0.030007988,0.0,0.030007988214492798
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",용어 질문,0.18070106,0.0,0.18070106208324432
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",인공지능,-0.006390732,0.0,0.006390731781721115
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",잠시 휴식,-0.009479301,0.0,0.009479301050305367
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",좋아하는 아이돌,0.021388788,0.0,0.021388787776231766
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",핵심 아이디어,-0.04741052,0.0,0.047410521656274796
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",확률 예측에서 MSE Loss 미 사용 이유,-0.030801957,0.0,0.030801957473158836
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,BCE 가 좋은 task,-0.014990256,0.0,0.014990256167948246
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,BCE 가 좋은 이유,-0.010584726,0.0,0.010584725998342037
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,LLM Fine-Tuning 의 PEFT,0.05360247,0.0,0.053602468222379684
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,LoRA,-0.00039119725,0.0,0.0003911972453352064
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,LoRA 와 QLoRA 의 차이,0.0030118115,0.0,0.003011811524629593
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,Loss Function 예시,-0.028975118,0.0,0.028975117951631546
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,Loss Function 정의,0.006162672,0.0,0.006162671837955713
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,MBTI,-0.04675594,0.0,0.04675593972206116
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,MSE Loss 설명,-0.0065853945,0.0,0.006585394497960806
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,MSE Loss 용도,-0.03297814,0.0,0.03297813981771469
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,Multi-Label 에서 CE + Softmax 적용 문제점,0.96745265,1.0,0.03254735469818115
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,PEFT 방법 5가지,0.005157277,0.0,0.00515727698802948
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,거대 언어 모델 정의,-0.038906753,0.0,0.038906753063201904
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,기본 경험,-0.037924953,0.0,0.03792495280504227
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,답변 실패,-0.04072393,0.0,0.040723931044340134
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,딥러닝,0.022753388,0.0,0.022753387689590454
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,마지막 할 말,-0.018519532,0.0,0.018519531935453415
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,머신러닝,-0.037716754,0.0,0.03771675378084183
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,면접 시작 인사,0.009449234,0.0,0.00944923423230648
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,상세 경험,-0.052361906,0.0,0.052361905574798584
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,수식,0.0070378482,0.0,0.007037848234176636
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,용어 질문,-0.011497378,0.0,0.0114973783493042
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,인공지능,-0.01762923,0.0,0.01762923039495945
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,잠시 휴식,-0.0054582604,0.0,0.005458260420709848
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,좋아하는 아이돌,-0.04580457,0.0,0.045804571360349655
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,핵심 아이디어,-0.019022064,0.0,0.01902206428349018
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,확률 예측에서 MSE Loss 미 사용 이유,0.043579254,0.0,0.04357925429940224
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,BCE 가 좋은 task,0.015628519,0.0,0.015628518536686897
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,BCE 가 좋은 이유,0.0076052216,0.0,0.007605221588164568
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,LLM Fine-Tuning 의 PEFT,0.014040721,0.0,0.014040720649063587
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,LoRA,0.004430009,0.0,0.004430009052157402
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,LoRA 와 QLoRA 의 차이,-0.004796782,0.0,0.0047967820428311825
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,Loss Function 예시,-0.015168617,0.0,0.01516861654818058
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,Loss Function 정의,-0.016148306,0.0,0.01614830642938614
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,MBTI,-0.010379009,0.0,0.010379008948802948
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,MSE Loss 설명,-0.0047746543,0.0,0.004774654284119606
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,MSE Loss 용도,-0.02541351,0.0,0.02541350945830345
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,Multi-Label 에서 CE + Softmax 적용 문제점,0.020624122,0.0,0.02062412165105343
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,PEFT 방법 5가지,0.017132368,0.0,0.017132367938756943
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,거대 언어 모델 정의,-0.0089125885,0.0,0.008912588469684124
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,기본 경험,-0.03408945,0.0,0.03408944979310036
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,답변 실패,0.96344894,1.0,0.036551058292388916
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,딥러닝,-0.0063435035,0.0,0.00634350348263979
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,마지막 할 말,-0.0026004813,0.0,0.002600481268018484
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,머신러닝,-0.0069981366,0.0,0.006998136639595032
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,면접 시작 인사,-0.016440362,0.0,0.016440361738204956
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,상세 경험,-0.01595624,0.0,0.01595623977482319
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,수식,-0.022962995,0.0,0.022962994873523712
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,용어 질문,-0.014486231,0.0,0.014486230909824371
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,인공지능,-0.009125193,0.0,0.009125192649662495
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,잠시 휴식,-0.0021472794,0.0,0.0021472794469445944
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,좋아하는 아이돌,0.0006481834,0.0,0.0006481833988800645
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,핵심 아이디어,0.0060867886,0.0,0.006086788605898619
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,확률 예측에서 MSE Loss 미 사용 이유,-0.008842249,0.0,0.008842249400913715
Loss Function 관련 실무 경험 -> [기본 경험] 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,BCE 가 좋은 task,-7.324535e-05,0.0,7.32453481759876e-05
Loss Function 관련 실무 경험 -> [기본 경험] 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,BCE 가 좋은 이유,-0.005126702,0.0,0.005126702133566141
Loss Function 관련 실무 경험 -> [기본 경험] 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,LLM Fine-Tuning 의 PEFT,-0.030639688,0.0,0.030639687553048134
Loss Function 관련 실무 경험 -> [기본 경험] 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,LoRA,-0.016826699,0.0,0.016826698556542397
Loss Function 관련 실무 경험 -> [기본 경험] 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,LoRA 와 QLoRA 의 차이,-0.009672474,0.0,0.00967247411608696
Loss Function 관련 실무 경험 -> [기본 경험] 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,Loss Function 예시,-0.014609494,0.0,0.014609494246542454
Loss Function 관련 실무 경험 -> [기본 경험] 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,Loss Function 정의,0.034798004,0.0,0.03479800373315811
Loss Function 관련 실무 경험 -> [기본 경험] 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,MBTI,0.014285714,0.0,0.014285714365541935
Loss Function 관련 실무 경험 -> [기본 경험] 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,MSE Loss 설명,-0.0069300267,0.0,0.006930026691406965
Loss Function 관련 실무 경험 -> [기본 경험] 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,MSE Loss 용도,0.008917853,0.0,0.008917853236198425
Loss Function 관련 실무 경험 -> [기본 경험] 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,Multi-Label 에서 CE + Softmax 적용 문제점,-0.01893928,0.0,0.01893928088247776
Loss Function 관련 실무 경험 -> [기본 경험] 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,PEFT 방법 5가지,-0.0101405205,0.0,0.01014052052050829
Loss Function 관련 실무 경험 -> [기본 경험] 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,거대 언어 모델 정의,0.013945809,0.0,0.01394580863416195
Loss Function 관련 실무 경험 -> [기본 경험] 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,기본 경험,0.9343101,1.0,0.06568992137908936
Loss Function 관련 실무 경험 -> [기본 경험] 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,답변 실패,0.024643917,0.0,0.0246439166367054
Loss Function 관련 실무 경험 -> [기본 경험] 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,딥러닝,0.0062997835,0.0,0.0062997834756970406
Loss Function 관련 실무 경험 -> [기본 경험] 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,마지막 할 말,0.000273584,0.0,0.0002735840098466724
Loss Function 관련 실무 경험 -> [기본 경험] 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,머신러닝,-0.043054856,0.0,0.04305485635995865
Loss Function 관련 실무 경험 -> [기본 경험] 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,면접 시작 인사,-0.014026793,0.0,0.014026792719960213
Loss Function 관련 실무 경험 -> [기본 경험] 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,상세 경험,-0.03168714,0.0,0.031687140464782715
Loss Function 관련 실무 경험 -> [기본 경험] 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,수식,-0.0029872356,0.0,0.002987235551699996
Loss Function 관련 실무 경험 -> [기본 경험] 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,용어 질문,-0.063521855,0.0,0.0635218545794487
Loss Function 관련 실무 경험 -> [기본 경험] 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,인공지능,-0.007287744,0.0,0.007287743967026472
Loss Function 관련 실무 경험 -> [기본 경험] 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,잠시 휴식,-0.0071334713,0.0,0.007133471313863993
Loss Function 관련 실무 경험 -> [기본 경험] 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,좋아하는 아이돌,0.009838359,0.0,0.00983835943043232
Loss Function 관련 실무 경험 -> [기본 경험] 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,핵심 아이디어,0.027101489,0.0,0.027101488783955574
Loss Function 관련 실무 경험 -> [기본 경험] 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,확률 예측에서 MSE Loss 미 사용 이유,0.005538684,0.0,0.0055386838503181934
Loss Function 관련 실무 경험 -> [상세 경험] 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,BCE 가 좋은 task,0.037173238,0.0,0.03717323765158653
Loss Function 관련 실무 경험 -> [상세 경험] 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,BCE 가 좋은 이유,-0.016997168,0.0,0.016997167840600014
Loss Function 관련 실무 경험 -> [상세 경험] 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,LLM Fine-Tuning 의 PEFT,-0.017452214,0.0,0.017452213913202286
Loss Function 관련 실무 경험 -> [상세 경험] 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,LoRA,-0.035341762,0.0,0.035341762006282806
Loss Function 관련 실무 경험 -> [상세 경험] 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,LoRA 와 QLoRA 의 차이,0.01649065,0.0,0.016490649431943893
Loss Function 관련 실무 경험 -> [상세 경험] 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,Loss Function 예시,-0.0071544717,0.0,0.007154471706598997
Loss Function 관련 실무 경험 -> [상세 경험] 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,Loss Function 정의,-0.0031817546,0.0,0.0031817546114325523
Loss Function 관련 실무 경험 -> [상세 경험] 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,MBTI,0.0032714303,0.0,0.003271430265158415
Loss Function 관련 실무 경험 -> [상세 경험] 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,MSE Loss 설명,-0.020701053,0.0,0.020701052621006966
Loss Function 관련 실무 경험 -> [상세 경험] 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,MSE Loss 용도,-0.021875203,0.0,0.021875202655792236
Loss Function 관련 실무 경험 -> [상세 경험] 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,Multi-Label 에서 CE + Softmax 적용 문제점,-0.032881387,0.0,0.03288138657808304
Loss Function 관련 실무 경험 -> [상세 경험] 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,PEFT 방법 5가지,-0.0020107238,0.0,0.0020107238087803125
Loss Function 관련 실무 경험 -> [상세 경험] 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,거대 언어 모델 정의,-0.0020387021,0.0,0.0020387021359056234
Loss Function 관련 실무 경험 -> [상세 경험] 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,기본 경험,-0.038643192,0.0,0.03864319249987602
Loss Function 관련 실무 경험 -> [상세 경험] 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,답변 실패,0.01848743,0.0,0.01848742924630642
Loss Function 관련 실무 경험 -> [상세 경험] 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,딥러닝,-0.020634808,0.0,0.020634807646274567
Loss Function 관련 실무 경험 -> [상세 경험] 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,마지막 할 말,-0.018361732,0.0,0.018361732363700867
Loss Function 관련 실무 경험 -> [상세 경험] 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,머신러닝,-0.032881115,0.0,0.03288111463189125
Loss Function 관련 실무 경험 -> [상세 경험] 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,면접 시작 인사,-0.0028765213,0.0,0.00287652132101357
Loss Function 관련 실무 경험 -> [상세 경험] 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,상세 경험,0.93442154,1.0,0.06557846069335938
Loss Function 관련 실무 경험 -> [상세 경험] 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,수식,-0.044811968,0.0,0.04481196776032448
Loss Function 관련 실무 경험 -> [상세 경험] 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,용어 질문,-0.03500922,0.0,0.035009220242500305
Loss Function 관련 실무 경험 -> [상세 경험] 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,인공지능,0.0039512506,0.0,0.003951250575482845
Loss Function 관련 실무 경험 -> [상세 경험] 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,잠시 휴식,-0.0038811506,0.0,0.003881150623783469
Loss Function 관련 실무 경험 -> [상세 경험] 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,좋아하는 아이돌,0.005443284,0.0,0.005443283822387457
Loss Function 관련 실무 경험 -> [상세 경험] 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,핵심 아이디어,-0.009273183,0.0,0.009273182600736618
Loss Function 관련 실무 경험 -> [상세 경험] 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,확률 예측에서 MSE Loss 미 사용 이유,-0.032669872,0.0,0.03266987204551697
Loss Function 관련 실무 경험 -> [상세 경험] 근데 기억 진짜 안나는데 어떡하지?,BCE 가 좋은 task,-0.004759466,0.0,0.0047594658099114895
Loss Function 관련 실무 경험 -> [상세 경험] 근데 기억 진짜 안나는데 어떡하지?,BCE 가 좋은 이유,0.014750608,0.0,0.014750608243048191
Loss Function 관련 실무 경험 -> [상세 경험] 근데 기억 진짜 안나는데 어떡하지?,LLM Fine-Tuning 의 PEFT,-0.004116914,0.0,0.004116913769394159
Loss Function 관련 실무 경험 -> [상세 경험] 근데 기억 진짜 안나는데 어떡하지?,LoRA,0.0035142277,0.0,0.0035142276901751757
Loss Function 관련 실무 경험 -> [상세 경험] 근데 기억 진짜 안나는데 어떡하지?,LoRA 와 QLoRA 의 차이,-0.0023823166,0.0,0.0023823166266083717
Loss Function 관련 실무 경험 -> [상세 경험] 근데 기억 진짜 안나는데 어떡하지?,Loss Function 예시,-0.0077193,0.0,0.0077193002216517925
Loss Function 관련 실무 경험 -> [상세 경험] 근데 기억 진짜 안나는데 어떡하지?,Loss Function 정의,0.00073794654,0.0,0.0007379465387202799
Loss Function 관련 실무 경험 -> [상세 경험] 근데 기억 진짜 안나는데 어떡하지?,MBTI,7.846752e-05,0.0,7.846752123441547e-05
Loss Function 관련 실무 경험 -> [상세 경험] 근데 기억 진짜 안나는데 어떡하지?,MSE Loss 설명,-0.000810997,0.0,0.0008109969785436988
Loss Function 관련 실무 경험 -> [상세 경험] 근데 기억 진짜 안나는데 어떡하지?,MSE Loss 용도,-0.0068322783,0.0,0.006832278333604336
Loss Function 관련 실무 경험 -> [상세 경험] 근데 기억 진짜 안나는데 어떡하지?,Multi-Label 에서 CE + Softmax 적용 문제점,-0.033511657,0.0,0.033511657267808914
Loss Function 관련 실무 경험 -> [상세 경험] 근데 기억 진짜 안나는데 어떡하지?,PEFT 방법 5가지,-0.006734129,0.0,0.006734129041433334
Loss Function 관련 실무 경험 -> [상세 경험] 근데 기억 진짜 안나는데 어떡하지?,거대 언어 모델 정의,-0.001987442,0.0,0.001987441908568144
Loss Function 관련 실무 경험 -> [상세 경험] 근데 기억 진짜 안나는데 어떡하지?,기본 경험,-0.017952736,0.0,0.01795273646712303
Loss Function 관련 실무 경험 -> [상세 경험] 근데 기억 진짜 안나는데 어떡하지?,답변 실패,0.9780115,1.0,0.021988511085510254
Loss Function 관련 실무 경험 -> [상세 경험] 근데 기억 진짜 안나는데 어떡하지?,딥러닝,-0.012302361,0.0,0.01230236142873764
Loss Function 관련 실무 경험 -> [상세 경험] 근데 기억 진짜 안나는데 어떡하지?,마지막 할 말,-0.0050199465,0.0,0.005019946489483118
Loss Function 관련 실무 경험 -> [상세 경험] 근데 기억 진짜 안나는데 어떡하지?,머신러닝,-0.01492001,0.0,0.014920010231435299
Loss Function 관련 실무 경험 -> [상세 경험] 근데 기억 진짜 안나는데 어떡하지?,면접 시작 인사,-0.023994537,0.0,0.023994537070393562
Loss Function 관련 실무 경험 -> [상세 경험] 근데 기억 진짜 안나는데 어떡하지?,상세 경험,0.06667696,0.0,0.06667695939540863
Loss Function 관련 실무 경험 -> [상세 경험] 근데 기억 진짜 안나는데 어떡하지?,수식,-0.021682441,0.0,0.021682441234588623
Loss Function 관련 실무 경험 -> [상세 경험] 근데 기억 진짜 안나는데 어떡하지?,용어 질문,0.003194617,0.0,0.0031946171075105667
Loss Function 관련 실무 경험 -> [상세 경험] 근데 기억 진짜 안나는데 어떡하지?,인공지능,-0.018220622,0.0,0.018220622092485428
Loss Function 관련 실무 경험 -> [상세 경험] 근데 기억 진짜 안나는데 어떡하지?,잠시 휴식,-0.01803784,0.0,0.018037840723991394
Loss Function 관련 실무 경험 -> [상세 경험] 근데 기억 진짜 안나는데 어떡하지?,좋아하는 아이돌,0.012340347,0.0,0.012340347282588482
Loss Function 관련 실무 경험 -> [상세 경험] 근데 기억 진짜 안나는데 어떡하지?,핵심 아이디어,0.0043653785,0.0,0.004365378525108099
Loss Function 관련 실무 경험 -> [상세 경험] 근데 기억 진짜 안나는데 어떡하지?,확률 예측에서 MSE Loss 미 사용 이유,-0.0012999405,0.0,0.0012999405153095722
Loss Function 관련 실무 경험 -> [기본 경험] 나 아직 실무 경험 없는데 ㅠㅠ,BCE 가 좋은 task,-0.016766692,0.0,0.016766691580414772
Loss Function 관련 실무 경험 -> [기본 경험] 나 아직 실무 경험 없는데 ㅠㅠ,BCE 가 좋은 이유,-0.02807587,0.0,0.028075870126485825
Loss Function 관련 실무 경험 -> [기본 경험] 나 아직 실무 경험 없는데 ㅠㅠ,LLM Fine-Tuning 의 PEFT,-0.017443443,0.0,0.017443442717194557
Loss Function 관련 실무 경험 -> [기본 경험] 나 아직 실무 경험 없는데 ㅠㅠ,LoRA,-0.013968794,0.0,0.01396879367530346
Loss Function 관련 실무 경험 -> [기본 경험] 나 아직 실무 경험 없는데 ㅠㅠ,LoRA 와 QLoRA 의 차이,0.0040248493,0.0,0.004024849273264408
Loss Function 관련 실무 경험 -> [기본 경험] 나 아직 실무 경험 없는데 ㅠㅠ,Loss Function 예시,-0.025994262,0.0,0.025994261726737022
Loss Function 관련 실무 경험 -> [기본 경험] 나 아직 실무 경험 없는데 ㅠㅠ,Loss Function 정의,-0.021536997,0.0,0.021536996588110924
Loss Function 관련 실무 경험 -> [기본 경험] 나 아직 실무 경험 없는데 ㅠㅠ,MBTI,0.020999627,0.0,0.02099962718784809
Loss Function 관련 실무 경험 -> [기본 경험] 나 아직 실무 경험 없는데 ㅠㅠ,MSE Loss 설명,0.011380053,0.0,0.011380053125321865
Loss Function 관련 실무 경험 -> [기본 경험] 나 아직 실무 경험 없는데 ㅠㅠ,MSE Loss 용도,0.0026501669,0.0,0.0026501668617129326
Loss Function 관련 실무 경험 -> [기본 경험] 나 아직 실무 경험 없는데 ㅠㅠ,Multi-Label 에서 CE + Softmax 적용 문제점,-0.03422579,0.0,0.034225791692733765
Loss Function 관련 실무 경험 -> [기본 경험] 나 아직 실무 경험 없는데 ㅠㅠ,PEFT 방법 5가지,-0.02003614,0.0,0.020036140456795692
Loss Function 관련 실무 경험 -> [기본 경험] 나 아직 실무 경험 없는데 ㅠㅠ,거대 언어 모델 정의,0.032421004,0.0,0.03242100402712822
Loss Function 관련 실무 경험 -> [기본 경험] 나 아직 실무 경험 없는데 ㅠㅠ,기본 경험,0.92151994,1.0,0.078480064868927
Loss Function 관련 실무 경험 -> [기본 경험] 나 아직 실무 경험 없는데 ㅠㅠ,답변 실패,0.15006147,0.0,0.1500614732503891
Loss Function 관련 실무 경험 -> [기본 경험] 나 아직 실무 경험 없는데 ㅠㅠ,딥러닝,0.009936932,0.0,0.009936931543052197
Loss Function 관련 실무 경험 -> [기본 경험] 나 아직 실무 경험 없는데 ㅠㅠ,마지막 할 말,0.003358345,0.0,0.0033583450131118298
Loss Function 관련 실무 경험 -> [기본 경험] 나 아직 실무 경험 없는데 ㅠㅠ,머신러닝,-0.06938163,0.0,0.06938163191080093
Loss Function 관련 실무 경험 -> [기본 경험] 나 아직 실무 경험 없는데 ㅠㅠ,면접 시작 인사,0.025725998,0.0,0.025725997984409332
Loss Function 관련 실무 경험 -> [기본 경험] 나 아직 실무 경험 없는데 ㅠㅠ,상세 경험,0.011324934,0.0,0.011324933730065823
Loss Function 관련 실무 경험 -> [기본 경험] 나 아직 실무 경험 없는데 ㅠㅠ,수식,-0.0017636046,0.0,0.0017636045813560486
Loss Function 관련 실무 경험 -> [기본 경험] 나 아직 실무 경험 없는데 ㅠㅠ,용어 질문,-0.028598038,0.0,0.028598038479685783
Loss Function 관련 실무 경험 -> [기본 경험] 나 아직 실무 경험 없는데 ㅠㅠ,인공지능,0.0010037931,0.0,0.0010037930915132165
Loss Function 관련 실무 경험 -> [기본 경험] 나 아직 실무 경험 없는데 ㅠㅠ,잠시 휴식,0.027584262,0.0,0.027584262192249298
Loss Function 관련 실무 경험 -> [기본 경험] 나 아직 실무 경험 없는데 ㅠㅠ,좋아하는 아이돌,0.029606272,0.0,0.029606271535158157
Loss Function 관련 실무 경험 -> [기본 경험] 나 아직 실무 경험 없는데 ㅠㅠ,핵심 아이디어,-0.0064116013,0.0,0.006411601323634386
Loss Function 관련 실무 경험 -> [기본 경험] 나 아직 실무 경험 없는데 ㅠㅠ,확률 예측에서 MSE Loss 미 사용 이유,-0.021215532,0.0,0.0212155319750309
Loss Function 관련 실무 경험 -> [기본 경험] 대답하기 싫은데 힝,BCE 가 좋은 task,0.0018223736,0.0,0.0018223735969513655
Loss Function 관련 실무 경험 -> [기본 경험] 대답하기 싫은데 힝,BCE 가 좋은 이유,0.018449549,0.0,0.01844954863190651
Loss Function 관련 실무 경험 -> [기본 경험] 대답하기 싫은데 힝,LLM Fine-Tuning 의 PEFT,-0.012678561,0.0,0.012678560800850391
Loss Function 관련 실무 경험 -> [기본 경험] 대답하기 싫은데 힝,LoRA,0.0018641074,0.0,0.0018641074420884252
Loss Function 관련 실무 경험 -> [기본 경험] 대답하기 싫은데 힝,LoRA 와 QLoRA 의 차이,-0.007878934,0.0,0.007878934033215046
Loss Function 관련 실무 경험 -> [기본 경험] 대답하기 싫은데 힝,Loss Function 예시,-0.022904519,0.0,0.022904518991708755
Loss Function 관련 실무 경험 -> [기본 경험] 대답하기 싫은데 힝,Loss Function 정의,-0.008619108,0.0,0.008619108237326145
Loss Function 관련 실무 경험 -> [기본 경험] 대답하기 싫은데 힝,MBTI,0.00012971862,0.0,0.00012971862452104688
Loss Function 관련 실무 경험 -> [기본 경험] 대답하기 싫은데 힝,MSE Loss 설명,0.012407854,0.0,0.012407854199409485
Loss Function 관련 실무 경험 -> [기본 경험] 대답하기 싫은데 힝,MSE Loss 용도,-0.011632571,0.0,0.011632570996880531
Loss Function 관련 실무 경험 -> [기본 경험] 대답하기 싫은데 힝,Multi-Label 에서 CE + Softmax 적용 문제점,-0.030427251,0.0,0.030427251011133194
Loss Function 관련 실무 경험 -> [기본 경험] 대답하기 싫은데 힝,PEFT 방법 5가지,-0.012463461,0.0,0.012463460676372051
Loss Function 관련 실무 경험 -> [기본 경험] 대답하기 싫은데 힝,거대 언어 모델 정의,-0.007518585,0.0,0.007518584839999676
Loss Function 관련 실무 경험 -> [기본 경험] 대답하기 싫은데 힝,기본 경험,0.10873555,0.0,0.10873554646968842
Loss Function 관련 실무 경험 -> [기본 경험] 대답하기 싫은데 힝,답변 실패,0.9700164,1.0,0.029983580112457275
Loss Function 관련 실무 경험 -> [기본 경험] 대답하기 싫은데 힝,딥러닝,-0.008967828,0.0,0.008967828005552292
Loss Function 관련 실무 경험 -> [기본 경험] 대답하기 싫은데 힝,마지막 할 말,-0.017593427,0.0,0.017593426629900932
Loss Function 관련 실무 경험 -> [기본 경험] 대답하기 싫은데 힝,머신러닝,-0.02813661,0.0,0.028136609122157097
Loss Function 관련 실무 경험 -> [기본 경험] 대답하기 싫은데 힝,면접 시작 인사,0.001545575,0.0,0.0015455749817192554
Loss Function 관련 실무 경험 -> [기본 경험] 대답하기 싫은데 힝,상세 경험,0.004918189,0.0,0.004918188787996769
Loss Function 관련 실무 경험 -> [기본 경험] 대답하기 싫은데 힝,수식,-0.008945832,0.0,0.008945832028985023
Loss Function 관련 실무 경험 -> [기본 경험] 대답하기 싫은데 힝,용어 질문,-0.0056992536,0.0,0.005699253641068935
Loss Function 관련 실무 경험 -> [기본 경험] 대답하기 싫은데 힝,인공지능,-0.012097618,0.0,0.012097617611289024
Loss Function 관련 실무 경험 -> [기본 경험] 대답하기 싫은데 힝,잠시 휴식,0.0008648522,0.0,0.0008648522198200226
Loss Function 관련 실무 경험 -> [기본 경험] 대답하기 싫은데 힝,좋아하는 아이돌,0.01176775,0.0,0.011767749674618244
Loss Function 관련 실무 경험 -> [기본 경험] 대답하기 싫은데 힝,핵심 아이디어,0.011103262,0.0,0.011103262193500996
Loss Function 관련 실무 경험 -> [기본 경험] 대답하기 싫은데 힝,확률 예측에서 MSE Loss 미 사용 이유,-0.0057510417,0.0,0.005751041695475578
Loss Function 관련 실무 경험 -> [기본 경험] 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ,BCE 가 좋은 task,-0.006439156,0.0,0.006439155898988247
Loss Function 관련 실무 경험 -> [기본 경험] 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ,BCE 가 좋은 이유,-0.030524217,0.0,0.03052421659231186
Loss Function 관련 실무 경험 -> [기본 경험] 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ,LLM Fine-Tuning 의 PEFT,-0.024987804,0.0,0.024987803772091866
Loss Function 관련 실무 경험 -> [기본 경험] 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ,LoRA,-0.020694999,0.0,0.020694999024271965
Loss Function 관련 실무 경험 -> [기본 경험] 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ,LoRA 와 QLoRA 의 차이,-0.010180505,0.0,0.010180504992604256
Loss Function 관련 실무 경험 -> [기본 경험] 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ,Loss Function 예시,-0.005487118,0.0,0.005487117916345596
Loss Function 관련 실무 경험 -> [기본 경험] 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ,Loss Function 정의,0.0064030048,0.0,0.006403004750609398
Loss Function 관련 실무 경험 -> [기본 경험] 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ,MBTI,0.015205567,0.0,0.01520556677132845
Loss Function 관련 실무 경험 -> [기본 경험] 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ,MSE Loss 설명,-0.019607557,0.0,0.019607556983828545
Loss Function 관련 실무 경험 -> [기본 경험] 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ,MSE Loss 용도,0.010267034,0.0,0.01026703417301178
Loss Function 관련 실무 경험 -> [기본 경험] 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ,Multi-Label 에서 CE + Softmax 적용 문제점,-0.020044282,0.0,0.02004428207874298
Loss Function 관련 실무 경험 -> [기본 경험] 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ,PEFT 방법 5가지,-0.00094952586,0.0,0.0009495258564129472
Loss Function 관련 실무 경험 -> [기본 경험] 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ,거대 언어 모델 정의,0.020647692,0.0,0.020647691562771797
Loss Function 관련 실무 경험 -> [기본 경험] 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ,기본 경험,0.94186586,1.0,0.05813413858413696
Loss Function 관련 실무 경험 -> [기본 경험] 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ,답변 실패,0.04289712,0.0,0.042897120118141174
Loss Function 관련 실무 경험 -> [기본 경험] 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ,딥러닝,0.004555205,0.0,0.004555204883217812
Loss Function 관련 실무 경험 -> [기본 경험] 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ,마지막 할 말,0.0032925943,0.0,0.003292594337835908
Loss Function 관련 실무 경험 -> [기본 경험] 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ,머신러닝,-0.05077905,0.0,0.05077904835343361
Loss Function 관련 실무 경험 -> [기본 경험] 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ,면접 시작 인사,-0.011159536,0.0,0.011159536428749561
Loss Function 관련 실무 경험 -> [기본 경험] 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ,상세 경험,-0.017261239,0.0,0.017261238768696785
Loss Function 관련 실무 경험 -> [기본 경험] 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ,수식,0.00031654254,0.0,0.00031654254416935146
Loss Function 관련 실무 경험 -> [기본 경험] 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ,용어 질문,-0.047557272,0.0,0.047557272017002106
Loss Function 관련 실무 경험 -> [기본 경험] 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ,인공지능,-0.005976128,0.0,0.005976127926260233
Loss Function 관련 실무 경험 -> [기본 경험] 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ,잠시 휴식,0.01472028,0.0,0.014720279723405838
Loss Function 관련 실무 경험 -> [기본 경험] 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ,좋아하는 아이돌,0.01590219,0.0,0.015902189537882805
Loss Function 관련 실무 경험 -> [기본 경험] 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ,핵심 아이디어,0.013672717,0.0,0.013672716915607452
Loss Function 관련 실무 경험 -> [기본 경험] 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ,확률 예측에서 MSE Loss 미 사용 이유,0.001152142,0.0,0.0011521419510245323
Loss Function 관련 실무 경험 -> [상세 경험] 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어,BCE 가 좋은 task,-0.0006427049,0.0,0.0006427048938348889
Loss Function 관련 실무 경험 -> [상세 경험] 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어,BCE 가 좋은 이유,0.022615,0.0,0.02261500060558319
Loss Function 관련 실무 경험 -> [상세 경험] 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어,LLM Fine-Tuning 의 PEFT,-0.026695164,0.0,0.026695163920521736
Loss Function 관련 실무 경험 -> [상세 경험] 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어,LoRA,-0.047581088,0.0,0.04758108779788017
Loss Function 관련 실무 경험 -> [상세 경험] 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어,LoRA 와 QLoRA 의 차이,0.01673655,0.0,0.016736550256609917
Loss Function 관련 실무 경험 -> [상세 경험] 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어,Loss Function 예시,-0.016932366,0.0,0.01693236641585827
Loss Function 관련 실무 경험 -> [상세 경험] 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어,Loss Function 정의,0.006161923,0.0,0.006161923054605722
Loss Function 관련 실무 경험 -> [상세 경험] 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어,MBTI,-0.004074701,0.0,0.0040747011080384254
Loss Function 관련 실무 경험 -> [상세 경험] 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어,MSE Loss 설명,-0.012572125,0.0,0.012572124600410461
Loss Function 관련 실무 경험 -> [상세 경험] 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어,MSE Loss 용도,-0.016304532,0.0,0.016304532065987587
Loss Function 관련 실무 경험 -> [상세 경험] 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어,Multi-Label 에서 CE + Softmax 적용 문제점,-0.032087624,0.0,0.032087624073028564
Loss Function 관련 실무 경험 -> [상세 경험] 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어,PEFT 방법 5가지,0.00048096868,0.0,0.000480968679767102
Loss Function 관련 실무 경험 -> [상세 경험] 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어,거대 언어 모델 정의,-0.010871385,0.0,0.010871385224163532
Loss Function 관련 실무 경험 -> [상세 경험] 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어,기본 경험,-0.06893845,0.0,0.06893844902515411
Loss Function 관련 실무 경험 -> [상세 경험] 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어,답변 실패,-0.008331332,0.0,0.008331332355737686
Loss Function 관련 실무 경험 -> [상세 경험] 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어,딥러닝,-0.010017945,0.0,0.010017945431172848
Loss Function 관련 실무 경험 -> [상세 경험] 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어,마지막 할 말,-0.03518056,0.0,0.03518056124448776
Loss Function 관련 실무 경험 -> [상세 경험] 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어,머신러닝,-0.035625968,0.0,0.035625968128442764
Loss Function 관련 실무 경험 -> [상세 경험] 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어,면접 시작 인사,-0.014924047,0.0,0.014924046583473682
Loss Function 관련 실무 경험 -> [상세 경험] 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어,상세 경험,0.943038,1.0,0.056962013244628906
Loss Function 관련 실무 경험 -> [상세 경험] 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어,수식,-0.028198121,0.0,0.0281981211155653
Loss Function 관련 실무 경험 -> [상세 경험] 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어,용어 질문,-0.045745786,0.0,0.045745786279439926
Loss Function 관련 실무 경험 -> [상세 경험] 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어,인공지능,0.0034420474,0.0,0.0034420473966747522
Loss Function 관련 실무 경험 -> [상세 경험] 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어,잠시 휴식,-0.0070178364,0.0,0.007017836440354586
Loss Function 관련 실무 경험 -> [상세 경험] 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어,좋아하는 아이돌,-0.0075988327,0.0,0.007598832715302706
Loss Function 관련 실무 경험 -> [상세 경험] 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어,핵심 아이디어,-0.0009616357,0.0,0.0009616357274353504
Loss Function 관련 실무 경험 -> [상세 경험] 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어,확률 예측에서 MSE Loss 미 사용 이유,-0.02781176,0.0,0.02781176008284092
Loss Function 관련 실무 경험 -> [상세 경험] 기억 진짜 안 나는데 가물가물하다,BCE 가 좋은 task,-0.0047878413,0.0,0.004787841346114874
Loss Function 관련 실무 경험 -> [상세 경험] 기억 진짜 안 나는데 가물가물하다,BCE 가 좋은 이유,0.017933294,0.0,0.01793329417705536
Loss Function 관련 실무 경험 -> [상세 경험] 기억 진짜 안 나는데 가물가물하다,LLM Fine-Tuning 의 PEFT,-0.0031513763,0.0,0.0031513762660324574
Loss Function 관련 실무 경험 -> [상세 경험] 기억 진짜 안 나는데 가물가물하다,LoRA,-0.0027851593,0.0,0.0027851592749357224
Loss Function 관련 실무 경험 -> [상세 경험] 기억 진짜 안 나는데 가물가물하다,LoRA 와 QLoRA 의 차이,-0.0032715166,0.0,0.0032715166453272104
Loss Function 관련 실무 경험 -> [상세 경험] 기억 진짜 안 나는데 가물가물하다,Loss Function 예시,-0.0048235343,0.0,0.004823534283787012
Loss Function 관련 실무 경험 -> [상세 경험] 기억 진짜 안 나는데 가물가물하다,Loss Function 정의,-0.0040807063,0.0,0.004080706275999546
Loss Function 관련 실무 경험 -> [상세 경험] 기억 진짜 안 나는데 가물가물하다,MBTI,0.0033452732,0.0,0.0033452732022851706
Loss Function 관련 실무 경험 -> [상세 경험] 기억 진짜 안 나는데 가물가물하다,MSE Loss 설명,-0.0040615383,0.0,0.004061538260430098
Loss Function 관련 실무 경험 -> [상세 경험] 기억 진짜 안 나는데 가물가물하다,MSE Loss 용도,-0.012044858,0.0,0.012044858187437057
Loss Function 관련 실무 경험 -> [상세 경험] 기억 진짜 안 나는데 가물가물하다,Multi-Label 에서 CE + Softmax 적용 문제점,-0.026647307,0.0,0.026647306978702545
Loss Function 관련 실무 경험 -> [상세 경험] 기억 진짜 안 나는데 가물가물하다,PEFT 방법 5가지,-0.012244865,0.0,0.01224486529827118
Loss Function 관련 실무 경험 -> [상세 경험] 기억 진짜 안 나는데 가물가물하다,거대 언어 모델 정의,0.005030279,0.0,0.0050302790477871895
Loss Function 관련 실무 경험 -> [상세 경험] 기억 진짜 안 나는데 가물가물하다,기본 경험,-0.0102141285,0.0,0.010214128531515598
Loss Function 관련 실무 경험 -> [상세 경험] 기억 진짜 안 나는데 가물가물하다,답변 실패,0.9724168,1.0,0.027583181858062744
Loss Function 관련 실무 경험 -> [상세 경험] 기억 진짜 안 나는데 가물가물하다,딥러닝,-0.016969793,0.0,0.016969792544841766
Loss Function 관련 실무 경험 -> [상세 경험] 기억 진짜 안 나는데 가물가물하다,마지막 할 말,0.00074245076,0.0,0.0007424507639370859
Loss Function 관련 실무 경험 -> [상세 경험] 기억 진짜 안 나는데 가물가물하다,머신러닝,-0.009877569,0.0,0.009877569042146206
Loss Function 관련 실무 경험 -> [상세 경험] 기억 진짜 안 나는데 가물가물하다,면접 시작 인사,-0.029560506,0.0,0.029560506343841553
Loss Function 관련 실무 경험 -> [상세 경험] 기억 진짜 안 나는데 가물가물하다,상세 경험,0.066270076,0.0,0.06627007573843002
Loss Function 관련 실무 경험 -> [상세 경험] 기억 진짜 안 나는데 가물가물하다,수식,-0.02148568,0.0,0.02148568071424961
Loss Function 관련 실무 경험 -> [상세 경험] 기억 진짜 안 나는데 가물가물하다,용어 질문,-0.007673937,0.0,0.0076739368960261345
Loss Function 관련 실무 경험 -> [상세 경험] 기억 진짜 안 나는데 가물가물하다,인공지능,-0.02195988,0.0,0.021959880366921425
Loss Function 관련 실무 경험 -> [상세 경험] 기억 진짜 안 나는데 가물가물하다,잠시 휴식,-0.006009804,0.0,0.006009804084897041
Loss Function 관련 실무 경험 -> [상세 경험] 기억 진짜 안 나는데 가물가물하다,좋아하는 아이돌,0.007485599,0.0,0.007485598791390657
Loss Function 관련 실무 경험 -> [상세 경험] 기억 진짜 안 나는데 가물가물하다,핵심 아이디어,-0.007198501,0.0,0.0071985009126365185
Loss Function 관련 실무 경험 -> [상세 경험] 기억 진짜 안 나는데 가물가물하다,확률 예측에서 MSE Loss 미 사용 이유,-0.00081764796,0.0,0.0008176479605026543
Loss Function 관련 실무 경험 -> [기본 경험] 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ,BCE 가 좋은 task,-0.020819765,0.0,0.020819764584302902
Loss Function 관련 실무 경험 -> [기본 경험] 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ,BCE 가 좋은 이유,-0.027231874,0.0,0.02723187394440174
Loss Function 관련 실무 경험 -> [기본 경험] 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ,LLM Fine-Tuning 의 PEFT,-0.011551262,0.0,0.011551261879503727
Loss Function 관련 실무 경험 -> [기본 경험] 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ,LoRA,-0.015845448,0.0,0.015845447778701782
Loss Function 관련 실무 경험 -> [기본 경험] 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ,LoRA 와 QLoRA 의 차이,0.008406669,0.0,0.008406668901443481
Loss Function 관련 실무 경험 -> [기본 경험] 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ,Loss Function 예시,-0.035287738,0.0,0.03528773784637451
Loss Function 관련 실무 경험 -> [기본 경험] 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ,Loss Function 정의,-0.02156126,0.0,0.021561259403824806
Loss Function 관련 실무 경험 -> [기본 경험] 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ,MBTI,0.033500176,0.0,0.033500175923109055
Loss Function 관련 실무 경험 -> [기본 경험] 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ,MSE Loss 설명,-0.0033570619,0.0,0.0033570618834346533
Loss Function 관련 실무 경험 -> [기본 경험] 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ,MSE Loss 용도,-0.012136917,0.0,0.012136916629970074
Loss Function 관련 실무 경험 -> [기본 경험] 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ,Multi-Label 에서 CE + Softmax 적용 문제점,-0.030972052,0.0,0.030972052365541458
Loss Function 관련 실무 경험 -> [기본 경험] 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ,PEFT 방법 5가지,-0.034901656,0.0,0.034901656210422516
Loss Function 관련 실무 경험 -> [기본 경험] 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ,거대 언어 모델 정의,0.04024939,0.0,0.04024938866496086
Loss Function 관련 실무 경험 -> [기본 경험] 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ,기본 경험,0.8876364,1.0,0.11236357688903809
Loss Function 관련 실무 경험 -> [기본 경험] 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ,답변 실패,0.23532367,0.0,0.23532366752624512
Loss Function 관련 실무 경험 -> [기본 경험] 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ,딥러닝,0.0007953207,0.0,0.0007953207241371274
Loss Function 관련 실무 경험 -> [기본 경험] 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ,마지막 할 말,0.009183349,0.0,0.009183349087834358
Loss Function 관련 실무 경험 -> [기본 경험] 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ,머신러닝,-0.06651793,0.0,0.06651792675256729
Loss Function 관련 실무 경험 -> [기본 경험] 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ,면접 시작 인사,0.055349793,0.0,0.055349793285131454
Loss Function 관련 실무 경험 -> [기본 경험] 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ,상세 경험,-0.01617437,0.0,0.016174370422959328
Loss Function 관련 실무 경험 -> [기본 경험] 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ,수식,0.0038424046,0.0,0.003842404577881098
Loss Function 관련 실무 경험 -> [기본 경험] 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ,용어 질문,-0.028645627,0.0,0.028645627200603485
Loss Function 관련 실무 경험 -> [기본 경험] 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ,인공지능,0.0075910366,0.0,0.0075910366140306
Loss Function 관련 실무 경험 -> [기본 경험] 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ,잠시 휴식,0.018737191,0.0,0.0187371913343668
Loss Function 관련 실무 경험 -> [기본 경험] 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ,좋아하는 아이돌,0.016254598,0.0,0.016254598274827003
Loss Function 관련 실무 경험 -> [기본 경험] 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ,핵심 아이디어,0.0016000759,0.0,0.0016000758623704314
Loss Function 관련 실무 경험 -> [기본 경험] 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ,확률 예측에서 MSE Loss 미 사용 이유,-0.033219714,0.0,0.03321971371769905
Loss Function 관련 실무 경험 -> [기본 경험] 이걸 왜 알려줘야 하지,BCE 가 좋은 task,-0.005173725,0.0,0.005173725076019764
Loss Function 관련 실무 경험 -> [기본 경험] 이걸 왜 알려줘야 하지,BCE 가 좋은 이유,0.007473763,0.0,0.007473763078451157
Loss Function 관련 실무 경험 -> [기본 경험] 이걸 왜 알려줘야 하지,LLM Fine-Tuning 의 PEFT,-0.021530246,0.0,0.02153024636209011
Loss Function 관련 실무 경험 -> [기본 경험] 이걸 왜 알려줘야 하지,LoRA,-0.011389459,0.0,0.011389458552002907
Loss Function 관련 실무 경험 -> [기본 경험] 이걸 왜 알려줘야 하지,LoRA 와 QLoRA 의 차이,0.0023984718,0.0,0.002398471813648939
Loss Function 관련 실무 경험 -> [기본 경험] 이걸 왜 알려줘야 하지,Loss Function 예시,-0.003373538,0.0,0.0033735379111021757
Loss Function 관련 실무 경험 -> [기본 경험] 이걸 왜 알려줘야 하지,Loss Function 정의,-0.015226882,0.0,0.015226881951093674
Loss Function 관련 실무 경험 -> [기본 경험] 이걸 왜 알려줘야 하지,MBTI,-0.0033395966,0.0,0.0033395965583622456
Loss Function 관련 실무 경험 -> [기본 경험] 이걸 왜 알려줘야 하지,MSE Loss 설명,0.012054903,0.0,0.01205490343272686
Loss Function 관련 실무 경험 -> [기본 경험] 이걸 왜 알려줘야 하지,MSE Loss 용도,-0.02106184,0.0,0.021061839535832405
Loss Function 관련 실무 경험 -> [기본 경험] 이걸 왜 알려줘야 하지,Multi-Label 에서 CE + Softmax 적용 문제점,-0.036151476,0.0,0.036151476204395294
Loss Function 관련 실무 경험 -> [기본 경험] 이걸 왜 알려줘야 하지,PEFT 방법 5가지,-0.016253965,0.0,0.016253964975476265
Loss Function 관련 실무 경험 -> [기본 경험] 이걸 왜 알려줘야 하지,거대 언어 모델 정의,-0.00026268756,0.0,0.0002626875648275018
Loss Function 관련 실무 경험 -> [기본 경험] 이걸 왜 알려줘야 하지,기본 경험,0.27284688,0.0,0.27284687757492065
Loss Function 관련 실무 경험 -> [기본 경험] 이걸 왜 알려줘야 하지,답변 실패,0.9114604,1.0,0.08853960037231445
Loss Function 관련 실무 경험 -> [기본 경험] 이걸 왜 알려줘야 하지,딥러닝,-0.004406558,0.0,0.004406557884067297
Loss Function 관련 실무 경험 -> [기본 경험] 이걸 왜 알려줘야 하지,마지막 할 말,-0.010723968,0.0,0.010723968036472797
Loss Function 관련 실무 경험 -> [기본 경험] 이걸 왜 알려줘야 하지,머신러닝,-0.045560826,0.0,0.04556082561612129
Loss Function 관련 실무 경험 -> [기본 경험] 이걸 왜 알려줘야 하지,면접 시작 인사,0.020997927,0.0,0.020997926592826843
Loss Function 관련 실무 경험 -> [기본 경험] 이걸 왜 알려줘야 하지,상세 경험,0.0003220643,0.0,0.0003220642975065857
Loss Function 관련 실무 경험 -> [기본 경험] 이걸 왜 알려줘야 하지,수식,0.0011968306,0.0,0.0011968306498602033
Loss Function 관련 실무 경험 -> [기본 경험] 이걸 왜 알려줘야 하지,용어 질문,0.0080998195,0.0,0.008099819533526897
Loss Function 관련 실무 경험 -> [기본 경험] 이걸 왜 알려줘야 하지,인공지능,-0.0057191076,0.0,0.005719107575714588
Loss Function 관련 실무 경험 -> [기본 경험] 이걸 왜 알려줘야 하지,잠시 휴식,-0.011435622,0.0,0.011435622349381447
Loss Function 관련 실무 경험 -> [기본 경험] 이걸 왜 알려줘야 하지,좋아하는 아이돌,0.023386853,0.0,0.02338685281574726
Loss Function 관련 실무 경험 -> [기본 경험] 이걸 왜 알려줘야 하지,핵심 아이디어,-0.0055951145,0.0,0.0055951145477592945
Loss Function 관련 실무 경험 -> [기본 경험] 이걸 왜 알려줘야 하지,확률 예측에서 MSE Loss 미 사용 이유,-0.015955316,0.0,0.01595531590282917
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,BCE 가 좋은 task,5.0214392e-05,0.0,5.021439210395329e-05
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,BCE 가 좋은 이유,0.0413513,0.0,0.04135129973292351
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,LLM Fine-Tuning 의 PEFT,-0.011248607,0.0,0.011248607188463211
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,LoRA,0.010432631,0.0,0.010432630777359009
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,LoRA 와 QLoRA 의 차이,-0.006855146,0.0,0.006855146028101444
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,Loss Function 예시,-0.007553569,0.0,0.007553569041192532
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,Loss Function 정의,0.006253574,0.0,0.006253574043512344
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,MBTI,0.95555216,1.0,0.04444783926010132
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,MSE Loss 설명,0.0407151,0.0,0.04071509838104248
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,MSE Loss 용도,-0.025400205,0.0,0.025400204584002495
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,Multi-Label 에서 CE + Softmax 적용 문제점,-0.05766053,0.0,0.057660531252622604
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,PEFT 방법 5가지,-0.018178128,0.0,0.018178127706050873
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,거대 언어 모델 정의,-0.018727198,0.0,0.018727198243141174
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,기본 경험,0.032103334,0.0,0.03210333362221718
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,답변 실패,-0.028356168,0.0,0.028356168419122696
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,딥러닝,-0.00063954474,0.0,0.0006395447417162359
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,마지막 할 말,-0.0016737519,0.0,0.0016737518599256873
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,머신러닝,0.012461889,0.0,0.0124618886038661
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,면접 시작 인사,0.006269619,0.0,0.00626961886882782
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,상세 경험,-0.0032914213,0.0,0.0032914213370531797
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,수식,-0.0019285706,0.0,0.001928570563904941
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,용어 질문,-0.018700603,0.0,0.018700603395700455
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,인공지능,-0.003417052,0.0,0.0034170520957559347
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,잠시 휴식,-0.019916264,0.0,0.019916264340281487
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,좋아하는 아이돌,0.018853528,0.0,0.018853528425097466
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,핵심 아이디어,0.0019273678,0.0,0.0019273677607998252
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,확률 예측에서 MSE Loss 미 사용 이유,-0.026131343,0.0,0.026131343096494675
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,BCE 가 좋은 task,9.924412e-05,0.0,9.924412006512284e-05
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,BCE 가 좋은 이유,-0.010324938,0.0,0.010324938222765923
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,LLM Fine-Tuning 의 PEFT,0.021153774,0.0,0.021153774112462997
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,LoRA,0.026038019,0.0,0.026038018986582756
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,LoRA 와 QLoRA 의 차이,-0.0035758198,0.0,0.0035758197773247957
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,Loss Function 예시,0.01264435,0.0,0.012644349597394466
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,Loss Function 정의,-0.005766635,0.0,0.005766634829342365
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,MBTI,0.0060548717,0.0,0.006054871715605259
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,MSE Loss 설명,0.012413619,0.0,0.012413619086146355
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,MSE Loss 용도,0.0009697498,0.0,0.0009697498171590269
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,Multi-Label 에서 CE + Softmax 적용 문제점,-0.03568831,0.0,0.035688310861587524
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,PEFT 방법 5가지,-0.014030872,0.0,0.014030871912837029
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,거대 언어 모델 정의,-0.0030552375,0.0,0.003055237466469407
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,기본 경험,0.0044879173,0.0,0.004487917292863131
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,답변 실패,-0.0036683688,0.0,0.0036683687940239906
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,딥러닝,0.007771929,0.0,0.007771928794682026
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,마지막 할 말,-0.016734071,0.0,0.01673407107591629
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,머신러닝,0.0023133485,0.0,0.002313348464667797
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,면접 시작 인사,-0.011462752,0.0,0.011462751775979996
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,상세 경험,-0.0014856511,0.0,0.0014856511261314154
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,수식,0.010885759,0.0,0.010885759256780148
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,용어 질문,0.019043347,0.0,0.019043346866965294
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,인공지능,0.008992825,0.0,0.008992824703454971
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,잠시 휴식,0.012506338,0.0,0.012506337836384773
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,좋아하는 아이돌,0.9695117,1.0,0.030488312244415283
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,핵심 아이디어,-0.008267272,0.0,0.008267272263765335
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,확률 예측에서 MSE Loss 미 사용 이유,-0.009724331,0.0,0.009724331088364124
잠시 휴식 -> 재미있는 이야기 해줄래?,BCE 가 좋은 task,-0.00093762414,0.0,0.0009376241359859705
잠시 휴식 -> 재미있는 이야기 해줄래?,BCE 가 좋은 이유,-0.0191891,0.0,0.019189100712537766
잠시 휴식 -> 재미있는 이야기 해줄래?,LLM Fine-Tuning 의 PEFT,-0.0078553725,0.0,0.007855372503399849
잠시 휴식 -> 재미있는 이야기 해줄래?,LoRA,-0.010806667,0.0,0.010806666687130928
잠시 휴식 -> 재미있는 이야기 해줄래?,LoRA 와 QLoRA 의 차이,0.0031504007,0.0,0.0031504007056355476
잠시 휴식 -> 재미있는 이야기 해줄래?,Loss Function 예시,0.0234254,0.0,0.023425400257110596
잠시 휴식 -> 재미있는 이야기 해줄래?,Loss Function 정의,0.0040208134,0.0,0.004020813386887312
잠시 휴식 -> 재미있는 이야기 해줄래?,MBTI,-0.037289385,0.0,0.03728938475251198
잠시 휴식 -> 재미있는 이야기 해줄래?,MSE Loss 설명,0.0022546807,0.0,0.0022546807304024696
잠시 휴식 -> 재미있는 이야기 해줄래?,MSE Loss 용도,-0.031282347,0.0,0.031282346695661545
잠시 휴식 -> 재미있는 이야기 해줄래?,Multi-Label 에서 CE + Softmax 적용 문제점,-0.0024587784,0.0,0.002458778442814946
잠시 휴식 -> 재미있는 이야기 해줄래?,PEFT 방법 5가지,-0.028834103,0.0,0.028834102675318718
잠시 휴식 -> 재미있는 이야기 해줄래?,거대 언어 모델 정의,-0.008431534,0.0,0.00843153428286314
잠시 휴식 -> 재미있는 이야기 해줄래?,기본 경험,0.011509063,0.0,0.011509062722325325
잠시 휴식 -> 재미있는 이야기 해줄래?,답변 실패,-0.0072703417,0.0,0.007270341739058495
잠시 휴식 -> 재미있는 이야기 해줄래?,딥러닝,-0.011462319,0.0,0.0114623187109828
잠시 휴식 -> 재미있는 이야기 해줄래?,마지막 할 말,0.012143006,0.0,0.01214300561696291
잠시 휴식 -> 재미있는 이야기 해줄래?,머신러닝,-0.010057323,0.0,0.010057322680950165
잠시 휴식 -> 재미있는 이야기 해줄래?,면접 시작 인사,-0.03139924,0.0,0.03139923885464668
잠시 휴식 -> 재미있는 이야기 해줄래?,상세 경험,0.008562524,0.0,0.008562523871660233
잠시 휴식 -> 재미있는 이야기 해줄래?,수식,-0.014063152,0.0,0.014063151553273201
잠시 휴식 -> 재미있는 이야기 해줄래?,용어 질문,-0.023507167,0.0,0.023507166653871536
잠시 휴식 -> 재미있는 이야기 해줄래?,인공지능,0.0037915832,0.0,0.0037915832363069057
잠시 휴식 -> 재미있는 이야기 해줄래?,잠시 휴식,0.9814179,1.0,0.01858210563659668
잠시 휴식 -> 재미있는 이야기 해줄래?,좋아하는 아이돌,-0.0018274115,0.0,0.001827411470003426
잠시 휴식 -> 재미있는 이야기 해줄래?,핵심 아이디어,-0.016480638,0.0,0.016480637714266777
잠시 휴식 -> 재미있는 이야기 해줄래?,확률 예측에서 MSE Loss 미 사용 이유,-0.0118281515,0.0,0.011828151531517506
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",BCE 가 좋은 task,-0.013831813,0.0,0.013831812888383865
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",BCE 가 좋은 이유,-0.00041687192,0.0,0.00041687191696837544
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",LLM Fine-Tuning 의 PEFT,0.931024,1.0,0.06897598505020142
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",LoRA,-0.017666694,0.0,0.017666693776845932
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",LoRA 와 QLoRA 의 차이,0.01245716,0.0,0.012457160279154778
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",Loss Function 예시,-0.06773319,0.0,0.06773319095373154
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",Loss Function 정의,-0.036694884,0.0,0.036694884300231934
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",MBTI,-0.022171829,0.0,0.022171828895807266
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",MSE Loss 설명,-0.025116546,0.0,0.02511654607951641
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",MSE Loss 용도,-0.040028628,0.0,0.04002862796187401
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",Multi-Label 에서 CE + Softmax 적용 문제점,0.057612643,0.0,0.057612642645835876
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",PEFT 방법 5가지,0.03266005,0.0,0.032660048454999924
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",거대 언어 모델 정의,-0.021130953,0.0,0.02113095298409462
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",기본 경험,-0.02571462,0.0,0.02571461908519268
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",답변 실패,0.05120467,0.0,0.05120467022061348
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",딥러닝,0.015500753,0.0,0.015500753186643124
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",마지막 할 말,-0.004179006,0.0,0.004179005976766348
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",머신러닝,-0.07127766,0.0,0.0712776631116867
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",면접 시작 인사,-0.045314733,0.0,0.0453147329390049
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",상세 경험,-0.06749689,0.0,0.0674968883395195
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",수식,0.05662601,0.0,0.056626010686159134
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",용어 질문,0.00900736,0.0,0.009007359854876995
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",인공지능,-0.017472582,0.0,0.017472581937909126
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",잠시 휴식,-0.0044382727,0.0,0.004438272677361965
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",좋아하는 아이돌,0.008342585,0.0,0.00834258459508419
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",핵심 아이디어,0.03341922,0.0,0.03341922163963318
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",확률 예측에서 MSE Loss 미 사용 이유,-0.016893812,0.0,0.016893811523914337
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,BCE 가 좋은 task,0.00083264057,0.0,0.000832640565931797
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,BCE 가 좋은 이유,0.017974727,0.0,0.017974726855754852
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,LLM Fine-Tuning 의 PEFT,0.021494031,0.0,0.021494030952453613
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,LoRA,-0.007974163,0.0,0.007974162697792053
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,LoRA 와 QLoRA 의 차이,-0.0035415092,0.0,0.0035415091551840305
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,Loss Function 예시,0.0032484897,0.0,0.0032484896946698427
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,Loss Function 정의,0.01220717,0.0,0.012207170017063618
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,MBTI,-0.00399548,0.0,0.003995480015873909
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,MSE Loss 설명,0.0016983921,0.0,0.0016983920941129327
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,MSE Loss 용도,-0.014022024,0.0,0.014022024348378181
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,Multi-Label 에서 CE + Softmax 적용 문제점,-0.019550866,0.0,0.01955086551606655
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,PEFT 방법 5가지,0.0045529893,0.0,0.004552989266812801
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,거대 언어 모델 정의,0.00061395235,0.0,0.000613952346611768
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,기본 경험,-0.02204561,0.0,0.02204561047255993
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,답변 실패,0.9732894,1.0,0.0267106294631958
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,딥러닝,-0.006468381,0.0,0.006468380801379681
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,마지막 할 말,0.001426613,0.0,0.0014266129583120346
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,머신러닝,-0.00759735,0.0,0.007597350049763918
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,면접 시작 인사,-0.009604917,0.0,0.009604916907846928
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,상세 경험,-0.013624875,0.0,0.013624874874949455
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,수식,-0.020074578,0.0,0.020074578002095222
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,용어 질문,0.0057671927,0.0,0.00576719269156456
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,인공지능,-0.018647004,0.0,0.018647003918886185
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,잠시 휴식,-0.002505545,0.0,0.002505545038729906
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,좋아하는 아이돌,-0.0073290826,0.0,0.007329082582145929
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,핵심 아이디어,0.0024402733,0.0,0.0024402732960879803
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,확률 예측에서 MSE Loss 미 사용 이유,-0.001119804,0.0,0.0011198039865121245
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야!,BCE 가 좋은 task,0.0006721928,0.0,0.0006721927784383297
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야!,BCE 가 좋은 이유,0.008333473,0.0,0.008333472535014153
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야!,LLM Fine-Tuning 의 PEFT,0.96123004,1.0,0.03876996040344238
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야!,LoRA,-0.017984673,0.0,0.017984673380851746
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야!,LoRA 와 QLoRA 의 차이,0.009308187,0.0,0.009308187291026115
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야!,Loss Function 예시,-0.06365191,0.0,0.0636519119143486
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야!,Loss Function 정의,-0.011822666,0.0,0.01182266604155302
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야!,MBTI,-0.033820197,0.0,0.033820196986198425
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야!,MSE Loss 설명,-0.039814092,0.0,0.039814092218875885
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야!,MSE Loss 용도,-0.026473658,0.0,0.02647365815937519
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야!,Multi-Label 에서 CE + Softmax 적용 문제점,0.017252116,0.0,0.017252115532755852
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야!,PEFT 방법 5가지,-0.0044864328,0.0,0.0044864327646791935
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야!,거대 언어 모델 정의,0.0013166687,0.0,0.0013166686985641718
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야!,기본 경험,-0.030464724,0.0,0.030464723706245422
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야!,답변 실패,-0.0012439659,0.0,0.001243965933099389
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야!,딥러닝,0.03239536,0.0,0.03239535912871361
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야!,마지막 할 말,0.012063056,0.0,0.012063056230545044
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야!,머신러닝,-0.01770013,0.0,0.017700130119919777
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야!,면접 시작 인사,-0.0262561,0.0,0.026256099343299866
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야!,상세 경험,-0.04508958,0.0,0.04508958011865616
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야!,수식,0.05124488,0.0,0.05124488100409508
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야!,용어 질문,0.008619993,0.0,0.00861999299377203
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야!,인공지능,0.0069462564,0.0,0.006946256384253502
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야!,잠시 휴식,-0.0040783505,0.0,0.004078350495547056
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야!,좋아하는 아이돌,0.022496438,0.0,0.022496437653899193
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야!,핵심 아이디어,0.00090564525,0.0,0.0009056452545337379
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야!,확률 예측에서 MSE Loss 미 사용 이유,-0.01469852,0.0,0.014698520302772522
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다,BCE 가 좋은 task,0.008959881,0.0,0.008959881030023098
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다,BCE 가 좋은 이유,0.008536811,0.0,0.008536810986697674
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다,LLM Fine-Tuning 의 PEFT,0.013003676,0.0,0.013003676198422909
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다,LoRA,-0.007321477,0.0,0.007321476936340332
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다,LoRA 와 QLoRA 의 차이,0.0032900309,0.0,0.003290030872449279
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다,Loss Function 예시,0.0016510285,0.0,0.0016510285204276443
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다,Loss Function 정의,0.009010405,0.0,0.009010405279695988
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다,MBTI,-0.017266605,0.0,0.01726660504937172
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다,MSE Loss 설명,0.006220395,0.0,0.006220395211130381
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다,MSE Loss 용도,-0.021754608,0.0,0.021754607558250427
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다,Multi-Label 에서 CE + Softmax 적용 문제점,-0.023093682,0.0,0.023093681782484055
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다,PEFT 방법 5가지,-0.007487992,0.0,0.007487991824746132
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다,거대 언어 모델 정의,0.028116466,0.0,0.028116466477513313
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다,기본 경험,-0.021624982,0.0,0.021624982357025146
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다,답변 실패,0.96708256,1.0,0.03291743993759155
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다,딥러닝,-0.01159278,0.0,0.011592780239880085
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다,마지막 할 말,-0.0098437,0.0,0.009843699634075165
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다,머신러닝,-0.013570394,0.0,0.013570394366979599
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다,면접 시작 인사,-0.010944686,0.0,0.010944685898721218
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다,상세 경험,-0.008425231,0.0,0.008425231091678143
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다,수식,0.0027530058,0.0,0.0027530058287084103
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다,용어 질문,0.007908295,0.0,0.007908294908702374
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다,인공지능,-0.020036727,0.0,0.0200367271900177
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다,잠시 휴식,-0.0016040318,0.0,0.0016040317714214325
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다,좋아하는 아이돌,0.0035867412,0.0,0.003586741164326668
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다,핵심 아이디어,-0.010042209,0.0,0.010042209178209305
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다,확률 예측에서 MSE Loss 미 사용 이유,0.0022736008,0.0,0.0022736007813364267
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",BCE 가 좋은 task,-0.0033240695,0.0,0.0033240695483982563
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",BCE 가 좋은 이유,-0.015840702,0.0,0.015840701758861542
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",LLM Fine-Tuning 의 PEFT,0.028783439,0.0,0.028783438727259636
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",LoRA,0.0010220325,0.0,0.0010220324620604515
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",LoRA 와 QLoRA 의 차이,-0.021870764,0.0,0.02187076397240162
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",Loss Function 예시,-0.0018042435,0.0,0.001804243540391326
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",Loss Function 정의,0.040038478,0.0,0.04003847762942314
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",MBTI,-0.006443289,0.0,0.00644328910857439
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",MSE Loss 설명,0.012100929,0.0,0.012100929394364357
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",MSE Loss 용도,-0.01807346,0.0,0.018073460087180138
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",Multi-Label 에서 CE + Softmax 적용 문제점,0.00743442,0.0,0.007434419821947813
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",PEFT 방법 5가지,0.9446369,1.0,0.05536311864852905
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",거대 언어 모델 정의,-0.02064936,0.0,0.020649360492825508
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",기본 경험,0.0108254915,0.0,0.01082549151033163
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",답변 실패,-0.014338641,0.0,0.014338641427457333
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",딥러닝,-0.02835128,0.0,0.028351280838251114
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",마지막 할 말,-0.0062884395,0.0,0.006288439501076937
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",머신러닝,0.017621314,0.0,0.017621314153075218
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",면접 시작 인사,-0.006142791,0.0,0.006142790894955397
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",상세 경험,-0.02765848,0.0,0.027658479288220406
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",수식,0.02357777,0.0,0.023577770218253136
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",용어 질문,-0.0041386513,0.0,0.004138651303946972
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",인공지능,-0.032993797,0.0,0.03299379721283913
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",잠시 휴식,-0.009334929,0.0,0.009334929287433624
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",좋아하는 아이돌,-0.018550314,0.0,0.018550314009189606
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",핵심 아이디어,0.03184299,0.0,0.03184299170970917
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",확률 예측에서 MSE Loss 미 사용 이유,-0.005726851,0.0,0.0057268510572612286
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,BCE 가 좋은 task,-0.0070560933,0.0,0.00705609330907464
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,BCE 가 좋은 이유,0.015116675,0.0,0.015116674825549126
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,LLM Fine-Tuning 의 PEFT,0.0066075064,0.0,0.006607506424188614
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,LoRA,0.021155417,0.0,0.02115541696548462
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,LoRA 와 QLoRA 의 차이,0.0028717073,0.0,0.0028717073146253824
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,Loss Function 예시,0.004211816,0.0,0.004211816005408764
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,Loss Function 정의,-0.0033595085,0.0,0.003359508467838168
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,MBTI,0.0026023355,0.0,0.0026023355312645435
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,MSE Loss 설명,-0.010293506,0.0,0.01029350608587265
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,MSE Loss 용도,-0.00985954,0.0,0.009859539568424225
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,Multi-Label 에서 CE + Softmax 적용 문제점,-0.015341798,0.0,0.015341797843575478
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,PEFT 방법 5가지,0.031573713,0.0,0.031573712825775146
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,거대 언어 모델 정의,-0.0061872,0.0,0.006187200080603361
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,기본 경험,-0.017307138,0.0,0.017307138070464134
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,답변 실패,0.9764844,1.0,0.02351558208465576
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,딥러닝,-0.005272052,0.0,0.00527205178514123
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,마지막 할 말,-0.0021176,0.0,0.0021176000591367483
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,머신러닝,-0.010796483,0.0,0.010796482674777508
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,면접 시작 인사,-0.01640986,0.0,0.0164098609238863
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,상세 경험,-0.012801532,0.0,0.012801531702280045
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,수식,-0.04091856,0.0,0.040918558835983276
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,용어 질문,0.0034354036,0.0,0.003435403574258089
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,인공지능,-0.01582604,0.0,0.015826040878891945
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,잠시 휴식,-0.0034999913,0.0,0.00349999126046896
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,좋아하는 아이돌,0.004288777,0.0,0.004288776777684689
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,핵심 아이디어,-0.00055585394,0.0,0.0005558539414778352
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,확률 예측에서 MSE Loss 미 사용 이유,-0.0016333249,0.0,0.0016333248931914568
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,BCE 가 좋은 task,-0.010313985,0.0,0.01031398493796587
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,BCE 가 좋은 이유,-0.03382914,0.0,0.03382914140820503
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,LLM Fine-Tuning 의 PEFT,-0.031428106,0.0,0.031428106129169464
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,LoRA,0.9277764,1.0,0.07222360372543335
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,LoRA 와 QLoRA 의 차이,-0.046422504,0.0,0.046422503888607025
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,Loss Function 예시,-0.020868039,0.0,0.02086803875863552
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,Loss Function 정의,-0.05646776,0.0,0.05646776035428047
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,MBTI,-0.03518084,0.0,0.03518084064126015
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,MSE Loss 설명,-0.05712155,0.0,0.05712154880166054
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,MSE Loss 용도,0.024720741,0.0,0.02472074143588543
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,Multi-Label 에서 CE + Softmax 적용 문제점,0.0041887886,0.0,0.004188788589090109
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,PEFT 방법 5가지,0.04256194,0.0,0.04256194084882736
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,거대 언어 모델 정의,-0.020852525,0.0,0.020852524787187576
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,기본 경험,0.0063472763,0.0,0.006347276270389557
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,답변 실패,0.12345361,0.0,0.12345360964536667
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,딥러닝,-0.012433465,0.0,0.012433464638888836
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,마지막 할 말,-0.033617314,0.0,0.03361731395125389
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,머신러닝,0.0012635485,0.0,0.0012635485036298633
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,면접 시작 인사,-0.030689385,0.0,0.030689384788274765
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,상세 경험,-0.0693733,0.0,0.06937330216169357
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,수식,0.028441546,0.0,0.028441546484827995
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,용어 질문,-0.03051285,0.0,0.030512850731611252
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,인공지능,-0.0019030129,0.0,0.0019030128605663776
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,잠시 휴식,-0.018816315,0.0,0.01881631463766098
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,좋아하는 아이돌,0.0012123237,0.0,0.001212323666550219
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,핵심 아이디어,0.04780863,0.0,0.047808628529310226
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,확률 예측에서 MSE Loss 미 사용 이유,-0.026231207,0.0,0.026231206953525543
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,BCE 가 좋은 task,-0.022595618,0.0,0.022595617920160294
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,BCE 가 좋은 이유,-0.0047549596,0.0,0.0047549596056342125
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,LLM Fine-Tuning 의 PEFT,0.0014014741,0.0,0.0014014741173014045
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,LoRA,0.049413454,0.0,0.04941345378756523
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,LoRA 와 QLoRA 의 차이,0.004110032,0.0,0.0041100322268903255
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,Loss Function 예시,0.019339757,0.0,0.019339757040143013
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,Loss Function 정의,-8.030971e-05,0.0,8.030970639083534e-05
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,MBTI,-0.00369834,0.0,0.003698339918628335
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,MSE Loss 설명,-0.015808815,0.0,0.015808815136551857
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,MSE Loss 용도,-0.015969954,0.0,0.015969954431056976
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,Multi-Label 에서 CE + Softmax 적용 문제점,-0.032689597,0.0,0.032689597457647324
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,PEFT 방법 5가지,-0.007203463,0.0,0.00720346299931407
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,거대 언어 모델 정의,0.010461167,0.0,0.010461167432367802
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,기본 경험,-0.007922313,0.0,0.007922313176095486
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,답변 실패,0.956091,1.0,0.04390901327133179
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,딥러닝,0.0024003987,0.0,0.0024003987200558186
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,마지막 할 말,0.00038773433,0.0,0.00038773432606831193
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,머신러닝,-0.013550657,0.0,0.013550656847655773
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,면접 시작 인사,-0.02481822,0.0,0.024818219244480133
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,상세 경험,-0.0008146864,0.0,0.000814686412923038
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,수식,-0.033770625,0.0,0.03377062454819679
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,용어 질문,-0.020234926,0.0,0.02023492567241192
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,인공지능,-0.026825033,0.0,0.026825033128261566
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,잠시 휴식,0.00803327,0.0,0.0080332700163126
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,좋아하는 아이돌,0.0136304125,0.0,0.013630412518978119
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,핵심 아이디어,0.013137703,0.0,0.013137702830135822
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,확률 예측에서 MSE Loss 미 사용 이유,0.011860932,0.0,0.011860932223498821
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야!,BCE 가 좋은 task,-0.015856411,0.0,0.015856411308050156
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야!,BCE 가 좋은 이유,-0.01772455,0.0,0.017724549397826195
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야!,LLM Fine-Tuning 의 PEFT,-0.019050844,0.0,0.01905084401369095
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야!,LoRA,0.9565049,1.0,0.043495118618011475
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야!,LoRA 와 QLoRA 의 차이,-0.0633023,0.0,0.06330230087041855
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야!,Loss Function 예시,-0.025670718,0.0,0.025670718401670456
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야!,Loss Function 정의,-0.040385168,0.0,0.0403851680457592
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야!,MBTI,-0.017264644,0.0,0.01726464368402958
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야!,MSE Loss 설명,-0.04129562,0.0,0.041295621544122696
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야!,MSE Loss 용도,0.018642625,0.0,0.018642624840140343
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야!,Multi-Label 에서 CE + Softmax 적용 문제점,0.004698862,0.0,0.0046988618560135365
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야!,PEFT 방법 5가지,0.0020514429,0.0,0.002051442861557007
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야!,거대 언어 모델 정의,0.021523926,0.0,0.02152392640709877
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야!,기본 경험,-0.013626134,0.0,0.013626134023070335
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야!,답변 실패,0.022412715,0.0,0.02241271547973156
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야!,딥러닝,0.0063854745,0.0,0.006385474465787411
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야!,마지막 할 말,-0.01823307,0.0,0.01823307015001774
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야!,머신러닝,0.0035590653,0.0,0.0035590652842074633
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야!,면접 시작 인사,-0.012534663,0.0,0.012534663081169128
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야!,상세 경험,-0.045668222,0.0,0.04566822201013565
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야!,수식,-0.0022650044,0.0,0.002265004441142082
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야!,용어 질문,-0.0397665,0.0,0.039766501635313034
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야!,인공지능,0.017638918,0.0,0.0176389180123806
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야!,잠시 휴식,-0.022420451,0.0,0.022420451045036316
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야!,좋아하는 아이돌,0.00882648,0.0,0.008826480247080326
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야!,핵심 아이디어,0.023326246,0.0,0.023326246067881584
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야!,확률 예측에서 MSE Loss 미 사용 이유,-0.020935282,0.0,0.020935282111167908
LoRA -> 무슨 OOM 없앤다는 것 같은데,BCE 가 좋은 task,-0.0045961053,0.0,0.004596105311065912
LoRA -> 무슨 OOM 없앤다는 것 같은데,BCE 가 좋은 이유,0.017028477,0.0,0.017028477042913437
LoRA -> 무슨 OOM 없앤다는 것 같은데,LLM Fine-Tuning 의 PEFT,-0.0020434435,0.0,0.002043443499132991
LoRA -> 무슨 OOM 없앤다는 것 같은데,LoRA,0.026794339,0.0,0.02679433859884739
LoRA -> 무슨 OOM 없앤다는 것 같은데,LoRA 와 QLoRA 의 차이,0.0020020485,0.0,0.0020020485389977694
LoRA -> 무슨 OOM 없앤다는 것 같은데,Loss Function 예시,0.0032199477,0.0,0.003219947684556246
LoRA -> 무슨 OOM 없앤다는 것 같은데,Loss Function 정의,-0.004736897,0.0,0.004736897069960833
LoRA -> 무슨 OOM 없앤다는 것 같은데,MBTI,-0.0024125506,0.0,0.0024125506170094013
LoRA -> 무슨 OOM 없앤다는 것 같은데,MSE Loss 설명,-0.009099559,0.0,0.009099558927118778
LoRA -> 무슨 OOM 없앤다는 것 같은데,MSE Loss 용도,-0.015033384,0.0,0.01503338385373354
LoRA -> 무슨 OOM 없앤다는 것 같은데,Multi-Label 에서 CE + Softmax 적용 문제점,-0.0143928705,0.0,0.014392870478332043
LoRA -> 무슨 OOM 없앤다는 것 같은데,PEFT 방법 5가지,-0.0043146755,0.0,0.004314675461500883
LoRA -> 무슨 OOM 없앤다는 것 같은데,거대 언어 모델 정의,0.0045866664,0.0,0.004586666356772184
LoRA -> 무슨 OOM 없앤다는 것 같은데,기본 경험,-0.01767502,0.0,0.017675019800662994
LoRA -> 무슨 OOM 없앤다는 것 같은데,답변 실패,0.97748786,1.0,0.02251213788986206
LoRA -> 무슨 OOM 없앤다는 것 같은데,딥러닝,-0.0062413085,0.0,0.0062413085252046585
LoRA -> 무슨 OOM 없앤다는 것 같은데,마지막 할 말,-0.004288417,0.0,0.0042884168215096
LoRA -> 무슨 OOM 없앤다는 것 같은데,머신러닝,-0.0151307555,0.0,0.015130755491554737
LoRA -> 무슨 OOM 없앤다는 것 같은데,면접 시작 인사,-0.010994501,0.0,0.010994501411914825
LoRA -> 무슨 OOM 없앤다는 것 같은데,상세 경험,-0.0012677188,0.0,0.0012677188497036695
LoRA -> 무슨 OOM 없앤다는 것 같은데,수식,-0.024080167,0.0,0.024080166593194008
LoRA -> 무슨 OOM 없앤다는 것 같은데,용어 질문,0.0007232172,0.0,0.0007232172065414488
LoRA -> 무슨 OOM 없앤다는 것 같은데,인공지능,-0.013262594,0.0,0.013262594118714333
LoRA -> 무슨 OOM 없앤다는 것 같은데,잠시 휴식,-0.00073335785,0.0,0.0007333578541874886
LoRA -> 무슨 OOM 없앤다는 것 같은데,좋아하는 아이돌,0.00015620087,0.0,0.00015620086924172938
LoRA -> 무슨 OOM 없앤다는 것 같은데,핵심 아이디어,0.008140507,0.0,0.008140507154166698
LoRA -> 무슨 OOM 없앤다는 것 같은데,확률 예측에서 MSE Loss 미 사용 이유,-0.0032056079,0.0,0.0032056078780442476
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,BCE 가 좋은 task,-0.046082333,0.0,0.046082332730293274
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,BCE 가 좋은 이유,0.0016808836,0.0,0.0016808835789561272
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,LLM Fine-Tuning 의 PEFT,0.015370584,0.0,0.015370584093034267
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,LoRA,-0.048261926,0.0,0.04826192557811737
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,LoRA 와 QLoRA 의 차이,0.965281,1.0,0.034718990325927734
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,Loss Function 예시,0.013139271,0.0,0.013139271177351475
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,Loss Function 정의,-0.015108294,0.0,0.015108293853700161
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,MBTI,-0.01591472,0.0,0.01591471955180168
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,MSE Loss 설명,-0.022699341,0.0,0.02269934117794037
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,MSE Loss 용도,-0.040960733,0.0,0.040960732847452164
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,Multi-Label 에서 CE + Softmax 적용 문제점,0.018744431,0.0,0.01874443143606186
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,PEFT 방법 5가지,-0.030506313,0.0,0.03050631284713745
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,거대 언어 모델 정의,-0.015424393,0.0,0.015424393117427826
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,기본 경험,0.0123443175,0.0,0.012344317510724068
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,답변 실패,-0.012056229,0.0,0.012056228704750538
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,딥러닝,0.02426263,0.0,0.024262629449367523
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,마지막 할 말,-0.005459721,0.0,0.005459721200168133
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,머신러닝,-0.028302167,0.0,0.028302166610956192
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,면접 시작 인사,-0.0148444455,0.0,0.014844445511698723
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,상세 경험,-0.016337836,0.0,0.016337836161255836
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,수식,-0.0030669137,0.0,0.003066913690418005
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,용어 질문,-0.009561315,0.0,0.009561315178871155
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,인공지능,-0.0025959497,0.0,0.002595949685201049
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,잠시 휴식,-0.003734843,0.0,0.0037348431069403887
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,좋아하는 아이돌,-0.01329555,0.0,0.013295549899339676
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,핵심 아이디어,0.005454953,0.0,0.0054549528285861015
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,확률 예측에서 MSE Loss 미 사용 이유,0.0055157286,0.0,0.005515728611499071
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,BCE 가 좋은 task,0.006088086,0.0,0.006088085938245058
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,BCE 가 좋은 이유,-0.008323413,0.0,0.00832341331988573
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,LLM Fine-Tuning 의 PEFT,-0.00786353,0.0,0.007863529957830906
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,LoRA,0.013401143,0.0,0.013401143252849579
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,LoRA 와 QLoRA 의 차이,0.06594351,0.0,0.06594350934028625
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,Loss Function 예시,0.01588789,0.0,0.01588789001107216
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,Loss Function 정의,-0.00978013,0.0,0.009780130349099636
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,MBTI,-0.002520149,0.0,0.0025201491080224514
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,MSE Loss 설명,-0.02090951,0.0,0.020909510552883148
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,MSE Loss 용도,-0.007110328,0.0,0.007110327947884798
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,Multi-Label 에서 CE + Softmax 적용 문제점,-0.022314979,0.0,0.022314978763461113
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,PEFT 방법 5가지,-0.015965091,0.0,0.015965091064572334
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,거대 언어 모델 정의,0.0024232925,0.0,0.0024232924915850163
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,기본 경험,-0.016341012,0.0,0.016341011971235275
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,답변 실패,0.9642301,1.0,0.035769879817962646
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,딥러닝,-0.008864586,0.0,0.008864586241543293
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,마지막 할 말,-0.0042470586,0.0,0.0042470586486160755
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,머신러닝,-0.01579577,0.0,0.015795769169926643
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,면접 시작 인사,-0.018044144,0.0,0.01804414391517639
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,상세 경험,-0.0016341119,0.0,0.0016341118607670069
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,수식,-0.034038924,0.0,0.03403892368078232
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,용어 질문,0.009510289,0.0,0.009510288946330547
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,인공지능,-0.006088407,0.0,0.006088406778872013
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,잠시 휴식,-0.0010191256,0.0,0.001019125571474433
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,좋아하는 아이돌,-0.003130067,0.0,0.0031300669070333242
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,핵심 아이디어,-0.0018837608,0.0,0.0018837607931345701
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,확률 예측에서 MSE Loss 미 사용 이유,0.009039676,0.0,0.009039675816893578
마지막 할 말 -> 로라야 정말 고마워!,BCE 가 좋은 task,-0.0019553408,0.0,0.001955340849235654
마지막 할 말 -> 로라야 정말 고마워!,BCE 가 좋은 이유,0.0070401477,0.0,0.007040147669613361
마지막 할 말 -> 로라야 정말 고마워!,LLM Fine-Tuning 의 PEFT,-0.0013194474,0.0,0.001319447415880859
마지막 할 말 -> 로라야 정말 고마워!,LoRA,-0.007951759,0.0,0.007951758801937103
마지막 할 말 -> 로라야 정말 고마워!,LoRA 와 QLoRA 의 차이,-0.0012981725,0.0,0.0012981725158169866
마지막 할 말 -> 로라야 정말 고마워!,Loss Function 예시,0.020563548,0.0,0.020563548430800438
마지막 할 말 -> 로라야 정말 고마워!,Loss Function 정의,0.010353097,0.0,0.010353096760809422
마지막 할 말 -> 로라야 정말 고마워!,MBTI,-0.013647486,0.0,0.013647486455738544
마지막 할 말 -> 로라야 정말 고마워!,MSE Loss 설명,-0.02604807,0.0,0.026048069819808006
마지막 할 말 -> 로라야 정말 고마워!,MSE Loss 용도,-0.0056644552,0.0,0.005664455238729715
마지막 할 말 -> 로라야 정말 고마워!,Multi-Label 에서 CE + Softmax 적용 문제점,-0.013926997,0.0,0.013926996849477291
마지막 할 말 -> 로라야 정말 고마워!,PEFT 방법 5가지,-0.0034931086,0.0,0.0034931085538119078
마지막 할 말 -> 로라야 정말 고마워!,거대 언어 모델 정의,0.007788698,0.0,0.007788698188960552
마지막 할 말 -> 로라야 정말 고마워!,기본 경험,0.0137784695,0.0,0.013778469525277615
마지막 할 말 -> 로라야 정말 고마워!,답변 실패,0.0018438132,0.0,0.001843813224695623
마지막 할 말 -> 로라야 정말 고마워!,딥러닝,-0.013484872,0.0,0.013484871946275234
마지막 할 말 -> 로라야 정말 고마워!,마지막 할 말,0.9831454,1.0,0.016854584217071533
마지막 할 말 -> 로라야 정말 고마워!,머신러닝,-0.0061755734,0.0,0.0061755734495818615
마지막 할 말 -> 로라야 정말 고마워!,면접 시작 인사,-0.014948783,0.0,0.014948783442378044
마지막 할 말 -> 로라야 정말 고마워!,상세 경험,-0.016796215,0.0,0.016796214506030083
마지막 할 말 -> 로라야 정말 고마워!,수식,-0.004011941,0.0,0.004011941142380238
마지막 할 말 -> 로라야 정말 고마워!,용어 질문,-0.02300876,0.0,0.023008760064840317
마지막 할 말 -> 로라야 정말 고마워!,인공지능,0.02205031,0.0,0.02205030992627144
마지막 할 말 -> 로라야 정말 고마워!,잠시 휴식,-0.0014700467,0.0,0.0014700466999784112
마지막 할 말 -> 로라야 정말 고마워!,좋아하는 아이돌,-0.0066452385,0.0,0.006645238492637873
마지막 할 말 -> 로라야 정말 고마워!,핵심 아이디어,-0.010848916,0.0,0.01084891613572836
마지막 할 말 -> 로라야 정말 고마워!,확률 예측에서 MSE Loss 미 사용 이유,0.011209054,0.0,0.011209053918719292
마지막 할 말 -> 로라야 사랑해,BCE 가 좋은 task,-0.006788783,0.0,0.006788782775402069
마지막 할 말 -> 로라야 사랑해,BCE 가 좋은 이유,0.007021861,0.0,0.0070218611508607864
마지막 할 말 -> 로라야 사랑해,LLM Fine-Tuning 의 PEFT,0.00085724326,0.0,0.0008572432561777532
마지막 할 말 -> 로라야 사랑해,LoRA,-0.009878752,0.0,0.009878751821815968
마지막 할 말 -> 로라야 사랑해,LoRA 와 QLoRA 의 차이,0.0055167414,0.0,0.0055167414247989655
마지막 할 말 -> 로라야 사랑해,Loss Function 예시,0.021973468,0.0,0.021973468363285065
마지막 할 말 -> 로라야 사랑해,Loss Function 정의,0.008170083,0.0,0.008170083165168762
마지막 할 말 -> 로라야 사랑해,MBTI,-0.01417741,0.0,0.014177409932017326
마지막 할 말 -> 로라야 사랑해,MSE Loss 설명,-0.03203518,0.0,0.03203517943620682
마지막 할 말 -> 로라야 사랑해,MSE Loss 용도,-0.002001012,0.0,0.0020010119769722223
마지막 할 말 -> 로라야 사랑해,Multi-Label 에서 CE + Softmax 적용 문제점,-0.021165498,0.0,0.021165497601032257
마지막 할 말 -> 로라야 사랑해,PEFT 방법 5가지,-0.010217916,0.0,0.01021791622042656
마지막 할 말 -> 로라야 사랑해,거대 언어 모델 정의,0.008446479,0.0,0.008446479216217995
마지막 할 말 -> 로라야 사랑해,기본 경험,0.012673422,0.0,0.012673421762883663
마지막 할 말 -> 로라야 사랑해,답변 실패,0.0017765368,0.0,0.0017765368102118373
마지막 할 말 -> 로라야 사랑해,딥러닝,-0.018535549,0.0,0.018535548821091652
마지막 할 말 -> 로라야 사랑해,마지막 할 말,0.9853079,1.0,0.014692127704620361
마지막 할 말 -> 로라야 사랑해,머신러닝,-0.00034478767,0.0,0.00034478766610845923
마지막 할 말 -> 로라야 사랑해,면접 시작 인사,-0.015213173,0.0,0.015213172882795334
마지막 할 말 -> 로라야 사랑해,상세 경험,-0.019192033,0.0,0.019192032516002655
마지막 할 말 -> 로라야 사랑해,수식,-0.0037203303,0.0,0.0037203303072601557
마지막 할 말 -> 로라야 사랑해,용어 질문,-0.020072112,0.0,0.02007211185991764
마지막 할 말 -> 로라야 사랑해,인공지능,0.020250821,0.0,0.020250821486115456
마지막 할 말 -> 로라야 사랑해,잠시 휴식,0.008841006,0.0,0.008841006085276604
마지막 할 말 -> 로라야 사랑해,좋아하는 아이돌,0.0059249103,0.0,0.005924910306930542
마지막 할 말 -> 로라야 사랑해,핵심 아이디어,-0.009391963,0.0,0.009391962550580502
마지막 할 말 -> 로라야 사랑해,확률 예측에서 MSE Loss 미 사용 이유,0.017310273,0.0,0.01731027290225029
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,BCE 가 좋은 task,-0.0033916833,0.0,0.0033916833344846964
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,BCE 가 좋은 이유,0.010507929,0.0,0.010507929138839245
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,LLM Fine-Tuning 의 PEFT,0.0062285108,0.0,0.00622851075604558
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,LoRA,-0.014961687,0.0,0.014961686916649342
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,LoRA 와 QLoRA 의 차이,0.005747211,0.0,0.005747211165726185
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,Loss Function 예시,0.030120438,0.0,0.03012043796479702
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,Loss Function 정의,0.007320909,0.0,0.007320908829569817
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,MBTI,-0.0073536816,0.0,0.007353681605309248
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,MSE Loss 설명,-0.0396125,0.0,0.03961250185966492
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,MSE Loss 용도,-0.0044261524,0.0,0.004426152445375919
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,Multi-Label 에서 CE + Softmax 적용 문제점,-0.018342359,0.0,0.018342358991503716
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,PEFT 방법 5가지,-0.012259858,0.0,0.01225985772907734
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,거대 언어 모델 정의,0.0040220795,0.0,0.004022079519927502
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,기본 경험,0.005909671,0.0,0.005909671075642109
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,답변 실패,-0.0009307664,0.0,0.0009307664004154503
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,딥러닝,-0.024439467,0.0,0.02443946711719036
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,마지막 할 말,0.9800187,1.0,0.019981324672698975
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,머신러닝,0.0051110582,0.0,0.005111058242619038
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,면접 시작 인사,-0.021088189,0.0,0.021088188514113426
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,상세 경험,-0.013352026,0.0,0.013352026231586933
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,수식,-0.0036995818,0.0,0.0036995818372815847
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,용어 질문,-0.032143153,0.0,0.03214315325021744
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,인공지능,0.020600649,0.0,0.02060064859688282
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,잠시 휴식,0.0068296427,0.0,0.006829642690718174
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,좋아하는 아이돌,-0.00159053,0.0,0.0015905300388112664
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,핵심 아이디어,-0.011915385,0.0,0.011915384791791439
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,확률 예측에서 MSE Loss 미 사용 이유,0.017948726,0.0,0.017948726192116737
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워,BCE 가 좋은 task,0.0036672004,0.0,0.0036672004498541355
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워,BCE 가 좋은 이유,-1.7788867e-05,0.0,1.778886689862702e-05
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워,LLM Fine-Tuning 의 PEFT,0.0066141984,0.0,0.006614198442548513
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워,LoRA,-0.024470443,0.0,0.024470442906022072
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워,LoRA 와 QLoRA 의 차이,-0.004727236,0.0,0.004727235995233059
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워,Loss Function 예시,0.014497136,0.0,0.014497135765850544
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워,Loss Function 정의,0.0024767003,0.0,0.002476700348779559
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워,MBTI,-0.024619749,0.0,0.024619748815894127
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워,MSE Loss 설명,-0.019312138,0.0,0.019312137737870216
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워,MSE Loss 용도,-0.013345437,0.0,0.013345437124371529
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워,Multi-Label 에서 CE + Softmax 적용 문제점,-0.007948624,0.0,0.007948623970150948
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워,PEFT 방법 5가지,-0.007959866,0.0,0.007959865964949131
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워,거대 언어 모델 정의,0.009146659,0.0,0.009146658703684807
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워,기본 경험,0.014603483,0.0,0.01460348255932331
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워,답변 실패,-0.0028136852,0.0,0.002813685219734907
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워,딥러닝,-0.013378935,0.0,0.013378934934735298
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워,마지막 할 말,0.9822904,1.0,0.01770961284637451
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워,머신러닝,-0.0019210548,0.0,0.0019210547907277942
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워,면접 시작 인사,-0.0073021646,0.0,0.007302164565771818
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워,상세 경험,-0.00022753692,0.0,0.00022753691882826388
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워,수식,0.0030783187,0.0,0.003078318666666746
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워,용어 질문,-0.017398162,0.0,0.017398161813616753
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워,인공지능,0.023754206,0.0,0.02375420555472374
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워,잠시 휴식,0.001157414,0.0,0.0011574140517041087
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워,좋아하는 아이돌,-0.006919413,0.0,0.00691941287368536
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워,핵심 아이디어,-0.015496785,0.0,0.015496784821152687
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워,확률 예측에서 MSE Loss 미 사용 이유,0.01219507,0.0,0.012195070274174213
