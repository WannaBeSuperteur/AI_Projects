input_part,output_answer,predicted_score,ground_truth_score,absolute_error
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,BCE 가 좋은 task,-0.02918688,0.0,0.029186880216002464
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,BCE 가 좋은 이유,-0.01968646,0.0,0.019686460494995117
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,LLM Fine-Tuning 의 PEFT,-0.0023235541,0.0,0.002323554130271077
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,LoRA,-0.0027601328,0.0,0.002760132774710655
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,LoRA 와 QLoRA 의 차이,-0.0052384953,0.0,0.005238495301455259
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,Loss Function 예시,-0.027006645,0.0,0.027006644755601883
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,Loss Function 정의,0.007971625,0.0,0.007971624843776226
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,MBTI,-0.004710382,0.0,0.004710381850600243
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,MSE Loss 설명,-0.008281757,0.0,0.00828175712376833
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,MSE Loss 용도,0.0089209,0.0,0.008920899592339993
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,Multi-Label 에서 CE + Softmax 적용 문제점,0.020473108,0.0,0.02047310769557953
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,PEFT 방법 5가지,-0.00856616,0.0,0.008566159754991531
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,거대 언어 모델 정의,-0.0019003885,0.0,0.001900388509966433
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,기본 경험,-0.017382598,0.0,0.01738259755074978
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,답변 실패,0.0119972015,0.0,0.011997201479971409
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,딥러닝,-0.0017952735,0.0,0.001795273507013917
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,마지막 할 말,0.003489555,0.0,0.0034895550925284624
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,머신러닝,-0.015634276,0.0,0.01563427597284317
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,면접 시작 인사,0.97310144,1.0,0.026898562908172607
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,상세 경험,-0.015749738,0.0,0.0157497376203537
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,수식,-0.013987065,0.0,0.01398706529289484
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,용어 질문,0.020138083,0.0,0.020138083025813103
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,인공지능,-0.0032764215,0.0,0.003276421455666423
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,잠시 휴식,-0.013533859,0.0,0.013533858582377434
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,좋아하는 아이돌,-0.0017007701,0.0,0.0017007701098918915
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,핵심 아이디어,-0.008705102,0.0,0.008705101907253265
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,확률 예측에서 MSE Loss 미 사용 이유,-0.00039827047,0.0,0.0003982704656664282
면접 시작 인사 -> 로라야 안녕 정말 반가워,BCE 가 좋은 task,-0.01028365,0.0,0.010283649899065495
면접 시작 인사 -> 로라야 안녕 정말 반가워,BCE 가 좋은 이유,-0.01103364,0.0,0.011033640243113041
면접 시작 인사 -> 로라야 안녕 정말 반가워,LLM Fine-Tuning 의 PEFT,0.01585609,0.0,0.01585608907043934
면접 시작 인사 -> 로라야 안녕 정말 반가워,LoRA,0.01770171,0.0,0.017701709643006325
면접 시작 인사 -> 로라야 안녕 정말 반가워,LoRA 와 QLoRA 의 차이,-0.010495191,0.0,0.010495191439986229
면접 시작 인사 -> 로라야 안녕 정말 반가워,Loss Function 예시,-0.028470255,0.0,0.028470255434513092
면접 시작 인사 -> 로라야 안녕 정말 반가워,Loss Function 정의,0.019833945,0.0,0.019833944737911224
면접 시작 인사 -> 로라야 안녕 정말 반가워,MBTI,0.0039957073,0.0,0.003995707258582115
면접 시작 인사 -> 로라야 안녕 정말 반가워,MSE Loss 설명,-0.008801768,0.0,0.008801767602562904
면접 시작 인사 -> 로라야 안녕 정말 반가워,MSE Loss 용도,-0.002411902,0.0,0.0024119019508361816
면접 시작 인사 -> 로라야 안녕 정말 반가워,Multi-Label 에서 CE + Softmax 적용 문제점,0.0029384333,0.0,0.0029384333174675703
면접 시작 인사 -> 로라야 안녕 정말 반가워,PEFT 방법 5가지,-0.03240794,0.0,0.032407939434051514
면접 시작 인사 -> 로라야 안녕 정말 반가워,거대 언어 모델 정의,-0.005667055,0.0,0.0056670550256967545
면접 시작 인사 -> 로라야 안녕 정말 반가워,기본 경험,-0.011653117,0.0,0.011653116904199123
면접 시작 인사 -> 로라야 안녕 정말 반가워,답변 실패,-0.014014748,0.0,0.01401474792510271
면접 시작 인사 -> 로라야 안녕 정말 반가워,딥러닝,-0.009406999,0.0,0.009406998753547668
면접 시작 인사 -> 로라야 안녕 정말 반가워,마지막 할 말,0.013624954,0.0,0.013624954037368298
면접 시작 인사 -> 로라야 안녕 정말 반가워,머신러닝,0.0036593138,0.0,0.003659313777461648
면접 시작 인사 -> 로라야 안녕 정말 반가워,면접 시작 인사,0.97953063,1.0,0.020469367504119873
면접 시작 인사 -> 로라야 안녕 정말 반가워,상세 경험,-0.008344449,0.0,0.00834444910287857
면접 시작 인사 -> 로라야 안녕 정말 반가워,수식,-0.013388402,0.0,0.013388401828706264
면접 시작 인사 -> 로라야 안녕 정말 반가워,용어 질문,-0.017694961,0.0,0.01769496127963066
면접 시작 인사 -> 로라야 안녕 정말 반가워,인공지능,-0.009154561,0.0,0.00915456097573042
면접 시작 인사 -> 로라야 안녕 정말 반가워,잠시 휴식,0.0011440206,0.0,0.0011440205853432417
면접 시작 인사 -> 로라야 안녕 정말 반가워,좋아하는 아이돌,0.0026512078,0.0,0.002651207847520709
면접 시작 인사 -> 로라야 안녕 정말 반가워,핵심 아이디어,-0.005177005,0.0,0.00517700519412756
면접 시작 인사 -> 로라야 안녕 정말 반가워,확률 예측에서 MSE Loss 미 사용 이유,0.004344657,0.0,0.004344657063484192
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,BCE 가 좋은 task,-0.0315197,0.0,0.03151969984173775
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,BCE 가 좋은 이유,-0.0065733674,0.0,0.006573367398232222
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,LLM Fine-Tuning 의 PEFT,-0.0070292954,0.0,0.0070292954333126545
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,LoRA,0.0029369665,0.0,0.002936966484412551
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,LoRA 와 QLoRA 의 차이,-0.009724844,0.0,0.009724844247102737
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,Loss Function 예시,-0.014154609,0.0,0.01415460929274559
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,Loss Function 정의,-0.0033516681,0.0,0.0033516681287437677
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,MBTI,0.001452438,0.0,0.0014524379512295127
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,MSE Loss 설명,-0.008779729,0.0,0.008779728785157204
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,MSE Loss 용도,-0.009892861,0.0,0.009892861358821392
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,Multi-Label 에서 CE + Softmax 적용 문제점,0.011060488,0.0,0.011060488410294056
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,PEFT 방법 5가지,-0.031301428,0.0,0.03130142763257027
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,거대 언어 모델 정의,-0.009957679,0.0,0.009957678616046906
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,기본 경험,-0.021632612,0.0,0.021632611751556396
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,답변 실패,0.025268145,0.0,0.02526814490556717
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,딥러닝,0.017604657,0.0,0.017604656517505646
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,마지막 할 말,0.0040085777,0.0,0.004008577670902014
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,머신러닝,-0.004869246,0.0,0.004869245924055576
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,면접 시작 인사,0.9606073,1.0,0.039392709732055664
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,상세 경험,-0.024937775,0.0,0.02493777498602867
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,수식,-0.007674759,0.0,0.007674758788198233
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,용어 질문,0.008685212,0.0,0.008685211651027203
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,인공지능,-0.028057735,0.0,0.028057735413312912
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,잠시 휴식,-0.019678488,0.0,0.01967848837375641
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,좋아하는 아이돌,-0.02095896,0.0,0.02095896005630493
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,핵심 아이디어,-0.013424314,0.0,0.013424313627183437
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,확률 예측에서 MSE Loss 미 사용 이유,0.0027736565,0.0,0.0027736565098166466
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,BCE 가 좋은 task,0.011956708,0.0,0.011956707574427128
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,BCE 가 좋은 이유,0.007335842,0.0,0.0073358421213924885
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,LLM Fine-Tuning 의 PEFT,0.013727009,0.0,0.013727009296417236
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,LoRA,0.02436764,0.0,0.024367639794945717
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,LoRA 와 QLoRA 의 차이,-0.0021961308,0.0,0.002196130808442831
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,Loss Function 예시,-0.029941881,0.0,0.029941881075501442
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,Loss Function 정의,0.0012944121,0.0,0.001294412068091333
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,MBTI,-0.0128730275,0.0,0.012873027473688126
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,MSE Loss 설명,-0.007944811,0.0,0.007944811135530472
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,MSE Loss 용도,0.009129372,0.0,0.009129372425377369
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,Multi-Label 에서 CE + Softmax 적용 문제점,0.0034786088,0.0,0.0034786087926477194
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,PEFT 방법 5가지,-0.029801723,0.0,0.02980172261595726
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,거대 언어 모델 정의,-0.0037930189,0.0,0.0037930188700556755
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,기본 경험,-0.0030752234,0.0,0.0030752234160900116
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,답변 실패,-0.010804533,0.0,0.010804533027112484
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,딥러닝,0.0015164214,0.0,0.0015164214419201016
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,마지막 할 말,0.028514689,0.0,0.028514688834547997
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,머신러닝,0.0009070444,0.0,0.0009070443920791149
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,면접 시작 인사,0.9674904,1.0,0.03250962495803833
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,상세 경험,-0.00027577562,0.0,0.00027577561559155583
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,수식,-0.008583694,0.0,0.008583693765103817
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,용어 질문,-0.049771775,0.0,0.04977177456021309
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,인공지능,-0.030281276,0.0,0.030281275510787964
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,잠시 휴식,-0.0138232885,0.0,0.01382328849285841
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,좋아하는 아이돌,-0.0044174823,0.0,0.0044174822978675365
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,핵심 아이디어,0.010875624,0.0,0.010875623673200607
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,확률 예측에서 MSE Loss 미 사용 이유,0.0036332768,0.0,0.003633276792243123
면접 시작 인사 -> 파이팅! 시작하자,BCE 가 좋은 task,0.0011053567,0.0,0.00110535672865808
면접 시작 인사 -> 파이팅! 시작하자,BCE 가 좋은 이유,-0.0064439694,0.0,0.006443969439715147
면접 시작 인사 -> 파이팅! 시작하자,LLM Fine-Tuning 의 PEFT,0.0126999235,0.0,0.012699923478066921
면접 시작 인사 -> 파이팅! 시작하자,LoRA,0.0062034377,0.0,0.006203437689691782
면접 시작 인사 -> 파이팅! 시작하자,LoRA 와 QLoRA 의 차이,-0.0162495,0.0,0.01624950021505356
면접 시작 인사 -> 파이팅! 시작하자,Loss Function 예시,-0.01493536,0.0,0.014935360290110111
면접 시작 인사 -> 파이팅! 시작하자,Loss Function 정의,0.005582198,0.0,0.0055821980349719524
면접 시작 인사 -> 파이팅! 시작하자,MBTI,0.008711581,0.0,0.008711581118404865
면접 시작 인사 -> 파이팅! 시작하자,MSE Loss 설명,-0.017293168,0.0,0.017293168231844902
면접 시작 인사 -> 파이팅! 시작하자,MSE Loss 용도,0.0035982905,0.0,0.0035982904955744743
면접 시작 인사 -> 파이팅! 시작하자,Multi-Label 에서 CE + Softmax 적용 문제점,0.016300293,0.0,0.016300292685627937
면접 시작 인사 -> 파이팅! 시작하자,PEFT 방법 5가지,-0.017696088,0.0,0.017696088179945946
면접 시작 인사 -> 파이팅! 시작하자,거대 언어 모델 정의,-0.0055254297,0.0,0.005525429733097553
면접 시작 인사 -> 파이팅! 시작하자,기본 경험,-0.0120179085,0.0,0.01201790850609541
면접 시작 인사 -> 파이팅! 시작하자,답변 실패,-0.018263776,0.0,0.01826377585530281
면접 시작 인사 -> 파이팅! 시작하자,딥러닝,0.0002883235,0.0,0.0002883234992623329
면접 시작 인사 -> 파이팅! 시작하자,마지막 할 말,0.0065167733,0.0,0.0065167732536792755
면접 시작 인사 -> 파이팅! 시작하자,머신러닝,0.0021071373,0.0,0.0021071373485028744
면접 시작 인사 -> 파이팅! 시작하자,면접 시작 인사,0.9831548,1.0,0.016845226287841797
면접 시작 인사 -> 파이팅! 시작하자,상세 경험,-0.010077223,0.0,0.010077223181724548
면접 시작 인사 -> 파이팅! 시작하자,수식,-0.012043263,0.0,0.012043262831866741
면접 시작 인사 -> 파이팅! 시작하자,용어 질문,-0.043854278,0.0,0.043854277580976486
면접 시작 인사 -> 파이팅! 시작하자,인공지능,-0.019528762,0.0,0.019528761506080627
면접 시작 인사 -> 파이팅! 시작하자,잠시 휴식,-0.009431466,0.0,0.009431466460227966
면접 시작 인사 -> 파이팅! 시작하자,좋아하는 아이돌,0.0018356789,0.0,0.0018356789369136095
면접 시작 인사 -> 파이팅! 시작하자,핵심 아이디어,0.015039401,0.0,0.01503940112888813
면접 시작 인사 -> 파이팅! 시작하자,확률 예측에서 MSE Loss 미 사용 이유,0.0048078974,0.0,0.004807897377759218
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",BCE 가 좋은 task,0.0023896438,0.0,0.002389643806964159
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",BCE 가 좋은 이유,-0.010361821,0.0,0.01036182139068842
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",LLM Fine-Tuning 의 PEFT,0.003991354,0.0,0.003991353791207075
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",LoRA,0.01060953,0.0,0.010609529912471771
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",LoRA 와 QLoRA 의 차이,-0.0270008,0.0,0.027000799775123596
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",Loss Function 예시,-0.0210001,0.0,0.021000100299715996
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",Loss Function 정의,-0.015868148,0.0,0.01586814783513546
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",MBTI,-0.015653176,0.0,0.015653176233172417
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",MSE Loss 설명,-0.015933055,0.0,0.01593305543065071
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",MSE Loss 용도,0.016970295,0.0,0.01697029545903206
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",Multi-Label 에서 CE + Softmax 적용 문제점,-0.024558175,0.0,0.024558175355196
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",PEFT 방법 5가지,-0.037652977,0.0,0.03765297681093216
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",거대 언어 모델 정의,-0.0012871927,0.0,0.0012871926883235574
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",기본 경험,0.14792252,0.0,0.14792251586914062
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",답변 실패,0.8725919,1.0,0.12740808725357056
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",딥러닝,0.033831272,0.0,0.03383127227425575
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",마지막 할 말,-0.019578502,0.0,0.01957850158214569
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",머신러닝,-0.026480716,0.0,0.026480715721845627
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",면접 시작 인사,-0.007960334,0.0,0.007960334420204163
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",상세 경험,-0.026380256,0.0,0.026380255818367004
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",수식,0.027927915,0.0,0.027927914634346962
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",용어 질문,-0.04805912,0.0,0.048059120774269104
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",인공지능,0.091097474,0.0,0.09109747409820557
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",잠시 휴식,-0.016522054,0.0,0.016522053629159927
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",좋아하는 아이돌,0.043284286,0.0,0.04328428581357002
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",핵심 아이디어,0.0025721884,0.0,0.002572188386693597
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",확률 예측에서 MSE Loss 미 사용 이유,-0.007094935,0.0,0.007094935048371553
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",BCE 가 좋은 task,-0.0025655974,0.0,0.002565597416833043
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",BCE 가 좋은 이유,-0.006113996,0.0,0.006113995797932148
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",LLM Fine-Tuning 의 PEFT,-0.013552388,0.0,0.013552388176321983
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",LoRA,0.013068699,0.0,0.013068699277937412
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",LoRA 와 QLoRA 의 차이,-0.0024856706,0.0,0.0024856706149876118
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",Loss Function 예시,-0.006485354,0.0,0.006485354155302048
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",Loss Function 정의,-0.0043769022,0.0,0.004376902244985104
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",MBTI,0.0060945028,0.0,0.006094502750784159
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",MSE Loss 설명,0.008007274,0.0,0.008007274009287357
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",MSE Loss 용도,0.008191096,0.0,0.008191095665097237
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",Multi-Label 에서 CE + Softmax 적용 문제점,-0.023297934,0.0,0.023297933861613274
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",PEFT 방법 5가지,0.0005531694,0.0,0.0005531694041565061
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",거대 언어 모델 정의,-0.018088419,0.0,0.01808841899037361
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",기본 경험,-0.037670054,0.0,0.03767005354166031
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",답변 실패,-0.02154041,0.0,0.021540410816669464
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",딥러닝,-0.017952723,0.0,0.017952723428606987
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",마지막 할 말,-0.004464277,0.0,0.004464277066290379
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",머신러닝,0.013445856,0.0,0.013445856049656868
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",면접 시작 인사,-0.0069612414,0.0,0.006961241364479065
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",상세 경험,0.01634908,0.0,0.016349079087376595
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",수식,0.028844392,0.0,0.02884439192712307
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",용어 질문,0.0025904032,0.0,0.0025904031936079264
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",인공지능,0.9198957,1.0,0.0801042914390564
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",잠시 휴식,-0.012530173,0.0,0.012530173175036907
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",좋아하는 아이돌,-0.0074814344,0.0,0.007481434382498264
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",핵심 아이디어,-0.0016932759,0.0,0.0016932758735492826
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",확률 예측에서 MSE Loss 미 사용 이유,-0.0024831828,0.0,0.00248318281956017
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",BCE 가 좋은 task,-0.0017875592,0.0,0.0017875592457130551
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",BCE 가 좋은 이유,0.003662208,0.0,0.0036622080951929092
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",LLM Fine-Tuning 의 PEFT,-0.016797828,0.0,0.016797827556729317
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",LoRA,-0.01882361,0.0,0.018823610618710518
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",LoRA 와 QLoRA 의 차이,-0.0060750833,0.0,0.006075083278119564
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",Loss Function 예시,0.019796431,0.0,0.019796431064605713
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",Loss Function 정의,-0.00010105155,0.0,0.00010105154797201976
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",MBTI,-0.026855577,0.0,0.026855576783418655
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",MSE Loss 설명,0.010166893,0.0,0.010166892781853676
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",MSE Loss 용도,-0.013307746,0.0,0.013307745568454266
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",Multi-Label 에서 CE + Softmax 적용 문제점,-0.006496988,0.0,0.006496987771242857
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",PEFT 방법 5가지,-0.0055983914,0.0,0.005598391406238079
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",거대 언어 모델 정의,0.003920994,0.0,0.003920993767678738
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",기본 경험,0.02484276,0.0,0.02484275959432125
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",답변 실패,-0.0036766767,0.0,0.003676676657050848
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",딥러닝,-0.056431267,0.0,0.05643126741051674
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",마지막 할 말,-0.0013296541,0.0,0.0013296541292220354
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",머신러닝,0.93637645,1.0,0.06362354755401611
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",면접 시작 인사,0.009025466,0.0,0.009025465697050095
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",상세 경험,-0.014278657,0.0,0.014278656803071499
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",수식,-0.0001624214,0.0,0.00016242140554822981
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",용어 질문,-0.0008489472,0.0,0.0008489472093060613
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",인공지능,-0.024736881,0.0,0.024736881256103516
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",잠시 휴식,-0.023750616,0.0,0.02375061623752117
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",좋아하는 아이돌,0.015832478,0.0,0.015832478180527687
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",핵심 아이디어,-0.02467569,0.0,0.02467568963766098
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",확률 예측에서 MSE Loss 미 사용 이유,0.0005774419,0.0,0.0005774418823421001
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",BCE 가 좋은 task,0.01381237,0.0,0.013812369666993618
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",BCE 가 좋은 이유,0.00069887284,0.0,0.0006988728418946266
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",LLM Fine-Tuning 의 PEFT,-0.037619196,0.0,0.03761919587850571
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",LoRA,0.003978358,0.0,0.003978358116000891
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",LoRA 와 QLoRA 의 차이,-0.0022015187,0.0,0.0022015187423676252
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",Loss Function 예시,0.007535458,0.0,0.007535458076745272
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",Loss Function 정의,-0.030614708,0.0,0.030614707618951797
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",MBTI,-0.01020108,0.0,0.01020107977092266
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",MSE Loss 설명,-0.010397712,0.0,0.010397711768746376
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",MSE Loss 용도,-0.021834716,0.0,0.021834716200828552
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",Multi-Label 에서 CE + Softmax 적용 문제점,0.0032660107,0.0,0.0032660106662660837
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",PEFT 방법 5가지,0.00092145713,0.0,0.0009214571327902377
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",거대 언어 모델 정의,0.0043088095,0.0,0.0043088095262646675
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",기본 경험,0.016184505,0.0,0.016184505075216293
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",답변 실패,0.008137079,0.0,0.008137078955769539
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",딥러닝,0.9570958,1.0,0.04290419816970825
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",마지막 할 말,-0.0065560634,0.0,0.006556063424795866
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",머신러닝,-0.09693991,0.0,0.09693990647792816
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",면접 시작 인사,-0.0057774116,0.0,0.005777411628514528
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",상세 경험,0.015124364,0.0,0.015124363824725151
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",수식,0.0063295667,0.0,0.006329566705971956
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",용어 질문,0.030811045,0.0,0.030811045318841934
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",인공지능,-0.06246243,0.0,0.06246243044734001
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",잠시 휴식,-0.007624351,0.0,0.00762435095384717
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",좋아하는 아이돌,-0.0055513256,0.0,0.005551325622946024
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",핵심 아이디어,-0.006041017,0.0,0.006041016895323992
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",확률 예측에서 MSE Loss 미 사용 이유,0.01665533,0.0,0.0166553296148777
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",BCE 가 좋은 task,0.0027236308,0.0,0.00272363075055182
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",BCE 가 좋은 이유,-0.0062581087,0.0,0.006258108653128147
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",LLM Fine-Tuning 의 PEFT,-0.046093132,0.0,0.046093132346868515
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",LoRA,-0.0105661545,0.0,0.01056615449488163
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",LoRA 와 QLoRA 의 차이,-0.0078429775,0.0,0.007842977531254292
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",Loss Function 예시,0.01457891,0.0,0.014578909613192081
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",Loss Function 정의,-0.010919662,0.0,0.010919662192463875
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",MBTI,-0.007394934,0.0,0.007394934073090553
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",MSE Loss 설명,-0.019560006,0.0,0.019560005515813828
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",MSE Loss 용도,-0.021228526,0.0,0.021228525787591934
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",Multi-Label 에서 CE + Softmax 적용 문제점,-0.0047252416,0.0,0.00472524156793952
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",PEFT 방법 5가지,0.0035881095,0.0,0.0035881095100194216
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",거대 언어 모델 정의,-0.008545848,0.0,0.008545847609639168
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",기본 경험,0.016548956,0.0,0.01654895581305027
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",답변 실패,0.011745085,0.0,0.011745085008442402
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",딥러닝,0.9551759,1.0,0.04482412338256836
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",마지막 할 말,-0.0026446944,0.0,0.002644694410264492
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",머신러닝,-0.087080374,0.0,0.08708037436008453
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",면접 시작 인사,-0.01076626,0.0,0.010766260325908661
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",상세 경험,0.007203565,0.0,0.00720356497913599
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",수식,-0.003933659,0.0,0.0039336588233709335
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",용어 질문,0.03448824,0.0,0.034488238394260406
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",인공지능,-0.031159442,0.0,0.03115944191813469
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",잠시 휴식,-0.011061064,0.0,0.011061063967645168
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",좋아하는 아이돌,0.0009880459,0.0,0.0009880459401756525
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",핵심 아이디어,-0.032681774,0.0,0.032681774348020554
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",확률 예측에서 MSE Loss 미 사용 이유,0.01934935,0.0,0.019349349662661552
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",BCE 가 좋은 task,-0.020238705,0.0,0.020238704979419708
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",BCE 가 좋은 이유,0.0039471244,0.0,0.003947124350816011
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",LLM Fine-Tuning 의 PEFT,-0.0021694596,0.0,0.002169459592550993
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",LoRA,-0.029313577,0.0,0.029313577339053154
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",LoRA 와 QLoRA 의 차이,-0.010977878,0.0,0.010977878235280514
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",Loss Function 예시,0.021044089,0.0,0.021044088527560234
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",Loss Function 정의,0.005348373,0.0,0.005348373204469681
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",MBTI,-0.014270828,0.0,0.014270828105509281
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",MSE Loss 설명,0.010517379,0.0,0.010517379269003868
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",MSE Loss 용도,-0.031379838,0.0,0.03137983754277229
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",Multi-Label 에서 CE + Softmax 적용 문제점,-0.00029635278,0.0,0.00029635278042405844
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",PEFT 방법 5가지,0.010701887,0.0,0.010701887309551239
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",거대 언어 모델 정의,0.00558809,0.0,0.005588090047240257
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",기본 경험,0.029599717,0.0,0.029599716886878014
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",답변 실패,0.007976365,1.0,0.992023634724319
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",딥러닝,-0.025201341,0.0,0.02520134113729
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",마지막 할 말,-0.013992836,0.0,0.013992835767567158
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",머신러닝,0.91021657,0.0,0.9102165699005127
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",면접 시작 인사,-0.012175327,0.0,0.01217532716691494
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",상세 경험,-0.03288234,0.0,0.032882340252399445
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",수식,0.0095200455,0.0,0.009520045481622219
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",용어 질문,0.01644882,0.0,0.016448820009827614
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",인공지능,-0.011240437,0.0,0.01124043669551611
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",잠시 휴식,0.003947181,0.0,0.003947181161493063
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",좋아하는 아이돌,-0.0047485773,0.0,0.004748577252030373
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",핵심 아이디어,-0.02417639,0.0,0.02417639084160328
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",확률 예측에서 MSE Loss 미 사용 이유,-0.022522144,0.0,0.02252214401960373
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아",BCE 가 좋은 task,0.0016998944,0.0,0.0016998944338411093
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아",BCE 가 좋은 이유,0.0005555871,0.0,0.0005555871175602078
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아",LLM Fine-Tuning 의 PEFT,-0.0068832813,0.0,0.006883281283080578
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아",LoRA,0.0034334194,0.0,0.003433419391512871
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아",LoRA 와 QLoRA 의 차이,-0.017766077,0.0,0.0177660770714283
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아",Loss Function 예시,-0.0044425386,0.0,0.004442538600414991
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아",Loss Function 정의,-0.01115232,0.0,0.011152319610118866
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아",MBTI,0.0028204343,0.0,0.002820434281602502
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아",MSE Loss 설명,0.00010864603,0.0,0.00010864603245863691
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아",MSE Loss 용도,0.0026256712,0.0,0.0026256712153553963
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아",Multi-Label 에서 CE + Softmax 적용 문제점,-0.007768836,0.0,0.007768835872411728
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아",PEFT 방법 5가지,-0.0035344695,0.0,0.003534469520673156
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아",거대 언어 모델 정의,-0.0064472337,0.0,0.006447233725339174
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아",기본 경험,0.0046607205,0.0,0.004660720471292734
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아",답변 실패,0.97636116,1.0,0.02363884449005127
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아",딥러닝,0.008171838,0.0,0.008171837776899338
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아",마지막 할 말,-0.01780751,0.0,0.017807509750127792
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아",머신러닝,-0.008212795,0.0,0.008212794549763203
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아",면접 시작 인사,-0.015234851,0.0,0.015234851278364658
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아",상세 경험,-0.013757875,0.0,0.013757875189185143
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아",수식,-0.016789427,0.0,0.016789427027106285
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아",용어 질문,-0.011568533,0.0,0.011568533256649971
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아",인공지능,0.0061013023,0.0,0.006101302336901426
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아",잠시 휴식,-0.00924992,0.0,0.009249920025467873
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아",좋아하는 아이돌,0.008402424,0.0,0.008402423933148384
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아",핵심 아이디어,-0.011143828,0.0,0.011143827810883522
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아",확률 예측에서 MSE Loss 미 사용 이유,-0.0072849337,0.0,0.00728493370115757
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지?",BCE 가 좋은 task,-0.011000902,0.0,0.011000902391970158
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지?",BCE 가 좋은 이유,-0.003615965,0.0,0.0036159649025648832
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지?",LLM Fine-Tuning 의 PEFT,-0.004910681,0.0,0.004910680931061506
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지?",LoRA,0.008516406,0.0,0.008516405709087849
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지?",LoRA 와 QLoRA 의 차이,-0.0035915573,0.0,0.003591557266190648
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지?",Loss Function 예시,-0.0136561105,0.0,0.013656110502779484
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지?",Loss Function 정의,-0.008407193,0.0,0.00840719323605299
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지?",MBTI,-0.0015725122,0.0,0.001572512206621468
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지?",MSE Loss 설명,0.014618299,0.0,0.014618298970162868
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지?",MSE Loss 용도,0.0054169563,0.0,0.005416956264525652
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지?",Multi-Label 에서 CE + Softmax 적용 문제점,-0.015730089,0.0,0.01573008857667446
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지?",PEFT 방법 5가지,0.0023021156,0.0,0.002302115550264716
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지?",거대 언어 모델 정의,-0.011907061,0.0,0.011907060630619526
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지?",기본 경험,-0.042823363,0.0,0.04282336309552193
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지?",답변 실패,-0.025394598,0.0,0.02539459802210331
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지?",딥러닝,0.0018150326,0.0,0.0018150325631722808
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지?",마지막 할 말,0.0015354413,0.0,0.0015354412607848644
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지?",머신러닝,0.0045522223,0.0,0.0045522223226726055
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지?",면접 시작 인사,-0.009061031,0.0,0.00906103104352951
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지?",상세 경험,0.020943245,0.0,0.02094324491918087
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지?",수식,0.04409227,0.0,0.044092271476984024
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지?",용어 질문,0.004927013,0.0,0.004927013069391251
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지?",인공지능,0.9100214,1.0,0.08997857570648193
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지?",잠시 휴식,-0.015402279,0.0,0.015402278862893581
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지?",좋아하는 아이돌,-0.006823152,0.0,0.006823151838034391
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지?",핵심 아이디어,-0.013911095,0.0,0.013911095447838306
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지?",확률 예측에서 MSE Loss 미 사용 이유,-0.006859904,0.0,0.006859904155135155
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야!",BCE 가 좋은 task,-0.0025253904,0.0,0.0025253903586417437
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야!",BCE 가 좋은 이유,0.0011690058,0.0,0.0011690057581290603
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야!",LLM Fine-Tuning 의 PEFT,-0.010898088,0.0,0.010898088105022907
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야!",LoRA,-0.004342353,0.0,0.004342352971434593
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야!",LoRA 와 QLoRA 의 차이,-0.0035323328,0.0,0.0035323328338563442
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야!",Loss Function 예시,0.012506254,0.0,0.012506254017353058
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야!",Loss Function 정의,-0.011197479,0.0,0.01119747944176197
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야!",MBTI,-0.027212609,0.0,0.027212608605623245
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야!",MSE Loss 설명,0.0004317126,0.0,0.000431712600402534
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야!",MSE Loss 용도,-0.014090633,0.0,0.014090633019804955
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야!",Multi-Label 에서 CE + Softmax 적용 문제점,-0.003046778,0.0,0.003046778030693531
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야!",PEFT 방법 5가지,-0.003988691,0.0,0.0039886911399662495
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야!",거대 언어 모델 정의,0.0020754682,0.0,0.0020754681900143623
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야!",기본 경험,0.02327779,0.0,0.02327778935432434
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야!",답변 실패,-0.010690646,0.0,0.01069064624607563
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야!",딥러닝,-0.042637147,0.0,0.04263714700937271
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야!",마지막 할 말,-0.0024357543,0.0,0.0024357542861253023
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야!",머신러닝,0.9331719,1.0,0.06682807207107544
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야!",면접 시작 인사,0.004938098,0.0,0.004938098136335611
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야!",상세 경험,-0.006101027,0.0,0.0061010271310806274
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야!",수식,0.0060502747,0.0,0.006050274707376957
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야!",용어 질문,-0.004539907,0.0,0.004539906978607178
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야!",인공지능,-0.041381557,0.0,0.041381556540727615
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야!",잠시 휴식,-0.023479024,0.0,0.023479023948311806
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야!",좋아하는 아이돌,0.011058362,0.0,0.011058362200856209
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야!",핵심 아이디어,-0.016580367,0.0,0.0165803674608469
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야!",확률 예측에서 MSE Loss 미 사용 이유,-0.0033967176,0.0,0.0033967175986617804
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지",BCE 가 좋은 task,0.011074903,0.0,0.011074903421103954
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지",BCE 가 좋은 이유,0.0038656252,0.0,0.0038656252436339855
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지",LLM Fine-Tuning 의 PEFT,-0.046333127,0.0,0.04633312672376633
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지",LoRA,0.013337913,0.0,0.01333791296929121
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지",LoRA 와 QLoRA 의 차이,-0.0017665613,0.0,0.001766561297699809
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지",Loss Function 예시,0.014371213,0.0,0.01437121257185936
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지",Loss Function 정의,-0.015534019,0.0,0.015534019097685814
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지",MBTI,-0.005427638,0.0,0.005427638068795204
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지",MSE Loss 설명,-0.012780382,0.0,0.012780382297933102
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지",MSE Loss 용도,-0.02362663,0.0,0.023626629263162613
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지",Multi-Label 에서 CE + Softmax 적용 문제점,0.0051553957,0.0,0.005155395716428757
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지",PEFT 방법 5가지,-0.006330291,0.0,0.00633029080927372
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지",거대 언어 모델 정의,-0.0031409455,0.0,0.003140945453196764
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지",기본 경험,0.019220985,0.0,0.0192209854722023
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지",답변 실패,0.0032290528,0.0,0.003229052759706974
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지",딥러닝,0.9544604,1.0,0.04553961753845215
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지",마지막 할 말,-0.0105428975,0.0,0.010542897507548332
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지",머신러닝,-0.065649435,0.0,0.06564943492412567
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지",면접 시작 인사,-0.009457522,0.0,0.009457522071897984
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지",상세 경험,0.010999407,0.0,0.010999406687915325
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지",수식,-6.324021e-05,0.0,6.324020796455443e-05
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지",용어 질문,0.03151877,0.0,0.03151876851916313
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지",인공지능,-0.05610284,0.0,0.05610283836722374
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지",잠시 휴식,-0.0053435424,0.0,0.00534354243427515
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지",좋아하는 아이돌,0.0030808398,0.0,0.0030808397568762302
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지",핵심 아이디어,-0.019732136,0.0,0.01973213627934456
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지",확률 예측에서 MSE Loss 미 사용 이유,0.015817083,0.0,0.015817083418369293
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야.",BCE 가 좋은 task,0.01568702,0.0,0.015687020495533943
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야.",BCE 가 좋은 이유,0.003136169,0.0,0.003136168932542205
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야.",LLM Fine-Tuning 의 PEFT,-0.039410885,0.0,0.03941088542342186
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야.",LoRA,0.0075455564,0.0,0.007545556407421827
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야.",LoRA 와 QLoRA 의 차이,-0.004821013,0.0,0.004821013193577528
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야.",Loss Function 예시,0.0069417735,0.0,0.00694177346304059
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야.",Loss Function 정의,-0.0256485,0.0,0.02564850077033043
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야.",MBTI,-0.009058065,0.0,0.00905806478112936
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야.",MSE Loss 설명,-0.004263804,0.0,0.004263803828507662
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야.",MSE Loss 용도,-0.02606857,0.0,0.026068570092320442
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야.",Multi-Label 에서 CE + Softmax 적용 문제점,0.00958072,0.0,0.009580720216035843
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야.",PEFT 방법 5가지,0.00074398925,0.0,0.0007439892506226897
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야.",거대 언어 모델 정의,-0.001741819,0.0,0.0017418189672753215
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야.",기본 경험,0.02483707,0.0,0.02483706921339035
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야.",답변 실패,0.0046935542,0.0,0.004693554248660803
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야.",딥러닝,0.9550484,1.0,0.04495161771774292
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야.",마지막 할 말,-0.004422404,0.0,0.004422403872013092
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야.",머신러닝,-0.070321746,0.0,0.07032174617052078
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야.",면접 시작 인사,-0.0060751345,0.0,0.006075134500861168
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야.",상세 경험,0.0118427165,0.0,0.011842716485261917
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야.",수식,0.005746724,0.0,0.005746724084019661
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야.",용어 질문,0.02365257,0.0,0.023652570322155952
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야.",인공지능,-0.03969418,0.0,0.039694178849458694
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야.",잠시 휴식,-0.0065418575,0.0,0.006541857495903969
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야.",좋아하는 아이돌,-0.004526546,0.0,0.004526546224951744
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야.",핵심 아이디어,-0.030348483,0.0,0.030348483473062515
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야.",확률 예측에서 MSE Loss 미 사용 이유,0.018085085,0.0,0.018085084855556488
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로",BCE 가 좋은 task,-0.013261488,0.0,0.01326148770749569
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로",BCE 가 좋은 이유,-0.0014786171,0.0,0.0014786170795559883
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로",LLM Fine-Tuning 의 PEFT,0.004493029,0.0,0.004493028856813908
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로",LoRA,-0.009000962,0.0,0.009000961668789387
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로",LoRA 와 QLoRA 의 차이,-0.016511235,0.0,0.016511235386133194
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로",Loss Function 예시,-0.021605229,0.0,0.021605229005217552
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로",Loss Function 정의,-0.009277614,0.0,0.009277613833546638
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로",MBTI,0.0055064606,0.0,0.005506460554897785
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로",MSE Loss 설명,-0.002994516,0.0,0.002994515933096409
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로",MSE Loss 용도,-0.0028339196,0.0,0.0028339195996522903
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로",Multi-Label 에서 CE + Softmax 적용 문제점,-0.013271643,0.0,0.013271642848849297
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로",PEFT 방법 5가지,-0.017521944,0.0,0.017521943897008896
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로",거대 언어 모델 정의,-0.0021487125,0.0,0.002148712519556284
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로",기본 경험,-0.00037519322,0.0,0.00037519322359003127
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로",답변 실패,0.96298254,1.0,0.03701746463775635
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로",딥러닝,-0.00096355216,0.0,0.0009635521564632654
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로",마지막 할 말,-0.0128995245,0.0,0.01289952453225851
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로",머신러닝,0.0950283,0.0,0.09502830356359482
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로",면접 시작 인사,-0.014737549,0.0,0.014737549237906933
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로",상세 경험,-0.030519083,0.0,0.03051908314228058
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로",수식,-0.013726049,0.0,0.013726049102842808
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로",용어 질문,0.0016171784,0.0,0.0016171784373000264
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로",인공지능,-0.004445243,0.0,0.004445243161171675
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로",잠시 휴식,-0.0022244656,0.0,0.002224465599283576
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로",좋아하는 아이돌,-0.008720247,0.0,0.008720247074961662
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로",핵심 아이디어,-0.02665032,0.0,0.026650320738554
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로",확률 예측에서 MSE Loss 미 사용 이유,0.007887919,0.0,0.007887919433414936
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,BCE 가 좋은 task,0.0077894595,0.0,0.0077894595451653
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,BCE 가 좋은 이유,0.009070264,0.0,0.009070264175534248
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,LLM Fine-Tuning 의 PEFT,0.00086998334,0.0,0.0008699833415448666
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,LoRA,-0.002759732,0.0,0.002759732073172927
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,LoRA 와 QLoRA 의 차이,0.011803241,0.0,0.011803241446614265
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,Loss Function 예시,-0.0022272824,0.0,0.0022272823844105005
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,Loss Function 정의,-0.02426981,0.0,0.02426980994641781
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,MBTI,0.00047624274,0.0,0.00047624274156987667
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,MSE Loss 설명,-0.0033290607,0.0,0.0033290607389062643
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,MSE Loss 용도,-0.021481683,0.0,0.02148168347775936
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,Multi-Label 에서 CE + Softmax 적용 문제점,0.010507087,0.0,0.010507087223231792
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,PEFT 방법 5가지,-0.006282323,0.0,0.00628232304006815
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,거대 언어 모델 정의,0.97668153,1.0,0.023318469524383545
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,기본 경험,0.0066028764,0.0,0.006602876354008913
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,답변 실패,-0.0004974296,0.0,0.0004974295734427869
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,딥러닝,-0.026196148,0.0,0.026196148246526718
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,마지막 할 말,0.0014047932,0.0,0.0014047932345420122
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,머신러닝,-0.010530193,0.0,0.010530193336308002
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,면접 시작 인사,-0.0040429477,0.0,0.004042947664856911
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,상세 경험,0.0033345814,0.0,0.003334581386297941
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,수식,-0.0068683787,0.0,0.006868378724902868
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,용어 질문,-0.032147218,0.0,0.03214721754193306
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,인공지능,-0.013256033,0.0,0.013256032951176167
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,잠시 휴식,-0.002070914,0.0,0.0020709140226244926
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,좋아하는 아이돌,0.021664089,0.0,0.02166408859193325
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,핵심 아이디어,0.0087686265,0.0,0.008768626488745213
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,확률 예측에서 MSE Loss 미 사용 이유,-0.010010119,0.0,0.01001011859625578
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,BCE 가 좋은 task,-0.007283252,0.0,0.007283252198249102
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,BCE 가 좋은 이유,-0.025867885,0.0,0.025867884978652
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,LLM Fine-Tuning 의 PEFT,0.011476574,0.0,0.011476574465632439
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,LoRA,-0.0045617525,0.0,0.004561752546578646
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,LoRA 와 QLoRA 의 차이,-0.0021567016,0.0,0.002156701637431979
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,Loss Function 예시,-0.016225629,0.0,0.016225628554821014
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,Loss Function 정의,-0.0066845156,0.0,0.006684515625238419
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,MBTI,-0.039757203,0.0,0.03975720331072807
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,MSE Loss 설명,0.03829625,0.0,0.03829624876379967
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,MSE Loss 용도,-0.015158004,0.0,0.015158004127442837
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,Multi-Label 에서 CE + Softmax 적용 문제점,-0.021601234,0.0,0.021601233631372452
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,PEFT 방법 5가지,0.0047004363,0.0,0.004700436256825924
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,거대 언어 모델 정의,0.9556338,0.0,0.955633819103241
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,기본 경험,0.028687648,0.0,0.028687648475170135
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,답변 실패,0.026962502,1.0,0.9730374980717897
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,딥러닝,-0.00745494,0.0,0.007454940117895603
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,마지막 할 말,0.0004893509,0.0,0.0004893508739769459
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,머신러닝,-0.024955858,0.0,0.024955857545137405
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,면접 시작 인사,-0.016863989,0.0,0.01686398871243
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,상세 경험,-0.0075349663,0.0,0.007534966338425875
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,수식,0.012560595,0.0,0.012560594826936722
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,용어 질문,0.0071221325,0.0,0.007122132461518049
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,인공지능,-0.01130051,0.0,0.011300509795546532
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,잠시 휴식,-0.01231048,0.0,0.012310479767620564
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,좋아하는 아이돌,0.011652796,0.0,0.011652795597910881
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,핵심 아이디어,-0.028168531,0.0,0.028168531134724617
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,확률 예측에서 MSE Loss 미 사용 이유,-0.0075494633,0.0,0.007549463305622339
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델,BCE 가 좋은 task,0.025985684,0.0,0.025985684245824814
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델,BCE 가 좋은 이유,0.0006292037,0.0,0.0006292036850936711
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델,LLM Fine-Tuning 의 PEFT,0.004655109,0.0,0.004655108787119389
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델,LoRA,-4.0116567e-05,0.0,4.011656710645184e-05
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델,LoRA 와 QLoRA 의 차이,0.009387162,0.0,0.009387161582708359
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델,Loss Function 예시,-0.010039712,0.0,0.01003971230238676
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델,Loss Function 정의,-0.014878543,0.0,0.014878543093800545
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델,MBTI,0.009646122,0.0,0.009646122343838215
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델,MSE Loss 설명,0.007561875,0.0,0.00756187504157424
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델,MSE Loss 용도,-0.02738503,0.0,0.027385029941797256
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델,Multi-Label 에서 CE + Softmax 적용 문제점,0.022829559,0.0,0.022829558700323105
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델,PEFT 방법 5가지,-0.014206845,0.0,0.014206845313310623
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델,거대 언어 모델 정의,0.95692647,1.0,0.04307353496551514
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델,기본 경험,-0.011050149,0.0,0.011050148867070675
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델,답변 실패,-0.0075968923,0.0,0.007596892304718494
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델,딥러닝,-0.035659105,0.0,0.03565910458564758
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델,마지막 할 말,0.00017270801,0.0,0.00017270800890401006
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델,머신러닝,-0.017314443,0.0,0.017314443364739418
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델,면접 시작 인사,-0.006061325,0.0,0.00606132484972477
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델,상세 경험,0.017908556,0.0,0.017908556386828423
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델,수식,-0.010709435,0.0,0.010709434747695923
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델,용어 질문,-0.022923606,0.0,0.022923605516552925
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델,인공지능,-0.02654968,0.0,0.026549680158495903
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델,잠시 휴식,-0.007140449,0.0,0.007140448782593012
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델,좋아하는 아이돌,0.026846848,0.0,0.02684684842824936
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델,핵심 아이디어,0.019776024,0.0,0.01977602392435074
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델,확률 예측에서 MSE Loss 미 사용 이유,-0.004715181,0.0,0.004715180955827236
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지.,BCE 가 좋은 task,0.018358657,0.0,0.018358657136559486
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지.,BCE 가 좋은 이유,0.0037363376,0.0,0.003736337646842003
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지.,LLM Fine-Tuning 의 PEFT,-0.012158562,0.0,0.012158562429249287
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지.,LoRA,0.019223543,0.0,0.019223542883992195
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지.,LoRA 와 QLoRA 의 차이,-0.011940055,0.0,0.011940054595470428
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지.,Loss Function 예시,-0.022048883,0.0,0.022048883140087128
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지.,Loss Function 정의,-0.022023574,0.0,0.022023573517799377
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지.,MBTI,0.029027358,0.0,0.029027357697486877
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지.,MSE Loss 설명,-0.0066190073,0.0,0.0066190073266625404
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지.,MSE Loss 용도,0.00848422,0.0,0.008484220132231712
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지.,Multi-Label 에서 CE + Softmax 적용 문제점,-0.020648506,0.0,0.02064850553870201
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지.,PEFT 방법 5가지,-0.017722977,0.0,0.017722977325320244
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지.,거대 언어 모델 정의,0.030409077,0.0,0.030409077182412148
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지.,기본 경험,-0.0004734931,0.0,0.00047349309897981584
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지.,답변 실패,0.9599341,1.0,0.040065884590148926
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지.,딥러닝,0.0010404746,0.0,0.001040474628098309
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지.,마지막 할 말,0.009229106,0.0,0.009229105897247791
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지.,머신러닝,-0.008475068,0.0,0.008475068025290966
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지.,면접 시작 인사,0.016062118,0.0,0.016062118113040924
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지.,상세 경험,-0.011545821,0.0,0.011545821093022823
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지.,수식,0.00010808114,0.0,0.00010808114166138694
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지.,용어 질문,-0.04074119,0.0,0.04074119031429291
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지.,인공지능,-0.034726832,0.0,0.034726832062006
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지.,잠시 휴식,-0.0145206535,0.0,0.014520653523504734
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지.,좋아하는 아이돌,-0.0021293964,0.0,0.0021293964236974716
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지.,핵심 아이디어,-0.0058630435,0.0,0.00586304347962141
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지.,확률 예측에서 MSE Loss 미 사용 이유,-0.000658312,0.0,0.0006583119975402951
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,BCE 가 좋은 task,0.0062251985,0.0,0.00622519850730896
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,BCE 가 좋은 이유,0.020844575,0.0,0.02084457501769066
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,LLM Fine-Tuning 의 PEFT,-0.05463281,0.0,0.05463280901312828
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,LoRA,0.0015438534,0.0,0.0015438534319400787
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,LoRA 와 QLoRA 의 차이,0.020822573,0.0,0.020822573453187943
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,Loss Function 예시,-0.020412039,0.0,0.020412039011716843
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,Loss Function 정의,0.9236929,0.0,0.9236928820610046
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,MBTI,-0.007945795,0.0,0.007945794612169266
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,MSE Loss 설명,-0.040180247,0.0,0.04018024727702141
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,MSE Loss 용도,0.017031973,0.0,0.017031973227858543
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,Multi-Label 에서 CE + Softmax 적용 문제점,-0.015090985,0.0,0.015090985223650932
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,PEFT 방법 5가지,-0.021794328,0.0,0.021794328466057777
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,거대 언어 모델 정의,-0.021775892,0.0,0.02177589200437069
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,기본 경험,-0.018788638,0.0,0.018788637593388557
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,답변 실패,0.02024207,1.0,0.9797579292207956
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,딥러닝,-0.022919407,0.0,0.02291940711438656
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,마지막 할 말,0.0015923491,0.0,0.001592349144630134
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,머신러닝,0.008820738,0.0,0.008820737712085247
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,면접 시작 인사,-0.014635672,0.0,0.014635671861469746
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,상세 경험,-0.031298414,0.0,0.03129841387271881
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,수식,0.019087551,0.0,0.019087551161646843
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,용어 질문,0.024837593,0.0,0.024837592616677284
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,인공지능,-0.028847381,0.0,0.028847381472587585
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,잠시 휴식,-0.016963614,0.0,0.016963614150881767
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,좋아하는 아이돌,-0.016159605,0.0,0.016159605234861374
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,핵심 아이디어,-0.004109425,0.0,0.004109425004571676
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,확률 예측에서 MSE Loss 미 사용 이유,0.00066586555,0.0,0.0006658655474893749
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",BCE 가 좋은 task,-0.024888098,0.0,0.02488809823989868
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",BCE 가 좋은 이유,0.056187198,0.0,0.05618719756603241
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",LLM Fine-Tuning 의 PEFT,-0.00031683673,0.0,0.0003168367256876081
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",LoRA,-0.014827619,0.0,0.014827619306743145
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",LoRA 와 QLoRA 의 차이,0.0034185385,0.0,0.003418538486585021
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",Loss Function 예시,-0.0030504845,0.0,0.003050484461709857
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",Loss Function 정의,0.9346521,1.0,0.06534790992736816
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",MBTI,-0.016160632,0.0,0.0161606315523386
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",MSE Loss 설명,-0.045351997,0.0,0.04535199701786041
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",MSE Loss 용도,-0.016296906,0.0,0.016296906396746635
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",Multi-Label 에서 CE + Softmax 적용 문제점,0.011154099,0.0,0.011154099367558956
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",PEFT 방법 5가지,-0.021623388,0.0,0.021623387932777405
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",거대 언어 모델 정의,-0.036071725,0.0,0.03607172518968582
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",기본 경험,-0.0304252,0.0,0.03042520023882389
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",답변 실패,-0.023617644,0.0,0.023617643862962723
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",딥러닝,-0.05501575,0.0,0.05501575022935867
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",마지막 할 말,0.0039538336,0.0,0.003953833598643541
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",머신러닝,-0.015430503,0.0,0.015430502593517303
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",면접 시작 인사,0.0076112403,0.0,0.0076112402603030205
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",상세 경험,-0.022509506,0.0,0.022509505972266197
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",수식,0.051530845,0.0,0.05153084546327591
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",용어 질문,-0.015680283,0.0,0.015680283308029175
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",인공지능,0.0036198907,0.0,0.003619890660047531
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",잠시 휴식,-0.02591962,0.0,0.02591961994767189
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",좋아하는 아이돌,-0.0016936178,0.0,0.0016936177853494883
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",핵심 아이디어,-0.06455697,0.0,0.06455697119235992
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",확률 예측에서 MSE Loss 미 사용 이유,-0.0065657655,0.0,0.006565765477716923
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야,BCE 가 좋은 task,0.007874613,0.0,0.00787461269646883
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야,BCE 가 좋은 이유,0.00018147579,0.0,0.00018147578521165997
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야,LLM Fine-Tuning 의 PEFT,-0.01850129,0.0,0.018501289188861847
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야,LoRA,0.0019499318,0.0,0.001949931844137609
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야,LoRA 와 QLoRA 의 차이,-0.024327062,0.0,0.02432706207036972
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야,Loss Function 예시,-0.018572986,0.0,0.018572986125946045
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야,Loss Function 정의,0.07522302,0.0,0.07522302120923996
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야,MBTI,-0.0013655508,0.0,0.0013655507937073708
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야,MSE Loss 설명,-0.011254836,0.0,0.01125483587384224
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야,MSE Loss 용도,0.006386656,0.0,0.0063866558484733105
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야,Multi-Label 에서 CE + Softmax 적용 문제점,-0.00030164246,0.0,0.0003016424598172307
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야,PEFT 방법 5가지,-0.020954175,0.0,0.020954174920916557
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야,거대 언어 모델 정의,-0.008989602,0.0,0.008989602327346802
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야,기본 경험,-0.01663715,0.0,0.016637150198221207
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야,답변 실패,0.9607214,1.0,0.039278626441955566
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야,딥러닝,-0.0039685215,0.0,0.003968521486967802
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야,마지막 할 말,-0.0089494875,0.0,0.00894948747009039
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야,머신러닝,-0.011308114,0.0,0.011308114044368267
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야,면접 시작 인사,-0.006275924,0.0,0.006275923922657967
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야,상세 경험,-0.02099746,0.0,0.020997459068894386
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야,수식,-0.0007903696,0.0,0.0007903695804998279
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야,용어 질문,-0.025044598,0.0,0.025044597685337067
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야,인공지능,-0.040847536,0.0,0.0408475361764431
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야,잠시 휴식,0.00014702519,0.0,0.00014702518819831312
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야,좋아하는 아이돌,0.001436519,0.0,0.0014365189708769321
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야,핵심 아이디어,-0.019512922,0.0,0.019512921571731567
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야,확률 예측에서 MSE Loss 미 사용 이유,0.010737005,0.0,0.010737004689872265
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지,BCE 가 좋은 task,0.012769068,0.0,0.012769067659974098
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지,BCE 가 좋은 이유,0.03981891,0.0,0.039818909019231796
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지,LLM Fine-Tuning 의 PEFT,-0.035146836,0.0,0.03514683619141579
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지,LoRA,-0.022963975,0.0,0.022963974624872208
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지,LoRA 와 QLoRA 의 차이,-0.004258713,0.0,0.004258713219314814
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지,Loss Function 예시,0.009572736,0.0,0.009572735987603664
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지,Loss Function 정의,0.95381033,1.0,0.04618966579437256
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지,MBTI,-0.0043717474,0.0,0.004371747374534607
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지,MSE Loss 설명,-0.028930388,0.0,0.028930388391017914
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지,MSE Loss 용도,0.017570019,0.0,0.017570018768310547
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지,Multi-Label 에서 CE + Softmax 적용 문제점,-0.0026225832,0.0,0.002622583182528615
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지,PEFT 방법 5가지,-0.022996569,0.0,0.0229965690523386
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지,거대 언어 모델 정의,-0.035965763,0.0,0.03596576303243637
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지,기본 경험,-0.027447263,0.0,0.027447262778878212
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지,답변 실패,-0.011940003,0.0,0.011940003372728825
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지,딥러닝,-0.040554687,0.0,0.04055468738079071
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지,마지막 할 말,0.004629121,0.0,0.004629121161997318
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지,머신러닝,0.034286916,0.0,0.03428691625595093
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지,면접 시작 인사,0.007301449,0.0,0.007301448844373226
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지,상세 경험,-0.034760132,0.0,0.03476013243198395
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지,수식,0.013722905,0.0,0.013722904957830906
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지,용어 질문,-0.020863915,0.0,0.020863914862275124
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지,인공지능,-0.030899227,0.0,0.030899226665496826
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지,잠시 휴식,-0.018774034,0.0,0.018774034455418587
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지,좋아하는 아이돌,-0.005853198,0.0,0.005853198003023863
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지,핵심 아이디어,-0.026115201,0.0,0.02611520141363144
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지,확률 예측에서 MSE Loss 미 사용 이유,-0.018694391,0.0,0.01869439147412777
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,BCE 가 좋은 task,-0.006244716,0.0,0.006244715768843889
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,BCE 가 좋은 이유,-0.010748772,0.0,0.010748771950602531
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,LLM Fine-Tuning 의 PEFT,-0.0070694988,0.0,0.0070694987662136555
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,LoRA,0.00045405183,0.0,0.0004540518275462091
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,LoRA 와 QLoRA 의 차이,-0.018996706,0.0,0.01899670623242855
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,Loss Function 예시,0.06692175,0.0,0.06692174822092056
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,Loss Function 정의,0.005171888,0.0,0.005171888042241335
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,MBTI,0.0041047195,0.0,0.0041047194972634315
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,MSE Loss 설명,0.0028743045,0.0,0.0028743045404553413
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,MSE Loss 용도,0.0017985585,0.0,0.0017985585145652294
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,Multi-Label 에서 CE + Softmax 적용 문제점,-0.004205325,0.0,0.004205325152724981
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,PEFT 방법 5가지,-0.008223486,0.0,0.008223486132919788
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,거대 언어 모델 정의,-0.018091101,0.0,0.018091101199388504
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,기본 경험,-0.0022194742,0.0,0.0022194741759449244
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,답변 실패,0.96780485,1.0,0.03219515085220337
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,딥러닝,-0.01161562,0.0,0.011615619994699955
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,마지막 할 말,-0.005246586,0.0,0.005246586166322231
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,머신러닝,-0.010016084,0.0,0.010016083717346191
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,면접 시작 인사,-0.019244201,0.0,0.019244201481342316
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,상세 경험,-0.0167987,0.0,0.016798699274659157
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,수식,-0.014916706,0.0,0.014916705898940563
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,용어 질문,-0.020216538,0.0,0.02021653763949871
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,인공지능,-0.020275911,0.0,0.020275911316275597
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,잠시 휴식,-0.0014382367,0.0,0.0014382366789504886
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,좋아하는 아이돌,0.026239714,0.0,0.02623971365392208
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,핵심 아이디어,-0.026390536,0.0,0.02639053575694561
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,확률 예측에서 MSE Loss 미 사용 이유,-0.002821864,0.0,0.0028218640945851803
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",BCE 가 좋은 task,0.00831944,0.0,0.008319440297782421
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",BCE 가 좋은 이유,-0.025211256,0.0,0.025211255997419357
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",LLM Fine-Tuning 의 PEFT,-0.0022084683,0.0,0.002208468271419406
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",LoRA,-0.0021001964,0.0,0.002100196434184909
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",LoRA 와 QLoRA 의 차이,0.007602749,0.0,0.007602748926728964
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",Loss Function 예시,0.97474533,1.0,0.025254666805267334
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",Loss Function 정의,-0.029450592,0.0,0.029450591653585434
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",MBTI,0.003306332,0.0,0.003306332044303417
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",MSE Loss 설명,0.0055809226,0.0,0.0055809225887060165
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",MSE Loss 용도,-0.0029345304,0.0,0.0029345303773880005
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",Multi-Label 에서 CE + Softmax 적용 문제점,-0.008871552,0.0,0.008871551603078842
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",PEFT 방법 5가지,0.009532473,0.0,0.009532473050057888
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",거대 언어 모델 정의,0.0072359187,0.0,0.0072359186597168446
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",기본 경험,-0.021586122,0.0,0.02158612199127674
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",답변 실패,-0.00654467,0.0,0.0065446700900793076
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",딥러닝,-0.0031898513,0.0,0.0031898512970656157
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",마지막 할 말,-0.0069064065,0.0,0.0069064064882695675
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",머신러닝,-0.006602961,0.0,0.006602961104363203
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",면접 시작 인사,-0.014030876,0.0,0.014030875638127327
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",상세 경험,-0.008713123,0.0,0.008713123388588428
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",수식,0.0029256807,0.0,0.0029256807174533606
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",용어 질문,0.0015558959,0.0,0.0015558958984911442
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",인공지능,-0.02192266,0.0,0.021922659128904343
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",잠시 휴식,-0.009357907,0.0,0.009357906877994537
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",좋아하는 아이돌,-0.004572847,0.0,0.004572846926748753
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",핵심 아이디어,0.00011375149,0.0,0.00011375149188097566
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",확률 예측에서 MSE Loss 미 사용 이유,0.0055371798,0.0,0.0055371797643601894
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어",BCE 가 좋은 task,0.006385238,0.0,0.006385237909853458
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어",BCE 가 좋은 이유,0.004117627,0.0,0.004117627162486315
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어",LLM Fine-Tuning 의 PEFT,-0.008872812,0.0,0.008872811682522297
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어",LoRA,0.02396719,0.0,0.023967189714312553
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어",LoRA 와 QLoRA 의 차이,-0.005509405,0.0,0.005509404931217432
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어",Loss Function 예시,0.969918,1.0,0.030081987380981445
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어",Loss Function 정의,-0.03406396,0.0,0.03406396135687828
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어",MBTI,0.0057083173,0.0,0.005708317272365093
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어",MSE Loss 설명,-0.011183958,0.0,0.011183957569301128
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어",MSE Loss 용도,-0.009214024,0.0,0.009214024059474468
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어",Multi-Label 에서 CE + Softmax 적용 문제점,0.020720096,0.0,0.020720096305012703
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어",PEFT 방법 5가지,0.013284766,0.0,0.013284766115248203
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어",거대 언어 모델 정의,0.019971387,0.0,0.019971387460827827
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어",기본 경험,-0.02614278,0.0,0.026142779737710953
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어",답변 실패,-0.016652148,0.0,0.016652148216962814
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어",딥러닝,2.729264e-05,0.0,2.7292640879750252e-05
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어",마지막 할 말,0.0001615278,0.0,0.00016152780153788626
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어",머신러닝,0.0016217645,0.0,0.0016217645024880767
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어",면접 시작 인사,-0.008723569,0.0,0.008723569102585316
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어",상세 경험,0.0072900075,0.0,0.007290007546544075
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어",수식,-0.0064354455,0.0,0.006435445509850979
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어",용어 질문,-0.007028837,0.0,0.007028837222605944
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어",인공지능,-0.010239185,0.0,0.010239184834063053
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어",잠시 휴식,-0.0035165583,0.0,0.003516558324918151
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어",좋아하는 아이돌,-0.008129076,0.0,0.008129076100885868
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어",핵심 아이디어,0.002695088,0.0,0.002695088041946292
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어",확률 예측에서 MSE Loss 미 사용 이유,-0.002656965,0.0,0.0026569650508463383
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야?",BCE 가 좋은 task,0.00043337562,0.0,0.00043337562237866223
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야?",BCE 가 좋은 이유,-0.0020338206,0.0,0.0020338206086307764
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야?",LLM Fine-Tuning 의 PEFT,0.00146108,0.0,0.0014610800426453352
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야?",LoRA,0.0004363795,0.0,0.00043637948692776263
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야?",LoRA 와 QLoRA 의 차이,0.00031279045,0.0,0.0003127904492430389
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야?",Loss Function 예시,0.040223375,0.0,0.0402233749628067
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야?",Loss Function 정의,-0.0035641408,0.0,0.003564140759408474
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야?",MBTI,0.0076322393,0.0,0.007632239256054163
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야?",MSE Loss 설명,-0.007923233,0.0,0.007923233322799206
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야?",MSE Loss 용도,-0.0073207403,0.0,0.007320740260183811
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야?",Multi-Label 에서 CE + Softmax 적용 문제점,0.0010858678,0.0,0.0010858677560463548
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야?",PEFT 방법 5가지,-0.0021436368,0.0,0.0021436368115246296
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야?",거대 언어 모델 정의,-0.013305929,0.0,0.01330592855811119
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야?",기본 경험,0.0014097845,0.0,0.001409784541465342
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야?",답변 실패,0.9707888,1.0,0.029211223125457764
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야?",딥러닝,0.0033173226,0.0,0.0033173225820064545
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야?",마지막 할 말,-0.015606239,0.0,0.015606239438056946
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야?",머신러닝,-0.018086068,0.0,0.018086068332195282
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야?",면접 시작 인사,-0.017202256,0.0,0.017202256247401237
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야?",상세 경험,-0.010903275,0.0,0.01090327464044094
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야?",수식,-0.015859995,0.0,0.015859995037317276
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야?",용어 질문,0.016107216,0.0,0.016107216477394104
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야?",인공지능,-0.0072549507,0.0,0.007254950702190399
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야?",잠시 휴식,0.002547049,0.0,0.0025470489636063576
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야?",좋아하는 아이돌,-0.0068943882,0.0,0.006894388236105442
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야?",핵심 아이디어,-0.028894547,0.0,0.028894547373056412
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야?",확률 예측에서 MSE Loss 미 사용 이유,-0.0014096838,0.0,0.0014096838422119617
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어",BCE 가 좋은 task,-0.00014490852,0.0,0.0001449085248168558
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어",BCE 가 좋은 이유,0.0092122955,0.0,0.009212295524775982
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어",LLM Fine-Tuning 의 PEFT,0.0028032307,0.0,0.002803230658173561
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어",LoRA,0.009762252,0.0,0.009762251749634743
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어",LoRA 와 QLoRA 의 차이,-0.00638564,0.0,0.006385639775544405
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어",Loss Function 예시,0.9696358,1.0,0.03036421537399292
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어",Loss Function 정의,-0.031593427,0.0,0.03159342706203461
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어",MBTI,0.013097214,0.0,0.013097213581204414
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어",MSE Loss 설명,-0.0063676457,0.0,0.006367645692080259
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어",MSE Loss 용도,0.0108877,0.0,0.010887700133025646
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어",Multi-Label 에서 CE + Softmax 적용 문제점,0.01503225,0.0,0.015032250434160233
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어",PEFT 방법 5가지,0.0034329598,0.0,0.003432959783822298
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어",거대 언어 모델 정의,0.004890789,0.0,0.004890788812190294
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어",기본 경험,-0.029496884,0.0,0.02949688397347927
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어",답변 실패,-0.018537324,0.0,0.01853732392191887
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어",딥러닝,0.0041402816,0.0,0.004140281584113836
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어",마지막 할 말,-0.0024527514,0.0,0.002452751388773322
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어",머신러닝,0.008780839,0.0,0.008780838921666145
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어",면접 시작 인사,-0.010495931,0.0,0.010495930910110474
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어",상세 경험,0.00017557586,0.0,0.0001755758567014709
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어",수식,-0.013382998,0.0,0.013382998295128345
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어",용어 질문,-0.005691582,0.0,0.00569158187136054
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어",인공지능,-0.017394869,0.0,0.017394868656992912
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어",잠시 휴식,-0.004590091,0.0,0.0045900908298790455
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어",좋아하는 아이돌,-0.009657792,0.0,0.009657791815698147
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어",핵심 아이디어,0.0058653983,0.0,0.005865398328751326
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어",확률 예측에서 MSE Loss 미 사용 이유,-0.0012323415,0.0,0.0012323415139690042
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아",BCE 가 좋은 task,0.008439847,0.0,0.008439847268164158
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아",BCE 가 좋은 이유,-0.006241805,0.0,0.0062418049201369286
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아",LLM Fine-Tuning 의 PEFT,-0.0018771349,0.0,0.0018771348986774683
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아",LoRA,0.002652849,0.0,0.002652849070727825
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아",LoRA 와 QLoRA 의 차이,0.0012013954,0.0,0.0012013954110443592
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아",Loss Function 예시,0.9734045,1.0,0.026595473289489746
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아",Loss Function 정의,-0.027897265,0.0,0.027897264808416367
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아",MBTI,0.0076871896,0.0,0.007687189616262913
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아",MSE Loss 설명,-0.012971534,0.0,0.01297153439372778
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아",MSE Loss 용도,-0.004763919,0.0,0.004763918928802013
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아",Multi-Label 에서 CE + Softmax 적용 문제점,0.0059391046,0.0,0.0059391045942902565
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아",PEFT 방법 5가지,0.003735825,0.0,0.003735824953764677
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아",거대 언어 모델 정의,0.010053083,0.0,0.010053083300590515
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아",기본 경험,-0.026723366,0.0,0.026723366230726242
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아",답변 실패,-0.018328624,0.0,0.018328623846173286
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아",딥러닝,0.004784001,0.0,0.004784001037478447
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아",마지막 할 말,0.0006276196,0.0,0.000627619621809572
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아",머신러닝,-0.0064269206,0.0,0.006426920648664236
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아",면접 시작 인사,-0.0018490732,0.0,0.0018490732181817293
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아",상세 경험,-0.0068281284,0.0,0.006828128360211849
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아",수식,-0.00371778,0.0,0.003717780113220215
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아",용어 질문,-0.013301909,0.0,0.01330190896987915
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아",인공지능,-0.017877454,0.0,0.017877453938126564
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아",잠시 휴식,-0.0125224525,0.0,0.012522452510893345
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아",좋아하는 아이돌,-0.0037083863,0.0,0.003708386328071356
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아",핵심 아이디어,0.013816096,0.0,0.013816095888614655
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아",확률 예측에서 MSE Loss 미 사용 이유,0.0012526938,0.0,0.001252693822607398
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,BCE 가 좋은 task,-0.009586785,0.0,0.009586784988641739
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,BCE 가 좋은 이유,0.0046863277,0.0,0.004686327651143074
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,LLM Fine-Tuning 의 PEFT,0.014199885,0.0,0.014199884608387947
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,LoRA,-0.030454662,0.0,0.030454661697149277
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,LoRA 와 QLoRA 의 차이,0.0008164947,0.0,0.0008164946921169758
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,Loss Function 예시,-0.01626946,0.0,0.016269460320472717
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,Loss Function 정의,-0.035364777,0.0,0.035364776849746704
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,MBTI,-0.041018236,0.0,0.04101823642849922
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,MSE Loss 설명,0.92770153,1.0,0.07229846715927124
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,MSE Loss 용도,-0.024211537,0.0,0.024211537092924118
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,Multi-Label 에서 CE + Softmax 적용 문제점,0.013231333,0.0,0.01323133334517479
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,PEFT 방법 5가지,-0.006729207,0.0,0.0067292070016264915
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,거대 언어 모델 정의,-0.0025100037,0.0,0.0025100037455558777
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,기본 경험,0.0018222963,0.0,0.0018222962971776724
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,답변 실패,0.00051938696,0.0,0.0005193869583308697
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,딥러닝,0.005285668,0.0,0.005285668186843395
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,마지막 할 말,0.012469941,0.0,0.012469940818846226
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,머신러닝,-0.015416153,0.0,0.015416152775287628
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,면접 시작 인사,-0.022696069,0.0,0.02269606851041317
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,상세 경험,0.0073950524,0.0,0.0073950523510575294
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,수식,0.00014909166,0.0,0.0001490916620241478
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,용어 질문,0.018363958,0.0,0.018363958224654198
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,인공지능,0.023315832,0.0,0.023315832018852234
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,잠시 휴식,-0.018820826,0.0,0.018820825964212418
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,좋아하는 아이돌,-0.0101841455,0.0,0.010184145532548428
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,핵심 아이디어,0.026166914,0.0,0.02616691403090954
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,확률 예측에서 MSE Loss 미 사용 이유,-0.044979893,0.0,0.044979892671108246
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,BCE 가 좋은 task,-0.038959768,0.0,0.038959767669439316
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,BCE 가 좋은 이유,0.04415822,0.0,0.044158220291137695
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,LLM Fine-Tuning 의 PEFT,0.021762175,0.0,0.021762175485491753
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,LoRA,-0.012339219,0.0,0.012339219450950623
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,LoRA 와 QLoRA 의 차이,-0.0060015796,0.0,0.006001579575240612
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,Loss Function 예시,6.451282e-05,0.0,6.45128166070208e-05
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,Loss Function 정의,0.06316842,0.0,0.06316842138767242
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,MBTI,-0.007393459,0.0,0.007393458858132362
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,MSE Loss 설명,0.43912077,0.0,0.4391207695007324
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,MSE Loss 용도,-0.10370916,0.0,0.1037091612815857
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,Multi-Label 에서 CE + Softmax 적용 문제점,0.007820046,0.0,0.007820045575499535
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,PEFT 방법 5가지,-0.05078352,0.0,0.05078351870179176
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,거대 언어 모델 정의,-0.019952552,0.0,0.019952552393078804
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,기본 경험,-0.042777255,0.0,0.042777255177497864
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,답변 실패,0.62035525,1.0,0.3796447515487671
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,딥러닝,-0.03088309,0.0,0.03088309057056904
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,마지막 할 말,-0.05893377,0.0,0.058933768421411514
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,머신러닝,-0.016617138,0.0,0.01661713793873787
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,면접 시작 인사,-0.044784553,0.0,0.0447845533490181
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,상세 경험,0.012155467,0.0,0.012155466713011265
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,수식,0.024734912,0.0,0.02473491244018078
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,용어 질문,0.0058912165,0.0,0.005891216453164816
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,인공지능,-0.0016491715,0.0,0.001649171463213861
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,잠시 휴식,-0.044166453,0.0,0.044166453182697296
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,좋아하는 아이돌,-0.0035937827,0.0,0.0035937826614826918
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,핵심 아이디어,0.038172726,0.0,0.03817272558808327
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,확률 예측에서 MSE Loss 미 사용 이유,0.014969379,0.0,0.01496937870979309
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",BCE 가 좋은 task,0.015113639,0.0,0.01511363871395588
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",BCE 가 좋은 이유,0.007641257,0.0,0.007641256786882877
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",LLM Fine-Tuning 의 PEFT,-0.035554275,0.0,0.035554274916648865
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",LoRA,0.03455522,0.0,0.03455521911382675
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",LoRA 와 QLoRA 의 차이,-0.013247652,0.0,0.013247651979327202
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",Loss Function 예시,-0.0056225504,0.0,0.005622550379484892
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",Loss Function 정의,0.019518372,0.0,0.019518371671438217
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",MBTI,0.019317431,0.0,0.01931743137538433
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",MSE Loss 설명,-0.01641163,0.0,0.01641163043677807
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",MSE Loss 용도,0.9657705,1.0,0.03422951698303223
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",Multi-Label 에서 CE + Softmax 적용 문제점,-0.0063584032,0.0,0.006358403246849775
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",PEFT 방법 5가지,-0.002059346,0.0,0.002059346064925194
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",거대 언어 모델 정의,-0.029371824,0.0,0.029371824115514755
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",기본 경험,-0.0044172583,0.0,0.0044172583147883415
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",답변 실패,0.008365141,0.0,0.008365141227841377
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",딥러닝,-0.0048340396,0.0,0.004834039602428675
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",마지막 할 말,-0.0034508693,0.0,0.0034508693497627974
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",머신러닝,0.003839178,0.0,0.0038391780108213425
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",면접 시작 인사,0.0068435837,0.0,0.006843583658337593
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",상세 경험,-0.0044239834,0.0,0.00442398339509964
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",수식,0.007770366,0.0,0.007770366035401821
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",용어 질문,-0.024684379,0.0,0.024684378877282143
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",인공지능,-0.029034568,0.0,0.02903456799685955
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",잠시 휴식,-0.017486235,0.0,0.01748623512685299
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",좋아하는 아이돌,0.0018320688,0.0,0.0018320687813684344
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",핵심 아이디어,-0.0077502667,0.0,0.007750266697257757
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",확률 예측에서 MSE Loss 미 사용 이유,-0.022607518,0.0,0.02260751836001873
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,BCE 가 좋은 task,0.0030538077,0.0,0.0030538076534867287
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,BCE 가 좋은 이유,0.0012871479,0.0,0.001287147868424654
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,LLM Fine-Tuning 의 PEFT,0.002425003,0.0,0.0024250030983239412
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,LoRA,0.011633243,0.0,0.011633243411779404
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,LoRA 와 QLoRA 의 차이,0.00074981677,0.0,0.0007498167688027024
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,Loss Function 예시,-0.009093995,0.0,0.009093995206058025
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,Loss Function 정의,-0.02264993,0.0,0.02264993079006672
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,MBTI,-0.0012556879,0.0,0.001255687908269465
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,MSE Loss 설명,-0.0001713159,0.0,0.00017131589993368834
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,MSE Loss 용도,0.065030344,0.0,0.06503034383058548
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,Multi-Label 에서 CE + Softmax 적용 문제점,-0.026729533,0.0,0.026729533448815346
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,PEFT 방법 5가지,-0.039744087,0.0,0.03974408656358719
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,거대 언어 모델 정의,0.00928541,0.0,0.009285409934818745
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,기본 경험,0.014835661,0.0,0.01483566127717495
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,답변 실패,0.957874,1.0,0.04212599992752075
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,딥러닝,-0.004220886,0.0,0.004220886155962944
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,마지막 할 말,0.0047513256,0.0,0.004751325584948063
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,머신러닝,-0.023289124,0.0,0.02328912355005741
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,면접 시작 인사,0.008157696,0.0,0.008157695643603802
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,상세 경험,-0.015623781,0.0,0.015623780898749828
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,수식,-0.026992548,0.0,0.026992548257112503
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,용어 질문,-0.021415053,0.0,0.02141505293548107
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,인공지능,-0.008863201,0.0,0.00886320136487484
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,잠시 휴식,0.0034584876,0.0,0.003458487568423152
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,좋아하는 아이돌,-0.007867932,0.0,0.007867932319641113
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,핵심 아이디어,-0.024688156,0.0,0.024688156321644783
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,확률 예측에서 MSE Loss 미 사용 이유,0.018757705,0.0,0.01875770464539528
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,BCE 가 좋은 task,0.0013404693,0.0,0.0013404693454504013
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,BCE 가 좋은 이유,0.02235094,0.0,0.022350940853357315
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,LLM Fine-Tuning 의 PEFT,0.016984826,0.0,0.01698482595384121
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,LoRA,0.03338899,0.0,0.03338899090886116
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,LoRA 와 QLoRA 의 차이,0.014981862,0.0,0.014981862157583237
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,Loss Function 예시,-0.012144571,0.0,0.012144571170210838
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,Loss Function 정의,-0.020939859,0.0,0.020939858630299568
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,MBTI,0.0010459552,0.0,0.0010459552286192775
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,MSE Loss 설명,0.0071700895,0.0,0.007170089520514011
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,MSE Loss 용도,-0.0047969753,0.0,0.004796975292265415
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,Multi-Label 에서 CE + Softmax 적용 문제점,0.007938112,0.0,0.007938112132251263
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,PEFT 방법 5가지,-0.025317146,0.0,0.025317145511507988
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,거대 언어 모델 정의,0.0059530973,0.0,0.005953097250312567
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,기본 경험,-0.04783974,0.0,0.04783973842859268
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,답변 실패,0.86869985,1.0,0.131300151348114
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,딥러닝,0.004739591,0.0,0.004739590920507908
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,마지막 할 말,-0.00027746535,0.0,0.00027746535488404334
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,머신러닝,-0.037461393,0.0,0.03746139258146286
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,면접 시작 인사,-0.028327359,0.0,0.02832735888659954
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,상세 경험,-0.012758489,0.0,0.012758488766849041
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,수식,0.034933824,0.0,0.034933824092149734
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,용어 질문,-0.034064595,0.0,0.03406459465622902
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,인공지능,0.009041442,0.0,0.009041441604495049
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,잠시 휴식,0.014353405,0.0,0.014353404752910137
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,좋아하는 아이돌,-0.02636589,0.0,0.026365889236330986
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,핵심 아이디어,-0.08035371,0.0,0.08035370707511902
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,확률 예측에서 MSE Loss 미 사용 이유,0.2527633,0.0,0.2527633011341095
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,BCE 가 좋은 task,0.0077648493,0.0,0.007764849346131086
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,BCE 가 좋은 이유,-0.056808747,0.0,0.056808747351169586
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,LLM Fine-Tuning 의 PEFT,0.00075802027,0.0,0.000758020265493542
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,LoRA,0.0004211391,0.0,0.0004211390914861113
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,LoRA 와 QLoRA 의 차이,-0.010924861,0.0,0.010924860835075378
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,Loss Function 예시,-0.010348201,0.0,0.010348200798034668
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,Loss Function 정의,-0.027615309,0.0,0.02761530876159668
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,MBTI,0.0061697294,0.0,0.006169729400426149
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,MSE Loss 설명,-0.009072842,0.0,0.009072842076420784
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,MSE Loss 용도,-0.024187388,0.0,0.02418738789856434
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,Multi-Label 에서 CE + Softmax 적용 문제점,-0.027048057,0.0,0.027048056945204735
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,PEFT 방법 5가지,0.010861983,0.0,0.010861982591450214
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,거대 언어 모델 정의,-0.024130708,0.0,0.02413070760667324
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,기본 경험,-0.031079387,0.0,0.031079387292265892
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,답변 실패,-0.019504007,0.0,0.019504006952047348
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,딥러닝,-0.01094496,0.0,0.010944959707558155
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,마지막 할 말,-0.0006551691,0.0,0.0006551690748892725
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,머신러닝,0.015182921,0.0,0.0151829207316041
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,면접 시작 인사,0.01147754,0.0,0.011477540247142315
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,상세 경험,0.011105357,0.0,0.011105356737971306
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,수식,0.002941937,0.0,0.0029419369529932737
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,용어 질문,-0.002656286,0.0,0.0026562858838588
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,인공지능,0.016110165,0.0,0.016110165044665337
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,잠시 휴식,-0.012065651,0.0,0.012065650895237923
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,좋아하는 아이돌,-0.0057768053,0.0,0.005776805337518454
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,핵심 아이디어,0.07722138,0.0,0.07722137868404388
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,확률 예측에서 MSE Loss 미 사용 이유,0.95359224,1.0,0.04640775918960571
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,BCE 가 좋은 task,-0.013024501,0.0,0.013024500571191311
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,BCE 가 좋은 이유,-0.0015723249,0.0,0.0015723248943686485
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,LLM Fine-Tuning 의 PEFT,0.043540135,0.0,0.04354013502597809
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,LoRA,-0.014456319,0.0,0.014456318691372871
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,LoRA 와 QLoRA 의 차이,-0.013739011,0.0,0.013739011250436306
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,Loss Function 예시,0.02112474,0.0,0.021124739199876785
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,Loss Function 정의,0.012968079,0.0,0.012968079186975956
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,MBTI,0.012851167,0.0,0.012851166538894176
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,MSE Loss 설명,0.018973103,0.0,0.018973102793097496
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,MSE Loss 용도,0.035389382,0.0,0.035389382392168045
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,Multi-Label 에서 CE + Softmax 적용 문제점,0.0024442747,0.0,0.0024442747235298157
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,PEFT 방법 5가지,0.006268491,0.0,0.0062684910371899605
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,거대 언어 모델 정의,-0.034169506,0.0,0.034169506281614304
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,기본 경험,0.05917985,0.0,0.05917984992265701
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,답변 실패,0.00084298995,0.0,0.0008429899462498724
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,딥러닝,0.02249908,0.0,0.02249908074736595
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,마지막 할 말,-0.0062839957,0.0,0.006283995695412159
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,머신러닝,0.015382636,0.0,0.015382636338472366
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,면접 시작 인사,0.00027626008,0.0,0.0002762600779533386
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,상세 경험,-0.03864581,0.0,0.03864581137895584
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,수식,0.71295106,1.0,0.28704893589019775
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,용어 질문,0.024592692,0.0,0.024592692032456398
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,인공지능,-0.004925816,0.0,0.004925815854221582
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,잠시 휴식,-0.00797044,0.0,0.007970440201461315
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,좋아하는 아이돌,0.005488337,0.0,0.005488337017595768
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,핵심 아이디어,0.01047421,0.0,0.010474209673702717
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,확률 예측에서 MSE Loss 미 사용 이유,0.008409211,0.0,0.008409211412072182
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",BCE 가 좋은 task,-0.0012459492,0.0,0.0012459491845220327
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",BCE 가 좋은 이유,-0.020461336,0.0,0.02046133577823639
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",LLM Fine-Tuning 의 PEFT,-0.00535075,0.0,0.0053507499396800995
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",LoRA,0.011452568,0.0,0.011452567763626575
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",LoRA 와 QLoRA 의 차이,-0.009236336,0.0,0.009236335754394531
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",Loss Function 예시,-0.012257338,0.0,0.01225733757019043
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",Loss Function 정의,-0.03723236,0.0,0.03723236173391342
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",MBTI,0.015461067,0.0,0.015461066737771034
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",MSE Loss 설명,-0.045779303,0.0,0.04577930271625519
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",MSE Loss 용도,0.0018181048,0.0,0.0018181047635152936
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",Multi-Label 에서 CE + Softmax 적용 문제점,-0.019696109,0.0,0.019696108996868134
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",PEFT 방법 5가지,-0.016132388,0.0,0.01613238826394081
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",거대 언어 모델 정의,0.011688918,0.0,0.011688917875289917
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",기본 경험,0.024150118,0.0,0.024150118231773376
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",답변 실패,-0.010560719,0.0,0.010560719296336174
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",딥러닝,-0.037451677,0.0,0.03745167702436447
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",마지막 할 말,-0.009456521,0.0,0.009456520900130272
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",머신러닝,-0.0023213634,0.0,0.002321363426744938
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",면접 시작 인사,0.00045857168,0.0,0.0004585716815199703
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",상세 경험,0.028509876,0.0,0.028509875759482384
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",수식,-0.013257296,0.0,0.013257295824587345
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",용어 질문,-0.0044483654,0.0,0.004448365420103073
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",인공지능,-0.016312286,0.0,0.016312286257743835
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",잠시 휴식,-0.018594177,0.0,0.018594177439808846
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",좋아하는 아이돌,0.017570227,0.0,0.01757022738456726
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",핵심 아이디어,0.9482487,1.0,0.05175131559371948
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",확률 예측에서 MSE Loss 미 사용 이유,-0.014190241,0.0,0.014190240763127804
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,BCE 가 좋은 task,-0.054336876,0.0,0.054336875677108765
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,BCE 가 좋은 이유,0.084853776,0.0,0.08485377579927444
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,LLM Fine-Tuning 의 PEFT,0.014567314,0.0,0.014567313715815544
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,LoRA,0.06574446,0.0,0.06574445962905884
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,LoRA 와 QLoRA 의 차이,0.0069369967,0.0,0.0069369967095553875
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,Loss Function 예시,-0.018319475,0.0,0.018319474533200264
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,Loss Function 정의,0.006410392,0.0,0.006410392001271248
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,MBTI,-0.02200298,0.0,0.02200298011302948
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,MSE Loss 설명,0.037109334,0.0,0.03710933402180672
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,MSE Loss 용도,-0.044359263,0.0,0.04435926303267479
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,Multi-Label 에서 CE + Softmax 적용 문제점,0.17160004,0.0,0.17160004377365112
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,PEFT 방법 5가지,0.0073472913,0.0,0.007347291335463524
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,거대 언어 모델 정의,0.082892716,0.0,0.08289271593093872
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,기본 경험,-0.05929035,0.0,0.05929034948348999
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,답변 실패,0.47295317,1.0,0.5270468294620514
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,딥러닝,-0.066354506,0.0,0.06635450571775436
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,마지막 할 말,0.06070067,0.0,0.0607006698846817
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,머신러닝,-0.07842379,0.0,0.07842379063367844
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,면접 시작 인사,-0.0015420322,0.0,0.0015420322306454182
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,상세 경험,0.037410136,0.0,0.03741013631224632
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,수식,0.08031013,0.0,0.08031012862920761
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,용어 질문,0.039935306,0.0,0.03993530571460724
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,인공지능,0.08049111,0.0,0.08049111068248749
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,잠시 휴식,0.053671118,0.0,0.05367111787199974
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,좋아하는 아이돌,-0.08177966,0.0,0.08177965879440308
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,핵심 아이디어,0.17845094,0.0,0.17845094203948975
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,확률 예측에서 MSE Loss 미 사용 이유,-0.09324241,0.0,0.09324240684509277
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",BCE 가 좋은 task,-0.0018884006,0.0,0.0018884006422013044
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",BCE 가 좋은 이유,-0.041569125,0.0,0.04156912490725517
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",LLM Fine-Tuning 의 PEFT,-0.012285523,0.0,0.012285523116588593
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",LoRA,0.009455012,0.0,0.009455012157559395
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",LoRA 와 QLoRA 의 차이,-0.011731156,0.0,0.011731156148016453
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",Loss Function 예시,-0.026012208,0.0,0.026012208312749863
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",Loss Function 정의,-0.055860154,0.0,0.05586015433073044
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",MBTI,-0.0038497157,0.0,0.003849715692922473
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",MSE Loss 설명,-0.019918649,0.0,0.019918648526072502
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",MSE Loss 용도,-0.010857263,0.0,0.010857262648642063
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",Multi-Label 에서 CE + Softmax 적용 문제점,0.0063148583,0.0,0.006314858328551054
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",PEFT 방법 5가지,-0.013110805,0.0,0.013110805302858353
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",거대 언어 모델 정의,0.0005496559,0.0,0.0005496558733284473
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",기본 경험,0.024743713,0.0,0.024743713438510895
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",답변 실패,0.005982819,0.0,0.005982819013297558
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",딥러닝,-0.035635073,0.0,0.035635072737932205
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",마지막 할 말,-0.008091791,0.0,0.008091790601611137
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",머신러닝,-0.00949091,0.0,0.009490909986197948
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",면접 시작 인사,0.00012222923,0.0,0.000122229233966209
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",상세 경험,-3.18175e-05,0.0,3.181750071235001e-05
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",수식,0.038696922,0.0,0.03869692236185074
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",용어 질문,-0.016559608,0.0,0.016559608280658722
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",인공지능,0.0078927055,0.0,0.007892705500125885
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",잠시 휴식,-0.014050799,0.0,0.014050799421966076
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",좋아하는 아이돌,0.0078677945,0.0,0.00786779448390007
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",핵심 아이디어,0.93563056,1.0,0.06436944007873535
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",확률 예측에서 MSE Loss 미 사용 이유,-0.02935554,0.0,0.029355540871620178
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')]",BCE 가 좋은 task,-0.011365402,0.0,0.011365401558578014
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')]",BCE 가 좋은 이유,-0.035664678,0.0,0.03566467761993408
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')]",LLM Fine-Tuning 의 PEFT,0.07288223,0.0,0.07288222759962082
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')]",LoRA,-0.05638998,0.0,0.056389980018138885
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')]",LoRA 와 QLoRA 의 차이,0.014468384,0.0,0.014468383975327015
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')]",Loss Function 예시,-0.03344195,0.0,0.033441949635744095
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')]",Loss Function 정의,-0.010819325,0.0,0.010819325223565102
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')]",MBTI,-0.04761932,0.0,0.04761932045221329
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')]",MSE Loss 설명,-0.015102522,0.0,0.015102522447705269
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')]",MSE Loss 용도,-0.006288993,0.0,0.006288993172347546
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')]",Multi-Label 에서 CE + Softmax 적용 문제점,-0.030599896,0.0,0.030599895864725113
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')]",PEFT 방법 5가지,-0.048196167,0.0,0.04819616675376892
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')]",거대 언어 모델 정의,-0.009892186,0.0,0.009892186149954796
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')]",기본 경험,-0.033506848,0.0,0.0335068479180336
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')]",답변 실패,0.058935948,0.0,0.058935947716236115
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')]",딥러닝,-0.020616181,0.0,0.020616181194782257
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')]",마지막 할 말,-0.029114397,0.0,0.02911439724266529
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')]",머신러닝,-0.02789746,0.0,0.027897460386157036
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')]",면접 시작 인사,-0.024948006,0.0,0.024948006495833397
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')]",상세 경험,-0.007246409,0.0,0.007246409077197313
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')]",수식,0.2727619,1.0,0.7272380888462067
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')]",용어 질문,0.062371783,0.0,0.06237178295850754
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')]",인공지능,-0.08174016,0.0,0.08174016326665878
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')]",잠시 휴식,-0.033265337,0.0,0.033265337347984314
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')]",좋아하는 아이돌,-0.013231833,0.0,0.013231833465397358
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')]",핵심 아이디어,0.60298777,0.0,0.6029877662658691
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')]",확률 예측에서 MSE Loss 미 사용 이유,-0.07923656,0.0,0.07923655956983566
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다",BCE 가 좋은 task,0.009422519,0.0,0.009422519244253635
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다",BCE 가 좋은 이유,-0.044129007,0.0,0.04412900656461716
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다",LLM Fine-Tuning 의 PEFT,-0.011334814,0.0,0.011334814131259918
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다",LoRA,0.0058014113,0.0,0.005801411345601082
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다",LoRA 와 QLoRA 의 차이,-0.019806406,0.0,0.019806405529379845
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다",Loss Function 예시,-0.015974559,0.0,0.015974558889865875
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다",Loss Function 정의,-0.018082773,0.0,0.018082773312926292
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다",MBTI,-0.0009685353,0.0,0.0009685353143140674
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다",MSE Loss 설명,-0.059573103,0.0,0.05957310274243355
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다",MSE Loss 용도,-0.0026925856,0.0,0.0026925855781883
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다",Multi-Label 에서 CE + Softmax 적용 문제점,0.01347987,0.0,0.013479869812726974
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다",PEFT 방법 5가지,-0.017790813,0.0,0.017790812999010086
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다",거대 언어 모델 정의,0.020222398,0.0,0.02022239752113819
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다",기본 경험,0.033275995,0.0,0.033275995403528214
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다",답변 실패,-0.017794328,0.0,0.017794327810406685
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다",딥러닝,-0.052439447,0.0,0.05243944749236107
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다",마지막 할 말,-0.0073323394,0.0,0.00733233941718936
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다",머신러닝,0.015624178,0.0,0.015624177642166615
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다",면접 시작 인사,0.00695465,0.0,0.006954649928957224
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다",상세 경험,0.022319233,0.0,0.022319233044981956
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다",수식,0.008970063,0.0,0.008970063179731369
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다",용어 질문,-0.018391846,0.0,0.018391845747828484
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다",인공지능,-0.020589294,0.0,0.020589293912053108
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다",잠시 휴식,-0.008300402,0.0,0.008300402201712132
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다",좋아하는 아이돌,0.02195169,0.0,0.021951690316200256
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다",핵심 아이디어,0.9450109,1.0,0.05498909950256348
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다",확률 예측에서 MSE Loss 미 사용 이유,0.005041225,0.0,0.005041224882006645
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거,BCE 가 좋은 task,-0.0024641359,0.0,0.0024641358759254217
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거,BCE 가 좋은 이유,0.009215213,0.0,0.009215213358402252
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거,LLM Fine-Tuning 의 PEFT,0.002790083,0.0,0.0027900829445570707
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거,LoRA,0.014842858,0.0,0.014842857606709003
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거,LoRA 와 QLoRA 의 차이,-0.0011920544,0.0,0.0011920543620362878
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거,Loss Function 예시,-0.0052412003,0.0,0.00524120032787323
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거,Loss Function 정의,-0.022876836,0.0,0.022876836359500885
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거,MBTI,0.008253265,0.0,0.008253265172243118
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거,MSE Loss 설명,-0.0046481714,0.0,0.004648171365261078
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거,MSE Loss 용도,-0.006443013,0.0,0.006443012971431017
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거,Multi-Label 에서 CE + Softmax 적용 문제점,-0.0037495648,0.0,0.003749564755707979
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거,PEFT 방법 5가지,-0.021321673,0.0,0.021321672946214676
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거,거대 언어 모델 정의,-0.0007818629,0.0,0.0007818628801032901
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거,기본 경험,-0.004344524,0.0,0.004344523884356022
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거,답변 실패,0.9731226,1.0,0.026877403259277344
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거,딥러닝,0.013525953,0.0,0.013525952585041523
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거,마지막 할 말,-0.0026264263,0.0,0.0026264262851327658
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거,머신러닝,-0.010724474,0.0,0.010724473744630814
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거,면접 시작 인사,0.0076020234,0.0,0.007602023426443338
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거,상세 경험,-0.012907877,0.0,0.012907876633107662
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거,수식,-0.0107939765,0.0,0.010793976485729218
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거,용어 질문,-0.021018276,0.0,0.02101827599108219
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거,인공지능,-0.028646512,0.0,0.02864651195704937
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거,잠시 휴식,-0.012919432,0.0,0.012919431552290916
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거,좋아하는 아이돌,0.0050871624,0.0,0.005087162367999554
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거,핵심 아이디어,0.010254612,0.0,0.010254612192511559
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거,확률 예측에서 MSE Loss 미 사용 이유,-0.005217099,0.0,0.005217099096626043
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지",BCE 가 좋은 task,0.003208005,0.0,0.003208005102351308
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지",BCE 가 좋은 이유,-0.0064568655,0.0,0.0064568654634058475
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지",LLM Fine-Tuning 의 PEFT,-0.019486235,0.0,0.019486235454678535
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지",LoRA,0.022198558,0.0,0.02219855785369873
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지",LoRA 와 QLoRA 의 차이,-0.015224854,0.0,0.015224854461848736
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지",Loss Function 예시,-0.011752521,0.0,0.011752520687878132
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지",Loss Function 정의,-0.045285046,0.0,0.045285046100616455
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지",MBTI,0.034768935,0.0,0.03476893529295921
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지",MSE Loss 설명,-0.00074101705,0.0,0.0007410170510411263
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지",MSE Loss 용도,0.0015544514,0.0,0.0015544514171779156
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지",Multi-Label 에서 CE + Softmax 적용 문제점,-0.015603885,0.0,0.015603885054588318
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지",PEFT 방법 5가지,-0.03145186,0.0,0.03145185858011246
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지",거대 언어 모델 정의,0.0055621513,0.0,0.005562151316553354
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지",기본 경험,0.0030434392,0.0,0.0030434392392635345
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지",답변 실패,-0.010643741,0.0,0.010643741115927696
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지",딥러닝,-0.03361726,0.0,0.03361726179718971
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지",마지막 할 말,-0.02015689,0.0,0.020156890153884888
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지",머신러닝,-0.028343126,0.0,0.02834312617778778
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지",면접 시작 인사,0.013441838,0.0,0.013441838324069977
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지",상세 경험,0.019157693,0.0,0.019157692790031433
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지",수식,0.009105708,0.0,0.009105708450078964
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지",용어 질문,-0.010987826,0.0,0.010987825691699982
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지",인공지능,-0.0021020742,0.0,0.0021020742133259773
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지",잠시 휴식,-0.01748819,0.0,0.01748819090425968
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지",좋아하는 아이돌,0.00035726637,0.0,0.00035726636997424066
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지",핵심 아이디어,0.93391645,1.0,0.06608355045318604
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지",확률 예측에서 MSE Loss 미 사용 이유,-0.02234032,0.0,0.0223403200507164
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야?",BCE 가 좋은 task,-0.008804708,0.0,0.008804707787930965
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야?",BCE 가 좋은 이유,-0.003271667,0.0,0.003271667053923011
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야?",LLM Fine-Tuning 의 PEFT,0.0069644987,0.0,0.006964498665183783
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야?",LoRA,0.008108294,0.0,0.008108293637633324
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야?",LoRA 와 QLoRA 의 차이,0.0072023435,0.0,0.007202343549579382
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야?",Loss Function 예시,0.017397072,0.0,0.017397072166204453
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야?",Loss Function 정의,0.0076916534,0.0,0.007691653445363045
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야?",MBTI,0.00852016,0.0,0.008520159870386124
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야?",MSE Loss 설명,0.024994187,0.0,0.02499418705701828
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야?",MSE Loss 용도,0.039427355,0.0,0.03942735493183136
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야?",Multi-Label 에서 CE + Softmax 적용 문제점,0.030203644,0.0,0.030203644186258316
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야?",PEFT 방법 5가지,0.001516037,0.0,0.001516037038527429
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야?",거대 언어 모델 정의,0.026882723,0.0,0.026882722973823547
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야?",기본 경험,-0.03255793,0.0,0.032557930797338486
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야?",답변 실패,0.032291334,0.0,0.03229133412241936
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야?",딥러닝,0.04283511,0.0,0.04283510893583298
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야?",마지막 할 말,-0.0017843563,0.0,0.0017843563109636307
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야?",머신러닝,-0.019368019,0.0,0.019368018954992294
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야?",면접 시작 인사,-0.008024316,0.0,0.008024316281080246
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야?",상세 경험,0.07895752,0.0,0.07895752042531967
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야?",수식,-0.006788934,0.0,0.006788934115320444
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야?",용어 질문,0.70223546,1.0,0.29776453971862793
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야?",인공지능,0.004405591,0.0,0.004405591171234846
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야?",잠시 휴식,-0.0065304483,0.0,0.006530448328703642
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야?",좋아하는 아이돌,-0.012123837,0.0,0.012123837135732174
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야?",핵심 아이디어,-0.04656058,0.0,0.046560581773519516
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야?",확률 예측에서 MSE Loss 미 사용 이유,-0.03154127,0.0,0.03154126927256584
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",BCE 가 좋은 task,0.16947523,0.0,0.1694752275943756
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",BCE 가 좋은 이유,-0.07456663,0.0,0.07456663250923157
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",LLM Fine-Tuning 의 PEFT,-0.015733583,0.0,0.01573358289897442
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",LoRA,0.006389233,0.0,0.0063892328180372715
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",LoRA 와 QLoRA 의 차이,-0.023521751,0.0,0.023521751165390015
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",Loss Function 예시,-0.015847258,0.0,0.015847258269786835
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",Loss Function 정의,-0.026961818,0.0,0.02696181833744049
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",MBTI,0.006878736,0.0,0.006878735963255167
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",MSE Loss 설명,0.011606252,0.0,0.011606251820921898
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",MSE Loss 용도,0.016879339,0.0,0.016879338771104813
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",Multi-Label 에서 CE + Softmax 적용 문제점,-0.025782596,0.0,0.025782596319913864
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",PEFT 방법 5가지,-0.025111077,0.0,0.02511107735335827
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",거대 언어 모델 정의,-0.03968843,0.0,0.03968843072652817
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",기본 경험,-0.016488416,0.0,0.016488416120409966
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",답변 실패,0.29123902,1.0,0.7087609767913818
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",딥러닝,-0.0084891,0.0,0.008489100262522697
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",마지막 할 말,0.009686488,0.0,0.009686487726867199
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",머신러닝,-0.020139089,0.0,0.020139088854193687
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",면접 시작 인사,-0.013857201,0.0,0.013857200741767883
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",상세 경험,0.05559461,0.0,0.055594608187675476
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",수식,0.020364968,0.0,0.020364968106150627
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",용어 질문,0.44200367,0.0,0.44200366735458374
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",인공지능,-0.0046233055,0.0,0.004623305518180132
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",잠시 휴식,-0.0012356474,0.0,0.0012356473598629236
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",좋아하는 아이돌,0.019946786,0.0,0.019946785643696785
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",핵심 아이디어,0.020745149,0.0,0.02074514888226986
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",확률 예측에서 MSE Loss 미 사용 이유,0.014051262,0.0,0.01405126228928566
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",BCE 가 좋은 task,-0.0024556175,1.0,1.0024556175339967
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",BCE 가 좋은 이유,0.0045971842,0.0,0.004597184248268604
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",LLM Fine-Tuning 의 PEFT,0.0013199472,0.0,0.001319947186857462
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",LoRA,-0.00028890045,0.0,0.0002889004535973072
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",LoRA 와 QLoRA 의 차이,-0.00027589392,0.0,0.00027589392266236246
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",Loss Function 예시,-0.015495588,0.0,0.015495588071644306
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",Loss Function 정의,0.0046426975,0.0,0.004642697516828775
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",MBTI,0.0021218194,0.0,0.0021218194160610437
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",MSE Loss 설명,-0.009541264,0.0,0.009541263803839684
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",MSE Loss 용도,0.008813314,0.0,0.008813314139842987
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",Multi-Label 에서 CE + Softmax 적용 문제점,-0.0031441397,0.0,0.0031441396567970514
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",PEFT 방법 5가지,-0.012998494,0.0,0.012998494319617748
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",거대 언어 모델 정의,0.007044013,0.0,0.007044013123959303
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",기본 경험,-0.015198977,0.0,0.01519897673279047
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",답변 실패,0.97059387,0.0,0.9705938696861267
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",딥러닝,0.001960506,0.0,0.0019605059642344713
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",마지막 할 말,-0.008963365,0.0,0.008963365107774734
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",머신러닝,-0.0028299184,0.0,0.0028299184050410986
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",면접 시작 인사,-0.0050745737,0.0,0.005074573680758476
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",상세 경험,-0.012261857,0.0,0.012261857278645039
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",수식,-0.023167273,0.0,0.02316727302968502
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",용어 질문,0.02688822,0.0,0.026888219639658928
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",인공지능,-0.015804492,0.0,0.015804491937160492
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",잠시 휴식,-0.0020540955,0.0,0.0020540955010801554
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",좋아하는 아이돌,-0.0043198727,0.0,0.004319872707128525
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",핵심 아이디어,-0.015163593,0.0,0.015163592994213104
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",확률 예측에서 MSE Loss 미 사용 이유,-0.0017789163,0.0,0.0017789163393899798
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",BCE 가 좋은 task,0.06360727,0.0,0.06360726803541183
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",BCE 가 좋은 이유,0.88051397,1.0,0.11948603391647339
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",LLM Fine-Tuning 의 PEFT,-0.050093547,0.0,0.05009354650974274
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",LoRA,0.0016913709,0.0,0.0016913708532229066
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",LoRA 와 QLoRA 의 차이,-0.0032985709,0.0,0.003298570867627859
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",Loss Function 예시,-0.013610728,0.0,0.013610728085041046
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",Loss Function 정의,0.027195053,0.0,0.027195053175091743
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",MBTI,-0.010538917,0.0,0.010538917034864426
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",MSE Loss 설명,-0.042760834,0.0,0.042760834097862244
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",MSE Loss 용도,-0.0071563683,0.0,0.0071563683450222015
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",Multi-Label 에서 CE + Softmax 적용 문제점,-0.01984167,0.0,0.019841669127345085
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",PEFT 방법 5가지,-0.013205691,0.0,0.013205691240727901
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",거대 언어 모델 정의,-0.0006141361,0.0,0.0006141361081972718
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",기본 경험,-5.1924122e-05,0.0,5.192412208998576e-05
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",답변 실패,-0.030808846,0.0,0.030808845534920692
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",딥러닝,-0.00873309,0.0,0.00873309001326561
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",마지막 할 말,0.009631264,0.0,0.0096312640234828
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",머신러닝,0.011390295,0.0,0.011390294879674911
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",면접 시작 인사,3.5960977e-05,0.0,3.596097667468712e-05
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",상세 경험,0.030256744,0.0,0.030256744474172592
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",수식,-0.013201182,0.0,0.013201181776821613
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",용어 질문,0.038261198,0.0,0.03826119750738144
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",인공지능,-0.020035911,0.0,0.020035911351442337
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",잠시 휴식,-0.018935151,0.0,0.018935151398181915
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",좋아하는 아이돌,-0.031119991,0.0,0.031119991093873978
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",핵심 아이디어,-0.07067014,0.0,0.07067014276981354
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",확률 예측에서 MSE Loss 미 사용 이유,-0.08176334,0.0,0.08176334202289581
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,BCE 가 좋은 task,-0.0052017192,0.0,0.005201719235628843
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,BCE 가 좋은 이유,-0.016606573,0.0,0.01660657301545143
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,LLM Fine-Tuning 의 PEFT,-0.009336515,0.0,0.009336515329778194
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,LoRA,0.028908126,0.0,0.028908126056194305
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,LoRA 와 QLoRA 의 차이,-0.0013159381,0.0,0.001315938076004386
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,Loss Function 예시,0.0047619026,0.0,0.004761902615427971
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,Loss Function 정의,-0.0059056086,0.0,0.005905608646571636
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,MBTI,0.00075457676,0.0,0.0007545767584815621
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,MSE Loss 설명,-0.0022056445,0.0,0.002205644501373172
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,MSE Loss 용도,-0.017942393,0.0,0.017942393198609352
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,Multi-Label 에서 CE + Softmax 적용 문제점,0.9662856,1.0,0.0337144136428833
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,PEFT 방법 5가지,-0.0075085633,0.0,0.007508563343435526
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,거대 언어 모델 정의,-0.013362127,0.0,0.013362127356231213
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,기본 경험,-0.020558093,0.0,0.02055809274315834
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,답변 실패,-0.004198942,0.0,0.004198941867798567
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,딥러닝,0.012232516,0.0,0.012232515960931778
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,마지막 할 말,-0.007535016,0.0,0.007535016164183617
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,머신러닝,0.015001517,0.0,0.015001516789197922
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,면접 시작 인사,0.0037940354,0.0,0.0037940354086458683
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,상세 경험,-0.0003172428,0.0,0.00031724281143397093
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,수식,-0.0004660878,0.0,0.0004660878039430827
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,용어 질문,0.024853315,0.0,0.024853315204381943
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,인공지능,0.01840741,0.0,0.018407410010695457
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,잠시 휴식,-0.015204456,0.0,0.015204455703496933
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,좋아하는 아이돌,0.007419717,0.0,0.0074197170324623585
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,핵심 아이디어,-0.017475434,0.0,0.0174754336476326
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,확률 예측에서 MSE Loss 미 사용 이유,-0.001307132,0.0,0.0013071319554001093
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,BCE 가 좋은 task,0.027954675,0.0,0.027954675257205963
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,BCE 가 좋은 이유,0.009004534,0.0,0.009004534222185612
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,LLM Fine-Tuning 의 PEFT,0.008900809,0.0,0.008900809101760387
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,LoRA,0.0054777153,0.0,0.0054777152836322784
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,LoRA 와 QLoRA 의 차이,-0.0150202345,0.0,0.015020234510302544
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,Loss Function 예시,-0.0014513844,0.0,0.001451384392566979
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,Loss Function 정의,0.000525291,0.0,0.0005252910195849836
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,MBTI,-0.01513011,0.0,0.015130110085010529
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,MSE Loss 설명,-0.009388855,0.0,0.00938885472714901
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,MSE Loss 용도,-0.0095589645,0.0,0.009558964520692825
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,Multi-Label 에서 CE + Softmax 적용 문제점,0.00981903,0.0,0.009819029830396175
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,PEFT 방법 5가지,0.0038886426,0.0,0.0038886426482349634
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,거대 언어 모델 정의,-0.015798362,0.0,0.015798361971974373
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,기본 경험,0.0023418907,0.0,0.0023418907076120377
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,답변 실패,0.97486615,1.0,0.025133848190307617
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,딥러닝,-0.0023173722,0.0,0.0023173722438514233
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,마지막 할 말,-0.0076072714,0.0,0.007607271429151297
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,머신러닝,-0.02118769,0.0,0.021187689155340195
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,면접 시작 인사,-0.010148366,0.0,0.01014836598187685
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,상세 경험,-0.0037690436,0.0,0.0037690436001867056
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,수식,-0.014119043,0.0,0.0141190430149436
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,용어 질문,-0.019873412,0.0,0.01987341232597828
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,인공지능,-0.007330209,0.0,0.007330209016799927
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,잠시 휴식,-0.009293273,0.0,0.009293273091316223
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,좋아하는 아이돌,0.002267065,0.0,0.002267064992338419
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,핵심 아이디어,-0.009367073,0.0,0.009367072954773903
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,확률 예측에서 MSE Loss 미 사용 이유,-0.019688593,0.0,0.019688593223690987
Loss Function 관련 실무 경험 -> 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,BCE 가 좋은 task,-0.0088015795,0.0,0.008801579475402832
Loss Function 관련 실무 경험 -> 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,BCE 가 좋은 이유,-0.0012277312,0.0,0.0012277312343940139
Loss Function 관련 실무 경험 -> 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,LLM Fine-Tuning 의 PEFT,-0.012782549,0.0,0.012782548554241657
Loss Function 관련 실무 경험 -> 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,LoRA,0.011156024,0.0,0.011156024411320686
Loss Function 관련 실무 경험 -> 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,LoRA 와 QLoRA 의 차이,-0.009596151,0.0,0.009596151299774647
Loss Function 관련 실무 경험 -> 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,Loss Function 예시,0.015335579,0.0,0.01533557940274477
Loss Function 관련 실무 경험 -> 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,Loss Function 정의,0.01194434,0.0,0.011944339610636234
Loss Function 관련 실무 경험 -> 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,MBTI,-0.0011798196,0.0,0.0011798195773735642
Loss Function 관련 실무 경험 -> 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,MSE Loss 설명,0.016961286,0.0,0.01696128584444523
Loss Function 관련 실무 경험 -> 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,MSE Loss 용도,0.0043431874,0.0,0.004343187436461449
Loss Function 관련 실무 경험 -> 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,Multi-Label 에서 CE + Softmax 적용 문제점,0.0053882194,0.0,0.005388219375163317
Loss Function 관련 실무 경험 -> 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,PEFT 방법 5가지,-0.028592952,0.0,0.028592951595783234
Loss Function 관련 실무 경험 -> 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,거대 언어 모델 정의,-0.004041258,0.0,0.004041257780045271
Loss Function 관련 실무 경험 -> 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,기본 경험,-0.022889858,1.0,1.0228898581117392
Loss Function 관련 실무 경험 -> 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,답변 실패,-0.0113474345,0.0,0.011347434483468533
Loss Function 관련 실무 경험 -> 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,딥러닝,-0.0071034646,0.0,0.007103464566171169
Loss Function 관련 실무 경험 -> 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,마지막 할 말,-0.014217879,0.0,0.014217878691852093
Loss Function 관련 실무 경험 -> 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,머신러닝,-0.012639943,0.0,0.012639942578971386
Loss Function 관련 실무 경험 -> 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,면접 시작 인사,-0.0027827197,0.0,0.002782719675451517
Loss Function 관련 실무 경험 -> 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,상세 경험,0.82897186,0.0,0.8289718627929688
Loss Function 관련 실무 경험 -> 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,수식,-0.066813454,0.0,0.06681345403194427
Loss Function 관련 실무 경험 -> 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,용어 질문,0.043349683,0.0,0.04334968328475952
Loss Function 관련 실무 경험 -> 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,인공지능,0.07301964,0.0,0.07301963865756989
Loss Function 관련 실무 경험 -> 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,잠시 휴식,-0.030901344,0.0,0.030901344493031502
Loss Function 관련 실무 경험 -> 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,좋아하는 아이돌,-0.0067098415,0.0,0.006709841545671225
Loss Function 관련 실무 경험 -> 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,핵심 아이디어,-0.026457842,0.0,0.02645784243941307
Loss Function 관련 실무 경험 -> 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,확률 예측에서 MSE Loss 미 사용 이유,-0.008968045,0.0,0.008968045003712177
Loss Function 관련 실무 경험 -> 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,BCE 가 좋은 task,0.00063497847,0.0,0.0006349784671328962
Loss Function 관련 실무 경험 -> 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,BCE 가 좋은 이유,-0.028074803,0.0,0.028074802830815315
Loss Function 관련 실무 경험 -> 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,LLM Fine-Tuning 의 PEFT,-0.0012537974,0.0,0.0012537974398583174
Loss Function 관련 실무 경험 -> 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,LoRA,-0.025291486,0.0,0.025291485711932182
Loss Function 관련 실무 경험 -> 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,LoRA 와 QLoRA 의 차이,0.017883683,0.0,0.017883682623505592
Loss Function 관련 실무 경험 -> 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,Loss Function 예시,-0.020902636,0.0,0.020902635529637337
Loss Function 관련 실무 경험 -> 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,Loss Function 정의,-0.004382091,0.0,0.004382091108709574
Loss Function 관련 실무 경험 -> 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,MBTI,-0.012971847,0.0,0.01297184731811285
Loss Function 관련 실무 경험 -> 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,MSE Loss 설명,0.011551217,0.0,0.011551217176020145
Loss Function 관련 실무 경험 -> 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,MSE Loss 용도,-0.021777118,0.0,0.021777117624878883
Loss Function 관련 실무 경험 -> 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,Multi-Label 에서 CE + Softmax 적용 문제점,-0.02543063,0.0,0.025430630892515182
Loss Function 관련 실무 경험 -> 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,PEFT 방법 5가지,-0.021512328,0.0,0.02151232771575451
Loss Function 관련 실무 경험 -> 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,거대 언어 모델 정의,0.054375105,0.0,0.05437510460615158
Loss Function 관련 실무 경험 -> 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,기본 경험,0.52113104,0.0,0.5211310386657715
Loss Function 관련 실무 경험 -> 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,답변 실패,0.0747704,0.0,0.07477039843797684
Loss Function 관련 실무 경험 -> 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,딥러닝,0.018778555,0.0,0.01877855509519577
Loss Function 관련 실무 경험 -> 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,마지막 할 말,0.00806807,0.0,0.008068069815635681
Loss Function 관련 실무 경험 -> 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,머신러닝,-0.052615453,0.0,0.0526154525578022
Loss Function 관련 실무 경험 -> 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,면접 시작 인사,-0.02164784,0.0,0.02164784073829651
Loss Function 관련 실무 경험 -> 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,상세 경험,0.39795485,1.0,0.6020451486110687
Loss Function 관련 실무 경험 -> 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,수식,0.009282501,0.0,0.00928250141441822
Loss Function 관련 실무 경험 -> 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,용어 질문,0.007584555,0.0,0.007584555074572563
Loss Function 관련 실무 경험 -> 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,인공지능,-0.003192367,0.0,0.0031923670321702957
Loss Function 관련 실무 경험 -> 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,잠시 휴식,-0.017255463,0.0,0.01725546270608902
Loss Function 관련 실무 경험 -> 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,좋아하는 아이돌,0.017309682,0.0,0.017309682443737984
Loss Function 관련 실무 경험 -> 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,핵심 아이디어,-0.016873453,0.0,0.016873452812433243
Loss Function 관련 실무 경험 -> 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,확률 예측에서 MSE Loss 미 사용 이유,0.0012990487,0.0,0.001299048657529056
Loss Function 관련 실무 경험 -> 근데 기억 진짜 안나는데 어떡하지?,BCE 가 좋은 task,-0.007181089,0.0,0.0071810889057815075
Loss Function 관련 실무 경험 -> 근데 기억 진짜 안나는데 어떡하지?,BCE 가 좋은 이유,0.004233934,0.0,0.004233933985233307
Loss Function 관련 실무 경험 -> 근데 기억 진짜 안나는데 어떡하지?,LLM Fine-Tuning 의 PEFT,-0.0020599086,0.0,0.0020599085837602615
Loss Function 관련 실무 경험 -> 근데 기억 진짜 안나는데 어떡하지?,LoRA,0.004974341,0.0,0.00497434101998806
Loss Function 관련 실무 경험 -> 근데 기억 진짜 안나는데 어떡하지?,LoRA 와 QLoRA 의 차이,-0.009090715,0.0,0.00909071508795023
Loss Function 관련 실무 경험 -> 근데 기억 진짜 안나는데 어떡하지?,Loss Function 예시,-0.011119692,0.0,0.011119691655039787
Loss Function 관련 실무 경험 -> 근데 기억 진짜 안나는데 어떡하지?,Loss Function 정의,0.01048475,0.0,0.010484750382602215
Loss Function 관련 실무 경험 -> 근데 기억 진짜 안나는데 어떡하지?,MBTI,-0.0002072911,0.0,0.0002072911011055112
Loss Function 관련 실무 경험 -> 근데 기억 진짜 안나는데 어떡하지?,MSE Loss 설명,-0.011946578,0.0,0.011946577578783035
Loss Function 관련 실무 경험 -> 근데 기억 진짜 안나는데 어떡하지?,MSE Loss 용도,-0.005694828,0.0,0.005694827996194363
Loss Function 관련 실무 경험 -> 근데 기억 진짜 안나는데 어떡하지?,Multi-Label 에서 CE + Softmax 적용 문제점,-0.017317133,0.0,0.017317133024334908
Loss Function 관련 실무 경험 -> 근데 기억 진짜 안나는데 어떡하지?,PEFT 방법 5가지,-0.00885428,0.0,0.008854280225932598
Loss Function 관련 실무 경험 -> 근데 기억 진짜 안나는데 어떡하지?,거대 언어 모델 정의,0.0042769792,0.0,0.004276979248970747
Loss Function 관련 실무 경험 -> 근데 기억 진짜 안나는데 어떡하지?,기본 경험,0.03434321,0.0,0.034343209117650986
Loss Function 관련 실무 경험 -> 근데 기억 진짜 안나는데 어떡하지?,답변 실패,0.9761798,1.0,0.023820221424102783
Loss Function 관련 실무 경험 -> 근데 기억 진짜 안나는데 어떡하지?,딥러닝,0.001067762,0.0,0.001067762030288577
Loss Function 관련 실무 경험 -> 근데 기억 진짜 안나는데 어떡하지?,마지막 할 말,-0.007494585,0.0,0.007494585122913122
Loss Function 관련 실무 경험 -> 근데 기억 진짜 안나는데 어떡하지?,머신러닝,-0.017611938,0.0,0.01761193759739399
Loss Function 관련 실무 경험 -> 근데 기억 진짜 안나는데 어떡하지?,면접 시작 인사,0.0018280951,0.0,0.0018280950607731938
Loss Function 관련 실무 경험 -> 근데 기억 진짜 안나는데 어떡하지?,상세 경험,-0.0006438127,0.0,0.000643812702037394
Loss Function 관련 실무 경험 -> 근데 기억 진짜 안나는데 어떡하지?,수식,-0.0059886384,0.0,0.005988638382405043
Loss Function 관련 실무 경험 -> 근데 기억 진짜 안나는데 어떡하지?,용어 질문,-0.03382697,0.0,0.03382696956396103
Loss Function 관련 실무 경험 -> 근데 기억 진짜 안나는데 어떡하지?,인공지능,-0.014151018,0.0,0.014151018112897873
Loss Function 관련 실무 경험 -> 근데 기억 진짜 안나는데 어떡하지?,잠시 휴식,-0.004981999,0.0,0.004981998819857836
Loss Function 관련 실무 경험 -> 근데 기억 진짜 안나는데 어떡하지?,좋아하는 아이돌,0.021072866,0.0,0.021072866395115852
Loss Function 관련 실무 경험 -> 근데 기억 진짜 안나는데 어떡하지?,핵심 아이디어,-0.004220913,0.0,0.004220913164317608
Loss Function 관련 실무 경험 -> 근데 기억 진짜 안나는데 어떡하지?,확률 예측에서 MSE Loss 미 사용 이유,-0.009235014,0.0,0.009235014207661152
Loss Function 관련 실무 경험 -> 나 아직 실무 경험 없는데 ㅠㅠ,BCE 가 좋은 task,-0.0036711798,0.0,0.003671179758384824
Loss Function 관련 실무 경험 -> 나 아직 실무 경험 없는데 ㅠㅠ,BCE 가 좋은 이유,0.022562709,0.0,0.02256270870566368
Loss Function 관련 실무 경험 -> 나 아직 실무 경험 없는데 ㅠㅠ,LLM Fine-Tuning 의 PEFT,0.013309896,0.0,0.013309895992279053
Loss Function 관련 실무 경험 -> 나 아직 실무 경험 없는데 ㅠㅠ,LoRA,-0.011975287,0.0,0.011975287459790707
Loss Function 관련 실무 경험 -> 나 아직 실무 경험 없는데 ㅠㅠ,LoRA 와 QLoRA 의 차이,-0.004312704,0.0,0.004312703851610422
Loss Function 관련 실무 경험 -> 나 아직 실무 경험 없는데 ㅠㅠ,Loss Function 예시,-0.007648865,0.0,0.0076488652266561985
Loss Function 관련 실무 경험 -> 나 아직 실무 경험 없는데 ㅠㅠ,Loss Function 정의,0.0070264493,0.0,0.00702644931152463
Loss Function 관련 실무 경험 -> 나 아직 실무 경험 없는데 ㅠㅠ,MBTI,0.0042621144,0.0,0.004262114409357309
Loss Function 관련 실무 경험 -> 나 아직 실무 경험 없는데 ㅠㅠ,MSE Loss 설명,0.015601903,0.0,0.015601903200149536
Loss Function 관련 실무 경험 -> 나 아직 실무 경험 없는데 ㅠㅠ,MSE Loss 용도,0.016920311,0.0,0.016920311376452446
Loss Function 관련 실무 경험 -> 나 아직 실무 경험 없는데 ㅠㅠ,Multi-Label 에서 CE + Softmax 적용 문제점,0.0031520624,0.0,0.0031520624179393053
Loss Function 관련 실무 경험 -> 나 아직 실무 경험 없는데 ㅠㅠ,PEFT 방법 5가지,-0.006302871,0.0,0.006302870810031891
Loss Function 관련 실무 경험 -> 나 아직 실무 경험 없는데 ㅠㅠ,거대 언어 모델 정의,0.02079573,0.0,0.020795730873942375
Loss Function 관련 실무 경험 -> 나 아직 실무 경험 없는데 ㅠㅠ,기본 경험,0.842061,1.0,0.15793901681900024
Loss Function 관련 실무 경험 -> 나 아직 실무 경험 없는데 ㅠㅠ,답변 실패,0.014314253,0.0,0.014314252883195877
Loss Function 관련 실무 경험 -> 나 아직 실무 경험 없는데 ㅠㅠ,딥러닝,0.017906703,0.0,0.017906703054904938
Loss Function 관련 실무 경험 -> 나 아직 실무 경험 없는데 ㅠㅠ,마지막 할 말,0.015739668,0.0,0.015739668160676956
Loss Function 관련 실무 경험 -> 나 아직 실무 경험 없는데 ㅠㅠ,머신러닝,0.010372319,0.0,0.010372319258749485
Loss Function 관련 실무 경험 -> 나 아직 실무 경험 없는데 ㅠㅠ,면접 시작 인사,-0.0052875895,0.0,0.005287589505314827
Loss Function 관련 실무 경험 -> 나 아직 실무 경험 없는데 ㅠㅠ,상세 경험,0.026372332,0.0,0.026372332125902176
Loss Function 관련 실무 경험 -> 나 아직 실무 경험 없는데 ㅠㅠ,수식,0.04090251,0.0,0.0409025102853775
Loss Function 관련 실무 경험 -> 나 아직 실무 경험 없는데 ㅠㅠ,용어 질문,-0.015633268,0.0,0.015633268281817436
Loss Function 관련 실무 경험 -> 나 아직 실무 경험 없는데 ㅠㅠ,인공지능,-0.0051498706,0.0,0.00514987064525485
Loss Function 관련 실무 경험 -> 나 아직 실무 경험 없는데 ㅠㅠ,잠시 휴식,-0.00790606,0.0,0.007906059734523296
Loss Function 관련 실무 경험 -> 나 아직 실무 경험 없는데 ㅠㅠ,좋아하는 아이돌,-0.008577165,0.0,0.008577165193855762
Loss Function 관련 실무 경험 -> 나 아직 실무 경험 없는데 ㅠㅠ,핵심 아이디어,0.024477307,0.0,0.024477306753396988
Loss Function 관련 실무 경험 -> 나 아직 실무 경험 없는데 ㅠㅠ,확률 예측에서 MSE Loss 미 사용 이유,-0.0019173465,0.0,0.001917346497066319
Loss Function 관련 실무 경험 -> 대답하기 싫은데 힝,BCE 가 좋은 task,-0.0032357727,0.0,0.003235772717744112
Loss Function 관련 실무 경험 -> 대답하기 싫은데 힝,BCE 가 좋은 이유,0.0029905885,0.0,0.0029905885457992554
Loss Function 관련 실무 경험 -> 대답하기 싫은데 힝,LLM Fine-Tuning 의 PEFT,-0.007886563,0.0,0.007886563427746296
Loss Function 관련 실무 경험 -> 대답하기 싫은데 힝,LoRA,-0.0014693437,0.0,0.0014693436678498983
Loss Function 관련 실무 경험 -> 대답하기 싫은데 힝,LoRA 와 QLoRA 의 차이,-0.008519038,0.0,0.008519037626683712
Loss Function 관련 실무 경험 -> 대답하기 싫은데 힝,Loss Function 예시,-0.016104214,0.0,0.016104213893413544
Loss Function 관련 실무 경험 -> 대답하기 싫은데 힝,Loss Function 정의,0.0030740628,0.0,0.003074062755331397
Loss Function 관련 실무 경험 -> 대답하기 싫은데 힝,MBTI,0.002635474,0.0,0.002635474083945155
Loss Function 관련 실무 경험 -> 대답하기 싫은데 힝,MSE Loss 설명,-0.012361542,0.0,0.012361542321741581
Loss Function 관련 실무 경험 -> 대답하기 싫은데 힝,MSE Loss 용도,-0.008315997,0.0,0.008315997198224068
Loss Function 관련 실무 경험 -> 대답하기 싫은데 힝,Multi-Label 에서 CE + Softmax 적용 문제점,-0.0020320816,0.0,0.0020320815965533257
Loss Function 관련 실무 경험 -> 대답하기 싫은데 힝,PEFT 방법 5가지,-0.017088411,0.0,0.01708841137588024
Loss Function 관련 실무 경험 -> 대답하기 싫은데 힝,거대 언어 모델 정의,-0.008505494,0.0,0.008505494333803654
Loss Function 관련 실무 경험 -> 대답하기 싫은데 힝,기본 경험,0.017438723,0.0,0.017438722774386406
Loss Function 관련 실무 경험 -> 대답하기 싫은데 힝,답변 실패,0.9787341,1.0,0.021265923976898193
Loss Function 관련 실무 경험 -> 대답하기 싫은데 힝,딥러닝,0.0055362666,0.0,0.005536266602575779
Loss Function 관련 실무 경험 -> 대답하기 싫은데 힝,마지막 할 말,-0.01573431,0.0,0.01573430933058262
Loss Function 관련 실무 경험 -> 대답하기 싫은데 힝,머신러닝,-0.02645989,0.0,0.026459889486432076
Loss Function 관련 실무 경험 -> 대답하기 싫은데 힝,면접 시작 인사,0.0026724506,0.0,0.002672450616955757
Loss Function 관련 실무 경험 -> 대답하기 싫은데 힝,상세 경험,0.008785953,0.0,0.008785952813923359
Loss Function 관련 실무 경험 -> 대답하기 싫은데 힝,수식,-0.013661278,0.0,0.013661278411746025
Loss Function 관련 실무 경험 -> 대답하기 싫은데 힝,용어 질문,-1.31702145e-05,0.0,1.3170214515412226e-05
Loss Function 관련 실무 경험 -> 대답하기 싫은데 힝,인공지능,-0.015326972,0.0,0.015326972119510174
Loss Function 관련 실무 경험 -> 대답하기 싫은데 힝,잠시 휴식,-0.005777345,0.0,0.005777345038950443
Loss Function 관련 실무 경험 -> 대답하기 싫은데 힝,좋아하는 아이돌,0.0032597855,0.0,0.00325978547334671
Loss Function 관련 실무 경험 -> 대답하기 싫은데 힝,핵심 아이디어,-0.011371619,0.0,0.011371619068086147
Loss Function 관련 실무 경험 -> 대답하기 싫은데 힝,확률 예측에서 MSE Loss 미 사용 이유,-0.0047007715,0.0,0.0047007715329527855
Loss Function 관련 실무 경험 -> 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ,BCE 가 좋은 task,-0.010110094,0.0,0.010110094211995602
Loss Function 관련 실무 경험 -> 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ,BCE 가 좋은 이유,-0.014025394,0.0,0.01402539387345314
Loss Function 관련 실무 경험 -> 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ,LLM Fine-Tuning 의 PEFT,-0.03211826,0.0,0.032118260860443115
Loss Function 관련 실무 경험 -> 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ,LoRA,-0.028443294,0.0,0.028443293645977974
Loss Function 관련 실무 경험 -> 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ,LoRA 와 QLoRA 의 차이,0.0021942125,0.0,0.002194212516769767
Loss Function 관련 실무 경험 -> 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ,Loss Function 예시,-0.006004694,0.0,0.006004693917930126
Loss Function 관련 실무 경험 -> 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ,Loss Function 정의,-0.017531583,0.0,0.017531583085656166
Loss Function 관련 실무 경험 -> 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ,MBTI,-0.023332275,0.0,0.023332275450229645
Loss Function 관련 실무 경험 -> 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ,MSE Loss 설명,-0.019383686,0.0,0.019383685663342476
Loss Function 관련 실무 경험 -> 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ,MSE Loss 용도,-0.016552292,0.0,0.016552291810512543
Loss Function 관련 실무 경험 -> 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ,Multi-Label 에서 CE + Softmax 적용 문제점,-0.0148064075,0.0,0.014806407503783703
Loss Function 관련 실무 경험 -> 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ,PEFT 방법 5가지,-0.009360872,0.0,0.009360872209072113
Loss Function 관련 실무 경험 -> 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ,거대 언어 모델 정의,0.008653482,0.0,0.008653482422232628
Loss Function 관련 실무 경험 -> 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ,기본 경험,0.45050752,1.0,0.5494924783706665
Loss Function 관련 실무 경험 -> 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ,답변 실패,0.14900973,0.0,0.14900973439216614
Loss Function 관련 실무 경험 -> 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ,딥러닝,0.0070634647,0.0,0.007063464727252722
Loss Function 관련 실무 경험 -> 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ,마지막 할 말,-0.026058031,0.0,0.026058031246066093
Loss Function 관련 실무 경험 -> 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ,머신러닝,-0.02810653,0.0,0.028106529265642166
Loss Function 관련 실무 경험 -> 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ,면접 시작 인사,-0.03345106,0.0,0.03345106169581413
Loss Function 관련 실무 경험 -> 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ,상세 경험,0.46734077,0.0,0.46734076738357544
Loss Function 관련 실무 경험 -> 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ,수식,0.01811926,0.0,0.018119260668754578
Loss Function 관련 실무 경험 -> 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ,용어 질문,0.03114624,0.0,0.03114623948931694
Loss Function 관련 실무 경험 -> 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ,인공지능,0.01243506,0.0,0.012435059994459152
Loss Function 관련 실무 경험 -> 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ,잠시 휴식,-0.03216996,0.0,0.03216996043920517
Loss Function 관련 실무 경험 -> 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ,좋아하는 아이돌,-0.017654926,0.0,0.01765492558479309
Loss Function 관련 실무 경험 -> 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ,핵심 아이디어,-0.008163637,0.0,0.008163636550307274
Loss Function 관련 실무 경험 -> 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ,확률 예측에서 MSE Loss 미 사용 이유,0.019542554,0.0,0.019542554393410683
Loss Function 관련 실무 경험 -> 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어,BCE 가 좋은 task,-0.0097494535,0.0,0.009749453514814377
Loss Function 관련 실무 경험 -> 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어,BCE 가 좋은 이유,-0.007296436,0.0,0.007296436000615358
Loss Function 관련 실무 경험 -> 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어,LLM Fine-Tuning 의 PEFT,-0.003114776,0.0,0.003114775987342
Loss Function 관련 실무 경험 -> 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어,LoRA,0.0020294967,0.0,0.0020294967107474804
Loss Function 관련 실무 경험 -> 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어,LoRA 와 QLoRA 의 차이,-0.016866343,0.0,0.016866343095898628
Loss Function 관련 실무 경험 -> 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어,Loss Function 예시,0.0024634253,0.0,0.00246342527680099
Loss Function 관련 실무 경험 -> 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어,Loss Function 정의,-0.019895412,0.0,0.019895412027835846
Loss Function 관련 실무 경험 -> 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어,MBTI,0.013807014,0.0,0.013807013630867004
Loss Function 관련 실무 경험 -> 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어,MSE Loss 설명,0.01442528,0.0,0.014425279572606087
Loss Function 관련 실무 경험 -> 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어,MSE Loss 용도,-0.023595847,0.0,0.023595847189426422
Loss Function 관련 실무 경험 -> 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어,Multi-Label 에서 CE + Softmax 적용 문제점,-0.04574261,0.0,0.04574260860681534
Loss Function 관련 실무 경험 -> 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어,PEFT 방법 5가지,-0.024478687,0.0,0.024478686973452568
Loss Function 관련 실무 경험 -> 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어,거대 언어 모델 정의,-0.023173848,0.0,0.023173848167061806
Loss Function 관련 실무 경험 -> 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어,기본 경험,-0.06874175,0.0,0.06874175369739532
Loss Function 관련 실무 경험 -> 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어,답변 실패,-0.014954011,0.0,0.01495401095598936
Loss Function 관련 실무 경험 -> 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어,딥러닝,-0.021281533,0.0,0.02128153294324875
Loss Function 관련 실무 경험 -> 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어,마지막 할 말,-0.0117157055,0.0,0.011715705506503582
Loss Function 관련 실무 경험 -> 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어,머신러닝,-0.020282106,0.0,0.02028210647404194
Loss Function 관련 실무 경험 -> 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어,면접 시작 인사,-0.020103082,0.0,0.020103082060813904
Loss Function 관련 실무 경험 -> 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어,상세 경험,0.827664,1.0,0.17233598232269287
Loss Function 관련 실무 경험 -> 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어,수식,-0.053563416,0.0,0.05356341600418091
Loss Function 관련 실무 경험 -> 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어,용어 질문,0.03424479,0.0,0.03424479067325592
Loss Function 관련 실무 경험 -> 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어,인공지능,0.09315196,0.0,0.09315195679664612
Loss Function 관련 실무 경험 -> 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어,잠시 휴식,-0.0045712017,0.0,0.004571201745420694
Loss Function 관련 실무 경험 -> 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어,좋아하는 아이돌,-0.006477005,0.0,0.00647700484842062
Loss Function 관련 실무 경험 -> 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어,핵심 아이디어,-0.011306366,0.0,0.011306365951895714
Loss Function 관련 실무 경험 -> 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어,확률 예측에서 MSE Loss 미 사용 이유,0.033781435,0.0,0.03378143534064293
Loss Function 관련 실무 경험 -> 기억 진짜 안 나는데 가물가물하다,BCE 가 좋은 task,0.0036277021,0.0,0.0036277021281421185
Loss Function 관련 실무 경험 -> 기억 진짜 안 나는데 가물가물하다,BCE 가 좋은 이유,0.0020102898,0.0,0.0020102898124605417
Loss Function 관련 실무 경험 -> 기억 진짜 안 나는데 가물가물하다,LLM Fine-Tuning 의 PEFT,-0.0073403344,0.0,0.007340334355831146
Loss Function 관련 실무 경험 -> 기억 진짜 안 나는데 가물가물하다,LoRA,0.00041797283,0.0,0.0004179728275630623
Loss Function 관련 실무 경험 -> 기억 진짜 안 나는데 가물가물하다,LoRA 와 QLoRA 의 차이,-0.0092141535,0.0,0.00921415351331234
Loss Function 관련 실무 경험 -> 기억 진짜 안 나는데 가물가물하다,Loss Function 예시,-0.00662331,0.0,0.006623310036957264
Loss Function 관련 실무 경험 -> 기억 진짜 안 나는데 가물가물하다,Loss Function 정의,0.0015795348,0.0,0.001579534844495356
Loss Function 관련 실무 경험 -> 기억 진짜 안 나는데 가물가물하다,MBTI,-0.0031159685,0.0,0.003115968545898795
Loss Function 관련 실무 경험 -> 기억 진짜 안 나는데 가물가물하다,MSE Loss 설명,-0.015437882,0.0,0.015437882393598557
Loss Function 관련 실무 경험 -> 기억 진짜 안 나는데 가물가물하다,MSE Loss 용도,-0.008655522,0.0,0.008655522018671036
Loss Function 관련 실무 경험 -> 기억 진짜 안 나는데 가물가물하다,Multi-Label 에서 CE + Softmax 적용 문제점,-0.009725474,0.0,0.009725473821163177
Loss Function 관련 실무 경험 -> 기억 진짜 안 나는데 가물가물하다,PEFT 방법 5가지,-0.015524227,0.0,0.015524227172136307
Loss Function 관련 실무 경험 -> 기억 진짜 안 나는데 가물가물하다,거대 언어 모델 정의,0.0024798624,0.0,0.0024798624217510223
Loss Function 관련 실무 경험 -> 기억 진짜 안 나는데 가물가물하다,기본 경험,0.02133672,0.0,0.021336719393730164
Loss Function 관련 실무 경험 -> 기억 진짜 안 나는데 가물가물하다,답변 실패,0.9769316,1.0,0.02306842803955078
Loss Function 관련 실무 경험 -> 기억 진짜 안 나는데 가물가물하다,딥러닝,-0.0040243915,0.0,0.004024391528218985
Loss Function 관련 실무 경험 -> 기억 진짜 안 나는데 가물가물하다,마지막 할 말,-0.009190987,0.0,0.00919098686426878
Loss Function 관련 실무 경험 -> 기억 진짜 안 나는데 가물가물하다,머신러닝,-0.006348507,0.0,0.006348507013171911
Loss Function 관련 실무 경험 -> 기억 진짜 안 나는데 가물가물하다,면접 시작 인사,0.012386425,0.0,0.012386425398290157
Loss Function 관련 실무 경험 -> 기억 진짜 안 나는데 가물가물하다,상세 경험,0.012385547,0.0,0.012385547161102295
Loss Function 관련 실무 경험 -> 기억 진짜 안 나는데 가물가물하다,수식,-0.014084703,0.0,0.014084703288972378
Loss Function 관련 실무 경험 -> 기억 진짜 안 나는데 가물가물하다,용어 질문,-0.03553786,0.0,0.03553786128759384
Loss Function 관련 실무 경험 -> 기억 진짜 안 나는데 가물가물하다,인공지능,-0.010242157,0.0,0.010242156684398651
Loss Function 관련 실무 경험 -> 기억 진짜 안 나는데 가물가물하다,잠시 휴식,-0.0053401887,0.0,0.00534018874168396
Loss Function 관련 실무 경험 -> 기억 진짜 안 나는데 가물가물하다,좋아하는 아이돌,0.011451807,0.0,0.011451806873083115
Loss Function 관련 실무 경험 -> 기억 진짜 안 나는데 가물가물하다,핵심 아이디어,-0.011486901,0.0,0.011486900970339775
Loss Function 관련 실무 경험 -> 기억 진짜 안 나는데 가물가물하다,확률 예측에서 MSE Loss 미 사용 이유,-0.0030274265,0.0,0.0030274265445768833
Loss Function 관련 실무 경험 -> 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ,BCE 가 좋은 task,-0.0037157107,0.0,0.0037157107144594193
Loss Function 관련 실무 경험 -> 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ,BCE 가 좋은 이유,0.021701109,0.0,0.021701108664274216
Loss Function 관련 실무 경험 -> 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ,LLM Fine-Tuning 의 PEFT,0.01199009,0.0,0.011990089900791645
Loss Function 관련 실무 경험 -> 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ,LoRA,-0.01757869,0.0,0.017578689381480217
Loss Function 관련 실무 경험 -> 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ,LoRA 와 QLoRA 의 차이,-0.0036094736,0.0,0.0036094735842198133
Loss Function 관련 실무 경험 -> 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ,Loss Function 예시,-0.010453875,0.0,0.010453875176608562
Loss Function 관련 실무 경험 -> 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ,Loss Function 정의,0.0061199223,0.0,0.006119922269135714
Loss Function 관련 실무 경험 -> 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ,MBTI,0.009643323,0.0,0.00964332278817892
Loss Function 관련 실무 경험 -> 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ,MSE Loss 설명,0.016217986,0.0,0.01621798612177372
Loss Function 관련 실무 경험 -> 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ,MSE Loss 용도,0.012561103,0.0,0.012561103329062462
Loss Function 관련 실무 경험 -> 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ,Multi-Label 에서 CE + Softmax 적용 문제점,-0.0040998613,0.0,0.00409986125305295
Loss Function 관련 실무 경험 -> 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ,PEFT 방법 5가지,-0.011344934,0.0,0.01134493388235569
Loss Function 관련 실무 경험 -> 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ,거대 언어 모델 정의,0.027275588,0.0,0.027275588363409042
Loss Function 관련 실무 경험 -> 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ,기본 경험,0.8298304,1.0,0.17016959190368652
Loss Function 관련 실무 경험 -> 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ,답변 실패,0.011481644,0.0,0.01148164365440607
Loss Function 관련 실무 경험 -> 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ,딥러닝,0.01878067,0.0,0.018780669197440147
Loss Function 관련 실무 경험 -> 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ,마지막 할 말,0.02230432,0.0,0.022304320707917213
Loss Function 관련 실무 경험 -> 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ,머신러닝,-0.0018741596,0.0,0.0018741595558822155
Loss Function 관련 실무 경험 -> 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ,면접 시작 인사,0.00992261,0.0,0.00992260966449976
Loss Function 관련 실무 경험 -> 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ,상세 경험,0.02773398,0.0,0.027733979746699333
Loss Function 관련 실무 경험 -> 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ,수식,0.02939117,0.0,0.029391169548034668
Loss Function 관련 실무 경험 -> 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ,용어 질문,-0.023488477,0.0,0.023488476872444153
Loss Function 관련 실무 경험 -> 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ,인공지능,-0.0048650457,0.0,0.0048650456592440605
Loss Function 관련 실무 경험 -> 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ,잠시 휴식,-0.008154229,0.0,0.008154229260981083
Loss Function 관련 실무 경험 -> 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ,좋아하는 아이돌,-0.0037359055,0.0,0.0037359055131673813
Loss Function 관련 실무 경험 -> 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ,핵심 아이디어,0.0180797,0.0,0.01807969994843006
Loss Function 관련 실무 경험 -> 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ,확률 예측에서 MSE Loss 미 사용 이유,-0.0022640426,0.0,0.002264042617753148
Loss Function 관련 실무 경험 -> 이걸 왜 알려줘야 하지,BCE 가 좋은 task,-0.013552131,0.0,0.01355213113129139
Loss Function 관련 실무 경험 -> 이걸 왜 알려줘야 하지,BCE 가 좋은 이유,0.0055111335,0.0,0.005511133465915918
Loss Function 관련 실무 경험 -> 이걸 왜 알려줘야 하지,LLM Fine-Tuning 의 PEFT,-0.0028529488,0.0,0.002852948848158121
Loss Function 관련 실무 경험 -> 이걸 왜 알려줘야 하지,LoRA,-0.0012891423,0.0,0.001289142295718193
Loss Function 관련 실무 경험 -> 이걸 왜 알려줘야 하지,LoRA 와 QLoRA 의 차이,1.7842789e-05,0.0,1.7842789020505734e-05
Loss Function 관련 실무 경험 -> 이걸 왜 알려줘야 하지,Loss Function 예시,-0.0061841817,0.0,0.006184181664139032
Loss Function 관련 실무 경험 -> 이걸 왜 알려줘야 하지,Loss Function 정의,0.0027581807,0.0,0.002758180722594261
Loss Function 관련 실무 경험 -> 이걸 왜 알려줘야 하지,MBTI,-0.012338082,0.0,0.012338082306087017
Loss Function 관련 실무 경험 -> 이걸 왜 알려줘야 하지,MSE Loss 설명,-0.02112502,0.0,0.02112502045929432
Loss Function 관련 실무 경험 -> 이걸 왜 알려줘야 하지,MSE Loss 용도,-0.022420678,0.0,0.022420678287744522
Loss Function 관련 실무 경험 -> 이걸 왜 알려줘야 하지,Multi-Label 에서 CE + Softmax 적용 문제점,-0.026501212,0.0,0.026501212269067764
Loss Function 관련 실무 경험 -> 이걸 왜 알려줘야 하지,PEFT 방법 5가지,-0.014074756,0.0,0.01407475583255291
Loss Function 관련 실무 경험 -> 이걸 왜 알려줘야 하지,거대 언어 모델 정의,0.0005571761,0.0,0.0005571761284954846
Loss Function 관련 실무 경험 -> 이걸 왜 알려줘야 하지,기본 경험,0.081100784,0.0,0.08110078424215317
Loss Function 관련 실무 경험 -> 이걸 왜 알려줘야 하지,답변 실패,0.96058524,1.0,0.03941476345062256
Loss Function 관련 실무 경험 -> 이걸 왜 알려줘야 하지,딥러닝,0.0043708384,0.0,0.004370838403701782
Loss Function 관련 실무 경험 -> 이걸 왜 알려줘야 하지,마지막 할 말,-0.0076978086,0.0,0.0076978085562586784
Loss Function 관련 실무 경험 -> 이걸 왜 알려줘야 하지,머신러닝,-0.022448938,0.0,0.022448938339948654
Loss Function 관련 실무 경험 -> 이걸 왜 알려줘야 하지,면접 시작 인사,-0.0056737824,0.0,0.005673782434314489
Loss Function 관련 실무 경험 -> 이걸 왜 알려줘야 하지,상세 경험,0.05340821,0.0,0.05340820923447609
Loss Function 관련 실무 경험 -> 이걸 왜 알려줘야 하지,수식,-0.011649126,0.0,0.011649126186966896
Loss Function 관련 실무 경험 -> 이걸 왜 알려줘야 하지,용어 질문,-0.0130945565,0.0,0.013094556517899036
Loss Function 관련 실무 경험 -> 이걸 왜 알려줘야 하지,인공지능,-0.010822939,0.0,0.01082293875515461
Loss Function 관련 실무 경험 -> 이걸 왜 알려줘야 하지,잠시 휴식,-0.018166013,0.0,0.018166013062000275
Loss Function 관련 실무 경험 -> 이걸 왜 알려줘야 하지,좋아하는 아이돌,0.006033618,0.0,0.006033618003129959
Loss Function 관련 실무 경험 -> 이걸 왜 알려줘야 하지,핵심 아이디어,-0.013979084,0.0,0.013979083858430386
Loss Function 관련 실무 경험 -> 이걸 왜 알려줘야 하지,확률 예측에서 MSE Loss 미 사용 이유,-0.0018068136,0.0,0.0018068136414512992
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,BCE 가 좋은 task,-0.0006494612,0.0,0.0006494611734524369
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,BCE 가 좋은 이유,0.010664702,0.0,0.010664702393114567
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,LLM Fine-Tuning 의 PEFT,0.00705301,0.0,0.007053010165691376
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,LoRA,-0.01970619,0.0,0.01970618963241577
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,LoRA 와 QLoRA 의 차이,-0.0007603204,0.0,0.0007603203994221985
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,Loss Function 예시,-0.004522007,0.0,0.004522006958723068
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,Loss Function 정의,-0.015985865,0.0,0.015985865145921707
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,MBTI,0.9670336,1.0,0.03296637535095215
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,MSE Loss 설명,-0.0013422116,0.0,0.0013422116171568632
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,MSE Loss 용도,-0.009797978,0.0,0.009797978214919567
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,Multi-Label 에서 CE + Softmax 적용 문제점,-0.047406275,0.0,0.04740627482533455
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,PEFT 방법 5가지,-0.016532637,0.0,0.016532637178897858
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,거대 언어 모델 정의,-0.009504257,0.0,0.009504256770014763
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,기본 경험,0.018015845,0.0,0.018015844747424126
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,답변 실패,-0.020168437,0.0,0.02016843669116497
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,딥러닝,-0.017466936,0.0,0.017466936260461807
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,마지막 할 말,-0.012657585,0.0,0.012657584622502327
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,머신러닝,0.0005576733,0.0,0.0005576732801273465
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,면접 시작 인사,-0.012673467,0.0,0.01267346739768982
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,상세 경험,0.012591291,0.0,0.012591291218996048
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,수식,-0.015682917,0.0,0.015682917088270187
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,용어 질문,-0.016583143,0.0,0.016583142802119255
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,인공지능,0.0041688946,0.0,0.004168894607573748
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,잠시 휴식,-0.053393383,0.0,0.05339338257908821
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,좋아하는 아이돌,-0.006077206,0.0,0.0060772062279284
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,핵심 아이디어,0.035110563,0.0,0.03511056303977966
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,확률 예측에서 MSE Loss 미 사용 이유,0.005129275,0.0,0.005129274912178516
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,BCE 가 좋은 task,0.008710974,0.0,0.008710973896086216
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,BCE 가 좋은 이유,-0.01072107,0.0,0.010721069760620594
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,LLM Fine-Tuning 의 PEFT,0.006602686,0.0,0.006602685898542404
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,LoRA,0.015698325,0.0,0.015698324888944626
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,LoRA 와 QLoRA 의 차이,0.014454934,0.0,0.014454933814704418
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,Loss Function 예시,-0.0029875431,0.0,0.0029875431209802628
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,Loss Function 정의,-0.0068011614,0.0,0.006801161449402571
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,MBTI,0.008771277,0.0,0.008771277032792568
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,MSE Loss 설명,-0.01897621,0.0,0.018976209685206413
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,MSE Loss 용도,1.6648652e-05,0.0,1.6648651580908336e-05
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,Multi-Label 에서 CE + Softmax 적용 문제점,0.0030334056,0.0,0.0030334056355059147
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,PEFT 방법 5가지,-0.0043458575,0.0,0.004345857538282871
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,거대 언어 모델 정의,0.016301908,0.0,0.01630190759897232
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,기본 경험,-0.012515772,0.0,0.012515772134065628
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,답변 실패,-0.012831677,0.0,0.012831676751375198
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,딥러닝,-0.004663877,0.0,0.004663877189159393
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,마지막 할 말,-0.0077834222,0.0,0.0077834222465753555
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,머신러닝,0.008207594,0.0,0.00820759404450655
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,면접 시작 인사,-6.2426598e-06,0.0,6.242659765121061e-06
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,상세 경험,-0.0040366924,0.0,0.004036692436784506
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,수식,-0.008962958,0.0,0.008962958119809628
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,용어 질문,-0.005659555,0.0,0.005659555085003376
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,인공지능,0.003080667,0.0,0.003080666996538639
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,잠시 휴식,0.0034048224,0.0,0.0034048224333673716
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,좋아하는 아이돌,0.9777335,1.0,0.022266507148742676
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,핵심 아이디어,0.0036325045,0.0,0.003632504492998123
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,확률 예측에서 MSE Loss 미 사용 이유,-0.011630275,0.0,0.011630275286734104
잠시 휴식 -> 재미있는 이야기 해줄래?,BCE 가 좋은 task,-0.005372623,0.0,0.005372622981667519
잠시 휴식 -> 재미있는 이야기 해줄래?,BCE 가 좋은 이유,-0.0110767055,0.0,0.011076705530285835
잠시 휴식 -> 재미있는 이야기 해줄래?,LLM Fine-Tuning 의 PEFT,-0.0036890365,0.0,0.0036890364717692137
잠시 휴식 -> 재미있는 이야기 해줄래?,LoRA,0.014530216,0.0,0.014530216343700886
잠시 휴식 -> 재미있는 이야기 해줄래?,LoRA 와 QLoRA 의 차이,-0.00834016,0.0,0.008340160362422466
잠시 휴식 -> 재미있는 이야기 해줄래?,Loss Function 예시,-0.015826588,0.0,0.01582658849656582
잠시 휴식 -> 재미있는 이야기 해줄래?,Loss Function 정의,-0.013181155,0.0,0.013181154616177082
잠시 휴식 -> 재미있는 이야기 해줄래?,MBTI,-0.02834592,0.0,0.028345920145511627
잠시 휴식 -> 재미있는 이야기 해줄래?,MSE Loss 설명,0.00039076657,0.0,0.00039076656685210764
잠시 휴식 -> 재미있는 이야기 해줄래?,MSE Loss 용도,-0.0028814368,0.0,0.0028814368415623903
잠시 휴식 -> 재미있는 이야기 해줄래?,Multi-Label 에서 CE + Softmax 적용 문제점,0.006648855,0.0,0.006648854818195105
잠시 휴식 -> 재미있는 이야기 해줄래?,PEFT 방법 5가지,-0.012268356,0.0,0.012268356047570705
잠시 휴식 -> 재미있는 이야기 해줄래?,거대 언어 모델 정의,-0.003059013,0.0,0.0030590130481868982
잠시 휴식 -> 재미있는 이야기 해줄래?,기본 경험,-0.025307205,0.0,0.025307204574346542
잠시 휴식 -> 재미있는 이야기 해줄래?,답변 실패,-0.011923804,0.0,0.011923803947865963
잠시 휴식 -> 재미있는 이야기 해줄래?,딥러닝,-0.010102928,0.0,0.010102927684783936
잠시 휴식 -> 재미있는 이야기 해줄래?,마지막 할 말,0.010267771,0.0,0.0102677708491683
잠시 휴식 -> 재미있는 이야기 해줄래?,머신러닝,-0.020593777,0.0,0.020593777298927307
잠시 휴식 -> 재미있는 이야기 해줄래?,면접 시작 인사,-0.0068144784,0.0,0.0068144784308969975
잠시 휴식 -> 재미있는 이야기 해줄래?,상세 경험,-0.0036998517,0.0,0.0036998516879975796
잠시 휴식 -> 재미있는 이야기 해줄래?,수식,-0.023069346,0.0,0.023069346323609352
잠시 휴식 -> 재미있는 이야기 해줄래?,용어 질문,0.018102352,0.0,0.01810235157608986
잠시 휴식 -> 재미있는 이야기 해줄래?,인공지능,0.004595887,0.0,0.004595886915922165
잠시 휴식 -> 재미있는 이야기 해줄래?,잠시 휴식,0.9831082,1.0,0.016891777515411377
잠시 휴식 -> 재미있는 이야기 해줄래?,좋아하는 아이돌,-0.014984048,0.0,0.01498404797166586
잠시 휴식 -> 재미있는 이야기 해줄래?,핵심 아이디어,-0.018620785,0.0,0.01862078532576561
잠시 휴식 -> 재미있는 이야기 해줄래?,확률 예측에서 MSE Loss 미 사용 이유,-0.0071734735,0.0,0.007173473481088877
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",BCE 가 좋은 task,-0.042644806,0.0,0.04264480620622635
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",BCE 가 좋은 이유,0.0044323117,0.0,0.004432311747223139
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",LLM Fine-Tuning 의 PEFT,0.92510486,1.0,0.07489514350891113
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",LoRA,0.012550338,0.0,0.012550338171422482
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",LoRA 와 QLoRA 의 차이,0.018021207,0.0,0.01802120730280876
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",Loss Function 예시,-0.06420815,0.0,0.06420814990997314
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",Loss Function 정의,-0.033719048,0.0,0.03371904790401459
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",MBTI,-0.02975878,0.0,0.02975877933204174
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",MSE Loss 설명,-0.025045583,0.0,0.02504558302462101
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",MSE Loss 용도,-0.0422982,0.0,0.042298201471567154
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",Multi-Label 에서 CE + Softmax 적용 문제점,0.038234003,0.0,0.03823400288820267
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",PEFT 방법 5가지,0.049019307,0.0,0.049019306898117065
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",거대 언어 모델 정의,-0.025042834,0.0,0.025042833760380745
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",기본 경험,-0.025874589,0.0,0.025874588638544083
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",답변 실패,-0.004969999,0.0,0.004969999194145203
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",딥러닝,-0.039808452,0.0,0.039808452129364014
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",마지막 할 말,0.01337508,0.0,0.013375080190598965
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",머신러닝,-0.049942095,0.0,0.04994209483265877
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",면접 시작 인사,-0.0066615185,0.0,0.006661518476903439
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",상세 경험,-0.015141965,0.0,0.015141964890062809
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",수식,0.012292801,0.0,0.012292801402509212
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",용어 질문,0.0111134965,0.0,0.011113496497273445
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",인공지능,-0.014604739,0.0,0.014604738913476467
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",잠시 휴식,0.008393699,0.0,0.008393699303269386
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",좋아하는 아이돌,-0.014931792,0.0,0.01493179239332676
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",핵심 아이디어,0.00361108,0.0,0.003611080115661025
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",확률 예측에서 MSE Loss 미 사용 이유,-0.019914258,0.0,0.019914258271455765
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,BCE 가 좋은 task,0.0030997233,0.0,0.0030997232533991337
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,BCE 가 좋은 이유,-0.0010119808,0.0,0.0010119808139279485
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,LLM Fine-Tuning 의 PEFT,0.00919706,0.0,0.009197060018777847
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,LoRA,-0.007277799,0.0,0.007277798838913441
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,LoRA 와 QLoRA 의 차이,-0.011076832,0.0,0.011076832190155983
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,Loss Function 예시,-0.022376595,0.0,0.022376595064997673
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,Loss Function 정의,0.0035600686,0.0,0.003560068551450968
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,MBTI,-0.005150388,0.0,0.005150387994945049
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,MSE Loss 설명,-0.002281288,0.0,0.002281287917867303
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,MSE Loss 용도,0.0069477204,0.0,0.006947720423340797
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,Multi-Label 에서 CE + Softmax 적용 문제점,0.0020841483,0.0,0.0020841483492404222
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,PEFT 방법 5가지,-0.006369939,0.0,0.00636993907392025
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,거대 언어 모델 정의,0.009359114,0.0,0.009359113872051239
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,기본 경험,-0.015818667,0.0,0.01581866666674614
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,답변 실패,0.97340256,1.0,0.026597440242767334
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,딥러닝,-0.0028814077,0.0,0.0028814077377319336
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,마지막 할 말,-0.004105684,0.0,0.004105683881789446
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,머신러닝,-0.010194169,0.0,0.010194169357419014
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,면접 시작 인사,-0.002744326,0.0,0.002744325902312994
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,상세 경험,-0.011208092,0.0,0.011208091862499714
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,수식,-0.02044437,0.0,0.020444370806217194
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,용어 질문,0.016444229,0.0,0.01644422858953476
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,인공지능,-0.01942408,0.0,0.019424080848693848
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,잠시 휴식,-0.0056853294,0.0,0.005685329437255859
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,좋아하는 아이돌,-0.0037098713,0.0,0.00370987132191658
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,핵심 아이디어,-0.017202217,0.0,0.017202217131853104
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,확률 예측에서 MSE Loss 미 사용 이유,-0.0024470375,0.0,0.0024470374919474125
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야!,BCE 가 좋은 task,-0.013228184,0.0,0.01322818361222744
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야!,BCE 가 좋은 이유,-0.0019349776,0.0,0.0019349775975570083
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야!,LLM Fine-Tuning 의 PEFT,0.955648,1.0,0.04435199499130249
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야!,LoRA,0.0055090357,0.0,0.005509035661816597
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야!,LoRA 와 QLoRA 의 차이,0.0007516955,0.0,0.0007516954792663455
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야!,Loss Function 예시,-0.021827312,0.0,0.02182731218636036
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야!,Loss Function 정의,-0.062612556,0.0,0.06261255592107773
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야!,MBTI,-0.02367402,0.0,0.023674020543694496
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야!,MSE Loss 설명,-0.039611127,0.0,0.039611127227544785
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야!,MSE Loss 용도,-0.033850946,0.0,0.03385094553232193
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야!,Multi-Label 에서 CE + Softmax 적용 문제점,0.013226907,0.0,0.013226906768977642
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야!,PEFT 방법 5가지,0.00034115798,0.0,0.0003411579818930477
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야!,거대 언어 모델 정의,-0.004693254,0.0,0.004693253897130489
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야!,기본 경험,0.0021409905,0.0,0.0021409904584288597
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야!,답변 실패,-0.0017945295,0.0,0.001794529496692121
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야!,딥러닝,-0.034622617,0.0,0.034622617065906525
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야!,마지막 할 말,0.0066456273,0.0,0.006645627319812775
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야!,머신러닝,-0.0005558731,0.0,0.0005558730917982757
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야!,면접 시작 인사,0.004867013,0.0,0.004867013078182936
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야!,상세 경험,0.014796227,0.0,0.014796227216720581
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야!,수식,0.029843247,0.0,0.029843246564269066
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야!,용어 질문,-0.0073317913,0.0,0.0073317913338541985
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야!,인공지능,0.006269558,0.0,0.0062695578671991825
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야!,잠시 휴식,-0.013041003,0.0,0.013041002675890923
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야!,좋아하는 아이돌,0.008201474,0.0,0.008201474323868752
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야!,핵심 아이디어,-0.007883742,0.0,0.007883742451667786
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야!,확률 예측에서 MSE Loss 미 사용 이유,-0.01807031,0.0,0.018070310354232788
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다,BCE 가 좋은 task,-0.0009793584,0.0,0.0009793584467843175
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다,BCE 가 좋은 이유,-0.0028266932,0.0,0.002826693234965205
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다,LLM Fine-Tuning 의 PEFT,0.0076395385,0.0,0.007639538496732712
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다,LoRA,-0.0007988707,0.0,0.000798870692960918
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다,LoRA 와 QLoRA 의 차이,-0.004115641,0.0,0.004115641117095947
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다,Loss Function 예시,-0.008475023,0.0,0.008475023321807384
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다,Loss Function 정의,0.0057450803,0.0,0.005745080299675465
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다,MBTI,-0.01077559,0.0,0.010775590315461159
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다,MSE Loss 설명,-0.008863291,0.0,0.008863290771842003
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다,MSE Loss 용도,-0.006209948,0.0,0.0062099481001496315
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다,Multi-Label 에서 CE + Softmax 적용 문제점,-0.0043667112,0.0,0.004366711247712374
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다,PEFT 방법 5가지,-0.019961974,0.0,0.019961973652243614
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다,거대 언어 모델 정의,0.008736524,0.0,0.008736523799598217
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다,기본 경험,0.00013169527,0.0,0.00013169526937417686
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다,답변 실패,0.98118323,1.0,0.018816769123077393
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다,딥러닝,-0.0037450693,0.0,0.0037450692616403103
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다,마지막 할 말,-0.0066394033,0.0,0.006639403291046619
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다,머신러닝,-0.011715727,0.0,0.011715726926922798
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다,면접 시작 인사,0.0010079723,0.0,0.0010079722851514816
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다,상세 경험,-0.004277284,0.0,0.004277283791452646
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다,수식,-0.023007045,0.0,0.023007044568657875
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다,용어 질문,-0.007699682,0.0,0.0076996819116175175
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다,인공지능,-0.013289853,0.0,0.013289852999150753
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다,잠시 휴식,-0.006645028,0.0,0.00664502801373601
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다,좋아하는 아이돌,0.0032555247,0.0,0.0032555246725678444
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다,핵심 아이디어,-0.01426033,0.0,0.014260330237448215
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다,확률 예측에서 MSE Loss 미 사용 이유,-0.00027559823,0.0,0.00027559822774492204
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",BCE 가 좋은 task,-0.013253912,0.0,0.013253912329673767
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",BCE 가 좋은 이유,-0.000530084,0.0,0.0005300840130075812
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",LLM Fine-Tuning 의 PEFT,-0.014560966,0.0,0.014560965821146965
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",LoRA,0.00870228,0.0,0.00870227999985218
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",LoRA 와 QLoRA 의 차이,-0.00066532777,0.0,0.0006653277669101954
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",Loss Function 예시,0.0040154974,0.0,0.004015497397631407
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",Loss Function 정의,-0.014874588,0.0,0.014874587766826153
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",MBTI,-0.005216667,0.0,0.005216666962951422
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",MSE Loss 설명,-0.00039262112,0.0,0.00039262112113647163
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",MSE Loss 용도,0.022161514,0.0,0.022161513566970825
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",Multi-Label 에서 CE + Softmax 적용 문제점,-0.01676593,0.0,0.016765929758548737
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",PEFT 방법 5가지,0.9549038,1.0,0.04509621858596802
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",거대 언어 모델 정의,-0.0037480716,0.0,0.003748071612790227
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",기본 경험,-0.040168148,0.0,0.040168147534132004
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",답변 실패,-0.023234796,0.0,0.023234795778989792
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",딥러닝,-0.028516654,0.0,0.028516653925180435
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",마지막 할 말,0.008873664,0.0,0.00887366384267807
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",머신러닝,4.683167e-05,0.0,4.683166844188236e-05
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",면접 시작 인사,-0.013955103,0.0,0.013955103233456612
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",상세 경험,-0.014711237,0.0,0.014711236581206322
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",수식,0.001546392,0.0,0.0015463919844478369
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",용어 질문,-0.010216441,0.0,0.010216441005468369
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",인공지능,-0.009231694,0.0,0.009231694042682648
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",잠시 휴식,-0.00038663353,0.0,0.0003866335318889469
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",좋아하는 아이돌,0.0058979695,0.0,0.005897969473153353
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",핵심 아이디어,-0.005606084,0.0,0.005606084130704403
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",확률 예측에서 MSE Loss 미 사용 이유,0.011769199,0.0,0.011769198812544346
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,BCE 가 좋은 task,-0.0044377297,0.0,0.004437729716300964
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,BCE 가 좋은 이유,0.00859101,0.0,0.008591010235249996
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,LLM Fine-Tuning 의 PEFT,-0.004834229,0.0,0.004834229126572609
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,LoRA,0.016883602,0.0,0.016883602365851402
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,LoRA 와 QLoRA 의 차이,-0.0026254223,0.0,0.0026254223193973303
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,Loss Function 예시,-0.0016102616,0.0,0.001610261620953679
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,Loss Function 정의,0.0017661835,0.0,0.0017661835299804807
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,MBTI,0.017636828,0.0,0.017636828124523163
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,MSE Loss 설명,-0.010499907,0.0,0.010499906726181507
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,MSE Loss 용도,-0.0076732673,0.0,0.007673267275094986
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,Multi-Label 에서 CE + Softmax 적용 문제점,0.0038322092,0.0,0.0038322091568261385
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,PEFT 방법 5가지,0.04096567,0.0,0.040965668857097626
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,거대 언어 모델 정의,-0.011068552,0.0,0.011068551801145077
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,기본 경험,0.012785745,0.0,0.012785744853317738
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,답변 실패,0.96996677,1.0,0.030033230781555176
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,딥러닝,0.007795981,0.0,0.007795981131494045
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,마지막 할 말,-0.023704514,0.0,0.023704513907432556
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,머신러닝,-0.019005,0.0,0.019005000591278076
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,면접 시작 인사,-0.005858617,0.0,0.005858616903424263
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,상세 경험,-0.011372733,0.0,0.011372732929885387
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,수식,-0.011973852,0.0,0.011973852291703224
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,용어 질문,-0.022089295,0.0,0.022089295089244843
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,인공지능,-0.017364528,0.0,0.01736452803015709
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,잠시 휴식,-0.009083586,0.0,0.009083585813641548
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,좋아하는 아이돌,0.023020457,0.0,0.023020457476377487
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,핵심 아이디어,-0.017400986,0.0,0.017400985583662987
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,확률 예측에서 MSE Loss 미 사용 이유,-0.011673971,0.0,0.011673971079289913
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,BCE 가 좋은 task,-0.019819152,0.0,0.019819151610136032
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,BCE 가 좋은 이유,0.0090368735,0.0,0.00903687346726656
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,LLM Fine-Tuning 의 PEFT,0.0093440255,0.0,0.009344025515019894
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,LoRA,0.95038706,1.0,0.04961293935775757
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,LoRA 와 QLoRA 의 차이,0.009584908,0.0,0.009584908373653889
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,Loss Function 예시,0.0053101,0.0,0.00531010003760457
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,Loss Function 정의,0.004191576,0.0,0.004191576037555933
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,MBTI,-0.01974664,0.0,0.01974664069712162
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,MSE Loss 설명,-0.06898003,0.0,0.06898003071546555
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,MSE Loss 용도,-0.0074911355,0.0,0.0074911355040967464
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,Multi-Label 에서 CE + Softmax 적용 문제점,0.021178441,0.0,0.021178441122174263
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,PEFT 방법 5가지,0.012777814,0.0,0.012777813710272312
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,거대 언어 모델 정의,-0.02022869,0.0,0.020228689536452293
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,기본 경험,-0.0020561083,0.0,0.002056108321994543
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,답변 실패,-4.9395967e-05,0.0,4.9395966925658286e-05
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,딥러닝,-0.01606192,0.0,0.016061920672655106
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,마지막 할 말,-0.011319836,0.0,0.011319835670292377
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,머신러닝,-0.044060983,0.0,0.04406098276376724
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,면접 시작 인사,0.004304015,0.0,0.004304015077650547
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,상세 경험,-0.018488176,0.0,0.01848817616701126
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,수식,0.0050728354,0.0,0.0050728353671729565
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,용어 질문,0.0021067697,0.0,0.002106769708916545
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,인공지능,0.0027741666,0.0,0.002774166641756892
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,잠시 휴식,-0.0018113818,0.0,0.0018113817786797881
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,좋아하는 아이돌,-0.020476952,0.0,0.02047695219516754
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,핵심 아이디어,-0.008546396,0.0,0.008546396158635616
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,확률 예측에서 MSE Loss 미 사용 이유,-0.0057655647,0.0,0.005765564739704132
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,BCE 가 좋은 task,0.0038689107,0.0,0.0038689107168465853
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,BCE 가 좋은 이유,-0.0038796319,0.0,0.003879631869494915
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,LLM Fine-Tuning 의 PEFT,0.00241363,0.0,0.0024136300198733807
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,LoRA,0.027818818,0.0,0.027818817645311356
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,LoRA 와 QLoRA 의 차이,-0.005414852,0.0,0.005414851941168308
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,Loss Function 예시,-0.0018586736,0.0,0.0018586736405268312
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,Loss Function 정의,0.013664593,0.0,0.013664592988789082
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,MBTI,-0.009108237,0.0,0.009108236990869045
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,MSE Loss 설명,-0.011431449,0.0,0.011431449092924595
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,MSE Loss 용도,0.0054796543,0.0,0.005479654297232628
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,Multi-Label 에서 CE + Softmax 적용 문제점,-0.020650275,0.0,0.02065027505159378
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,PEFT 방법 5가지,-0.023327315,0.0,0.023327315226197243
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,거대 언어 모델 정의,0.00074134354,0.0,0.0007413435378111899
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,기본 경험,0.011538093,0.0,0.011538092978298664
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,답변 실패,0.95753866,1.0,0.0424613356590271
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,딥러닝,0.019170031,0.0,0.01917003095149994
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,마지막 할 말,-0.01983103,0.0,0.019831029698252678
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,머신러닝,-0.006350781,0.0,0.006350780837237835
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,면접 시작 인사,-0.00890706,0.0,0.008907060138881207
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,상세 경험,-0.016252285,0.0,0.01625228486955166
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,수식,-0.00061345036,0.0,0.0006134503637440503
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,용어 질문,-0.05412554,0.0,0.05412553995847702
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,인공지능,-0.017458767,0.0,0.01745876669883728
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,잠시 휴식,-0.0015686186,0.0,0.0015686185797676444
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,좋아하는 아이돌,0.033501696,0.0,0.03350169584155083
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,핵심 아이디어,0.04123309,0.0,0.041233088821172714
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,확률 예측에서 MSE Loss 미 사용 이유,-0.014631077,0.0,0.014631076715886593
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야!,BCE 가 좋은 task,-0.002151706,0.0,0.0021517060231417418
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야!,BCE 가 좋은 이유,-0.0034960874,0.0,0.0034960873890668154
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야!,LLM Fine-Tuning 의 PEFT,0.00039204434,0.0,0.0003920443414244801
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야!,LoRA,0.9602226,1.0,0.039777398109436035
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야!,LoRA 와 QLoRA 의 차이,-0.020544978,0.0,0.020544977858662605
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야!,Loss Function 예시,-0.009756212,0.0,0.009756212122738361
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야!,Loss Function 정의,0.0025057816,0.0,0.0025057815946638584
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야!,MBTI,-0.00075085263,0.0,0.0007508526323363185
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야!,MSE Loss 설명,-0.026406307,0.0,0.02640630677342415
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야!,MSE Loss 용도,0.016935907,0.0,0.016935907304286957
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야!,Multi-Label 에서 CE + Softmax 적용 문제점,0.023984393,0.0,0.02398439310491085
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야!,PEFT 방법 5가지,-0.0024649836,0.0,0.0024649836122989655
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야!,거대 언어 모델 정의,0.006990435,0.0,0.006990435067564249
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야!,기본 경험,-0.0034283157,0.0,0.003428315743803978
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야!,답변 실패,-0.0020784782,0.0,0.0020784782245755196
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야!,딥러닝,-0.008607356,0.0,0.008607355877757072
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야!,마지막 할 말,-0.00043538358,0.0,0.00043538358295336366
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야!,머신러닝,-0.017834438,0.0,0.017834438011050224
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야!,면접 시작 인사,0.0014302019,0.0,0.0014302019262686372
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야!,상세 경험,0.015115223,0.0,0.0151152228936553
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야!,수식,-0.017382827,0.0,0.017382826656103134
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야!,용어 질문,-0.020133736,0.0,0.020133735612034798
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야!,인공지능,0.019095967,0.0,0.01909596659243107
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야!,잠시 휴식,-0.004455013,0.0,0.004455013200640678
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야!,좋아하는 아이돌,-0.0045062583,0.0,0.0045062582939863205
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야!,핵심 아이디어,-0.012275463,0.0,0.012275462970137596
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야!,확률 예측에서 MSE Loss 미 사용 이유,-0.0046381247,0.0,0.004638124722987413
LoRA -> 무슨 OOM 없앤다는 것 같은데,BCE 가 좋은 task,0.007027454,0.0,0.00702745420858264
LoRA -> 무슨 OOM 없앤다는 것 같은데,BCE 가 좋은 이유,0.012651255,0.0,0.01265125535428524
LoRA -> 무슨 OOM 없앤다는 것 같은데,LLM Fine-Tuning 의 PEFT,0.0009128819,0.0,0.0009128819219768047
LoRA -> 무슨 OOM 없앤다는 것 같은데,LoRA,0.014486746,0.0,0.014486745931208134
LoRA -> 무슨 OOM 없앤다는 것 같은데,LoRA 와 QLoRA 의 차이,-0.0019790058,0.0,0.001979005755856633
LoRA -> 무슨 OOM 없앤다는 것 같은데,Loss Function 예시,-0.00825941,0.0,0.008259410038590431
LoRA -> 무슨 OOM 없앤다는 것 같은데,Loss Function 정의,-0.004762522,0.0,0.00476252194494009
LoRA -> 무슨 OOM 없앤다는 것 같은데,MBTI,0.013551929,0.0,0.013551929034292698
LoRA -> 무슨 OOM 없앤다는 것 같은데,MSE Loss 설명,-0.011959059,0.0,0.011959059163928032
LoRA -> 무슨 OOM 없앤다는 것 같은데,MSE Loss 용도,0.002537462,0.0,0.002537461929023266
LoRA -> 무슨 OOM 없앤다는 것 같은데,Multi-Label 에서 CE + Softmax 적용 문제점,-0.0019709733,0.0,0.0019709733314812183
LoRA -> 무슨 OOM 없앤다는 것 같은데,PEFT 방법 5가지,-0.0072077503,0.0,0.007207750342786312
LoRA -> 무슨 OOM 없앤다는 것 같은데,거대 언어 모델 정의,0.008121464,0.0,0.008121464401483536
LoRA -> 무슨 OOM 없앤다는 것 같은데,기본 경험,-0.013525142,0.0,0.013525142334401608
LoRA -> 무슨 OOM 없앤다는 것 같은데,답변 실패,0.9791579,1.0,0.02084207534790039
LoRA -> 무슨 OOM 없앤다는 것 같은데,딥러닝,0.0016850878,0.0,0.001685087801888585
LoRA -> 무슨 OOM 없앤다는 것 같은데,마지막 할 말,-0.011424478,0.0,0.011424478143453598
LoRA -> 무슨 OOM 없앤다는 것 같은데,머신러닝,-0.01883235,0.0,0.01883235014975071
LoRA -> 무슨 OOM 없앤다는 것 같은데,면접 시작 인사,0.003692315,0.0,0.003692314960062504
LoRA -> 무슨 OOM 없앤다는 것 같은데,상세 경험,0.0027274226,0.0,0.0027274226304143667
LoRA -> 무슨 OOM 없앤다는 것 같은데,수식,-0.0119692115,0.0,0.011969211511313915
LoRA -> 무슨 OOM 없앤다는 것 같은데,용어 질문,-0.025017856,0.0,0.025017855688929558
LoRA -> 무슨 OOM 없앤다는 것 같은데,인공지능,-0.019104274,0.0,0.01910427398979664
LoRA -> 무슨 OOM 없앤다는 것 같은데,잠시 휴식,-0.011271713,0.0,0.011271713301539421
LoRA -> 무슨 OOM 없앤다는 것 같은데,좋아하는 아이돌,0.0015942049,0.0,0.0015942049212753773
LoRA -> 무슨 OOM 없앤다는 것 같은데,핵심 아이디어,-0.018823331,0.0,0.018823331221938133
LoRA -> 무슨 OOM 없앤다는 것 같은데,확률 예측에서 MSE Loss 미 사용 이유,-0.0113718,0.0,0.011371799744665623
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,BCE 가 좋은 task,0.0023710434,0.0,0.0023710434325039387
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,BCE 가 좋은 이유,-0.010571251,0.0,0.010571250692009926
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,LLM Fine-Tuning 의 PEFT,-0.0017656161,0.0,0.0017656161217018962
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,LoRA,-0.015834121,0.0,0.01583412103354931
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,LoRA 와 QLoRA 의 차이,0.9633956,1.0,0.03660440444946289
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,Loss Function 예시,-0.013976147,0.0,0.013976147398352623
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,Loss Function 정의,0.0016685813,0.0,0.001668581273406744
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,MBTI,-0.002857393,0.0,0.0028573928866535425
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,MSE Loss 설명,-0.02155367,0.0,0.02155366912484169
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,MSE Loss 용도,-0.008760046,0.0,0.00876004621386528
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,Multi-Label 에서 CE + Softmax 적용 문제점,0.0011397591,0.0,0.001139759086072445
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,PEFT 방법 5가지,0.00082798727,0.0,0.0008279872708953917
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,거대 언어 모델 정의,0.009505319,0.0,0.0095053194090724
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,기본 경험,-0.018809069,0.0,0.018809068948030472
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,답변 실패,-0.018283421,0.0,0.01828342117369175
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,딥러닝,0.016301816,0.0,0.016301816329360008
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,마지막 할 말,0.00890682,0.0,0.008906819857656956
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,머신러닝,-0.032849982,0.0,0.032849982380867004
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,면접 시작 인사,-0.03109695,0.0,0.03109695017337799
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,상세 경험,0.00074358855,0.0,0.0007435885490849614
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,수식,-0.00092200085,0.0,0.0009220008505508304
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,용어 질문,0.0033354564,0.0,0.0033354563638567924
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,인공지능,-0.014671286,0.0,0.014671285636723042
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,잠시 휴식,-0.0050939047,0.0,0.0050939046777784824
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,좋아하는 아이돌,-0.011510048,0.0,0.011510048061609268
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,핵심 아이디어,-0.022361627,0.0,0.022361626848578453
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,확률 예측에서 MSE Loss 미 사용 이유,-0.022283338,0.0,0.022283338010311127
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,BCE 가 좋은 task,-0.005263893,0.0,0.005263892933726311
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,BCE 가 좋은 이유,-0.0016767256,0.0,0.0016767255729064345
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,LLM Fine-Tuning 의 PEFT,0.0066856327,0.0,0.00668563274666667
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,LoRA,-0.00047219495,0.0,0.0004721949517261237
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,LoRA 와 QLoRA 의 차이,0.10567192,0.0,0.10567191988229752
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,Loss Function 예시,0.016791264,0.0,0.016791263595223427
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,Loss Function 정의,-0.03208964,0.0,0.03208963945508003
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,MBTI,-0.0019912426,0.0,0.0019912426359951496
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,MSE Loss 설명,-0.02095565,0.0,0.020955650135874748
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,MSE Loss 용도,-0.0073219235,0.0,0.00732192350551486
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,Multi-Label 에서 CE + Softmax 적용 문제점,-0.027505293,0.0,0.027505293488502502
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,PEFT 방법 5가지,-0.022738611,0.0,0.022738611325621605
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,거대 언어 모델 정의,-0.0149246445,0.0,0.014924644492566586
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,기본 경험,0.046007987,0.0,0.04600798711180687
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,답변 실패,0.9298632,1.0,0.07013678550720215
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,딥러닝,0.009304487,0.0,0.009304487146437168
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,마지막 할 말,-0.017341407,0.0,0.017341407015919685
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,머신러닝,-0.013858376,0.0,0.013858376070857048
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,면접 시작 인사,-0.008976135,0.0,0.008976135402917862
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,상세 경험,0.019125871,0.0,0.01912587136030197
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,수식,-0.024316614,0.0,0.024316614493727684
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,용어 질문,-0.047520954,0.0,0.0475209541618824
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,인공지능,-0.019504506,0.0,0.019504506140947342
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,잠시 휴식,-0.015279207,0.0,0.01527920737862587
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,좋아하는 아이돌,0.030167477,0.0,0.0301674772053957
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,핵심 아이디어,0.0024729427,0.0,0.0024729426950216293
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,확률 예측에서 MSE Loss 미 사용 이유,-0.006794185,0.0,0.006794184911996126
마지막 할 말 -> 로라야 정말 고마워!,BCE 가 좋은 task,-0.010478598,0.0,0.010478598065674305
마지막 할 말 -> 로라야 정말 고마워!,BCE 가 좋은 이유,-0.0016867749,0.0,0.001686774892732501
마지막 할 말 -> 로라야 정말 고마워!,LLM Fine-Tuning 의 PEFT,-0.0070376955,0.0,0.007037695497274399
마지막 할 말 -> 로라야 정말 고마워!,LoRA,0.004095195,0.0,0.004095194861292839
마지막 할 말 -> 로라야 정말 고마워!,LoRA 와 QLoRA 의 차이,-0.010515636,0.0,0.010515635833144188
마지막 할 말 -> 로라야 정말 고마워!,Loss Function 예시,-0.002161134,0.0,0.002161134034395218
마지막 할 말 -> 로라야 정말 고마워!,Loss Function 정의,0.013163547,0.0,0.013163547031581402
마지막 할 말 -> 로라야 정말 고마워!,MBTI,0.00017333914,0.0,0.00017333914001937956
마지막 할 말 -> 로라야 정말 고마워!,MSE Loss 설명,-0.019268602,0.0,0.01926860213279724
마지막 할 말 -> 로라야 정말 고마워!,MSE Loss 용도,-0.0024900343,0.0,0.0024900343269109726
마지막 할 말 -> 로라야 정말 고마워!,Multi-Label 에서 CE + Softmax 적용 문제점,-0.004857472,0.0,0.0048574721440672874
마지막 할 말 -> 로라야 정말 고마워!,PEFT 방법 5가지,0.0029737581,0.0,0.0029737581498920918
마지막 할 말 -> 로라야 정말 고마워!,거대 언어 모델 정의,-0.010270994,0.0,0.010270994156599045
마지막 할 말 -> 로라야 정말 고마워!,기본 경험,0.013741561,0.0,0.013741561211645603
마지막 할 말 -> 로라야 정말 고마워!,답변 실패,-0.009987784,0.0,0.00998778361827135
마지막 할 말 -> 로라야 정말 고마워!,딥러닝,0.0013537288,0.0,0.0013537288177758455
마지막 할 말 -> 로라야 정말 고마워!,마지막 할 말,0.9860216,1.0,0.013978421688079834
마지막 할 말 -> 로라야 정말 고마워!,머신러닝,0.012860608,0.0,0.012860608287155628
마지막 할 말 -> 로라야 정말 고마워!,면접 시작 인사,-0.007155548,0.0,0.007155547849833965
마지막 할 말 -> 로라야 정말 고마워!,상세 경험,0.0019522319,0.0,0.0019522318616509438
마지막 할 말 -> 로라야 정말 고마워!,수식,-0.007992861,0.0,0.007992860861122608
마지막 할 말 -> 로라야 정말 고마워!,용어 질문,0.0015883514,0.0,0.0015883514424785972
마지막 할 말 -> 로라야 정말 고마워!,인공지능,0.010077774,0.0,0.010077773593366146
마지막 할 말 -> 로라야 정말 고마워!,잠시 휴식,-0.016854232,0.0,0.01685423217713833
마지막 할 말 -> 로라야 정말 고마워!,좋아하는 아이돌,-0.0060598743,0.0,0.006059874314814806
마지막 할 말 -> 로라야 정말 고마워!,핵심 아이디어,0.0059536365,0.0,0.005953636486083269
마지막 할 말 -> 로라야 정말 고마워!,확률 예측에서 MSE Loss 미 사용 이유,-0.0052592717,0.0,0.005259271711111069
마지막 할 말 -> 로라야 사랑해,BCE 가 좋은 task,-0.015632981,0.0,0.015632981434464455
마지막 할 말 -> 로라야 사랑해,BCE 가 좋은 이유,-0.0042870943,0.0,0.004287094343453646
마지막 할 말 -> 로라야 사랑해,LLM Fine-Tuning 의 PEFT,-0.008389843,0.0,0.008389842696487904
마지막 할 말 -> 로라야 사랑해,LoRA,0.00612905,0.0,0.00612905016168952
마지막 할 말 -> 로라야 사랑해,LoRA 와 QLoRA 의 차이,-0.0057098377,0.0,0.005709837656468153
마지막 할 말 -> 로라야 사랑해,Loss Function 예시,-0.0048348387,0.0,0.004834838677197695
마지막 할 말 -> 로라야 사랑해,Loss Function 정의,0.0105149,0.0,0.010514900088310242
마지막 할 말 -> 로라야 사랑해,MBTI,0.0033379188,0.0,0.0033379187807440758
마지막 할 말 -> 로라야 사랑해,MSE Loss 설명,-0.02015461,0.0,0.02015461027622223
마지막 할 말 -> 로라야 사랑해,MSE Loss 용도,-0.0029295187,0.0,0.002929518697783351
마지막 할 말 -> 로라야 사랑해,Multi-Label 에서 CE + Softmax 적용 문제점,-0.0091736615,0.0,0.009173661470413208
마지막 할 말 -> 로라야 사랑해,PEFT 방법 5가지,-0.0039013557,0.0,0.003901355667039752
마지막 할 말 -> 로라야 사랑해,거대 언어 모델 정의,-0.009561492,0.0,0.009561492130160332
마지막 할 말 -> 로라야 사랑해,기본 경험,0.01517584,0.0,0.015175839886069298
마지막 할 말 -> 로라야 사랑해,답변 실패,-0.0130325975,0.0,0.013032597489655018
마지막 할 말 -> 로라야 사랑해,딥러닝,0.00036002716,0.0,0.0003600271593313664
마지막 할 말 -> 로라야 사랑해,마지막 할 말,0.9872053,1.0,0.012794673442840576
마지막 할 말 -> 로라야 사랑해,머신러닝,0.0140500255,0.0,0.01405002549290657
마지막 할 말 -> 로라야 사랑해,면접 시작 인사,-0.011141138,0.0,0.011141138151288033
마지막 할 말 -> 로라야 사랑해,상세 경험,0.0025046486,0.0,0.0025046486407518387
마지막 할 말 -> 로라야 사랑해,수식,-0.019766904,0.0,0.019766904413700104
마지막 할 말 -> 로라야 사랑해,용어 질문,0.010463152,0.0,0.010463152080774307
마지막 할 말 -> 로라야 사랑해,인공지능,0.010743055,0.0,0.010743054561316967
마지막 할 말 -> 로라야 사랑해,잠시 휴식,-0.003904852,0.0,0.003904852084815502
마지막 할 말 -> 로라야 사랑해,좋아하는 아이돌,0.0034023474,0.0,0.003402347443625331
마지막 할 말 -> 로라야 사랑해,핵심 아이디어,0.009544373,0.0,0.00954437255859375
마지막 할 말 -> 로라야 사랑해,확률 예측에서 MSE Loss 미 사용 이유,-0.004419339,0.0,0.0044193388894200325
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,BCE 가 좋은 task,-0.010809123,0.0,0.010809122584760189
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,BCE 가 좋은 이유,-0.0056649754,0.0,0.005664975382387638
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,LLM Fine-Tuning 의 PEFT,-0.005669035,0.0,0.005669035017490387
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,LoRA,0.0071515604,0.0,0.007151560392230749
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,LoRA 와 QLoRA 의 차이,-0.0066547818,0.0,0.0066547817550599575
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,Loss Function 예시,0.0025634796,0.0,0.0025634795892983675
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,Loss Function 정의,0.0075239483,0.0,0.007523948326706886
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,MBTI,0.00647943,0.0,0.006479430012404919
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,MSE Loss 설명,-0.021638853,0.0,0.02163885347545147
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,MSE Loss 용도,-0.005931889,0.0,0.005931889172643423
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,Multi-Label 에서 CE + Softmax 적용 문제점,-0.013650192,0.0,0.013650191947817802
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,PEFT 방법 5가지,-0.0071300725,0.0,0.0071300724521279335
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,거대 언어 모델 정의,-0.012006233,0.0,0.01200623344630003
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,기본 경험,0.011039834,0.0,0.011039833538234234
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,답변 실패,-0.0062771263,0.0,0.006277126260101795
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,딥러닝,0.003118494,0.0,0.0031184940598905087
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,마지막 할 말,0.98283833,1.0,0.017161667346954346
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,머신러닝,0.015139894,0.0,0.015139893628656864
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,면접 시작 인사,-0.019925194,0.0,0.0199251938611269
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,상세 경험,7.186518e-05,0.0,7.186517905211076e-05
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,수식,-0.019546082,0.0,0.019546082243323326
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,용어 질문,-0.0021048412,0.0,0.00210484117269516
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,인공지능,0.0074404404,0.0,0.007440440356731415
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,잠시 휴식,0.0027523784,0.0,0.002752378350123763
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,좋아하는 아이돌,-0.0012368165,0.0,0.0012368165189400315
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,핵심 아이디어,0.011394453,0.0,0.01139445323497057
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,확률 예측에서 MSE Loss 미 사용 이유,-0.006173209,0.0,0.006173208821564913
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워,BCE 가 좋은 task,-0.004150778,0.0,0.00415077805519104
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워,BCE 가 좋은 이유,0.00082418416,0.0,0.0008241841569542885
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워,LLM Fine-Tuning 의 PEFT,-0.0036289655,0.0,0.0036289654672145844
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워,LoRA,-0.0003294059,0.0,0.00032940591336227953
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워,LoRA 와 QLoRA 의 차이,-0.013459203,0.0,0.013459202833473682
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워,Loss Function 예시,-0.009369821,0.0,0.009369821287691593
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워,Loss Function 정의,0.0045659975,0.0,0.004565997514873743
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워,MBTI,-0.014532561,0.0,0.014532561413943768
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워,MSE Loss 설명,-0.0053761667,0.0,0.0053761666640639305
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워,MSE Loss 용도,-0.0155656235,0.0,0.01556562352925539
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워,Multi-Label 에서 CE + Softmax 적용 문제점,-0.0018686302,0.0,0.0018686301773414016
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워,PEFT 방법 5가지,-0.00926768,0.0,0.00926768034696579
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워,거대 언어 모델 정의,-0.0041716285,0.0,0.004171628504991531
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워,기본 경험,0.0125415465,0.0,0.012541546486318111
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워,답변 실패,-0.008886673,0.0,0.008886672556400299
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워,딥러닝,-0.009861894,0.0,0.009861893951892853
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워,마지막 할 말,0.9832396,1.0,0.016760408878326416
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워,머신러닝,0.011002152,0.0,0.011002152226865292
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워,면접 시작 인사,0.0056479652,0.0,0.005647965241223574
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워,상세 경험,0.004990595,0.0,0.004990594927221537
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워,수식,-0.013201151,0.0,0.013201151043176651
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워,용어 질문,-0.0007975477,0.0,0.0007975476910360157
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워,인공지능,0.0076811886,0.0,0.007681188639253378
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워,잠시 휴식,-0.0025230644,0.0,0.0025230643805116415
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워,좋아하는 아이돌,-0.0029915099,0.0,0.0029915098566561937
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워,핵심 아이디어,0.0056202933,0.0,0.005620293319225311
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워,확률 예측에서 MSE Loss 미 사용 이유,0.0017121914,0.0,0.0017121913842856884
