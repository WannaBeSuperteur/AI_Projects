input_part,output_answer,predicted_score,ground_truth_score,absolute_error
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,BCE 가 좋은 task,-0.00057738245,0.0,0.0005773824523203075
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,"BCE 가 좋은 task, 이유",-0.0056186114,0.0,0.005618611350655556
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,LLM Fine-Tuning 의 PEFT,-0.0100472355,0.0,0.010047235526144505
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,LoRA,-0.012901009,0.0,0.012901009060442448
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,LoRA 와 QLoRA 의 차이,0.020022621,0.0,0.020022621378302574
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,Loss Function 예시,0.003242877,0.0,0.0032428770791739225
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,Loss Function 정의,-0.0124352425,0.0,0.012435242533683777
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,MBTI,-0.023848053,0.0,0.023848053067922592
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,MSE Loss 설명,0.012327443,0.0,0.01232744287699461
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,MSE Loss 용도,0.00896731,0.0,0.008967310190200806
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,Multi-Label 에서 CE + Softmax 적용 문제점,0.019748835,0.0,0.019748834893107414
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,PEFT 방법 5가지,0.013537107,0.0,0.013537107035517693
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,거대 언어 모델 정의,-0.001242035,0.0,0.0012420349521562457
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,기본 경험,-0.036185503,0.0,0.036185503005981445
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,답변 실패,-0.02535329,0.0,0.025353290140628815
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,딥러닝,-0.036838315,0.0,0.036838315427303314
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,마지막 할 말,-0.054836243,0.0,0.05483624339103699
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,머신러닝,-0.053641967,0.0,0.053641967475414276
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,면접 시작 인사,0.9414811,1.0,0.05851888656616211
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,상세 경험,-0.058084317,0.0,0.05808431655168533
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,수식,-0.016238252,0.0,0.016238251700997353
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,인공지능,-0.027961783,0.0,0.02796178311109543
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,잠시 휴식,0.0046794764,0.0,0.004679476376622915
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,좋아하는 아이돌,0.0049434174,0.0,0.004943417385220528
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,핵심 아이디어,-0.03736291,0.0,0.03736291080713272
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,확률 예측에서 MSE Loss 미 사용 이유,0.02158473,0.0,0.021584730595350266
면접 시작 인사 -> 로라야 안녕 정말 반가워,BCE 가 좋은 task,-0.015248563,0.0,0.015248563140630722
면접 시작 인사 -> 로라야 안녕 정말 반가워,"BCE 가 좋은 task, 이유",-0.017658982,0.0,0.017658982425928116
면접 시작 인사 -> 로라야 안녕 정말 반가워,LLM Fine-Tuning 의 PEFT,-0.004257813,0.0,0.004257813096046448
면접 시작 인사 -> 로라야 안녕 정말 반가워,LoRA,0.00034499372,0.0,0.0003449937212280929
면접 시작 인사 -> 로라야 안녕 정말 반가워,LoRA 와 QLoRA 의 차이,0.025682217,0.0,0.02568221651017666
면접 시작 인사 -> 로라야 안녕 정말 반가워,Loss Function 예시,-0.008015915,0.0,0.00801591482013464
면접 시작 인사 -> 로라야 안녕 정말 반가워,Loss Function 정의,-0.012807363,0.0,0.012807362712919712
면접 시작 인사 -> 로라야 안녕 정말 반가워,MBTI,-0.014653781,0.0,0.014653781428933144
면접 시작 인사 -> 로라야 안녕 정말 반가워,MSE Loss 설명,0.016017167,0.0,0.016017166897654533
면접 시작 인사 -> 로라야 안녕 정말 반가워,MSE Loss 용도,0.0155256605,0.0,0.01552566047757864
면접 시작 인사 -> 로라야 안녕 정말 반가워,Multi-Label 에서 CE + Softmax 적용 문제점,0.013632775,0.0,0.013632775284349918
면접 시작 인사 -> 로라야 안녕 정말 반가워,PEFT 방법 5가지,-0.007225037,0.0,0.007225037086755037
면접 시작 인사 -> 로라야 안녕 정말 반가워,거대 언어 모델 정의,0.003188649,0.0,0.003188648959621787
면접 시작 인사 -> 로라야 안녕 정말 반가워,기본 경험,-0.015459446,0.0,0.015459446236491203
면접 시작 인사 -> 로라야 안녕 정말 반가워,답변 실패,-0.012276489,0.0,0.012276489287614822
면접 시작 인사 -> 로라야 안녕 정말 반가워,딥러닝,-0.03563386,0.0,0.03563385829329491
면접 시작 인사 -> 로라야 안녕 정말 반가워,마지막 할 말,-0.009754466,0.0,0.009754465892910957
면접 시작 인사 -> 로라야 안녕 정말 반가워,머신러닝,-0.034152042,0.0,0.034152042120695114
면접 시작 인사 -> 로라야 안녕 정말 반가워,면접 시작 인사,0.9402969,1.0,0.05970311164855957
면접 시작 인사 -> 로라야 안녕 정말 반가워,상세 경험,-0.036708973,0.0,0.03670897334814072
면접 시작 인사 -> 로라야 안녕 정말 반가워,수식,-0.027619025,0.0,0.027619024738669395
면접 시작 인사 -> 로라야 안녕 정말 반가워,인공지능,-0.026010294,0.0,0.026010293513536453
면접 시작 인사 -> 로라야 안녕 정말 반가워,잠시 휴식,-0.024647687,0.0,0.024647686630487442
면접 시작 인사 -> 로라야 안녕 정말 반가워,좋아하는 아이돌,0.002146564,0.0,0.002146563958376646
면접 시작 인사 -> 로라야 안녕 정말 반가워,핵심 아이디어,-0.04873129,0.0,0.04873128980398178
면접 시작 인사 -> 로라야 안녕 정말 반가워,확률 예측에서 MSE Loss 미 사용 이유,0.027361855,0.0,0.027361854910850525
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,BCE 가 좋은 task,-0.007640459,0.0,0.007640459109097719
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,"BCE 가 좋은 task, 이유",-0.010766464,0.0,0.010766464285552502
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,LLM Fine-Tuning 의 PEFT,-0.025748378,0.0,0.025748377665877342
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,LoRA,-0.0031090735,0.0,0.0031090734992176294
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,LoRA 와 QLoRA 의 차이,0.01734481,0.0,0.01734481006860733
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,Loss Function 예시,-0.004331788,0.0,0.004331788048148155
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,Loss Function 정의,-0.035242215,0.0,0.03524221479892731
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,MBTI,-0.040521182,0.0,0.040521182119846344
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,MSE Loss 설명,0.015368173,0.0,0.015368172898888588
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,MSE Loss 용도,0.011415606,0.0,0.011415606364607811
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,Multi-Label 에서 CE + Softmax 적용 문제점,0.016266625,0.0,0.016266625374555588
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,PEFT 방법 5가지,-0.0059529273,0.0,0.005952927283942699
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,거대 언어 모델 정의,-0.01865567,0.0,0.018655670806765556
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,기본 경험,-0.038965285,0.0,0.03896528482437134
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,답변 실패,-0.0046499996,0.0,0.004649999551475048
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,딥러닝,-0.040586673,0.0,0.040586672723293304
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,마지막 할 말,-0.053724665,0.0,0.05372466519474983
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,머신러닝,-0.04787437,0.0,0.047874368727207184
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,면접 시작 인사,0.9371282,1.0,0.06287181377410889
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,상세 경험,-0.049797237,0.0,0.049797236919403076
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,수식,-0.03139156,0.0,0.03139156103134155
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,인공지능,-0.042712502,0.0,0.04271250218153
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,잠시 휴식,-0.017727258,0.0,0.017727257683873177
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,좋아하는 아이돌,-0.013366878,0.0,0.013366878032684326
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,핵심 아이디어,-0.049839042,0.0,0.049839042127132416
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,확률 예측에서 MSE Loss 미 사용 이유,0.017999668,0.0,0.017999667674303055
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,BCE 가 좋은 task,-0.011408011,0.0,0.011408011429011822
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,"BCE 가 좋은 task, 이유",-0.008249958,0.0,0.008249958045780659
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,LLM Fine-Tuning 의 PEFT,0.011039279,0.0,0.011039279401302338
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,LoRA,0.021234564,0.0,0.02123456448316574
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,LoRA 와 QLoRA 의 차이,0.023711389,0.0,0.023711388930678368
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,Loss Function 예시,-0.026480582,0.0,0.026480581611394882
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,Loss Function 정의,-0.023120824,0.0,0.023120824247598648
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,MBTI,-0.058112573,0.0,0.05811257287859917
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,MSE Loss 설명,0.016333096,0.0,0.016333095729351044
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,MSE Loss 용도,0.014108821,0.0,0.01410882081836462
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,Multi-Label 에서 CE + Softmax 적용 문제점,0.018043604,0.0,0.018043603748083115
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,PEFT 방법 5가지,0.0237092,0.0,0.02370920032262802
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,거대 언어 모델 정의,-0.010673147,0.0,0.010673146694898605
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,기본 경험,-0.020913081,0.0,0.020913081243634224
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,답변 실패,-0.027378373,0.0,0.027378372848033905
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,딥러닝,-0.033756927,0.0,0.03375692665576935
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,마지막 할 말,-0.0056913067,0.0,0.0056913066655397415
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,머신러닝,-0.047940135,0.0,0.04794013500213623
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,면접 시작 인사,0.92885774,1.0,0.07114225625991821
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,상세 경험,-0.028559612,0.0,0.02855961211025715
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,수식,-0.01130466,0.0,0.011304659768939018
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,인공지능,-0.038492136,0.0,0.03849213570356369
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,잠시 휴식,-0.008770907,0.0,0.008770907297730446
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,좋아하는 아이돌,-0.006713878,0.0,0.006713877897709608
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,핵심 아이디어,-0.049161702,0.0,0.049161702394485474
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,확률 예측에서 MSE Loss 미 사용 이유,0.023316458,0.0,0.023316457867622375
면접 시작 인사 -> 파이팅! 시작하자,BCE 가 좋은 task,-0.020844024,0.0,0.020844023674726486
면접 시작 인사 -> 파이팅! 시작하자,"BCE 가 좋은 task, 이유",-0.023730014,0.0,0.023730013519525528
면접 시작 인사 -> 파이팅! 시작하자,LLM Fine-Tuning 의 PEFT,0.010423011,0.0,0.010423011146485806
면접 시작 인사 -> 파이팅! 시작하자,LoRA,-0.0068828557,0.0,0.006882855668663979
면접 시작 인사 -> 파이팅! 시작하자,LoRA 와 QLoRA 의 차이,0.015597882,0.0,0.015597881749272346
면접 시작 인사 -> 파이팅! 시작하자,Loss Function 예시,-0.0025040219,0.0,0.0025040218606591225
면접 시작 인사 -> 파이팅! 시작하자,Loss Function 정의,-0.013227717,0.0,0.013227717019617558
면접 시작 인사 -> 파이팅! 시작하자,MBTI,-0.02203844,0.0,0.02203843928873539
면접 시작 인사 -> 파이팅! 시작하자,MSE Loss 설명,0.023417864,0.0,0.023417863994836807
면접 시작 인사 -> 파이팅! 시작하자,MSE Loss 용도,0.019736795,0.0,0.019736794754862785
면접 시작 인사 -> 파이팅! 시작하자,Multi-Label 에서 CE + Softmax 적용 문제점,0.013488315,0.0,0.013488315045833588
면접 시작 인사 -> 파이팅! 시작하자,PEFT 방법 5가지,0.013811034,0.0,0.01381103415042162
면접 시작 인사 -> 파이팅! 시작하자,거대 언어 모델 정의,0.001579096,0.0,0.0015790959587320685
면접 시작 인사 -> 파이팅! 시작하자,기본 경험,-0.032774884,0.0,0.03277488425374031
면접 시작 인사 -> 파이팅! 시작하자,답변 실패,-0.034563698,0.0,0.03456369787454605
면접 시작 인사 -> 파이팅! 시작하자,딥러닝,-0.0440628,0.0,0.04406280070543289
면접 시작 인사 -> 파이팅! 시작하자,마지막 할 말,-0.026089726,0.0,0.026089726015925407
면접 시작 인사 -> 파이팅! 시작하자,머신러닝,-0.041836355,0.0,0.04183635488152504
면접 시작 인사 -> 파이팅! 시작하자,면접 시작 인사,0.9562577,1.0,0.04374229907989502
면접 시작 인사 -> 파이팅! 시작하자,상세 경험,-0.054687176,0.0,0.054687175899744034
면접 시작 인사 -> 파이팅! 시작하자,수식,-0.0076299445,0.0,0.0076299444772303104
면접 시작 인사 -> 파이팅! 시작하자,인공지능,-0.026880512,0.0,0.02688051201403141
면접 시작 인사 -> 파이팅! 시작하자,잠시 휴식,-0.007079333,0.0,0.007079333066940308
면접 시작 인사 -> 파이팅! 시작하자,좋아하는 아이돌,-0.009776479,0.0,0.009776478633284569
면접 시작 인사 -> 파이팅! 시작하자,핵심 아이디어,-0.02166644,0.0,0.02166643925011158
면접 시작 인사 -> 파이팅! 시작하자,확률 예측에서 MSE Loss 미 사용 이유,0.013988853,0.0,0.013988853432238102
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",BCE 가 좋은 task,0.014155752,0.0,0.014155752025544643
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데","BCE 가 좋은 task, 이유",0.0063012764,0.0,0.006301276385784149
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",LLM Fine-Tuning 의 PEFT,-0.007009171,0.0,0.007009170949459076
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",LoRA,0.004974164,0.0,0.004974164068698883
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",LoRA 와 QLoRA 의 차이,0.0072682085,0.0,0.007268208544701338
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",Loss Function 예시,-0.091960534,0.0,0.09196053445339203
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",Loss Function 정의,-0.043951835,0.0,0.04395183548331261
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",MBTI,-0.010650823,0.0,0.010650822892785072
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",MSE Loss 설명,-0.04183079,0.0,0.04183078929781914
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",MSE Loss 용도,-0.028033262,0.0,0.028033262118697166
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",Multi-Label 에서 CE + Softmax 적용 문제점,0.005998707,0.0,0.005998706910759211
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",PEFT 방법 5가지,0.069679506,0.0,0.06967950612306595
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",거대 언어 모델 정의,-0.048696496,0.0,0.04869649559259415
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",기본 경험,0.034245104,0.0,0.03424510359764099
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",답변 실패,0.88330764,1.0,0.11669236421585083
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",딥러닝,0.04275707,0.0,0.0427570715546608
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",마지막 할 말,-0.036156867,0.0,0.03615686669945717
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",머신러닝,0.017385047,0.0,0.017385046929121017
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",면접 시작 인사,-0.01685469,0.0,0.01685469038784504
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",상세 경험,-0.016303938,0.0,0.016303937882184982
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",수식,-0.06940051,0.0,0.06940051168203354
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",인공지능,0.022162074,0.0,0.022162074223160744
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",잠시 휴식,-0.005911038,0.0,0.005911037791520357
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",좋아하는 아이돌,0.0053557735,0.0,0.005355773493647575
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",핵심 아이디어,-0.027568795,0.0,0.027568794786930084
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",확률 예측에서 MSE Loss 미 사용 이유,-0.06256872,0.0,0.06256871670484543
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",BCE 가 좋은 task,0.082490675,0.0,0.08249067515134811
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!","BCE 가 좋은 task, 이유",0.08864115,0.0,0.08864115178585052
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",LLM Fine-Tuning 의 PEFT,-0.015443054,0.0,0.015443054027855396
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",LoRA,-0.030853791,0.0,0.030853791162371635
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",LoRA 와 QLoRA 의 차이,0.0153413,0.0,0.015341299585998058
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",Loss Function 예시,-0.034572456,0.0,0.034572456032037735
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",Loss Function 정의,-0.016271956,0.0,0.016271956264972687
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",MBTI,0.03127108,0.0,0.031271081417798996
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",MSE Loss 설명,-0.049312383,0.0,0.04931238293647766
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",MSE Loss 용도,-0.037082236,0.0,0.037082236260175705
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",Multi-Label 에서 CE + Softmax 적용 문제점,-0.07120131,0.0,0.07120130956172943
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",PEFT 방법 5가지,0.03783191,0.0,0.03783190995454788
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",거대 언어 모델 정의,-0.02454716,0.0,0.024547159671783447
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",기본 경험,0.014289051,0.0,0.014289051294326782
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",답변 실패,0.052165516,0.0,0.05216551572084427
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",딥러닝,0.5785009,0.0,0.5785009264945984
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",마지막 할 말,-0.021281628,0.0,0.02128162793815136
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",머신러닝,0.22725125,0.0,0.22725124657154083
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",면접 시작 인사,-0.029993046,0.0,0.029993046075105667
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",상세 경험,-0.01295533,0.0,0.012955330312252045
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",수식,0.06525172,0.0,0.06525172293186188
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",인공지능,0.20036802,1.0,0.7996319830417633
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",잠시 휴식,-0.0038332976,0.0,0.0038332976400852203
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",좋아하는 아이돌,0.06851312,0.0,0.06851311773061752
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",핵심 아이디어,0.10211739,0.0,0.1021173894405365
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",확률 예측에서 MSE Loss 미 사용 이유,0.021140492,0.0,0.02114049158990383
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",BCE 가 좋은 task,0.04814992,0.0,0.048149921000003815
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지","BCE 가 좋은 task, 이유",0.042164356,0.0,0.042164355516433716
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",LLM Fine-Tuning 의 PEFT,-0.021439152,0.0,0.02143915183842182
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",LoRA,-0.0580354,0.0,0.05803539976477623
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",LoRA 와 QLoRA 의 차이,-0.022738146,0.0,0.022738145664334297
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",Loss Function 예시,0.013867867,0.0,0.013867867179214954
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",Loss Function 정의,-0.050443266,0.0,0.050443265587091446
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",MBTI,0.039612737,0.0,0.03961273655295372
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",MSE Loss 설명,0.032570705,0.0,0.03257070481777191
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",MSE Loss 용도,0.045741234,0.0,0.045741233974695206
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",Multi-Label 에서 CE + Softmax 적용 문제점,0.023777751,0.0,0.023777751252055168
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",PEFT 방법 5가지,0.023282345,0.0,0.02328234538435936
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",거대 언어 모델 정의,0.021678926,0.0,0.021678926423192024
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",기본 경험,-0.027934752,0.0,0.02793475240468979
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",답변 실패,0.016179167,0.0,0.016179166734218597
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",딥러닝,0.62078863,0.0,0.6207886338233948
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",마지막 할 말,-0.046217494,0.0,0.04621749371290207
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",머신러닝,0.59183556,1.0,0.40816444158554077
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",면접 시작 인사,0.009064584,0.0,0.009064584039151669
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",상세 경험,0.020497944,0.0,0.020497944205999374
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",수식,0.01296917,0.0,0.01296916976571083
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",인공지능,0.109419554,0.0,0.1094195544719696
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",잠시 휴식,0.020856673,0.0,0.020856672897934914
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",좋아하는 아이돌,-0.052558713,0.0,0.05255871266126633
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",핵심 아이디어,-0.027416611,0.0,0.027416611090302467
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",확률 예측에서 MSE Loss 미 사용 이유,0.02447738,0.0,0.024477379396557808
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",BCE 가 좋은 task,0.0714688,0.0,0.07146880030632019
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?","BCE 가 좋은 task, 이유",0.061310664,0.0,0.06131066381931305
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",LLM Fine-Tuning 의 PEFT,-0.013291336,0.0,0.01329133566468954
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",LoRA,-0.067133844,0.0,0.0671338438987732
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",LoRA 와 QLoRA 의 차이,0.020296922,0.0,0.020296921953558922
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",Loss Function 예시,-0.057597663,0.0,0.05759766325354576
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",Loss Function 정의,0.011988094,0.0,0.011988094076514244
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",MBTI,-0.053461034,0.0,0.05346103385090828
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",MSE Loss 설명,0.011844836,0.0,0.011844836175441742
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",MSE Loss 용도,0.022549381,0.0,0.022549381479620934
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",Multi-Label 에서 CE + Softmax 적용 문제점,-0.062012926,0.0,0.0620129257440567
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",PEFT 방법 5가지,0.006621116,0.0,0.00662111584097147
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",거대 언어 모델 정의,-0.023624303,0.0,0.023624302819371223
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",기본 경험,0.015300864,0.0,0.015300864353775978
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",답변 실패,0.24418671,0.0,0.24418671429157257
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",딥러닝,0.6285766,1.0,0.3714234232902527
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",마지막 할 말,-0.059626833,0.0,0.059626832604408264
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",머신러닝,0.3018979,0.0,0.30189791321754456
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",면접 시작 인사,-0.015104977,0.0,0.015104977414011955
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",상세 경험,0.047730405,0.0,0.04773040488362312
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",수식,0.017311389,0.0,0.01731138862669468
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",인공지능,0.15255617,0.0,0.1525561660528183
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",잠시 휴식,0.018004065,0.0,0.01800406537950039
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",좋아하는 아이돌,0.02705524,0.0,0.02705523930490017
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",핵심 아이디어,0.03772589,0.0,0.037725888192653656
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",확률 예측에서 MSE Loss 미 사용 이유,-0.05187851,0.0,0.05187850818037987
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",BCE 가 좋은 task,0.057864644,0.0,0.05786464363336563
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야","BCE 가 좋은 task, 이유",0.04741011,0.0,0.047410108149051666
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",LLM Fine-Tuning 의 PEFT,-0.0328443,0.0,0.03284430131316185
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",LoRA,-0.06667304,0.0,0.06667304039001465
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",LoRA 와 QLoRA 의 차이,0.041684758,0.0,0.04168475791811943
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",Loss Function 예시,-0.092775926,0.0,0.09277592599391937
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",Loss Function 정의,0.012446184,0.0,0.01244618371129036
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",MBTI,-0.044641305,0.0,0.04464130476117134
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",MSE Loss 설명,-0.0022649847,0.0,0.0022649846505373716
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",MSE Loss 용도,0.009736929,0.0,0.009736929088830948
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",Multi-Label 에서 CE + Softmax 적용 문제점,-0.03603301,0.0,0.03603300824761391
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",PEFT 방법 5가지,0.050519135,0.0,0.05051913484930992
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",거대 언어 모델 정의,-0.012371515,0.0,0.012371514923870564
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",기본 경험,0.012178037,0.0,0.012178037315607071
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",답변 실패,0.35756218,0.0,0.35756218433380127
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",딥러닝,0.5995135,1.0,0.4004865288734436
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",마지막 할 말,-0.047865417,0.0,0.04786541685461998
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",머신러닝,0.30298737,0.0,0.30298736691474915
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",면접 시작 인사,-0.018433973,0.0,0.01843397319316864
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",상세 경험,0.044280168,0.0,0.044280167669057846
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",수식,0.0069535053,0.0,0.0069535053335130215
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",인공지능,0.1536941,0.0,0.15369409322738647
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",잠시 휴식,0.015483424,0.0,0.015483424067497253
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",좋아하는 아이돌,0.027844137,0.0,0.027844136580824852
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",핵심 아이디어,-0.00373085,0.0,0.003730850061401725
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",확률 예측에서 MSE Loss 미 사용 이유,-0.05699575,0.0,0.05699574947357178
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",BCE 가 좋은 task,0.02659722,0.0,0.026597220450639725
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?","BCE 가 좋은 task, 이유",0.0033820765,0.0,0.003382076509296894
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",LLM Fine-Tuning 의 PEFT,0.003636222,0.0,0.0036362220998853445
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",LoRA,-0.057635583,0.0,0.057635582983493805
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",LoRA 와 QLoRA 의 차이,-0.017470881,0.0,0.01747088134288788
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",Loss Function 예시,-0.08077475,0.0,0.08077474683523178
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",Loss Function 정의,-0.03965395,0.0,0.039653949439525604
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",MBTI,-0.0071109305,0.0,0.007110930513590574
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",MSE Loss 설명,0.035261903,0.0,0.03526190295815468
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",MSE Loss 용도,0.06325652,0.0,0.06325651705265045
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",Multi-Label 에서 CE + Softmax 적용 문제점,-0.053195484,0.0,0.05319548398256302
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",PEFT 방법 5가지,0.020013412,0.0,0.020013412460684776
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",거대 언어 모델 정의,-0.007855936,0.0,0.007855935953557491
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",기본 경험,-0.0017952396,0.0,0.0017952396301552653
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",답변 실패,0.62427723,1.0,0.3757227659225464
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",딥러닝,0.3179301,0.0,0.31793010234832764
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",마지막 할 말,-0.014841144,0.0,0.014841143973171711
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",머신러닝,0.28974327,0.0,0.2897432744503021
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",면접 시작 인사,-0.059537716,0.0,0.05953771620988846
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",상세 경험,-0.039616503,0.0,0.039616502821445465
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",수식,-0.007592453,0.0,0.00759245315566659
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",인공지능,0.11350064,0.0,0.11350063979625702
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",잠시 휴식,0.039293393,0.0,0.03929339349269867
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",좋아하는 아이돌,-0.02904886,0.0,0.0290488600730896
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",핵심 아이디어,0.0034964462,0.0,0.003496446181088686
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",확률 예측에서 MSE Loss 미 사용 이유,-0.04556921,0.0,0.04556921124458313
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,BCE 가 좋은 task,0.018293042,0.0,0.018293041735887527
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,"BCE 가 좋은 task, 이유",0.01964435,0.0,0.019644349813461304
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,LLM Fine-Tuning 의 PEFT,0.01574034,0.0,0.01574034057557583
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,LoRA,0.013981263,0.0,0.013981263153254986
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,LoRA 와 QLoRA 의 차이,0.023187198,0.0,0.023187197744846344
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,Loss Function 예시,0.084392235,0.0,0.0843922346830368
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,Loss Function 정의,0.024513675,0.0,0.024513674899935722
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,MBTI,0.0005185597,0.0,0.0005185597110539675
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,MSE Loss 설명,0.02787116,0.0,0.027871159836649895
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,MSE Loss 용도,0.017142937,0.0,0.01714293658733368
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,Multi-Label 에서 CE + Softmax 적용 문제점,0.054326676,0.0,0.054326675832271576
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,PEFT 방법 5가지,0.05260185,0.0,0.052601851522922516
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,거대 언어 모델 정의,0.8997986,1.0,0.10020142793655396
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,기본 경험,0.054613993,0.0,0.05461399257183075
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,답변 실패,0.01970679,0.0,0.019706789404153824
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,딥러닝,-0.044404626,0.0,0.04440462589263916
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,마지막 할 말,0.01541663,0.0,0.015416629612445831
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,머신러닝,0.010422791,0.0,0.010422791354358196
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,면접 시작 인사,-0.009070932,0.0,0.009070931933820248
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,상세 경험,0.005790488,0.0,0.005790487863123417
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,수식,-0.064775385,0.0,0.06477538496255875
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,인공지능,0.019779248,0.0,0.019779248163104057
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,잠시 휴식,0.00045551546,0.0,0.00045551545917987823
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,좋아하는 아이돌,0.020272939,0.0,0.020272938534617424
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,핵심 아이디어,0.07843333,0.0,0.0784333273768425
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,확률 예측에서 MSE Loss 미 사용 이유,0.022238165,0.0,0.022238165140151978
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,BCE 가 좋은 task,-0.006784684,0.0,0.006784684024751186
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,"BCE 가 좋은 task, 이유",-0.015041798,0.0,0.01504179835319519
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,LLM Fine-Tuning 의 PEFT,0.010928101,0.0,0.010928100906312466
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,LoRA,0.019913249,0.0,0.01991324871778488
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,LoRA 와 QLoRA 의 차이,0.0004911228,0.0,0.0004911227733828127
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,Loss Function 예시,-0.04443778,0.0,0.04443778097629547
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,Loss Function 정의,-0.016527936,0.0,0.0165279358625412
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,MBTI,-0.0075262776,0.0,0.007526277564466
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,MSE Loss 설명,-0.03677573,0.0,0.036775730550289154
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,MSE Loss 용도,-0.024860322,0.0,0.02486032247543335
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,Multi-Label 에서 CE + Softmax 적용 문제점,-0.019012788,0.0,0.01901278831064701
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,PEFT 방법 5가지,0.045926698,0.0,0.04592669755220413
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,거대 언어 모델 정의,0.117502145,0.0,0.11750214546918869
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,기본 경험,0.016575918,0.0,0.016575917601585388
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,답변 실패,0.88367105,1.0,0.11632895469665527
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,딥러닝,-0.0060506933,0.0,0.006050693336874247
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,마지막 할 말,0.00094928377,0.0,0.0009492837707512081
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,머신러닝,0.0017771951,0.0,0.0017771951388567686
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,면접 시작 인사,-0.043756183,0.0,0.04375618323683739
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,상세 경험,0.0023351456,0.0,0.002335145603865385
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,수식,-0.08558396,0.0,0.08558396250009537
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,인공지능,0.02516424,0.0,0.02516423910856247
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,잠시 휴식,-0.01624579,0.0,0.01624578982591629
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,좋아하는 아이돌,-0.006740824,0.0,0.006740823853760958
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,핵심 아이디어,-0.017052246,0.0,0.017052246257662773
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,확률 예측에서 MSE Loss 미 사용 이유,-0.06373195,0.0,0.06373195350170135
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,BCE 가 좋은 task,0.04223237,0.0,0.042232368141412735
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,"BCE 가 좋은 task, 이유",0.015829477,0.0,0.015829477459192276
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,LLM Fine-Tuning 의 PEFT,-0.037205435,0.0,0.03720543533563614
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,LoRA,0.008204809,0.0,0.00820480939000845
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,LoRA 와 QLoRA 의 차이,0.021593835,0.0,0.021593835204839706
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,Loss Function 예시,0.08018675,0.0,0.08018674701452255
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,Loss Function 정의,0.69526833,0.0,0.6952683329582214
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,MBTI,-0.00997148,0.0,0.009971479885280132
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,MSE Loss 설명,0.033742957,0.0,0.033742956817150116
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,MSE Loss 용도,0.030495016,0.0,0.030495015904307365
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,Multi-Label 에서 CE + Softmax 적용 문제점,-0.03422834,0.0,0.03422833979129791
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,PEFT 방법 5가지,0.070726894,0.0,0.0707268938422203
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,거대 언어 모델 정의,0.0104576005,0.0,0.010457600466907024
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,기본 경험,0.008091695,0.0,0.008091694675385952
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,답변 실패,0.052926816,1.0,0.947073183953762
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,딥러닝,0.020910786,0.0,0.02091078646481037
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,마지막 할 말,0.0098586995,0.0,0.009858699515461922
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,머신러닝,-0.03179304,0.0,0.03179303929209709
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,면접 시작 인사,-0.06356678,0.0,0.06356678158044815
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,상세 경험,0.048595525,0.0,0.048595525324344635
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,수식,0.04398362,0.0,0.043983619660139084
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,인공지능,-0.00043630763,0.0,0.00043630762957036495
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,잠시 휴식,-0.007768622,0.0,0.007768622133880854
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,좋아하는 아이돌,-0.020084592,0.0,0.020084591582417488
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,핵심 아이디어,0.0056417454,0.0,0.005641745403409004
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,확률 예측에서 MSE Loss 미 사용 이유,0.009050505,0.0,0.009050505235791206
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",BCE 가 좋은 task,0.04542834,0.0,0.04542833939194679
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!","BCE 가 좋은 task, 이유",0.035525877,0.0,0.03552587702870369
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",LLM Fine-Tuning 의 PEFT,-0.00822932,0.0,0.00822931993752718
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",LoRA,0.021782868,0.0,0.02178286761045456
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",LoRA 와 QLoRA 의 차이,-0.0023061829,0.0,0.0023061828687787056
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",Loss Function 예시,-0.029768355,0.0,0.029768355190753937
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",Loss Function 정의,0.7911326,1.0,0.2088673710823059
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",MBTI,0.024806758,0.0,0.024806758388876915
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",MSE Loss 설명,0.03759031,0.0,0.03759030997753143
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",MSE Loss 용도,0.039568573,0.0,0.039568573236465454
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",Multi-Label 에서 CE + Softmax 적용 문제점,-0.035340246,0.0,0.03534024581313133
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",PEFT 방법 5가지,0.06117883,0.0,0.06117882952094078
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",거대 언어 모델 정의,0.05074495,0.0,0.05074495077133179
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",기본 경험,-0.03172596,0.0,0.031725961714982986
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",답변 실패,0.0055283294,0.0,0.0055283294059336185
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",딥러닝,-0.020130137,0.0,0.020130136981606483
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",마지막 할 말,-0.04874566,0.0,0.04874565824866295
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",머신러닝,-0.076001726,0.0,0.07600172609090805
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",면접 시작 인사,-0.0021875768,0.0,0.0021875768434256315
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",상세 경험,-0.011761236,0.0,0.011761236004531384
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",수식,0.007229485,0.0,0.007229485083371401
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",인공지능,-0.016751738,0.0,0.016751738265156746
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",잠시 휴식,0.0065570734,0.0,0.0065570734441280365
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",좋아하는 아이돌,-0.026363784,0.0,0.026363784447312355
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",핵심 아이디어,-0.061847933,0.0,0.061847932636737823
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",확률 예측에서 MSE Loss 미 사용 이유,0.01166943,0.0,0.011669429950416088
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,BCE 가 좋은 task,0.01111654,0.0,0.011116540059447289
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,"BCE 가 좋은 task, 이유",0.004650214,0.0,0.004650214221328497
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,LLM Fine-Tuning 의 PEFT,-0.001087981,0.0,0.0010879810433834791
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,LoRA,0.00069664064,0.0,0.0006966406363062561
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,LoRA 와 QLoRA 의 차이,0.010538808,0.0,0.010538808070123196
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,Loss Function 예시,0.26327732,0.0,0.2632773220539093
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,Loss Function 정의,-0.055289682,0.0,0.05528968200087547
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,MBTI,-0.019033398,0.0,0.01903339847922325
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,MSE Loss 설명,-0.032948006,0.0,0.03294800594449043
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,MSE Loss 용도,-0.01728167,0.0,0.0172816701233387
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,Multi-Label 에서 CE + Softmax 적용 문제점,0.018699033,0.0,0.018699033185839653
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,PEFT 방법 5가지,0.08526022,0.0,0.08526021987199783
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,거대 언어 모델 정의,-0.010960467,0.0,0.010960467159748077
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,기본 경험,-0.035001412,0.0,0.03500141203403473
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,답변 실패,0.82862484,1.0,0.17137515544891357
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,딥러닝,-0.024593249,0.0,0.024593248963356018
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,마지막 할 말,-0.0042679585,0.0,0.0042679584585130215
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,머신러닝,0.017661497,0.0,0.017661496996879578
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,면접 시작 인사,-0.015259418,0.0,0.015259417705237865
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,상세 경험,-0.023260057,0.0,0.023260056972503662
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,수식,-0.04723153,0.0,0.0472315289080143
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,인공지능,0.010380398,0.0,0.0103803975507617
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,잠시 휴식,-0.005480258,0.0,0.005480257794260979
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,좋아하는 아이돌,-0.0010425285,0.0,0.0010425285436213017
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,핵심 아이디어,-0.042030092,0.0,0.04203009232878685
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,확률 예측에서 MSE Loss 미 사용 이유,-0.052825972,0.0,0.05282597243785858
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",BCE 가 좋은 task,0.019626113,0.0,0.019626112654805183
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지","BCE 가 좋은 task, 이유",0.015298304,0.0,0.01529830414801836
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",LLM Fine-Tuning 의 PEFT,-0.0043055867,0.0,0.004305586684495211
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",LoRA,-0.021336058,0.0,0.021336058154702187
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",LoRA 와 QLoRA 의 차이,0.020628365,0.0,0.020628364756703377
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",Loss Function 예시,0.5889118,1.0,0.411088228225708
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",Loss Function 정의,-0.062091433,0.0,0.06209143251180649
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",MBTI,-0.080513425,0.0,0.08051342517137527
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",MSE Loss 설명,0.028093582,0.0,0.028093582019209862
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",MSE Loss 용도,0.040558193,0.0,0.04055819287896156
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",Multi-Label 에서 CE + Softmax 적용 문제점,-0.007337232,0.0,0.007337232120335102
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",PEFT 방법 5가지,0.01697163,0.0,0.016971629112958908
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",거대 언어 모델 정의,-0.010234692,0.0,0.010234692133963108
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",기본 경험,-0.07414047,0.0,0.07414046674966812
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",답변 실패,0.54336315,0.0,0.5433631539344788
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",딥러닝,-0.04866863,0.0,0.04866863042116165
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",마지막 할 말,-0.020739561,0.0,0.020739560946822166
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",머신러닝,0.00616942,0.0,0.006169420201331377
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",면접 시작 인사,-0.024281694,0.0,0.024281693622469902
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",상세 경험,-0.030516878,0.0,0.03051687777042389
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",수식,-0.056741927,0.0,0.056741926819086075
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",인공지능,-0.03230239,0.0,0.03230239078402519
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",잠시 휴식,0.021828087,0.0,0.02182808704674244
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",좋아하는 아이돌,-0.010050479,0.0,0.01005047932267189
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",핵심 아이디어,-0.07196721,0.0,0.07196720689535141
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",확률 예측에서 MSE Loss 미 사용 이유,-0.010649293,0.0,0.010649292729794979
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,BCE 가 좋은 task,0.06051792,0.0,0.060517918318510056
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,"BCE 가 좋은 task, 이유",0.0505802,0.0,0.05058019980788231
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,LLM Fine-Tuning 의 PEFT,-0.030373363,0.0,0.030373362824320793
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,LoRA,-0.08482787,0.0,0.08482787013053894
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,LoRA 와 QLoRA 의 차이,0.027790926,0.0,0.02779092639684677
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,Loss Function 예시,0.078844234,0.0,0.07884423434734344
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,Loss Function 정의,0.021629635,0.0,0.021629635244607925
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,MBTI,-0.10611136,0.0,0.10611136257648468
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,MSE Loss 설명,0.7569837,1.0,0.2430163025856018
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,MSE Loss 용도,0.7353753,0.0,0.7353752851486206
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,Multi-Label 에서 CE + Softmax 적용 문제점,0.010809842,0.0,0.010809841565787792
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,PEFT 방법 5가지,0.03749169,0.0,0.03749169036746025
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,거대 언어 모델 정의,0.0051921243,0.0,0.005192124284803867
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,기본 경험,-0.010900678,0.0,0.010900678113102913
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,답변 실패,-0.0032677923,0.0,0.003267792286351323
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,딥러닝,-0.047640476,0.0,0.04764047637581825
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,마지막 할 말,0.028929863,0.0,0.02892986312508583
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,머신러닝,-0.056438915,0.0,0.05643891543149948
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,면접 시작 인사,0.017914632,0.0,0.017914632335305214
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,상세 경험,0.028179232,0.0,0.02817923203110695
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,수식,0.07156139,0.0,0.07156138867139816
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,인공지능,0.0050694714,0.0,0.005069471430033445
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,잠시 휴식,0.018752908,0.0,0.01875290833413601
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,좋아하는 아이돌,0.012609562,0.0,0.012609561905264854
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,핵심 아이디어,-0.023946837,0.0,0.023946836590766907
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,확률 예측에서 MSE Loss 미 사용 이유,0.014745839,0.0,0.014745838940143585
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,BCE 가 좋은 task,0.0677853,0.0,0.06778530031442642
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,"BCE 가 좋은 task, 이유",0.05411891,0.0,0.05411890894174576
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,LLM Fine-Tuning 의 PEFT,-0.020007215,0.0,0.020007215440273285
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,LoRA,-0.08363767,0.0,0.08363766968250275
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,LoRA 와 QLoRA 의 차이,-0.0037936687,0.0,0.0037936687003821135
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,Loss Function 예시,0.04678311,0.0,0.04678310826420784
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,Loss Function 정의,0.04317751,0.0,0.04317751154303551
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,MBTI,-0.082378566,0.0,0.0823785662651062
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,MSE Loss 설명,0.71774477,0.0,0.717744767665863
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,MSE Loss 용도,0.70579594,0.0,0.70579594373703
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,Multi-Label 에서 CE + Softmax 적용 문제점,-0.033340976,0.0,0.033340975642204285
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,PEFT 방법 5가지,0.0298623,0.0,0.02986229956150055
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,거대 언어 모델 정의,0.0012762189,0.0,0.0012762189144268632
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,기본 경험,-0.008502648,0.0,0.008502648212015629
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,답변 실패,0.09895084,1.0,0.9010491594672203
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,딥러닝,-0.036519952,0.0,0.03651995211839676
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,마지막 할 말,-0.008570379,0.0,0.00857037864625454
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,머신러닝,-0.056737524,0.0,0.05673752352595329
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,면접 시작 인사,0.0035104332,0.0,0.0035104332491755486
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,상세 경험,0.02566086,0.0,0.025660859420895576
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,수식,0.058436904,0.0,0.05843690410256386
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,인공지능,0.021009196,0.0,0.02100919559597969
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,잠시 휴식,0.0098842345,0.0,0.009884234517812729
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,좋아하는 아이돌,-0.00038505907,0.0,0.0003850590728688985
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,핵심 아이디어,-0.00082766067,0.0,0.0008276606677100062
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,확률 예측에서 MSE Loss 미 사용 이유,0.024366707,0.0,0.024366706609725952
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",BCE 가 좋은 task,0.0470551,0.0,0.04705509915947914
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지","BCE 가 좋은 task, 이유",0.034484103,0.0,0.034484103322029114
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",LLM Fine-Tuning 의 PEFT,-0.06602549,0.0,0.06602548807859421
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",LoRA,-0.035900973,0.0,0.03590097278356552
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",LoRA 와 QLoRA 의 차이,-0.046271585,0.0,0.046271584928035736
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",Loss Function 예시,0.07730098,0.0,0.07730098068714142
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",Loss Function 정의,0.03597927,0.0,0.035979270935058594
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",MBTI,-0.010470732,0.0,0.010470732115209103
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",MSE Loss 설명,0.7655645,0.0,0.765564501285553
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",MSE Loss 용도,0.7939846,1.0,0.20601540803909302
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",Multi-Label 에서 CE + Softmax 적용 문제점,0.025300741,0.0,0.02530074119567871
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",PEFT 방법 5가지,0.047210433,0.0,0.04721043258905411
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",거대 언어 모델 정의,0.029758938,0.0,0.029758937656879425
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",기본 경험,-0.036844186,0.0,0.03684418648481369
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",답변 실패,-0.03403826,0.0,0.03403826057910919
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",딥러닝,0.030333644,0.0,0.030333643779158592
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",마지막 할 말,0.002652299,0.0,0.002652298891916871
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",머신러닝,0.043572605,0.0,0.04357260465621948
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",면접 시작 인사,0.02321988,0.0,0.02321987971663475
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",상세 경험,-0.045747656,0.0,0.045747656375169754
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",수식,-0.009217598,0.0,0.009217597544193268
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",인공지능,-0.04457275,0.0,0.044572748243808746
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",잠시 휴식,0.024770819,0.0,0.024770818650722504
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",좋아하는 아이돌,-0.017313197,0.0,0.017313197255134583
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",핵심 아이디어,-0.0394024,0.0,0.039402399212121964
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",확률 예측에서 MSE Loss 미 사용 이유,0.0042374018,0.0,0.004237401764839888
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,BCE 가 좋은 task,-0.0026845192,0.0,0.002684519160538912
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,"BCE 가 좋은 task, 이유",-0.012525874,0.0,0.012525874190032482
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,LLM Fine-Tuning 의 PEFT,0.029624265,0.0,0.02962426468729973
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,LoRA,-0.016896643,0.0,0.016896642744541168
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,LoRA 와 QLoRA 의 차이,0.043345645,0.0,0.04334564507007599
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,Loss Function 예시,-0.0038076977,0.0,0.0038076976779848337
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,Loss Function 정의,0.0019788905,0.0,0.0019788905046880245
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,MBTI,-0.06133391,0.0,0.06133390963077545
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,MSE Loss 설명,0.23292078,0.0,0.2329207807779312
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,MSE Loss 용도,0.26757523,0.0,0.2675752341747284
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,Multi-Label 에서 CE + Softmax 적용 문제점,-0.022094384,0.0,0.02209438383579254
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,PEFT 방법 5가지,0.0735865,0.0,0.07358650118112564
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,거대 언어 모델 정의,0.010281223,0.0,0.010281222872436047
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,기본 경험,-0.034637243,0.0,0.034637242555618286
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,답변 실패,0.71090597,1.0,0.2890940308570862
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,딥러닝,-0.052633815,0.0,0.05263381451368332
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,마지막 할 말,-0.023781696,0.0,0.02378169633448124
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,머신러닝,-0.042125765,0.0,0.04212576523423195
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,면접 시작 인사,-0.031220883,0.0,0.03122088313102722
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,상세 경험,-0.015870064,0.0,0.01587006449699402
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,수식,-0.04734218,0.0,0.04734218120574951
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,인공지능,0.009275636,0.0,0.009275635704398155
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,잠시 휴식,0.018400816,0.0,0.01840081624686718
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,좋아하는 아이돌,-0.0029367309,0.0,0.002936730859801173
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,핵심 아이디어,-0.0991706,0.0,0.09917060285806656
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,확률 예측에서 MSE Loss 미 사용 이유,0.008568213,0.0,0.008568213321268559
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,BCE 가 좋은 task,-0.0059521673,0.0,0.005952167324721813
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,"BCE 가 좋은 task, 이유",-0.008740492,0.0,0.008740492165088654
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,LLM Fine-Tuning 의 PEFT,0.023614064,0.0,0.0236140638589859
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,LoRA,0.016635826,0.0,0.016635825857520103
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,LoRA 와 QLoRA 의 차이,0.0017499202,0.0,0.001749920193105936
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,Loss Function 예시,-0.037400123,0.0,0.03740012273192406
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,Loss Function 정의,-0.028018039,0.0,0.028018038719892502
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,MBTI,-0.039040778,0.0,0.03904077783226967
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,MSE Loss 설명,-0.017464159,0.0,0.017464159056544304
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,MSE Loss 용도,0.0013436897,0.0,0.0013436897424980998
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,Multi-Label 에서 CE + Softmax 적용 문제점,-0.033172604,0.0,0.0331726036965847
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,PEFT 방법 5가지,0.046132237,0.0,0.04613223671913147
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,거대 언어 모델 정의,-0.05776247,0.0,0.057762470096349716
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,기본 경험,-0.06494001,0.0,0.06494001299142838
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,답변 실패,0.8894877,1.0,0.11051231622695923
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,딥러닝,-0.03169297,0.0,0.03169297054409981
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,마지막 할 말,-0.013676331,0.0,0.013676331378519535
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,머신러닝,-0.024202136,0.0,0.02420213632285595
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,면접 시작 인사,-0.040918056,0.0,0.040918055921792984
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,상세 경험,-0.050025504,0.0,0.05002550408244133
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,수식,-0.061040577,0.0,0.06104057654738426
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,인공지능,-0.008236715,0.0,0.008236714638769627
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,잠시 휴식,-0.0042846636,0.0,0.004284663591533899
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,좋아하는 아이돌,-0.027653884,0.0,0.027653884142637253
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,핵심 아이디어,-0.04408982,0.0,0.044089820235967636
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,확률 예측에서 MSE Loss 미 사용 이유,0.07991483,0.0,0.07991483062505722
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,BCE 가 좋은 task,-0.018956322,0.0,0.018956322222948074
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,"BCE 가 좋은 task, 이유",-0.006099077,0.0,0.006099076941609383
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,LLM Fine-Tuning 의 PEFT,0.010090174,0.0,0.010090174153447151
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,LoRA,-0.011566005,0.0,0.01156600471585989
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,LoRA 와 QLoRA 의 차이,-0.002888973,0.0,0.0028889731038361788
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,Loss Function 예시,0.016691383,0.0,0.016691382974386215
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,Loss Function 정의,-0.031984314,0.0,0.03198431432247162
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,MBTI,-0.014223213,0.0,0.01422321330755949
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,MSE Loss 설명,0.03228844,0.0,0.03228843957185745
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,MSE Loss 용도,0.033496983,0.0,0.03349698334932327
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,Multi-Label 에서 CE + Softmax 적용 문제점,-0.05698431,0.0,0.0569843091070652
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,PEFT 방법 5가지,0.024156168,0.0,0.02415616810321808
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,거대 언어 모델 정의,-0.017404918,0.0,0.017404917627573013
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,기본 경험,-0.0068753697,0.0,0.006875369697809219
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,답변 실패,0.06867808,0.0,0.06867808103561401
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,딥러닝,0.0037225776,0.0,0.003722577588632703
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,마지막 할 말,0.0057814075,0.0,0.005781407468020916
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,머신러닝,0.007697919,0.0,0.00769791891798377
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,면접 시작 인사,0.0010562857,0.0,0.001056285691447556
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,상세 경험,0.03608416,0.0,0.03608416020870209
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,수식,-0.033362158,0.0,0.03336215764284134
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,인공지능,0.013254806,0.0,0.013254806399345398
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,잠시 휴식,0.00058546395,0.0,0.0005854639457538724
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,좋아하는 아이돌,0.0040997155,0.0,0.004099715501070023
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,핵심 아이디어,0.0054260306,0.0,0.005426030606031418
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,확률 예측에서 MSE Loss 미 사용 이유,0.88628787,1.0,0.1137121319770813
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,BCE 가 좋은 task,0.02485903,0.0,0.024859029799699783
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,"BCE 가 좋은 task, 이유",0.012197169,0.0,0.012197168543934822
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,LLM Fine-Tuning 의 PEFT,0.040893152,0.0,0.040893152356147766
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,LoRA,-0.008145308,0.0,0.008145308122038841
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,LoRA 와 QLoRA 의 차이,-0.007826668,0.0,0.007826668210327625
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,Loss Function 예시,-0.03780076,0.0,0.037800759077072144
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,Loss Function 정의,-0.028470604,0.0,0.028470603749155998
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,MBTI,-0.0076577603,0.0,0.007657760288566351
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,MSE Loss 설명,-0.030386768,0.0,0.03038676828145981
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,MSE Loss 용도,-0.022215322,0.0,0.02221532166004181
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,Multi-Label 에서 CE + Softmax 적용 문제점,-0.043559838,0.0,0.04355983808636665
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,PEFT 방법 5가지,0.05564522,0.0,0.05564521998167038
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,거대 언어 모델 정의,-0.05784118,0.0,0.05784118175506592
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,기본 경험,-0.03965634,0.0,0.03965634107589722
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,답변 실패,0.71113497,0.0,0.7111349701881409
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,딥러닝,-0.03171656,0.0,0.03171655908226967
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,마지막 할 말,-0.038812805,0.0,0.03881280496716499
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,머신러닝,-0.045133878,0.0,0.04513387754559517
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,면접 시작 인사,-0.045303363,0.0,0.04530336335301399
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,상세 경험,-0.040862773,0.0,0.04086277261376381
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,수식,0.19910476,1.0,0.8008952438831329
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,인공지능,0.08059655,0.0,0.08059655129909515
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,잠시 휴식,-0.013902325,0.0,0.013902325183153152
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,좋아하는 아이돌,-0.01099119,0.0,0.010991189628839493
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,핵심 아이디어,-0.025157344,0.0,0.025157343596220016
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,확률 예측에서 MSE Loss 미 사용 이유,-0.04223879,0.0,0.04223879054188728
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",BCE 가 좋은 task,0.04866478,0.0,0.04866477847099304
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거","BCE 가 좋은 task, 이유",0.044950258,0.0,0.04495025798678398
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",LLM Fine-Tuning 의 PEFT,0.03470185,0.0,0.03470185026526451
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",LoRA,-0.10051487,0.0,0.10051486641168594
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",LoRA 와 QLoRA 의 차이,0.025218967,0.0,0.025218967348337173
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",Loss Function 예시,-0.038902085,0.0,0.03890208527445793
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",Loss Function 정의,0.025876,0.0,0.0258760005235672
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",MBTI,-0.058678336,0.0,0.058678336441516876
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",MSE Loss 설명,0.01610905,0.0,0.016109049320220947
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",MSE Loss 용도,0.017179154,0.0,0.017179153859615326
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",Multi-Label 에서 CE + Softmax 적용 문제점,-0.017761046,0.0,0.017761046066880226
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",PEFT 방법 5가지,0.039791062,0.0,0.03979106247425079
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",거대 언어 모델 정의,-0.0051728417,0.0,0.005172841716557741
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",기본 경험,0.021918342,0.0,0.021918341517448425
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",답변 실패,-0.05111102,0.0,0.051111020147800446
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",딥러닝,0.037135925,0.0,0.03713592514395714
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",마지막 할 말,0.03245315,0.0,0.03245314955711365
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",머신러닝,-0.057486195,0.0,0.057486195117235184
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",면접 시작 인사,-0.028424459,0.0,0.02842445857822895
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",상세 경험,0.011645125,0.0,0.011645125225186348
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",수식,0.15109476,0.0,0.15109476447105408
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",인공지능,0.052436143,0.0,0.05243614315986633
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",잠시 휴식,0.008547024,0.0,0.008547023870050907
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",좋아하는 아이돌,0.05127201,0.0,0.05127200856804848
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",핵심 아이디어,0.63905776,1.0,0.3609422445297241
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",확률 예측에서 MSE Loss 미 사용 이유,-0.07986648,0.0,0.07986647635698318
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,BCE 가 좋은 task,0.06357423,0.0,0.06357423216104507
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,"BCE 가 좋은 task, 이유",0.058051523,0.0,0.05805152282118797
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,LLM Fine-Tuning 의 PEFT,-0.09310312,0.0,0.09310311824083328
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,LoRA,-0.119417466,0.0,0.1194174662232399
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,LoRA 와 QLoRA 의 차이,-0.076908655,0.0,0.0769086554646492
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,Loss Function 예시,-0.0182104,0.0,0.01821039989590645
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,Loss Function 정의,0.055581775,0.0,0.055581774562597275
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,MBTI,-0.05823811,0.0,0.058238111436367035
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,MSE Loss 설명,0.07377141,0.0,0.0737714096903801
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,MSE Loss 용도,0.07865878,0.0,0.07865878194570541
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,Multi-Label 에서 CE + Softmax 적용 문제점,0.06795064,0.0,0.06795064359903336
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,PEFT 방법 5가지,0.085453756,0.0,0.08545375615358353
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,거대 언어 모델 정의,0.027946362,0.0,0.027946362271904945
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,기본 경험,0.042856593,0.0,0.04285659268498421
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,답변 실패,0.24943598,1.0,0.7505640238523483
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,딥러닝,0.047662605,0.0,0.047662604600191116
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,마지막 할 말,-0.051850263,0.0,0.05185026302933693
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,머신러닝,-0.044864457,0.0,0.044864457100629807
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,면접 시작 인사,-0.0663684,0.0,0.06636840105056763
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,상세 경험,-0.038807012,0.0,0.038807012140750885
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,수식,0.13759944,0.0,0.13759943842887878
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,인공지능,0.058674537,0.0,0.058674536645412445
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,잠시 휴식,-0.009142967,0.0,0.009142966940999031
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,좋아하는 아이돌,0.019923445,0.0,0.019923444837331772
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,핵심 아이디어,0.4536188,0.0,0.4536187946796417
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,확률 예측에서 MSE Loss 미 사용 이유,-0.16921864,0.0,0.16921864449977875
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",BCE 가 좋은 task,0.046960607,0.0,0.046960607171058655
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고","BCE 가 좋은 task, 이유",0.05381095,0.0,0.05381095036864281
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",LLM Fine-Tuning 의 PEFT,0.011953942,0.0,0.01195394154638052
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",LoRA,-0.108760096,0.0,0.10876009613275528
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",LoRA 와 QLoRA 의 차이,0.010304576,0.0,0.01030457578599453
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",Loss Function 예시,-0.047046248,0.0,0.047046247869729996
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",Loss Function 정의,0.022281034,0.0,0.022281033918261528
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",MBTI,-0.08348462,0.0,0.08348461985588074
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",MSE Loss 설명,0.005111698,0.0,0.0051116980612277985
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",MSE Loss 용도,-0.0018851586,0.0,0.0018851585919037461
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",Multi-Label 에서 CE + Softmax 적용 문제점,0.017212551,0.0,0.017212551087141037
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",PEFT 방법 5가지,0.043220885,0.0,0.0432208850979805
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",거대 언어 모델 정의,-0.010524015,0.0,0.010524014942348003
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",기본 경험,0.044240918,0.0,0.04424091801047325
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",답변 실패,-0.043738257,0.0,0.04373825713992119
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",딥러닝,0.057946242,0.0,0.05794624239206314
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",마지막 할 말,0.014897701,0.0,0.01489770133048296
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",머신러닝,-0.050895892,0.0,0.05089589208364487
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",면접 시작 인사,-0.028337378,0.0,0.028337378054857254
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",상세 경험,0.016406419,0.0,0.01640641875565052
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",수식,0.16769272,0.0,0.16769272089004517
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",인공지능,0.063457444,0.0,0.0634574443101883
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",잠시 휴식,0.0016981416,0.0,0.0016981415683403611
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",좋아하는 아이돌,0.04766818,0.0,0.047668181359767914
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",핵심 아이디어,0.61688733,1.0,0.38311266899108887
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",확률 예측에서 MSE Loss 미 사용 이유,-0.097683296,0.0,0.09768329560756683
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",BCE 가 좋은 task,0.29742777,0.0,0.297427773475647
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!","BCE 가 좋은 task, 이유",0.25967148,0.0,0.25967147946357727
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",LLM Fine-Tuning 의 PEFT,0.030847386,0.0,0.03084738552570343
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",LoRA,-0.013116303,0.0,0.013116302900016308
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",LoRA 와 QLoRA 의 차이,0.008812104,0.0,0.008812104351818562
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",Loss Function 예시,-0.007916194,0.0,0.007916194386780262
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",Loss Function 정의,-0.0016605428,0.0,0.0016605427954345942
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",MBTI,0.009959598,0.0,0.009959598071873188
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",MSE Loss 설명,0.021399874,0.0,0.02139987424015999
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",MSE Loss 용도,0.035839643,0.0,0.03583964332938194
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",Multi-Label 에서 CE + Softmax 적용 문제점,-0.0058637494,0.0,0.005863749422132969
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",PEFT 방법 5가지,0.06944973,0.0,0.06944973021745682
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",거대 언어 모델 정의,-0.006836084,0.0,0.006836084183305502
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",기본 경험,0.006317239,0.0,0.006317238789051771
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",답변 실패,0.3340639,1.0,0.6659361124038696
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",딥러닝,-0.016589617,0.0,0.016589617356657982
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",마지막 할 말,-0.017983349,0.0,0.017983349040150642
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",머신러닝,-0.030630391,0.0,0.030630391091108322
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",면접 시작 인사,-0.022734988,0.0,0.02273498848080635
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",상세 경험,0.014605006,0.0,0.014605006203055382
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",수식,-0.01991477,0.0,0.019914770498871803
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",인공지능,0.08505089,0.0,0.08505088835954666
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",잠시 휴식,0.022112058,0.0,0.022112058475613594
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",좋아하는 아이돌,-0.011533423,0.0,0.011533423326909542
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",핵심 아이디어,-0.036890753,0.0,0.036890752613544464
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",확률 예측에서 MSE Loss 미 사용 이유,-0.020348165,0.0,0.020348165184259415
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",BCE 가 좋은 task,0.11279478,1.0,0.887205220758915
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어","BCE 가 좋은 task, 이유",0.087804124,0.0,0.08780412375926971
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",LLM Fine-Tuning 의 PEFT,0.020012114,0.0,0.020012114197015762
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",LoRA,-0.01585062,0.0,0.015850620344281197
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",LoRA 와 QLoRA 의 차이,0.0059555117,0.0,0.005955511704087257
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",Loss Function 예시,-0.051760517,0.0,0.05176051706075668
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",Loss Function 정의,-0.022218058,0.0,0.02221805788576603
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",MBTI,-0.022532752,0.0,0.0225327517837286
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",MSE Loss 설명,-0.027549312,0.0,0.02754931151866913
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",MSE Loss 용도,-0.012714078,0.0,0.012714077718555927
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",Multi-Label 에서 CE + Softmax 적용 문제점,-0.020713773,0.0,0.020713772624731064
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",PEFT 방법 5가지,0.043565724,0.0,0.04356572404503822
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",거대 언어 모델 정의,-0.02051074,0.0,0.02051074057817459
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",기본 경험,-0.036717054,0.0,0.03671705350279808
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",답변 실패,0.7622828,0.0,0.7622827887535095
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",딥러닝,-0.030597305,0.0,0.030597304925322533
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",마지막 할 말,-0.033890747,0.0,0.0338907465338707
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",머신러닝,-0.018085362,0.0,0.018085362389683723
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",면접 시작 인사,-0.04272268,0.0,0.0427226796746254
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",상세 경험,-0.016430864,0.0,0.016430864110589027
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",수식,-0.06777964,0.0,0.06777963787317276
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",인공지능,0.020932954,0.0,0.02093295380473137
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",잠시 휴식,0.0010807932,0.0,0.0010807932121679187
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",좋아하는 아이돌,-0.024990985,0.0,0.024990985170006752
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",핵심 아이디어,-0.046452057,0.0,0.046452056616544724
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",확률 예측에서 MSE Loss 미 사용 이유,-0.029593144,0.0,0.029593143612146378
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",BCE 가 좋은 task,0.41527018,0.0,0.4152701795101166
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아","BCE 가 좋은 task, 이유",0.42412522,1.0,0.5758747756481171
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",LLM Fine-Tuning 의 PEFT,0.0047883806,0.0,0.004788380581885576
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",LoRA,-0.0018009227,0.0,0.0018009226769208908
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",LoRA 와 QLoRA 의 차이,-0.0008705165,0.0,0.0008705165237188339
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",Loss Function 예시,0.02407406,0.0,0.02407406084239483
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",Loss Function 정의,-0.034576938,0.0,0.034576937556266785
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",MBTI,-0.05366963,0.0,0.053669631481170654
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",MSE Loss 설명,0.05882454,0.0,0.05882453918457031
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",MSE Loss 용도,0.05116213,0.0,0.05116213113069534
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",Multi-Label 에서 CE + Softmax 적용 문제점,0.059933327,0.0,0.05993332713842392
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",PEFT 방법 5가지,-0.009421564,0.0,0.00942156370729208
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",거대 언어 모델 정의,-0.0029972626,0.0,0.0029972626361995935
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",기본 경험,-0.03689407,0.0,0.036894071847200394
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",답변 실패,0.048036058,0.0,0.048036057502031326
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",딥러닝,0.008565531,0.0,0.008565531112253666
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",마지막 할 말,-0.023564732,0.0,0.02356473170220852
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",머신러닝,-0.045967355,0.0,0.045967355370521545
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",면접 시작 인사,-0.042716645,0.0,0.04271664470434189
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",상세 경험,0.032516398,0.0,0.032516397535800934
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",수식,0.004946696,0.0,0.0049466961063444614
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",인공지능,0.053133275,0.0,0.053133275359869
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",잠시 휴식,-0.0027784891,0.0,0.0027784891426563263
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",좋아하는 아이돌,-0.007298537,0.0,0.007298537064343691
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",핵심 아이디어,-0.021740634,0.0,0.021740633994340897
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",확률 예측에서 MSE Loss 미 사용 이유,0.02895938,0.0,0.028959380462765694
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,BCE 가 좋은 task,0.015278276,0.0,0.015278276056051254
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,"BCE 가 좋은 task, 이유",0.026691444,0.0,0.026691444218158722
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,LLM Fine-Tuning 의 PEFT,-0.07247325,0.0,0.07247325032949448
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,LoRA,0.016037354,0.0,0.0160373542457819
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,LoRA 와 QLoRA 의 차이,-0.008324609,0.0,0.008324609138071537
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,Loss Function 예시,0.009970972,0.0,0.009970972314476967
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,Loss Function 정의,-0.072371304,0.0,0.07237130403518677
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,MBTI,-0.048053708,0.0,0.04805370792746544
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,MSE Loss 설명,0.008113176,0.0,0.008113175630569458
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,MSE Loss 용도,-0.014776533,0.0,0.014776532538235188
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,Multi-Label 에서 CE + Softmax 적용 문제점,0.8794795,1.0,0.12052047252655029
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,PEFT 방법 5가지,-0.014462405,0.0,0.014462404884397984
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,거대 언어 모델 정의,0.0366556,0.0,0.03665560111403465
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,기본 경험,-0.016826138,0.0,0.016826137900352478
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,답변 실패,-0.012015234,0.0,0.012015233747661114
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,딥러닝,-0.035671845,0.0,0.03567184507846832
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,마지막 할 말,0.017390573,0.0,0.017390573397278786
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,머신러닝,-0.024127088,0.0,0.024127088487148285
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,면접 시작 인사,0.016046472,0.0,0.016046471893787384
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,상세 경험,-0.042064227,0.0,0.042064227163791656
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,수식,0.023778034,0.0,0.02377803437411785
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,인공지능,-0.0041065086,0.0,0.004106508567929268
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,잠시 휴식,0.00072376884,0.0,0.0007237688405439258
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,좋아하는 아이돌,-0.0052345777,0.0,0.005234577693045139
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,핵심 아이디어,-0.01785077,0.0,0.01785076968371868
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,확률 예측에서 MSE Loss 미 사용 이유,-0.011917423,0.0,0.011917423456907272
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,BCE 가 좋은 task,0.008221296,0.0,0.008221295662224293
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,"BCE 가 좋은 task, 이유",0.0009840475,0.0,0.0009840475395321846
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,LLM Fine-Tuning 의 PEFT,-0.0010096637,0.0,0.0010096636833623052
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,LoRA,0.0146332225,0.0,0.014633222483098507
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,LoRA 와 QLoRA 의 차이,-0.0034756274,0.0,0.0034756273962557316
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,Loss Function 예시,-0.041661717,0.0,0.041661716997623444
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,Loss Function 정의,-0.03450963,0.0,0.034509629011154175
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,MBTI,-0.019040087,0.0,0.01904008723795414
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,MSE Loss 설명,-0.052806318,0.0,0.052806317806243896
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,MSE Loss 용도,-0.037707914,0.0,0.03770791366696358
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,Multi-Label 에서 CE + Softmax 적용 문제점,0.06505364,0.0,0.06505364179611206
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,PEFT 방법 5가지,0.099402614,0.0,0.09940261393785477
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,거대 언어 모델 정의,-0.06410087,0.0,0.06410086899995804
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,기본 경험,-0.029854069,0.0,0.029854068532586098
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,답변 실패,0.89184904,1.0,0.10815095901489258
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,딥러닝,-0.023628607,0.0,0.023628607392311096
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,마지막 할 말,-0.013566285,0.0,0.013566285371780396
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,머신러닝,-0.017363552,0.0,0.017363552004098892
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,면접 시작 인사,-0.026170256,0.0,0.02617025561630726
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,상세 경험,-0.040814985,0.0,0.04081498458981514
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,수식,-0.08094007,0.0,0.08094006776809692
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,인공지능,0.0024873035,0.0,0.0024873034562915564
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,잠시 휴식,-0.010103719,0.0,0.010103719308972359
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,좋아하는 아이돌,0.0017365816,0.0,0.0017365815583616495
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,핵심 아이디어,-0.028261675,0.0,0.02826167456805706
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,확률 예측에서 MSE Loss 미 사용 이유,-0.054482423,0.0,0.05448242276906967
Loss Function 관련 실무 경험 -> 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,BCE 가 좋은 task,0.009178025,0.0,0.009178024716675282
Loss Function 관련 실무 경험 -> 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,"BCE 가 좋은 task, 이유",0.009700813,0.0,0.009700813330709934
Loss Function 관련 실무 경험 -> 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,LLM Fine-Tuning 의 PEFT,-0.06379768,0.0,0.06379768252372742
Loss Function 관련 실무 경험 -> 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,LoRA,-0.028409578,0.0,0.028409577906131744
Loss Function 관련 실무 경험 -> 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,LoRA 와 QLoRA 의 차이,-0.0001622927,0.0,0.0001622926938580349
Loss Function 관련 실무 경험 -> 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,Loss Function 예시,0.12678559,0.0,0.12678559124469757
Loss Function 관련 실무 경험 -> 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,Loss Function 정의,0.010093028,0.0,0.010093027725815773
Loss Function 관련 실무 경험 -> 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,MBTI,-0.052973848,0.0,0.05297384783625603
Loss Function 관련 실무 경험 -> 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,MSE Loss 설명,0.0273825,0.0,0.0273825004696846
Loss Function 관련 실무 경험 -> 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,MSE Loss 용도,0.0101169925,0.0,0.010116992518305779
Loss Function 관련 실무 경험 -> 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,Multi-Label 에서 CE + Softmax 적용 문제점,-0.0039819786,0.0,0.003981978632509708
Loss Function 관련 실무 경험 -> 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,PEFT 방법 5가지,0.032089967,0.0,0.0320899672806263
Loss Function 관련 실무 경험 -> 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,거대 언어 모델 정의,-0.016902147,0.0,0.016902146860957146
Loss Function 관련 실무 경험 -> 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,기본 경험,0.5363519,1.0,0.46364808082580566
Loss Function 관련 실무 경험 -> 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,답변 실패,-0.091598675,0.0,0.09159867465496063
Loss Function 관련 실무 경험 -> 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,딥러닝,0.0065884935,0.0,0.006588493473827839
Loss Function 관련 실무 경험 -> 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,마지막 할 말,-0.04752325,0.0,0.04752324894070625
Loss Function 관련 실무 경험 -> 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,머신러닝,0.020821083,0.0,0.020821083337068558
Loss Function 관련 실무 경험 -> 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,면접 시작 인사,-0.0061131897,0.0,0.006113189738243818
Loss Function 관련 실무 경험 -> 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,상세 경험,0.34680808,0.0,0.3468080759048462
Loss Function 관련 실무 경험 -> 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,수식,-0.0111749545,0.0,0.01117495447397232
Loss Function 관련 실무 경험 -> 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,인공지능,-0.0061573125,0.0,0.0061573125422000885
Loss Function 관련 실무 경험 -> 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,잠시 휴식,0.023434322,0.0,0.023434322327375412
Loss Function 관련 실무 경험 -> 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,좋아하는 아이돌,0.013054491,0.0,0.013054491020739079
Loss Function 관련 실무 경험 -> 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,핵심 아이디어,0.011660319,0.0,0.011660318821668625
Loss Function 관련 실무 경험 -> 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,확률 예측에서 MSE Loss 미 사용 이유,-0.0114459405,0.0,0.011445940472185612
Loss Function 관련 실무 경험 -> 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,BCE 가 좋은 task,0.043190014,0.0,0.043190013617277145
Loss Function 관련 실무 경험 -> 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,"BCE 가 좋은 task, 이유",0.04340895,0.0,0.04340894892811775
Loss Function 관련 실무 경험 -> 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,LLM Fine-Tuning 의 PEFT,-0.0668052,0.0,0.06680519878864288
Loss Function 관련 실무 경험 -> 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,LoRA,-0.021570006,0.0,0.021570006385445595
Loss Function 관련 실무 경험 -> 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,LoRA 와 QLoRA 의 차이,0.027848119,0.0,0.027848118916153908
Loss Function 관련 실무 경험 -> 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,Loss Function 예시,-0.052319303,0.0,0.052319303154945374
Loss Function 관련 실무 경험 -> 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,Loss Function 정의,-0.04734193,0.0,0.047341931611299515
Loss Function 관련 실무 경험 -> 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,MBTI,0.00420645,0.0,0.004206450190395117
Loss Function 관련 실무 경험 -> 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,MSE Loss 설명,0.000193506,0.0,0.00019350599905010313
Loss Function 관련 실무 경험 -> 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,MSE Loss 용도,0.00019632539,0.0,0.00019632538896985352
Loss Function 관련 실무 경험 -> 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,Multi-Label 에서 CE + Softmax 적용 문제점,-0.002880647,0.0,0.0028806470800191164
Loss Function 관련 실무 경험 -> 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,PEFT 방법 5가지,0.041858405,0.0,0.041858404874801636
Loss Function 관련 실무 경험 -> 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,거대 언어 모델 정의,-0.015469966,0.0,0.01546996645629406
Loss Function 관련 실무 경험 -> 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,기본 경험,0.6142826,0.0,0.6142826080322266
Loss Function 관련 실무 경험 -> 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,답변 실패,0.1591137,0.0,0.15911370515823364
Loss Function 관련 실무 경험 -> 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,딥러닝,-0.0114265885,0.0,0.011426588520407677
Loss Function 관련 실무 경험 -> 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,마지막 할 말,-0.006597017,0.0,0.00659701693803072
Loss Function 관련 실무 경험 -> 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,머신러닝,-0.013803251,0.0,0.013803251087665558
Loss Function 관련 실무 경험 -> 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,면접 시작 인사,-0.0068861954,0.0,0.00688619539141655
Loss Function 관련 실무 경험 -> 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,상세 경험,0.31488988,1.0,0.6851101219654083
Loss Function 관련 실무 경험 -> 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,수식,-0.0048433426,0.0,0.004843342583626509
Loss Function 관련 실무 경험 -> 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,인공지능,0.042100005,0.0,0.042100004851818085
Loss Function 관련 실무 경험 -> 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,잠시 휴식,-0.0029549662,0.0,0.0029549661558121443
Loss Function 관련 실무 경험 -> 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,좋아하는 아이돌,0.0417156,0.0,0.0417155995965004
Loss Function 관련 실무 경험 -> 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,핵심 아이디어,-0.0074083568,0.0,0.007408356759697199
Loss Function 관련 실무 경험 -> 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,확률 예측에서 MSE Loss 미 사용 이유,-0.0479822,0.0,0.04798220098018646
Loss Function 관련 실무 경험 -> 근데 기억 진짜 안나는데 어떡하지?,BCE 가 좋은 task,-0.010986302,0.0,0.01098630204796791
Loss Function 관련 실무 경험 -> 근데 기억 진짜 안나는데 어떡하지?,"BCE 가 좋은 task, 이유",-0.017423918,0.0,0.01742391847074032
Loss Function 관련 실무 경험 -> 근데 기억 진짜 안나는데 어떡하지?,LLM Fine-Tuning 의 PEFT,0.002609831,0.0,0.0026098310481756926
Loss Function 관련 실무 경험 -> 근데 기억 진짜 안나는데 어떡하지?,LoRA,0.013999431,0.0,0.013999431394040585
Loss Function 관련 실무 경험 -> 근데 기억 진짜 안나는데 어떡하지?,LoRA 와 QLoRA 의 차이,-0.013302975,0.0,0.013302975334227085
Loss Function 관련 실무 경험 -> 근데 기억 진짜 안나는데 어떡하지?,Loss Function 예시,-0.023832662,0.0,0.023832662031054497
Loss Function 관련 실무 경험 -> 근데 기억 진짜 안나는데 어떡하지?,Loss Function 정의,-0.03096444,0.0,0.03096443973481655
Loss Function 관련 실무 경험 -> 근데 기억 진짜 안나는데 어떡하지?,MBTI,-0.0027917037,0.0,0.0027917036786675453
Loss Function 관련 실무 경험 -> 근데 기억 진짜 안나는데 어떡하지?,MSE Loss 설명,-0.05433185,0.0,0.05433185026049614
Loss Function 관련 실무 경험 -> 근데 기억 진짜 안나는데 어떡하지?,MSE Loss 용도,-0.035853855,0.0,0.035853855311870575
Loss Function 관련 실무 경험 -> 근데 기억 진짜 안나는데 어떡하지?,Multi-Label 에서 CE + Softmax 적용 문제점,-0.0034361014,0.0,0.0034361013676971197
Loss Function 관련 실무 경험 -> 근데 기억 진짜 안나는데 어떡하지?,PEFT 방법 5가지,0.06190245,0.0,0.061902448534965515
Loss Function 관련 실무 경험 -> 근데 기억 진짜 안나는데 어떡하지?,거대 언어 모델 정의,-0.038585145,0.0,0.03858514502644539
Loss Function 관련 실무 경험 -> 근데 기억 진짜 안나는데 어떡하지?,기본 경험,0.01596229,0.0,0.01596228964626789
Loss Function 관련 실무 경험 -> 근데 기억 진짜 안나는데 어떡하지?,답변 실패,0.9174599,1.0,0.08254009485244751
Loss Function 관련 실무 경험 -> 근데 기억 진짜 안나는데 어떡하지?,딥러닝,-0.021915346,0.0,0.021915346384048462
Loss Function 관련 실무 경험 -> 근데 기억 진짜 안나는데 어떡하지?,마지막 할 말,-0.032938734,0.0,0.03293873369693756
Loss Function 관련 실무 경험 -> 근데 기억 진짜 안나는데 어떡하지?,머신러닝,-0.0113883875,0.0,0.011388387531042099
Loss Function 관련 실무 경험 -> 근데 기억 진짜 안나는데 어떡하지?,면접 시작 인사,-0.021911472,0.0,0.02191147208213806
Loss Function 관련 실무 경험 -> 근데 기억 진짜 안나는데 어떡하지?,상세 경험,-0.031329636,0.0,0.03132963553071022
Loss Function 관련 실무 경험 -> 근데 기억 진짜 안나는데 어떡하지?,수식,-0.06904153,0.0,0.06904152780771255
Loss Function 관련 실무 경험 -> 근데 기억 진짜 안나는데 어떡하지?,인공지능,-0.008176155,0.0,0.008176155388355255
Loss Function 관련 실무 경험 -> 근데 기억 진짜 안나는데 어떡하지?,잠시 휴식,-0.013343574,0.0,0.013343573547899723
Loss Function 관련 실무 경험 -> 근데 기억 진짜 안나는데 어떡하지?,좋아하는 아이돌,-0.0043367906,0.0,0.004336790647357702
Loss Function 관련 실무 경험 -> 근데 기억 진짜 안나는데 어떡하지?,핵심 아이디어,-0.02461998,0.0,0.02461997978389263
Loss Function 관련 실무 경험 -> 근데 기억 진짜 안나는데 어떡하지?,확률 예측에서 MSE Loss 미 사용 이유,-0.037344594,0.0,0.037344593554735184
Loss Function 관련 실무 경험 -> 나 아직 실무 경험 없는데 ㅠㅠ,BCE 가 좋은 task,-0.004105465,0.0,0.004105465020984411
Loss Function 관련 실무 경험 -> 나 아직 실무 경험 없는데 ㅠㅠ,"BCE 가 좋은 task, 이유",-0.009038949,0.0,0.009038949385285378
Loss Function 관련 실무 경험 -> 나 아직 실무 경험 없는데 ㅠㅠ,LLM Fine-Tuning 의 PEFT,-0.030900177,0.0,0.030900176614522934
Loss Function 관련 실무 경험 -> 나 아직 실무 경험 없는데 ㅠㅠ,LoRA,-0.015060943,0.0,0.015060942620038986
Loss Function 관련 실무 경험 -> 나 아직 실무 경험 없는데 ㅠㅠ,LoRA 와 QLoRA 의 차이,-0.008184242,0.0,0.008184242062270641
Loss Function 관련 실무 경험 -> 나 아직 실무 경험 없는데 ㅠㅠ,Loss Function 예시,-0.041979194,0.0,0.041979193687438965
Loss Function 관련 실무 경험 -> 나 아직 실무 경험 없는데 ㅠㅠ,Loss Function 정의,-0.041162618,0.0,0.04116261750459671
Loss Function 관련 실무 경험 -> 나 아직 실무 경험 없는데 ㅠㅠ,MBTI,0.015690546,0.0,0.015690546482801437
Loss Function 관련 실무 경험 -> 나 아직 실무 경험 없는데 ㅠㅠ,MSE Loss 설명,-0.018771928,0.0,0.018771927803754807
Loss Function 관련 실무 경험 -> 나 아직 실무 경험 없는데 ㅠㅠ,MSE Loss 용도,-0.01221214,0.0,0.012212139554321766
Loss Function 관련 실무 경험 -> 나 아직 실무 경험 없는데 ㅠㅠ,Multi-Label 에서 CE + Softmax 적용 문제점,-0.027966093,0.0,0.02796609327197075
Loss Function 관련 실무 경험 -> 나 아직 실무 경험 없는데 ㅠㅠ,PEFT 방법 5가지,0.018111834,0.0,0.018111834302544594
Loss Function 관련 실무 경험 -> 나 아직 실무 경험 없는데 ㅠㅠ,거대 언어 모델 정의,0.006020616,0.0,0.006020615808665752
Loss Function 관련 실무 경험 -> 나 아직 실무 경험 없는데 ㅠㅠ,기본 경험,0.6912263,1.0,0.3087736964225769
Loss Function 관련 실무 경험 -> 나 아직 실무 경험 없는데 ㅠㅠ,답변 실패,0.24149004,0.0,0.24149003624916077
Loss Function 관련 실무 경험 -> 나 아직 실무 경험 없는데 ㅠㅠ,딥러닝,-0.049074043,0.0,0.049074042588472366
Loss Function 관련 실무 경험 -> 나 아직 실무 경험 없는데 ㅠㅠ,마지막 할 말,-0.044504266,0.0,0.04450426623225212
Loss Function 관련 실무 경험 -> 나 아직 실무 경험 없는데 ㅠㅠ,머신러닝,-0.015928714,0.0,0.015928713604807854
Loss Function 관련 실무 경험 -> 나 아직 실무 경험 없는데 ㅠㅠ,면접 시작 인사,0.018930411,0.0,0.018930410966277122
Loss Function 관련 실무 경험 -> 나 아직 실무 경험 없는데 ㅠㅠ,상세 경험,0.23600353,0.0,0.23600353300571442
Loss Function 관련 실무 경험 -> 나 아직 실무 경험 없는데 ㅠㅠ,수식,-0.028697122,0.0,0.028697121888399124
Loss Function 관련 실무 경험 -> 나 아직 실무 경험 없는데 ㅠㅠ,인공지능,0.023574345,0.0,0.0235743448138237
Loss Function 관련 실무 경험 -> 나 아직 실무 경험 없는데 ㅠㅠ,잠시 휴식,0.0052227345,0.0,0.005222734529525042
Loss Function 관련 실무 경험 -> 나 아직 실무 경험 없는데 ㅠㅠ,좋아하는 아이돌,0.022077255,0.0,0.022077254951000214
Loss Function 관련 실무 경험 -> 나 아직 실무 경험 없는데 ㅠㅠ,핵심 아이디어,0.016379954,0.0,0.016379954293370247
Loss Function 관련 실무 경험 -> 나 아직 실무 경험 없는데 ㅠㅠ,확률 예측에서 MSE Loss 미 사용 이유,-0.036904484,0.0,0.036904484033584595
Loss Function 관련 실무 경험 -> 대답하기 싫은데 힝,BCE 가 좋은 task,-0.0051749866,0.0,0.005174986552447081
Loss Function 관련 실무 경험 -> 대답하기 싫은데 힝,"BCE 가 좋은 task, 이유",-0.01140248,0.0,0.01140248030424118
Loss Function 관련 실무 경험 -> 대답하기 싫은데 힝,LLM Fine-Tuning 의 PEFT,-0.015212484,0.0,0.015212483704090118
Loss Function 관련 실무 경험 -> 대답하기 싫은데 힝,LoRA,0.008449451,0.0,0.008449451066553593
Loss Function 관련 실무 경험 -> 대답하기 싫은데 힝,LoRA 와 QLoRA 의 차이,-0.009425108,0.0,0.009425108321011066
Loss Function 관련 실무 경험 -> 대답하기 싫은데 힝,Loss Function 예시,-0.040248234,0.0,0.04024823382496834
Loss Function 관련 실무 경험 -> 대답하기 싫은데 힝,Loss Function 정의,-0.038998052,0.0,0.03899805247783661
Loss Function 관련 실무 경험 -> 대답하기 싫은데 힝,MBTI,-0.005515054,0.0,0.005515053868293762
Loss Function 관련 실무 경험 -> 대답하기 싫은데 힝,MSE Loss 설명,-0.055317607,0.0,0.055317606776952744
Loss Function 관련 실무 경험 -> 대답하기 싫은데 힝,MSE Loss 용도,-0.04084127,0.0,0.04084127023816109
Loss Function 관련 실무 경험 -> 대답하기 싫은데 힝,Multi-Label 에서 CE + Softmax 적용 문제점,-0.008818238,0.0,0.008818238042294979
Loss Function 관련 실무 경험 -> 대답하기 싫은데 힝,PEFT 방법 5가지,0.047322113,0.0,0.0473221130669117
Loss Function 관련 실무 경험 -> 대답하기 싫은데 힝,거대 언어 모델 정의,-0.046166476,0.0,0.04616647586226463
Loss Function 관련 실무 경험 -> 대답하기 싫은데 힝,기본 경험,0.031264685,0.0,0.03126468509435654
Loss Function 관련 실무 경험 -> 대답하기 싫은데 힝,답변 실패,0.9087227,1.0,0.09127730131149292
Loss Function 관련 실무 경험 -> 대답하기 싫은데 힝,딥러닝,-0.025596907,0.0,0.02559690736234188
Loss Function 관련 실무 경험 -> 대답하기 싫은데 힝,마지막 할 말,-0.020259479,0.0,0.02025947906076908
Loss Function 관련 실무 경험 -> 대답하기 싫은데 힝,머신러닝,-0.023006711,0.0,0.023006711155176163
Loss Function 관련 실무 경험 -> 대답하기 싫은데 힝,면접 시작 인사,-0.0046469592,0.0,0.004646959248930216
Loss Function 관련 실무 경험 -> 대답하기 싫은데 힝,상세 경험,-0.015998615,0.0,0.015998614951968193
Loss Function 관련 실무 경험 -> 대답하기 싫은데 힝,수식,-0.051765744,0.0,0.051765743643045425
Loss Function 관련 실무 경험 -> 대답하기 싫은데 힝,인공지능,-0.0030417203,0.0,0.003041720250621438
Loss Function 관련 실무 경험 -> 대답하기 싫은데 힝,잠시 휴식,-0.014014443,0.0,0.014014443382620811
Loss Function 관련 실무 경험 -> 대답하기 싫은데 힝,좋아하는 아이돌,-0.008020906,0.0,0.008020905777812004
Loss Function 관련 실무 경험 -> 대답하기 싫은데 힝,핵심 아이디어,-0.030336123,0.0,0.03033612295985222
Loss Function 관련 실무 경험 -> 대답하기 싫은데 힝,확률 예측에서 MSE Loss 미 사용 이유,-0.051063146,0.0,0.05106314644217491
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,BCE 가 좋은 task,0.019242998,0.0,0.019242998212575912
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,"BCE 가 좋은 task, 이유",0.027065648,0.0,0.027065647765994072
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,LLM Fine-Tuning 의 PEFT,-0.06870673,0.0,0.06870672851800919
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,LoRA,-0.058292262,0.0,0.05829226225614548
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,LoRA 와 QLoRA 의 차이,0.017462779,0.0,0.017462778836488724
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,Loss Function 예시,-0.015121859,0.0,0.015121858566999435
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,Loss Function 정의,0.076156415,0.0,0.07615641504526138
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,MBTI,0.907295,1.0,0.09270501136779785
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,MSE Loss 설명,0.012370778,0.0,0.012370778247714043
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,MSE Loss 용도,0.035891622,0.0,0.03589162230491638
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,Multi-Label 에서 CE + Softmax 적용 문제점,-0.04816052,0.0,0.04816051945090294
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,PEFT 방법 5가지,0.032064334,0.0,0.03206433355808258
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,거대 언어 모델 정의,0.0035700349,0.0,0.0035700348671525717
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,기본 경험,-0.0046246974,0.0,0.004624697379767895
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,답변 실패,-0.06389914,0.0,0.06389913707971573
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,딥러닝,-0.03205906,0.0,0.03205905854701996
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,마지막 할 말,-0.047793306,0.0,0.04779330641031265
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,머신러닝,0.027175348,0.0,0.02717534825205803
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,면접 시작 인사,-0.0081288945,0.0,0.008128894492983818
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,상세 경험,-0.016779155,0.0,0.016779154539108276
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,수식,-0.06956435,0.0,0.0695643499493599
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,인공지능,-0.016414028,0.0,0.01641402766108513
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,잠시 휴식,-0.059211534,0.0,0.05921153351664543
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,좋아하는 아이돌,-0.010405824,0.0,0.010405823588371277
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,핵심 아이디어,0.012562384,0.0,0.012562383897602558
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,확률 예측에서 MSE Loss 미 사용 이유,-0.0036405902,0.0,0.0036405902355909348
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,BCE 가 좋은 task,0.007286331,0.0,0.00728633115068078
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,"BCE 가 좋은 task, 이유",0.019143475,0.0,0.019143475219607353
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,LLM Fine-Tuning 의 PEFT,0.027924182,0.0,0.027924181893467903
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,LoRA,0.0062039415,0.0,0.006203941535204649
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,LoRA 와 QLoRA 의 차이,0.07438919,0.0,0.07438918948173523
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,Loss Function 예시,0.03201652,0.0,0.03201651945710182
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,Loss Function 정의,-0.006061828,0.0,0.006061828229576349
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,MBTI,0.0079439925,0.0,0.007943992502987385
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,MSE Loss 설명,0.0005429474,0.0,0.0005429473822005093
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,MSE Loss 용도,-0.0072268182,0.0,0.007226818241178989
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,Multi-Label 에서 CE + Softmax 적용 문제점,0.009014692,0.0,0.009014692157506943
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,PEFT 방법 5가지,-0.004959949,0.0,0.00495994882658124
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,거대 언어 모델 정의,0.032919798,0.0,0.03291979804635048
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,기본 경험,-0.0012670761,0.0,0.001267076120711863
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,답변 실패,-0.03576534,0.0,0.03576533868908882
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,딥러닝,0.036800917,0.0,0.036800917237997055
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,마지막 할 말,-0.015404687,0.0,0.015404687263071537
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,머신러닝,-0.04344471,0.0,0.043444711714982986
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,면접 시작 인사,-0.010808239,0.0,0.010808238759636879
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,상세 경험,-0.029452497,0.0,0.029452497139573097
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,수식,0.011101304,0.0,0.01110130362212658
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,인공지능,0.023391822,0.0,0.02339182235300541
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,잠시 휴식,0.015048015,0.0,0.015048014931380749
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,좋아하는 아이돌,0.9400681,1.0,0.05993187427520752
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,핵심 아이디어,-0.008684251,0.0,0.008684251457452774
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,확률 예측에서 MSE Loss 미 사용 이유,0.0121221375,0.0,0.0121221374720335
잠시 휴식 -> 재미있는 이야기 해줄래?,BCE 가 좋은 task,0.020246891,0.0,0.02024689130485058
잠시 휴식 -> 재미있는 이야기 해줄래?,"BCE 가 좋은 task, 이유",0.022659415,0.0,0.022659415379166603
잠시 휴식 -> 재미있는 이야기 해줄래?,LLM Fine-Tuning 의 PEFT,0.008950632,0.0,0.008950632065534592
잠시 휴식 -> 재미있는 이야기 해줄래?,LoRA,-0.0013187968,0.0,0.0013187967706471682
잠시 휴식 -> 재미있는 이야기 해줄래?,LoRA 와 QLoRA 의 차이,-0.0020405306,0.0,0.0020405305549502373
잠시 휴식 -> 재미있는 이야기 해줄래?,Loss Function 예시,0.0118117565,0.0,0.011811756528913975
잠시 휴식 -> 재미있는 이야기 해줄래?,Loss Function 정의,-0.001841969,0.0,0.0018419689731672406
잠시 휴식 -> 재미있는 이야기 해줄래?,MBTI,-0.041210663,0.0,0.041210662573575974
잠시 휴식 -> 재미있는 이야기 해줄래?,MSE Loss 설명,0.014390587,0.0,0.014390586875379086
잠시 휴식 -> 재미있는 이야기 해줄래?,MSE Loss 용도,0.013548702,0.0,0.013548702001571655
잠시 휴식 -> 재미있는 이야기 해줄래?,Multi-Label 에서 CE + Softmax 적용 문제점,-0.0028706184,0.0,0.0028706183657050133
잠시 휴식 -> 재미있는 이야기 해줄래?,PEFT 방법 5가지,0.016441362,0.0,0.016441361978650093
잠시 휴식 -> 재미있는 이야기 해줄래?,거대 언어 모델 정의,0.0006007971,0.0,0.0006007971242070198
잠시 휴식 -> 재미있는 이야기 해줄래?,기본 경험,-0.027003875,0.0,0.027003875002264977
잠시 휴식 -> 재미있는 이야기 해줄래?,답변 실패,-0.026817884,0.0,0.026817884296178818
잠시 휴식 -> 재미있는 이야기 해줄래?,딥러닝,0.0007631983,0.0,0.0007631983025930822
잠시 휴식 -> 재미있는 이야기 해줄래?,마지막 할 말,0.015542483,0.0,0.01554248295724392
잠시 휴식 -> 재미있는 이야기 해줄래?,머신러닝,-0.001854053,0.0,0.0018540529999881983
잠시 휴식 -> 재미있는 이야기 해줄래?,면접 시작 인사,0.011864925,0.0,0.011864924803376198
잠시 휴식 -> 재미있는 이야기 해줄래?,상세 경험,-0.044170648,0.0,0.044170647859573364
잠시 휴식 -> 재미있는 이야기 해줄래?,수식,-0.053951673,0.0,0.053951673209667206
잠시 휴식 -> 재미있는 이야기 해줄래?,인공지능,-0.012940722,0.0,0.012940721586346626
잠시 휴식 -> 재미있는 이야기 해줄래?,잠시 휴식,0.9655465,1.0,0.034453511238098145
잠시 휴식 -> 재미있는 이야기 해줄래?,좋아하는 아이돌,0.0016357688,0.0,0.0016357688000425696
잠시 휴식 -> 재미있는 이야기 해줄래?,핵심 아이디어,-0.012513312,0.0,0.012513311579823494
잠시 휴식 -> 재미있는 이야기 해줄래?,확률 예측에서 MSE Loss 미 사용 이유,0.0014617787,0.0,0.0014617786509916186
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",BCE 가 좋은 task,-0.010695187,0.0,0.010695187374949455
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야","BCE 가 좋은 task, 이유",0.012082225,0.0,0.01208222471177578
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",LLM Fine-Tuning 의 PEFT,0.8152356,1.0,0.18476438522338867
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",LoRA,-0.0074710352,0.0,0.007471035234630108
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",LoRA 와 QLoRA 의 차이,0.00971279,0.0,0.00971279013901949
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",Loss Function 예시,-0.06731228,0.0,0.06731227785348892
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",Loss Function 정의,-0.07569839,0.0,0.07569839060306549
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",MBTI,-0.07597772,0.0,0.07597772032022476
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",MSE Loss 설명,-0.009982144,0.0,0.009982144460082054
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",MSE Loss 용도,-0.0299378,0.0,0.029937800019979477
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",Multi-Label 에서 CE + Softmax 적용 문제점,0.0094970325,0.0,0.00949703250080347
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",PEFT 방법 5가지,0.043278184,0.0,0.04327818378806114
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",거대 언어 모델 정의,0.010437094,0.0,0.010437093675136566
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",기본 경험,-0.009138067,0.0,0.009138067252933979
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",답변 실패,-0.035931442,0.0,0.03593144193291664
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",딥러닝,-0.045899365,0.0,0.04589936509728432
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",마지막 할 말,0.024174713,0.0,0.024174712598323822
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",머신러닝,-0.052110527,0.0,0.05211052671074867
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",면접 시작 인사,-0.052289642,0.0,0.05228964239358902
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",상세 경험,0.07048576,0.0,0.07048576325178146
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",수식,0.048155297,0.0,0.048155296593904495
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",인공지능,0.016068336,0.0,0.016068335622549057
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",잠시 휴식,0.024671001,0.0,0.024671001359820366
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",좋아하는 아이돌,0.002797004,0.0,0.0027970040682703257
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",핵심 아이디어,0.066306174,0.0,0.06630617380142212
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",확률 예측에서 MSE Loss 미 사용 이유,0.0066369963,0.0,0.006636996287852526
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,BCE 가 좋은 task,0.0018169774,0.0,0.0018169773975387216
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,"BCE 가 좋은 task, 이유",-0.010363053,0.0,0.010363052599132061
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,LLM Fine-Tuning 의 PEFT,0.25783882,0.0,0.25783881545066833
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,LoRA,-0.01592751,0.0,0.01592751033604145
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,LoRA 와 QLoRA 의 차이,0.0063732993,0.0,0.006373299285769463
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,Loss Function 예시,-0.04396095,0.0,0.04396095126867294
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,Loss Function 정의,-0.045430563,0.0,0.045430563390254974
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,MBTI,-0.011723665,0.0,0.011723664589226246
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,MSE Loss 설명,0.016170656,0.0,0.01617065630853176
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,MSE Loss 용도,0.026145922,0.0,0.026145922020077705
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,Multi-Label 에서 CE + Softmax 적용 문제점,-0.02730949,0.0,0.027309490367770195
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,PEFT 방법 5가지,0.11148047,0.0,0.1114804670214653
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,거대 언어 모델 정의,0.0053742873,0.0,0.0053742872551083565
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,기본 경험,-0.034434028,0.0,0.03443402796983719
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,답변 실패,0.78905004,1.0,0.21094995737075806
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,딥러닝,-0.043957476,0.0,0.04395747557282448
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,마지막 할 말,-0.018518748,0.0,0.01851874776184559
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,머신러닝,-0.0063564386,0.0,0.006356438621878624
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,면접 시작 인사,-0.02575293,0.0,0.025752929970622063
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,상세 경험,-0.00737433,0.0,0.007374329958111048
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,수식,-0.040142324,0.0,0.040142323821783066
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,인공지능,0.03549385,0.0,0.03549385070800781
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,잠시 휴식,-0.0039431816,0.0,0.003943181596696377
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,좋아하는 아이돌,-0.022320116,0.0,0.022320115938782692
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,핵심 아이디어,-0.06351205,0.0,0.06351204961538315
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,확률 예측에서 MSE Loss 미 사용 이유,-0.055566512,0.0,0.055566512048244476
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",BCE 가 좋은 task,0.0048453966,0.0,0.004845396615564823
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?","BCE 가 좋은 task, 이유",0.013740082,0.0,0.013740082271397114
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",LLM Fine-Tuning 의 PEFT,-0.015274181,0.0,0.01527418103069067
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",LoRA,0.016942268,0.0,0.01694226823747158
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",LoRA 와 QLoRA 의 차이,0.051783185,0.0,0.051783185452222824
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",Loss Function 예시,-0.01365508,0.0,0.013655080460011959
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",Loss Function 정의,0.038289726,0.0,0.03828972578048706
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",MBTI,0.0285511,0.0,0.028551099821925163
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",MSE Loss 설명,0.000120492194,0.0,0.00012049219367327169
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",MSE Loss 용도,0.023613064,0.0,0.023613063618540764
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",Multi-Label 에서 CE + Softmax 적용 문제점,-0.0107542,0.0,0.01075419969856739
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",PEFT 방법 5가지,0.85936785,1.0,0.14063215255737305
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",거대 언어 모델 정의,0.080686286,0.0,0.0806862860918045
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",기본 경험,-0.02476889,0.0,0.02476889081299305
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",답변 실패,0.21819879,0.0,0.21819879114627838
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",딥러닝,-0.011308616,0.0,0.011308616027235985
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",마지막 할 말,0.0060028904,0.0,0.006002890411764383
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",머신러닝,-0.062187966,0.0,0.06218796595931053
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",면접 시작 인사,0.009673024,0.0,0.009673023596405983
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",상세 경험,-0.041560695,0.0,0.04156069457530975
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",수식,-0.07604592,0.0,0.076045922935009
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",인공지능,-0.014678499,0.0,0.014678498730063438
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",잠시 휴식,0.03411146,0.0,0.03411145880818367
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",좋아하는 아이돌,0.015145293,0.0,0.015145293436944485
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",핵심 아이디어,-0.0018571471,0.0,0.0018571470864117146
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",확률 예측에서 MSE Loss 미 사용 이유,0.008097132,0.0,0.008097131736576557
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,BCE 가 좋은 task,-0.0337174,0.0,0.03371740132570267
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,"BCE 가 좋은 task, 이유",-0.032570485,0.0,0.0325704850256443
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,LLM Fine-Tuning 의 PEFT,-0.009519386,0.0,0.009519386105239391
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,LoRA,0.01590248,0.0,0.015902480110526085
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,LoRA 와 QLoRA 의 차이,0.007847971,0.0,0.00784797128289938
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,Loss Function 예시,-0.055743508,0.0,0.055743508040905
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,Loss Function 정의,-0.04973712,0.0,0.0497371181845665
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,MBTI,0.029623607,0.0,0.02962360717356205
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,MSE Loss 설명,-0.050375443,0.0,0.05037544295191765
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,MSE Loss 용도,-0.021236125,0.0,0.021236125379800797
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,Multi-Label 에서 CE + Softmax 적용 문제점,-0.0009485558,0.0,0.0009485558257438242
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,PEFT 방법 5가지,0.3426326,0.0,0.34263259172439575
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,거대 언어 모델 정의,0.0009766951,0.0,0.0009766950970515609
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,기본 경험,-0.020786222,0.0,0.02078622207045555
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,답변 실패,0.7513482,1.0,0.24865180253982544
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,딥러닝,-0.016247766,0.0,0.016247766092419624
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,마지막 할 말,-0.015498251,0.0,0.015498250722885132
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,머신러닝,-0.018593805,0.0,0.018593804910779
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,면접 시작 인사,-0.05161272,0.0,0.051612719893455505
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,상세 경험,-0.058095966,0.0,0.058095965534448624
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,수식,-0.06969049,0.0,0.06969048827886581
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,인공지능,0.013394349,0.0,0.013394349254667759
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,잠시 휴식,-0.0060878806,0.0,0.006087880581617355
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,좋아하는 아이돌,0.07886847,0.0,0.07886847108602524
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,핵심 아이디어,-0.047596265,0.0,0.04759626463055611
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,확률 예측에서 MSE Loss 미 사용 이유,-0.038440786,0.0,0.03844078630208969
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,BCE 가 좋은 task,-0.016820583,0.0,0.01682058349251747
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,"BCE 가 좋은 task, 이유",-0.00079867983,0.0,0.0007986798300407827
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,LLM Fine-Tuning 의 PEFT,-0.061226282,0.0,0.06122628226876259
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,LoRA,0.7672433,1.0,0.23275667428970337
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,LoRA 와 QLoRA 의 차이,0.23933713,0.0,0.23933713138103485
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,Loss Function 예시,0.0094901705,0.0,0.009490170516073704
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,Loss Function 정의,-0.027269186,0.0,0.027269186452031136
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,MBTI,-0.047104716,0.0,0.047104716300964355
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,MSE Loss 설명,-0.08764894,0.0,0.08764894306659698
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,MSE Loss 용도,-0.090082064,0.0,0.0900820642709732
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,Multi-Label 에서 CE + Softmax 적용 문제점,-0.03243043,0.0,0.03243042901158333
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,PEFT 방법 5가지,0.026684182,0.0,0.02668418176472187
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,거대 언어 모델 정의,-0.0031055058,0.0,0.003105505835264921
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,기본 경험,-0.0043491116,0.0,0.004349111579358578
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,답변 실패,-0.036786426,0.0,0.03678642585873604
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,딥러닝,-0.042049013,0.0,0.04204901307821274
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,마지막 할 말,-0.018100915,0.0,0.018100915476679802
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,머신러닝,-0.037385195,0.0,0.03738519549369812
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,면접 시작 인사,-0.0073802858,0.0,0.007380285765975714
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,상세 경험,-0.06690934,0.0,0.06690934300422668
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,수식,0.004899158,0.0,0.004899158142507076
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,인공지능,-0.010382924,0.0,0.010382924228906631
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,잠시 휴식,-0.011747942,0.0,0.011747942306101322
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,좋아하는 아이돌,-0.036963746,0.0,0.03696374595165253
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,핵심 아이디어,-0.08335863,0.0,0.08335863053798676
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,확률 예측에서 MSE Loss 미 사용 이유,-0.009164364,0.0,0.009164364077150822
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,BCE 가 좋은 task,-0.030948753,0.0,0.030948752537369728
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,"BCE 가 좋은 task, 이유",-0.045030262,0.0,0.04503026232123375
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,LLM Fine-Tuning 의 PEFT,-0.016890628,0.0,0.0168906282633543
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,LoRA,0.09726886,0.0,0.09726885706186295
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,LoRA 와 QLoRA 의 차이,-0.023071887,0.0,0.023071886971592903
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,Loss Function 예시,-0.04506467,0.0,0.045064669102430344
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,Loss Function 정의,-0.028896404,0.0,0.028896404430270195
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,MBTI,-0.0049364697,0.0,0.004936469718813896
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,MSE Loss 설명,-0.047141522,0.0,0.04714152216911316
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,MSE Loss 용도,-0.027433762,0.0,0.027433762326836586
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,Multi-Label 에서 CE + Softmax 적용 문제점,-0.046208624,0.0,0.04620862379670143
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,PEFT 방법 5가지,0.045197856,0.0,0.045197855681180954
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,거대 언어 모델 정의,-0.015683467,0.0,0.01568346656858921
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,기본 경험,-0.028761156,0.0,0.028761155903339386
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,답변 실패,0.8979435,1.0,0.10205650329589844
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,딥러닝,-0.008531384,0.0,0.00853138417005539
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,마지막 할 말,-0.026948351,0.0,0.02694835141301155
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,머신러닝,-0.014809512,0.0,0.014809511601924896
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,면접 시작 인사,-0.028554108,0.0,0.02855410799384117
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,상세 경험,-0.053050224,0.0,0.05305022373795509
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,수식,-0.07764664,0.0,0.0776466429233551
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,인공지능,-0.013230519,0.0,0.013230519369244576
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,잠시 휴식,0.011874322,0.0,0.011874321848154068
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,좋아하는 아이돌,0.0042078304,0.0,0.004207830410450697
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,핵심 아이디어,-0.020726377,0.0,0.02072637714445591
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,확률 예측에서 MSE Loss 미 사용 이유,-0.014587047,0.0,0.014587046578526497
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,BCE 가 좋은 task,-0.011949255,0.0,0.011949255131185055
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,"BCE 가 좋은 task, 이유",0.02122853,0.0,0.021228529512882233
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,LLM Fine-Tuning 의 PEFT,-0.027027078,0.0,0.027027077972888947
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,LoRA,0.046591584,0.0,0.046591583639383316
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,LoRA 와 QLoRA 의 차이,0.8559181,1.0,0.14408189058303833
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,Loss Function 예시,0.0044939,0.0,0.00449390010908246
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,Loss Function 정의,-0.003154962,0.0,0.0031549620907753706
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,MBTI,-0.004633727,0.0,0.004633727017790079
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,MSE Loss 설명,-0.035059985,0.0,0.035059984773397446
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,MSE Loss 용도,-0.034766372,0.0,0.03476637229323387
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,Multi-Label 에서 CE + Softmax 적용 문제점,0.043316513,0.0,0.04331651329994202
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,PEFT 방법 5가지,0.02999727,0.0,0.029997270554304123
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,거대 언어 모델 정의,0.011788144,0.0,0.011788143776357174
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,기본 경험,-0.019981176,0.0,0.019981175661087036
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,답변 실패,-0.03466635,0.0,0.03466634824872017
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,딥러닝,0.008844542,0.0,0.008844542317092419
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,마지막 할 말,-0.0034397966,0.0,0.0034397966228425503
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,머신러닝,-0.02710226,0.0,0.027102259919047356
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,면접 시작 인사,0.01698104,0.0,0.016981039196252823
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,상세 경험,0.041431908,0.0,0.04143190756440163
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,수식,-0.013560475,0.0,0.01356047485023737
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,인공지능,0.005827316,0.0,0.005827316083014011
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,잠시 휴식,-0.0032541845,0.0,0.0032541844993829727
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,좋아하는 아이돌,0.0060096947,0.0,0.006009694654494524
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,핵심 아이디어,-0.04330719,0.0,0.043307188898324966
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,확률 예측에서 MSE Loss 미 사용 이유,-0.024704343,0.0,0.0247043427079916
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,BCE 가 좋은 task,0.002595931,0.0,0.0025959310587495565
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,"BCE 가 좋은 task, 이유",-0.0049784672,0.0,0.004978467244654894
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,LLM Fine-Tuning 의 PEFT,0.0028046705,0.0,0.0028046704828739166
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,LoRA,0.036066093,0.0,0.03606609255075455
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,LoRA 와 QLoRA 의 차이,0.08381228,0.0,0.08381228148937225
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,Loss Function 예시,-0.021067616,0.0,0.02106761559844017
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,Loss Function 정의,-0.038694903,0.0,0.03869490325450897
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,MBTI,0.020617675,0.0,0.02061767503619194
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,MSE Loss 설명,-0.025097763,0.0,0.025097763165831566
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,MSE Loss 용도,-0.0053161294,0.0,0.005316129419952631
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,Multi-Label 에서 CE + Softmax 적용 문제점,-0.019128751,0.0,0.019128751009702682
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,PEFT 방법 5가지,0.070163526,0.0,0.07016352564096451
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,거대 언어 모델 정의,-0.05291421,0.0,0.05291420966386795
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,기본 경험,-0.028675783,0.0,0.028675783425569534
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,답변 실패,0.88494897,1.0,0.1150510311126709
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,딥러닝,-0.010964126,0.0,0.010964126326143742
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,마지막 할 말,-0.027615273,0.0,0.027615273371338844
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,머신러닝,-0.009650728,0.0,0.009650727733969688
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,면접 시작 인사,-0.03621091,0.0,0.036210909485816956
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,상세 경험,-0.041552763,0.0,0.04155276343226433
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,수식,-0.09271687,0.0,0.09271687269210815
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,인공지능,0.00566914,0.0,0.005669139791280031
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,잠시 휴식,-0.008731331,0.0,0.008731330744922161
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,좋아하는 아이돌,-0.005495965,0.0,0.005495965015143156
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,핵심 아이디어,-0.0068438025,0.0,0.006843802519142628
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,확률 예측에서 MSE Loss 미 사용 이유,-0.03683385,0.0,0.03683384880423546
마지막 할 말 -> 로라야 정말 고마워!,BCE 가 좋은 task,-0.014135406,0.0,0.014135406352579594
마지막 할 말 -> 로라야 정말 고마워!,"BCE 가 좋은 task, 이유",-0.026245799,0.0,0.02624579891562462
마지막 할 말 -> 로라야 정말 고마워!,LLM Fine-Tuning 의 PEFT,0.028442953,0.0,0.028442952781915665
마지막 할 말 -> 로라야 정말 고마워!,LoRA,0.01936911,0.0,0.019369110465049744
마지막 할 말 -> 로라야 정말 고마워!,LoRA 와 QLoRA 의 차이,0.010488836,0.0,0.010488836094737053
마지막 할 말 -> 로라야 정말 고마워!,Loss Function 예시,-0.0033102718,0.0,0.0033102717716246843
마지막 할 말 -> 로라야 정말 고마워!,Loss Function 정의,0.009951336,0.0,0.009951336309313774
마지막 할 말 -> 로라야 정말 고마워!,MBTI,-0.017442258,0.0,0.017442258074879646
마지막 할 말 -> 로라야 정말 고마워!,MSE Loss 설명,-0.025738962,0.0,0.02573896199464798
마지막 할 말 -> 로라야 정말 고마워!,MSE Loss 용도,0.007882238,0.0,0.007882238365709782
마지막 할 말 -> 로라야 정말 고마워!,Multi-Label 에서 CE + Softmax 적용 문제점,-0.015581298,0.0,0.015581297688186169
마지막 할 말 -> 로라야 정말 고마워!,PEFT 방법 5가지,0.029143851,0.0,0.02914385125041008
마지막 할 말 -> 로라야 정말 고마워!,거대 언어 모델 정의,-0.0010098206,0.0,0.0010098206112161279
마지막 할 말 -> 로라야 정말 고마워!,기본 경험,-0.03664598,0.0,0.036645978689193726
마지막 할 말 -> 로라야 정말 고마워!,답변 실패,-0.006517417,0.0,0.006517416797578335
마지막 할 말 -> 로라야 정말 고마워!,딥러닝,-0.03121122,0.0,0.03121121972799301
마지막 할 말 -> 로라야 정말 고마워!,마지막 할 말,0.9433973,1.0,0.05660271644592285
마지막 할 말 -> 로라야 정말 고마워!,머신러닝,-0.0030591683,0.0,0.0030591683462262154
마지막 할 말 -> 로라야 정말 고마워!,면접 시작 인사,-0.037237942,0.0,0.03723794221878052
마지막 할 말 -> 로라야 정말 고마워!,상세 경험,0.022227218,0.0,0.022227218374609947
마지막 할 말 -> 로라야 정말 고마워!,수식,0.01975046,0.0,0.019750459119677544
마지막 할 말 -> 로라야 정말 고마워!,인공지능,-0.0063330806,0.0,0.0063330805860459805
마지막 할 말 -> 로라야 정말 고마워!,잠시 휴식,-0.0049093594,0.0,0.004909359384328127
마지막 할 말 -> 로라야 정말 고마워!,좋아하는 아이돌,0.018613804,0.0,0.018613804131746292
마지막 할 말 -> 로라야 정말 고마워!,핵심 아이디어,-0.004599655,0.0,0.004599655047059059
마지막 할 말 -> 로라야 정말 고마워!,확률 예측에서 MSE Loss 미 사용 이유,0.014904458,0.0,0.014904458075761795
마지막 할 말 -> 로라야 사랑해,BCE 가 좋은 task,-0.007976231,0.0,0.007976231165230274
마지막 할 말 -> 로라야 사랑해,"BCE 가 좋은 task, 이유",-0.017706145,0.0,0.017706144601106644
마지막 할 말 -> 로라야 사랑해,LLM Fine-Tuning 의 PEFT,0.023469904,0.0,0.02346990443766117
마지막 할 말 -> 로라야 사랑해,LoRA,0.022882635,0.0,0.02288263477385044
마지막 할 말 -> 로라야 사랑해,LoRA 와 QLoRA 의 차이,0.019473031,0.0,0.019473031163215637
마지막 할 말 -> 로라야 사랑해,Loss Function 예시,-0.0057331184,0.0,0.005733118392527103
마지막 할 말 -> 로라야 사랑해,Loss Function 정의,0.0032315762,0.0,0.0032315761782228947
마지막 할 말 -> 로라야 사랑해,MBTI,-0.024135513,0.0,0.024135513231158257
마지막 할 말 -> 로라야 사랑해,MSE Loss 설명,-0.023734221,0.0,0.02373422123491764
마지막 할 말 -> 로라야 사랑해,MSE Loss 용도,0.008456178,0.0,0.00845617800951004
마지막 할 말 -> 로라야 사랑해,Multi-Label 에서 CE + Softmax 적용 문제점,-0.022914786,0.0,0.022914785891771317
마지막 할 말 -> 로라야 사랑해,PEFT 방법 5가지,0.02449021,0.0,0.02449020929634571
마지막 할 말 -> 로라야 사랑해,거대 언어 모델 정의,0.007760868,0.0,0.007760867942124605
마지막 할 말 -> 로라야 사랑해,기본 경험,-0.03759314,0.0,0.037593141198158264
마지막 할 말 -> 로라야 사랑해,답변 실패,0.00035101137,0.0,0.00035101137473247945
마지막 할 말 -> 로라야 사랑해,딥러닝,-0.03553083,0.0,0.035530831664800644
마지막 할 말 -> 로라야 사랑해,마지막 할 말,0.94651496,1.0,0.05348503589630127
마지막 할 말 -> 로라야 사랑해,머신러닝,-0.005498529,0.0,0.0054985289461910725
마지막 할 말 -> 로라야 사랑해,면접 시작 인사,-0.03505099,0.0,0.03505098819732666
마지막 할 말 -> 로라야 사랑해,상세 경험,0.011957597,0.0,0.011957596987485886
마지막 할 말 -> 로라야 사랑해,수식,0.027286444,0.0,0.02728644385933876
마지막 할 말 -> 로라야 사랑해,인공지능,-0.0036783079,0.0,0.003678307868540287
마지막 할 말 -> 로라야 사랑해,잠시 휴식,0.00982697,0.0,0.009826970286667347
마지막 할 말 -> 로라야 사랑해,좋아하는 아이돌,0.040775456,0.0,0.04077545553445816
마지막 할 말 -> 로라야 사랑해,핵심 아이디어,-0.0057790936,0.0,0.005779093597084284
마지막 할 말 -> 로라야 사랑해,확률 예측에서 MSE Loss 미 사용 이유,0.026611535,0.0,0.026611534878611565
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,BCE 가 좋은 task,0.0012811166,0.0,0.0012811166234314442
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,"BCE 가 좋은 task, 이유",-0.0075480933,0.0,0.00754809333011508
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,LLM Fine-Tuning 의 PEFT,0.025844185,0.0,0.025844184681773186
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,LoRA,0.017147738,0.0,0.017147738486528397
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,LoRA 와 QLoRA 의 차이,0.021857511,0.0,0.02185751125216484
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,Loss Function 예시,-0.0064531094,0.0,0.006453109439462423
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,Loss Function 정의,0.0030977565,0.0,0.0030977565329521894
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,MBTI,-0.022593537,0.0,0.022593537345528603
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,MSE Loss 설명,-0.037221726,0.0,0.03722172603011131
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,MSE Loss 용도,-0.006472843,0.0,0.006472842767834663
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,Multi-Label 에서 CE + Softmax 적용 문제점,-0.0153851025,0.0,0.015385102480649948
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,PEFT 방법 5가지,0.028383078,0.0,0.028383078053593636
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,거대 언어 모델 정의,-0.0015890665,0.0,0.0015890664653852582
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,기본 경험,-0.046725623,0.0,0.046725623309612274
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,답변 실패,-0.015213476,0.0,0.015213475562632084
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,딥러닝,-0.03946951,0.0,0.039469510316848755
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,마지막 할 말,0.9473354,1.0,0.05266457796096802
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,머신러닝,-0.0046509295,0.0,0.004650929477065802
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,면접 시작 인사,-0.05683074,0.0,0.056830741465091705
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,상세 경험,0.02290703,0.0,0.02290702983736992
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,수식,0.011031471,0.0,0.011031471192836761
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,인공지능,-0.021610357,0.0,0.021610356867313385
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,잠시 휴식,0.015881753,0.0,0.015881752595305443
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,좋아하는 아이돌,0.019885244,0.0,0.019885243847966194
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,핵심 아이디어,-0.013117642,0.0,0.013117642141878605
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,확률 예측에서 MSE Loss 미 사용 이유,0.021687865,0.0,0.021687865257263184
