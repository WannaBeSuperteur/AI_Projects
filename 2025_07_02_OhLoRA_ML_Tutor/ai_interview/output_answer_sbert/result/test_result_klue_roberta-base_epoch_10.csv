input_part,output_answer,predicted_score,ground_truth_score,absolute_error
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,BCE 가 좋은 task,-0.012485702,0.0,0.012485701590776443
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,BCE 가 좋은 이유,-0.017256757,0.0,0.017256757244467735
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,LLM Fine-Tuning 의 PEFT,-0.015942827,0.0,0.015942826867103577
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,LoRA,-0.0050190855,0.0,0.005019085481762886
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,LoRA 와 QLoRA 의 차이,-0.013692195,0.0,0.01369219459593296
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,Loss Function 예시,0.0036460792,0.0,0.0036460792180150747
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,Loss Function 정의,0.020171044,0.0,0.020171044394373894
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,MBTI,-0.025066564,0.0,0.025066563859581947
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,MSE Loss 설명,0.017424362,0.0,0.017424361780285835
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,MSE Loss 용도,0.0023415599,0.0,0.0023415598552674055
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,Multi-Label 에서 CE + Softmax 적용 문제점,0.0038975666,0.0,0.003897566581144929
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,PEFT 방법 5가지,0.0032816175,0.0,0.0032816175371408463
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,거대 언어 모델 정의,0.001752069,0.0,0.0017520689871162176
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,기본 경험,-0.00826141,0.0,0.00826140958815813
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,답변 실패,-0.008704295,0.0,0.008704295381903648
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,딥러닝,0.011023133,0.0,0.011023133061826229
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,마지막 할 말,-0.0035010097,0.0,0.003501009661704302
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,머신러닝,0.013615414,0.0,0.013615413568913937
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,면접 시작 인사,0.98709226,1.0,0.012907743453979492
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,상세 경험,3.9990657e-05,0.0,3.9990656659938395e-05
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,수식,-0.0037507256,0.0,0.0037507256492972374
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,용어 질문,0.022181705,0.0,0.02218170464038849
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,인공지능,-0.005251838,0.0,0.005251837894320488
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,잠시 휴식,-0.002377894,0.0,0.0023778940085321665
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,좋아하는 아이돌,-0.02142938,0.0,0.021429380401968956
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,핵심 아이디어,0.0074593807,0.0,0.00745938066393137
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,확률 예측에서 MSE Loss 미 사용 이유,-0.00085236103,0.0,0.0008523610304109752
면접 시작 인사 -> 로라야 안녕 정말 반가워,BCE 가 좋은 task,-0.0086703785,0.0,0.008670378476381302
면접 시작 인사 -> 로라야 안녕 정말 반가워,BCE 가 좋은 이유,-0.016507998,0.0,0.01650799810886383
면접 시작 인사 -> 로라야 안녕 정말 반가워,LLM Fine-Tuning 의 PEFT,-0.008826668,0.0,0.008826668374240398
면접 시작 인사 -> 로라야 안녕 정말 반가워,LoRA,-0.0076050838,0.0,0.007605083752423525
면접 시작 인사 -> 로라야 안녕 정말 반가워,LoRA 와 QLoRA 의 차이,-0.009107366,0.0,0.00910736620426178
면접 시작 인사 -> 로라야 안녕 정말 반가워,Loss Function 예시,0.004935629,0.0,0.004935629200190306
면접 시작 인사 -> 로라야 안녕 정말 반가워,Loss Function 정의,0.024273165,0.0,0.024273164570331573
면접 시작 인사 -> 로라야 안녕 정말 반가워,MBTI,-0.013604246,0.0,0.013604246079921722
면접 시작 인사 -> 로라야 안녕 정말 반가워,MSE Loss 설명,0.0121438615,0.0,0.012143861502408981
면접 시작 인사 -> 로라야 안녕 정말 반가워,MSE Loss 용도,-0.0026514113,0.0,0.0026514113415032625
면접 시작 인사 -> 로라야 안녕 정말 반가워,Multi-Label 에서 CE + Softmax 적용 문제점,0.006739134,0.0,0.006739133968949318
면접 시작 인사 -> 로라야 안녕 정말 반가워,PEFT 방법 5가지,-0.007007646,0.0,0.007007645908743143
면접 시작 인사 -> 로라야 안녕 정말 반가워,거대 언어 모델 정의,0.00683597,0.0,0.0068359700962901115
면접 시작 인사 -> 로라야 안녕 정말 반가워,기본 경험,-0.0071552196,0.0,0.007155219558626413
면접 시작 인사 -> 로라야 안녕 정말 반가워,답변 실패,-0.004154398,0.0,0.00415439810603857
면접 시작 인사 -> 로라야 안녕 정말 반가워,딥러닝,0.01222947,0.0,0.01222946960479021
면접 시작 인사 -> 로라야 안녕 정말 반가워,마지막 할 말,0.012428779,0.0,0.012428779155015945
면접 시작 인사 -> 로라야 안녕 정말 반가워,머신러닝,0.022685131,0.0,0.022685131058096886
면접 시작 인사 -> 로라야 안녕 정말 반가워,면접 시작 인사,0.9873841,1.0,0.01261591911315918
면접 시작 인사 -> 로라야 안녕 정말 반가워,상세 경험,0.0046485793,0.0,0.0046485792845487595
면접 시작 인사 -> 로라야 안녕 정말 반가워,수식,-0.014861785,0.0,0.014861784875392914
면접 시작 인사 -> 로라야 안녕 정말 반가워,용어 질문,0.0005932534,0.0,0.0005932534113526344
면접 시작 인사 -> 로라야 안녕 정말 반가워,인공지능,-0.0016410056,0.0,0.0016410056268796325
면접 시작 인사 -> 로라야 안녕 정말 반가워,잠시 휴식,-0.0064416653,0.0,0.006441665347665548
면접 시작 인사 -> 로라야 안녕 정말 반가워,좋아하는 아이돌,-0.016130256,0.0,0.01613025553524494
면접 시작 인사 -> 로라야 안녕 정말 반가워,핵심 아이디어,-0.0018556002,0.0,0.0018556001596152782
면접 시작 인사 -> 로라야 안녕 정말 반가워,확률 예측에서 MSE Loss 미 사용 이유,-0.0023074998,0.0,0.002307499758899212
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,BCE 가 좋은 task,-0.00028690966,0.0,0.00028690966428257525
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,BCE 가 좋은 이유,-0.022186555,0.0,0.022186554968357086
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,LLM Fine-Tuning 의 PEFT,-0.014349561,0.0,0.014349561184644699
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,LoRA,-0.0007890606,0.0,0.0007890606066212058
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,LoRA 와 QLoRA 의 차이,-0.01121041,0.0,0.011210409924387932
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,Loss Function 예시,0.011399246,0.0,0.01139924582093954
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,Loss Function 정의,0.021149397,0.0,0.021149396896362305
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,MBTI,-0.019543942,0.0,0.01954394206404686
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,MSE Loss 설명,0.011574782,0.0,0.011574782431125641
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,MSE Loss 용도,-0.0036943096,0.0,0.0036943096201866865
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,Multi-Label 에서 CE + Softmax 적용 문제점,0.00026349523,0.0,0.0002634952252265066
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,PEFT 방법 5가지,-0.0070403754,0.0,0.007040375377982855
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,거대 언어 모델 정의,-0.0037576514,0.0,0.0037576514296233654
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,기본 경험,-0.01236121,0.0,0.012361209839582443
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,답변 실패,-0.0032187963,0.0,0.0032187963370233774
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,딥러닝,0.014644696,0.0,0.01464469637721777
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,마지막 할 말,0.0015634195,0.0,0.0015634194714948535
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,머신러닝,0.01981444,0.0,0.019814439117908478
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,면접 시작 인사,0.9852874,1.0,0.01471257209777832
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,상세 경험,-0.0069974484,0.0,0.006997448392212391
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,수식,-0.0116113005,0.0,0.011611300520598888
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,용어 질문,-0.0047474382,0.0,0.004747438244521618
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,인공지능,-0.0064992295,0.0,0.0064992294646799564
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,잠시 휴식,-0.018070143,0.0,0.018070142716169357
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,좋아하는 아이돌,-0.018846707,0.0,0.018846707418560982
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,핵심 아이디어,0.0099562695,0.0,0.009956269524991512
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,확률 예측에서 MSE Loss 미 사용 이유,0.0040683043,0.0,0.004068304318934679
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,BCE 가 좋은 task,-0.0058867233,0.0,0.0058867232874035835
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,BCE 가 좋은 이유,-0.010213941,0.0,0.0102139413356781
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,LLM Fine-Tuning 의 PEFT,-0.012983416,0.0,0.012983416207134724
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,LoRA,-0.0027384227,0.0,0.002738422714173794
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,LoRA 와 QLoRA 의 차이,-0.010325455,0.0,0.010325455106794834
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,Loss Function 예시,0.008730651,0.0,0.008730650879442692
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,Loss Function 정의,0.019398523,0.0,0.01939852349460125
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,MBTI,-0.01789345,0.0,0.01789345033466816
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,MSE Loss 설명,0.0077285646,0.0,0.00772856455296278
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,MSE Loss 용도,-0.003975102,0.0,0.003975102212280035
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,Multi-Label 에서 CE + Softmax 적용 문제점,0.0026722625,0.0,0.002672262489795685
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,PEFT 방법 5가지,0.0033206202,0.0,0.0033206201624125242
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,거대 언어 모델 정의,0.0014891131,0.0,0.0014891130849719048
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,기본 경험,-0.0044400776,0.0,0.00444007758051157
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,답변 실패,-0.0127921095,0.0,0.01279210951179266
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,딥러닝,0.012942444,0.0,0.01294244360178709
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,마지막 할 말,0.028425537,0.0,0.028425537049770355
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,머신러닝,0.014674042,0.0,0.014674042351543903
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,면접 시작 인사,0.98075193,1.0,0.01924806833267212
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,상세 경험,-0.0025094156,0.0,0.002509415615350008
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,수식,-0.0051527084,0.0,0.005152708385139704
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,용어 질문,-0.015503437,0.0,0.015503437258303165
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,인공지능,-0.0065040574,0.0,0.006504057440906763
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,잠시 휴식,-0.004308983,0.0,0.004308983217924833
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,좋아하는 아이돌,-0.02824112,0.0,0.028241120278835297
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,핵심 아이디어,0.0064029587,0.0,0.006402958650141954
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,확률 예측에서 MSE Loss 미 사용 이유,0.00657441,0.0,0.006574410013854504
면접 시작 인사 -> 파이팅! 시작하자,BCE 가 좋은 task,-0.0038683214,0.0,0.0038683214224874973
면접 시작 인사 -> 파이팅! 시작하자,BCE 가 좋은 이유,-0.009420219,0.0,0.009420218877494335
면접 시작 인사 -> 파이팅! 시작하자,LLM Fine-Tuning 의 PEFT,-0.006786568,0.0,0.0067865680903196335
면접 시작 인사 -> 파이팅! 시작하자,LoRA,-0.00710058,0.0,0.007100579794496298
면접 시작 인사 -> 파이팅! 시작하자,LoRA 와 QLoRA 의 차이,-0.012475714,0.0,0.012475714087486267
면접 시작 인사 -> 파이팅! 시작하자,Loss Function 예시,0.0080865,0.0,0.008086499758064747
면접 시작 인사 -> 파이팅! 시작하자,Loss Function 정의,0.020037001,0.0,0.020037000998854637
면접 시작 인사 -> 파이팅! 시작하자,MBTI,-0.014287,0.0,0.014286999590694904
면접 시작 인사 -> 파이팅! 시작하자,MSE Loss 설명,0.011164832,0.0,0.011164831928908825
면접 시작 인사 -> 파이팅! 시작하자,MSE Loss 용도,0.0015434236,0.0,0.0015434236265718937
면접 시작 인사 -> 파이팅! 시작하자,Multi-Label 에서 CE + Softmax 적용 문제점,0.008321118,0.0,0.008321117609739304
면접 시작 인사 -> 파이팅! 시작하자,PEFT 방법 5가지,2.7667398e-05,0.0,2.7667398171615787e-05
면접 시작 인사 -> 파이팅! 시작하자,거대 언어 모델 정의,0.0023181145,0.0,0.002318114507943392
면접 시작 인사 -> 파이팅! 시작하자,기본 경험,-0.005869845,0.0,0.005869844928383827
면접 시작 인사 -> 파이팅! 시작하자,답변 실패,-0.0135126375,0.0,0.013512637466192245
면접 시작 인사 -> 파이팅! 시작하자,딥러닝,0.010231357,0.0,0.01023135706782341
면접 시작 인사 -> 파이팅! 시작하자,마지막 할 말,0.0102869235,0.0,0.010286923497915268
면접 시작 인사 -> 파이팅! 시작하자,머신러닝,0.019984663,0.0,0.019984662532806396
면접 시작 인사 -> 파이팅! 시작하자,면접 시작 인사,0.9898552,1.0,0.010144829750061035
면접 시작 인사 -> 파이팅! 시작하자,상세 경험,-0.005548078,0.0,0.00554807810112834
면접 시작 인사 -> 파이팅! 시작하자,수식,-0.009526253,0.0,0.009526252746582031
면접 시작 인사 -> 파이팅! 시작하자,용어 질문,-0.013464257,0.0,0.01346425712108612
면접 시작 인사 -> 파이팅! 시작하자,인공지능,-0.004178017,0.0,0.004178016912192106
면접 시작 인사 -> 파이팅! 시작하자,잠시 휴식,-0.0009953303,0.0,0.0009953302796930075
면접 시작 인사 -> 파이팅! 시작하자,좋아하는 아이돌,-0.017959924,0.0,0.017959924414753914
면접 시작 인사 -> 파이팅! 시작하자,핵심 아이디어,0.017148716,0.0,0.017148716375231743
면접 시작 인사 -> 파이팅! 시작하자,확률 예측에서 MSE Loss 미 사용 이유,-0.0013481757,0.0,0.001348175690509379
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",BCE 가 좋은 task,0.0050565554,0.0,0.005056555382907391
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",BCE 가 좋은 이유,-0.010417299,0.0,0.010417299345135689
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",LLM Fine-Tuning 의 PEFT,-0.0011157349,0.0,0.0011157349217683077
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",LoRA,-0.0010147588,0.0,0.0010147588327527046
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",LoRA 와 QLoRA 의 차이,-0.00015446433,0.0,0.00015446433098986745
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",Loss Function 예시,-0.0111000445,0.0,0.0111000444740057
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",Loss Function 정의,-0.01145347,0.0,0.011453470215201378
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",MBTI,-0.0032122573,0.0,0.0032122572883963585
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",MSE Loss 설명,0.00094580924,0.0,0.0009458092390559614
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",MSE Loss 용도,-0.009202207,0.0,0.009202207438647747
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",Multi-Label 에서 CE + Softmax 적용 문제점,-0.00041480537,0.0,0.0004148053703829646
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",PEFT 방법 5가지,-0.00018929318,0.0,0.00018929317593574524
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",거대 언어 모델 정의,-0.0046714325,0.0,0.004671432543545961
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",기본 경험,-0.010104233,0.0,0.010104233399033546
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",답변 실패,0.9901318,1.0,0.009868204593658447
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",딥러닝,0.00020680492,0.0,0.00020680492161773145
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",마지막 할 말,-0.007358879,0.0,0.00735887885093689
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",머신러닝,-0.0022660922,0.0,0.002266092225909233
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",면접 시작 인사,-0.0017856954,0.0,0.001785695436410606
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",상세 경험,-0.0032931608,0.0,0.003293160814791918
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",수식,-0.007463934,0.0,0.007463933899998665
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",용어 질문,0.009181104,0.0,0.00918110366910696
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",인공지능,-0.008756909,0.0,0.0087569085881114
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",잠시 휴식,-0.004196367,0.0,0.004196367226541042
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",좋아하는 아이돌,0.013343457,0.0,0.013343457132577896
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",핵심 아이디어,0.008100708,0.0,0.00810070801526308
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",확률 예측에서 MSE Loss 미 사용 이유,-0.0045038583,0.0,0.004503858275711536
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",BCE 가 좋은 task,-0.0016071557,0.0,0.0016071556601673365
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",BCE 가 좋은 이유,-0.001402896,0.0,0.0014028960140421987
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",LLM Fine-Tuning 의 PEFT,-0.0012451399,0.0,0.0012451398652046919
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",LoRA,0.004421314,0.0,0.004421314224600792
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",LoRA 와 QLoRA 의 차이,0.00459518,0.0,0.004595180042088032
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",Loss Function 예시,-0.00079519296,0.0,0.0007951929583214223
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",Loss Function 정의,0.019973645,0.0,0.019973644986748695
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",MBTI,0.0049223173,0.0,0.004922317340970039
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",MSE Loss 설명,0.022157596,0.0,0.022157596424221992
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",MSE Loss 용도,0.0019328719,0.0,0.0019328718772158027
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",Multi-Label 에서 CE + Softmax 적용 문제점,0.0017157505,0.0,0.0017157505499199033
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",PEFT 방법 5가지,-0.007567741,0.0,0.007567740976810455
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",거대 언어 모델 정의,-0.014795855,0.0,0.014795854687690735
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",기본 경험,0.00669738,0.0,0.006697379983961582
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",답변 실패,-0.0010787494,0.0,0.001078749424777925
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",딥러닝,0.009855959,0.0,0.009855958633124828
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",마지막 할 말,0.011676612,0.0,0.011676612310111523
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",머신러닝,-0.012577852,0.0,0.012577852234244347
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",면접 시작 인사,-0.0060389927,0.0,0.006038992665708065
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",상세 경험,-0.0038619835,0.0,0.0038619835395365953
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",수식,0.002820609,0.0,0.0028206089045852423
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",용어 질문,0.0063509294,0.0,0.006350929383188486
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",인공지능,0.9703571,1.0,0.029642879962921143
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",잠시 휴식,0.014946375,0.0,0.014946375042200089
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",좋아하는 아이돌,-0.00976716,0.0,0.009767159819602966
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",핵심 아이디어,0.007436847,0.0,0.007436846848577261
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",확률 예측에서 MSE Loss 미 사용 이유,-0.0175474,0.0,0.017547400668263435
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",BCE 가 좋은 task,0.0083521465,0.0,0.008352146483957767
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",BCE 가 좋은 이유,-0.0026984122,0.0,0.0026984121650457382
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",LLM Fine-Tuning 의 PEFT,-0.031143045,0.0,0.03114304505288601
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",LoRA,0.0032186345,0.0,0.003218634519726038
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",LoRA 와 QLoRA 의 차이,-0.012613017,0.0,0.012613017112016678
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",Loss Function 예시,-0.0066493214,0.0,0.006649321410804987
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",Loss Function 정의,0.00388733,0.0,0.003887329949066043
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",MBTI,-0.04001457,0.0,0.04001456871628761
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",MSE Loss 설명,0.021867812,0.0,0.021867811679840088
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",MSE Loss 용도,0.003396356,0.0,0.003396356012672186
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",Multi-Label 에서 CE + Softmax 적용 문제점,0.026440684,0.0,0.026440683752298355
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",PEFT 방법 5가지,0.0095464345,0.0,0.009546434506773949
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",거대 언어 모델 정의,-0.016480537,0.0,0.01648053713142872
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",기본 경험,-0.0046409355,0.0,0.004640935454517603
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",답변 실패,-0.0008213952,0.0,0.0008213951950892806
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",딥러닝,-0.041019518,0.0,0.04101951792836189
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",마지막 할 말,0.0077441027,0.0,0.007744102738797665
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",머신러닝,0.97664034,1.0,0.02335965633392334
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",면접 시작 인사,0.017976234,0.0,0.01797623373568058
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",상세 경험,0.024451157,0.0,0.024451157078146935
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",수식,-0.0032598684,0.0,0.003259868361055851
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",용어 질문,-0.004331328,0.0,0.004331327974796295
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",인공지능,-0.043434046,0.0,0.04343404620885849
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",잠시 휴식,0.00088946236,0.0,0.0008894623606465757
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",좋아하는 아이돌,-0.0015142803,0.0,0.0015142803313210607
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",핵심 아이디어,0.005641034,0.0,0.005641033872961998
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",확률 예측에서 MSE Loss 미 사용 이유,-0.0013483936,0.0,0.001348393619991839
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",BCE 가 좋은 task,-0.010990487,0.0,0.010990487411618233
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",BCE 가 좋은 이유,0.011657364,0.0,0.01165736373513937
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",LLM Fine-Tuning 의 PEFT,0.003686635,0.0,0.0036866350565105677
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",LoRA,-0.00294865,0.0,0.002948649926111102
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",LoRA 와 QLoRA 의 차이,-0.030471336,0.0,0.030471336096525192
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",Loss Function 예시,-0.019521406,0.0,0.019521405920386314
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",Loss Function 정의,-0.029777955,0.0,0.029777955263853073
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",MBTI,-0.0150024295,0.0,0.015002429485321045
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",MSE Loss 설명,0.0036631352,0.0,0.003663135226815939
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",MSE Loss 용도,-0.008361602,0.0,0.008361602202057838
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",Multi-Label 에서 CE + Softmax 적용 문제점,-0.0007573247,0.0,0.000757324683945626
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",PEFT 방법 5가지,-0.033544622,0.0,0.033544622361660004
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",거대 언어 모델 정의,0.00017630077,0.0,0.00017630077491048723
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",기본 경험,0.0056141065,0.0,0.005614106543362141
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",답변 실패,0.0016948098,0.0,0.0016948097618296742
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",딥러닝,0.9707394,1.0,0.029260575771331787
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",마지막 할 말,-0.008991294,0.0,0.008991293609142303
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",머신러닝,-0.045334592,0.0,0.045334592461586
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",면접 시작 인사,0.003426433,0.0,0.0034264330752193928
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",상세 경험,-0.028536493,0.0,0.028536492958664894
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",수식,-0.035130102,0.0,0.035130102187395096
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",용어 질문,-0.015878025,0.0,0.01587802544236183
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",인공지능,-0.009592746,0.0,0.009592746384441853
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",잠시 휴식,-0.0076048635,0.0,0.007604863494634628
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",좋아하는 아이돌,-0.0036834967,0.0,0.003683496732264757
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",핵심 아이디어,-0.023218606,0.0,0.023218605667352676
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",확률 예측에서 MSE Loss 미 사용 이유,-0.0076418496,0.0,0.00764184957370162
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",BCE 가 좋은 task,-0.011762868,0.0,0.01176286768168211
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",BCE 가 좋은 이유,0.0072502964,0.0,0.007250296417623758
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",LLM Fine-Tuning 의 PEFT,0.0038503704,0.0,0.0038503704126924276
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",LoRA,-0.007365615,0.0,0.007365615107119083
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",LoRA 와 QLoRA 의 차이,-0.027264072,0.0,0.027264071628451347
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",Loss Function 예시,-0.017348066,0.0,0.017348065972328186
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",Loss Function 정의,-0.022082962,0.0,0.022082962095737457
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",MBTI,-0.009633063,0.0,0.009633063338696957
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",MSE Loss 설명,0.0036487547,0.0,0.0036487546749413013
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",MSE Loss 용도,-0.0128039075,0.0,0.012803907506167889
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",Multi-Label 에서 CE + Softmax 적용 문제점,-0.0041380157,0.0,0.004138015676289797
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",PEFT 방법 5가지,-0.030423434,0.0,0.03042343445122242
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",거대 언어 모델 정의,0.0011374862,0.0,0.0011374861933290958
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",기본 경험,0.00056892564,0.0,0.0005689256358891726
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",답변 실패,0.04398634,0.0,0.04398633912205696
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",딥러닝,0.9536975,1.0,0.04630249738693237
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",마지막 할 말,-0.0041277986,0.0,0.004127798601984978
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",머신러닝,-0.04732419,0.0,0.04732419177889824
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",면접 시작 인사,0.00068435003,0.0,0.0006843500304967165
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",상세 경험,-0.038330045,0.0,0.038330044597387314
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",수식,-0.041128825,0.0,0.04112882539629936
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",용어 질문,-0.01522761,0.0,0.015227610245347023
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",인공지능,0.004433634,0.0,0.004433634225279093
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",잠시 휴식,-0.0122039,0.0,0.012203900143504143
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",좋아하는 아이돌,-0.0037660587,0.0,0.003766058711335063
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",핵심 아이디어,-0.033241235,0.0,0.033241234719753265
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",확률 예측에서 MSE Loss 미 사용 이유,-0.010764726,0.0,0.01076472643762827
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",BCE 가 좋은 task,0.012719462,0.0,0.01271946169435978
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",BCE 가 좋은 이유,-0.015902521,0.0,0.015902521088719368
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",LLM Fine-Tuning 의 PEFT,-0.007300913,0.0,0.007300912868231535
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",LoRA,-0.0018227343,0.0,0.0018227342516183853
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",LoRA 와 QLoRA 의 차이,-0.009781285,0.0,0.00978128518909216
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",Loss Function 예시,0.0015251873,0.0,0.0015251872828230262
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",Loss Function 정의,0.007549938,0.0,0.007549937814474106
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",MBTI,-0.044526674,0.0,0.04452667385339737
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",MSE Loss 설명,0.017363962,0.0,0.017363961786031723
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",MSE Loss 용도,-0.015371761,0.0,0.015371761284768581
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",Multi-Label 에서 CE + Softmax 적용 문제점,0.02101436,0.0,0.021014360710978508
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",PEFT 방법 5가지,0.01050336,0.0,0.010503360070288181
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",거대 언어 모델 정의,-0.015637642,0.0,0.01563764177262783
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",기본 경험,-0.005620909,0.0,0.005620908923447132
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",답변 실패,0.009807856,1.0,0.9901921441778541
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",딥러닝,-0.05629311,0.0,0.05629311129450798
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",마지막 할 말,0.012499801,0.0,0.012499800883233547
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",머신러닝,0.9565565,0.0,0.956556499004364
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",면접 시작 인사,0.0061648693,0.0,0.0061648692935705185
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",상세 경험,0.022383818,0.0,0.02238381840288639
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",수식,0.007814144,0.0,0.007814143784344196
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",용어 질문,-0.0014200669,0.0,0.001420066924765706
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",인공지능,0.006021681,0.0,0.006021680776029825
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",잠시 휴식,0.01230242,0.0,0.012302420102059841
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",좋아하는 아이돌,-0.011525635,0.0,0.011525634676218033
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",핵심 아이디어,0.008797458,0.0,0.008797458373010159
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",확률 예측에서 MSE Loss 미 사용 이유,-0.0076238313,0.0,0.0076238312758505344
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아",BCE 가 좋은 task,0.004398627,0.0,0.004398627206683159
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아",BCE 가 좋은 이유,-0.0119652,0.0,0.011965200304985046
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아",LLM Fine-Tuning 의 PEFT,0.0017155419,0.0,0.0017155419336631894
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아",LoRA,-0.00082718185,0.0,0.000827181851491332
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아",LoRA 와 QLoRA 의 차이,-0.0018917156,0.0,0.0018917155684903264
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아",Loss Function 예시,-0.015728235,0.0,0.015728235244750977
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아",Loss Function 정의,-0.011330449,0.0,0.011330449022352695
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아",MBTI,0.00038784544,0.0,0.00038784544449299574
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아",MSE Loss 설명,0.0019843667,0.0,0.0019843666814267635
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아",MSE Loss 용도,-0.006124116,0.0,0.006124116014689207
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아",Multi-Label 에서 CE + Softmax 적용 문제점,0.0035110814,0.0,0.003511081449687481
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아",PEFT 방법 5가지,-0.00065441185,0.0,0.0006544118514284492
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아",거대 언어 모델 정의,-0.006924287,0.0,0.00692428695037961
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아",기본 경험,-0.000747906,0.0,0.000747905985917896
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아",답변 실패,0.99208724,1.0,0.007912755012512207
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아",딥러닝,-0.003315628,0.0,0.0033156280405819416
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아",마지막 할 말,-0.007591272,0.0,0.00759127177298069
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아",머신러닝,0.0013020321,0.0,0.0013020321493968368
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아",면접 시작 인사,0.00024022911,0.0,0.00024022911384236068
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아",상세 경험,-0.0052285385,0.0,0.005228538531810045
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아",수식,-0.017155595,0.0,0.017155595123767853
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아",용어 질문,0.008318557,0.0,0.008318557403981686
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아",인공지능,-0.012147342,0.0,0.01214734185487032
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아",잠시 휴식,-0.0021376035,0.0,0.002137603471055627
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아",좋아하는 아이돌,0.0070356354,0.0,0.007035635411739349
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아",핵심 아이디어,0.00994704,0.0,0.009947040118277073
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아",확률 예측에서 MSE Loss 미 사용 이유,-0.00086871005,0.0,0.0008687100489623845
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지?",BCE 가 좋은 task,-0.0037911928,0.0,0.003791192779317498
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지?",BCE 가 좋은 이유,0.009005365,0.0,0.009005364961922169
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지?",LLM Fine-Tuning 의 PEFT,-0.005233758,0.0,0.005233758129179478
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지?",LoRA,0.0054851547,0.0,0.005485154688358307
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지?",LoRA 와 QLoRA 의 차이,0.00080424984,0.0,0.0008042498375289142
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지?",Loss Function 예시,0.00051566795,0.0,0.0005156679544597864
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지?",Loss Function 정의,0.023665562,0.0,0.02366556227207184
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지?",MBTI,0.00059701275,0.0,0.0005970127531327307
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지?",MSE Loss 설명,0.031328514,0.0,0.03132851421833038
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지?",MSE Loss 용도,0.0072853914,0.0,0.007285391446202993
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지?",Multi-Label 에서 CE + Softmax 적용 문제점,-0.0016664548,0.0,0.0016664548311382532
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지?",PEFT 방법 5가지,-0.0112568205,0.0,0.011256820522248745
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지?",거대 언어 모델 정의,-0.012706514,0.0,0.012706514447927475
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지?",기본 경험,0.004799931,0.0,0.004799930844455957
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지?",답변 실패,-8.07174e-05,0.0,8.071740012383088e-05
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지?",딥러닝,0.017649407,0.0,0.01764940656721592
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지?",마지막 할 말,0.014687849,0.0,0.014687849208712578
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지?",머신러닝,-0.012234534,0.0,0.01223453413695097
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지?",면접 시작 인사,-0.005705447,0.0,0.005705446936190128
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지?",상세 경험,-0.01826164,0.0,0.018261639401316643
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지?",수식,0.012236639,0.0,0.0122366389259696
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지?",용어 질문,0.002501002,0.0,0.0025010020472109318
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지?",인공지능,0.9671764,1.0,0.03282362222671509
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지?",잠시 휴식,0.014496019,0.0,0.01449601911008358
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지?",좋아하는 아이돌,-0.010189285,0.0,0.010189284570515156
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지?",핵심 아이디어,0.006155052,0.0,0.006155052222311497
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지?",확률 예측에서 MSE Loss 미 사용 이유,-0.019562982,0.0,0.0195629820227623
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야!",BCE 가 좋은 task,0.005779385,0.0,0.0057793851010501385
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야!",BCE 가 좋은 이유,-0.0011446879,0.0,0.0011446878779679537
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야!",LLM Fine-Tuning 의 PEFT,-0.031602908,0.0,0.03160290792584419
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야!",LoRA,0.01028289,0.0,0.010282889939844608
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야!",LoRA 와 QLoRA 의 차이,-0.0030301495,0.0,0.0030301494989544153
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야!",Loss Function 예시,-0.014474778,0.0,0.014474778436124325
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야!",Loss Function 정의,-0.0011077328,0.0,0.0011077327653765678
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야!",MBTI,-0.035618193,0.0,0.035618193447589874
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야!",MSE Loss 설명,0.011175082,0.0,0.011175082065165043
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야!",MSE Loss 용도,0.012038191,0.0,0.012038190849125385
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야!",Multi-Label 에서 CE + Softmax 적용 문제점,0.031023016,0.0,0.031023016199469566
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야!",PEFT 방법 5가지,0.0026207727,0.0,0.0026207726914435625
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야!",거대 언어 모델 정의,-0.018065182,0.0,0.018065182492136955
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야!",기본 경험,-0.0012349751,0.0,0.001234975061379373
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야!",답변 실패,0.0029435134,0.0,0.002943513449281454
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야!",딥러닝,-0.036198407,0.0,0.03619840741157532
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야!",마지막 할 말,0.005759629,0.0,0.00575962895527482
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야!",머신러닝,0.97295284,1.0,0.027047157287597656
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야!",면접 시작 인사,0.014918967,0.0,0.01491896715015173
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야!",상세 경험,0.028470209,0.0,0.02847020886838436
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야!",수식,0.0022342354,0.0,0.002234235405921936
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야!",용어 질문,0.0039153737,0.0,0.0039153737016022205
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야!",인공지능,-0.047401655,0.0,0.047401655465364456
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야!",잠시 휴식,0.003095077,0.0,0.003095076885074377
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야!",좋아하는 아이돌,-0.0046358593,0.0,0.004635859280824661
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야!",핵심 아이디어,0.0034784032,0.0,0.003478403203189373
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야!",확률 예측에서 MSE Loss 미 사용 이유,-0.0031447385,0.0,0.003144738497212529
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지",BCE 가 좋은 task,-0.01596368,0.0,0.015963679179549217
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지",BCE 가 좋은 이유,0.004433393,0.0,0.004433393012732267
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지",LLM Fine-Tuning 의 PEFT,0.0041495766,0.0,0.004149576649069786
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지",LoRA,-0.0034613737,0.0,0.0034613737370818853
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지",LoRA 와 QLoRA 의 차이,-0.02898669,0.0,0.028986690565943718
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지",Loss Function 예시,-0.019977143,0.0,0.01997714303433895
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지",Loss Function 정의,-0.025273439,0.0,0.025273438543081284
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지",MBTI,-0.007947433,0.0,0.007947432808578014
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지",MSE Loss 설명,0.00612048,0.0,0.006120480131357908
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지",MSE Loss 용도,-0.010869218,0.0,0.010869218036532402
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지",Multi-Label 에서 CE + Softmax 적용 문제점,-0.0035351778,0.0,0.003535177791491151
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지",PEFT 방법 5가지,-0.032043178,0.0,0.032043177634477615
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지",거대 언어 모델 정의,-0.0031700323,0.0,0.003170032287016511
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지",기본 경험,0.0038185804,0.0,0.0038185804150998592
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지",답변 실패,0.0070088357,0.0,0.007008835673332214
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지",딥러닝,0.9726878,1.0,0.02731221914291382
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지",마지막 할 말,-0.005226038,0.0,0.005226037930697203
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지",머신러닝,-0.047861185,0.0,0.04786118492484093
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지",면접 시작 인사,0.0047160457,0.0,0.004716045688837767
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지",상세 경험,-0.020177323,0.0,0.02017732337117195
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지",수식,-0.047677845,0.0,0.047677844762802124
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지",용어 질문,-0.010841743,0.0,0.01084174308925867
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지",인공지능,-0.01564953,0.0,0.015649529173970222
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지",잠시 휴식,-0.008349816,0.0,0.00834981631487608
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지",좋아하는 아이돌,-0.006841604,0.0,0.006841604132205248
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지",핵심 아이디어,-0.022499204,0.0,0.0224992036819458
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지",확률 예측에서 MSE Loss 미 사용 이유,-0.010847047,0.0,0.010847046971321106
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야.",BCE 가 좋은 task,-0.016737392,0.0,0.01673739217221737
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야.",BCE 가 좋은 이유,0.0026586384,0.0,0.0026586384046822786
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야.",LLM Fine-Tuning 의 PEFT,0.005074553,0.0,0.005074553191661835
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야.",LoRA,-0.00060818484,0.0,0.0006081848405301571
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야.",LoRA 와 QLoRA 의 차이,-0.028735751,0.0,0.028735751286149025
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야.",Loss Function 예시,-0.02273964,0.0,0.02273963950574398
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야.",Loss Function 정의,-0.023474796,0.0,0.02347479574382305
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야.",MBTI,-0.012610708,0.0,0.012610708363354206
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야.",MSE Loss 설명,0.0016722337,0.0,0.0016722336877137423
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야.",MSE Loss 용도,-0.009154079,0.0,0.009154078550636768
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야.",Multi-Label 에서 CE + Softmax 적용 문제점,0.003066904,0.0,0.0030669039115309715
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야.",PEFT 방법 5가지,-0.03701582,0.0,0.037015821784734726
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야.",거대 언어 모델 정의,0.002132112,0.0,0.0021321119274944067
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야.",기본 경험,0.009275368,0.0,0.00927536841481924
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야.",답변 실패,-0.00059523503,0.0,0.0005952350329607725
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야.",딥러닝,0.9734215,1.0,0.02657848596572876
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야.",마지막 할 말,-0.008912309,0.0,0.00891230907291174
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야.",머신러닝,-0.04484315,0.0,0.044843148440122604
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야.",면접 시작 인사,0.0024498864,0.0,0.0024498864077031612
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야.",상세 경험,-0.020952104,0.0,0.020952103659510612
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야.",수식,-0.04209841,0.0,0.04209841042757034
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야.",용어 질문,-0.014603023,0.0,0.014603023417294025
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야.",인공지능,-0.010934389,0.0,0.01093438919633627
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야.",잠시 휴식,-0.0048497757,0.0,0.004849775694310665
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야.",좋아하는 아이돌,-0.0039385566,0.0,0.003938556648790836
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야.",핵심 아이디어,-0.024689494,0.0,0.02468949370086193
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야.",확률 예측에서 MSE Loss 미 사용 이유,-0.011227133,0.0,0.011227132752537727
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로",BCE 가 좋은 task,0.0021830488,0.0,0.002183048753067851
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로",BCE 가 좋은 이유,-0.010655084,0.0,0.010655083693563938
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로",LLM Fine-Tuning 의 PEFT,0.00030101073,0.0,0.00030101073207333684
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로",LoRA,-0.0033738834,0.0,0.003373883431777358
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로",LoRA 와 QLoRA 의 차이,-0.00041481358,0.0,0.0004148135776631534
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로",Loss Function 예시,-0.01817384,0.0,0.018173839896917343
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로",Loss Function 정의,-0.01012923,0.0,0.010129230096936226
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로",MBTI,0.0024245963,0.0,0.002424596343189478
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로",MSE Loss 설명,-0.002316168,0.0,0.0023161680437624454
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로",MSE Loss 용도,-0.009511212,0.0,0.009511211887001991
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로",Multi-Label 에서 CE + Softmax 적용 문제점,-0.0021453674,0.0,0.002145367441698909
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로",PEFT 방법 5가지,-0.0012870894,0.0,0.0012870894279330969
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로",거대 언어 모델 정의,-0.006439174,0.0,0.006439174059778452
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로",기본 경험,-0.0051392945,0.0,0.005139294546097517
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로",답변 실패,0.9890944,1.0,0.010905623435974121
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로",딥러닝,-0.0019212902,0.0,0.0019212901825085282
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로",마지막 할 말,0.0038887986,0.0,0.0038887986447662115
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로",머신러닝,0.008017877,0.0,0.008017877116799355
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로",면접 시작 인사,0.006723134,0.0,0.006723133847117424
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로",상세 경험,-0.0054349015,0.0,0.00543490145355463
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로",수식,-0.0031867004,0.0,0.003186700399965048
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로",용어 질문,0.008760691,0.0,0.008760690689086914
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로",인공지능,-0.015943132,0.0,0.01594313234090805
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로",잠시 휴식,0.00036165476,0.0,0.0003616547619458288
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로",좋아하는 아이돌,-0.0041152453,0.0,0.004115245305001736
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로",핵심 아이디어,0.005483252,0.0,0.0054832519963383675
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로",확률 예측에서 MSE Loss 미 사용 이유,0.001644929,0.0,0.0016449290560558438
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,BCE 가 좋은 task,0.0009366689,0.0,0.0009366688900627196
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,BCE 가 좋은 이유,0.004512766,0.0,0.004512765910476446
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,LLM Fine-Tuning 의 PEFT,0.020910751,0.0,0.020910751074552536
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,LoRA,0.008720754,0.0,0.008720753714442253
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,LoRA 와 QLoRA 의 차이,-0.017152123,0.0,0.017152123153209686
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,Loss Function 예시,-0.0049875593,0.0,0.0049875592812895775
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,Loss Function 정의,-0.013229553,0.0,0.013229552656412125
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,MBTI,-0.0084976535,0.0,0.008497653529047966
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,MSE Loss 설명,-0.036175303,0.0,0.03617530316114426
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,MSE Loss 용도,0.020100452,0.0,0.02010045200586319
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,Multi-Label 에서 CE + Softmax 적용 문제점,-0.0018518945,0.0,0.001851894543506205
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,PEFT 방법 5가지,0.027791046,0.0,0.027791045606136322
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,거대 언어 모델 정의,0.9881654,1.0,0.01183462142944336
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,기본 경험,0.0009405491,0.0,0.0009405490709468722
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,답변 실패,-0.0035151965,0.0,0.0035151964984834194
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,딥러닝,0.0005802085,0.0,0.0005802084924653172
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,마지막 할 말,-0.0014424039,0.0,0.0014424038818106055
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,머신러닝,-0.01633674,0.0,0.01633674092590809
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,면접 시작 인사,0.004572199,0.0,0.0045721991918981075
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,상세 경험,-0.0041718846,0.0,0.004171884618699551
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,수식,-0.013778501,0.0,0.013778501190245152
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,용어 질문,0.021940013,0.0,0.021940013393759727
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,인공지능,-0.012728766,0.0,0.012728765606880188
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,잠시 휴식,0.0017043281,0.0,0.001704328111372888
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,좋아하는 아이돌,-0.008787501,0.0,0.00878750067204237
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,핵심 아이디어,0.017354658,0.0,0.017354657873511314
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,확률 예측에서 MSE Loss 미 사용 이유,0.0030418427,0.0,0.00304184271954
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,BCE 가 좋은 task,-0.01188986,0.0,0.011889860033988953
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,BCE 가 좋은 이유,-0.010641794,0.0,0.010641793720424175
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,LLM Fine-Tuning 의 PEFT,0.040027753,0.0,0.04002775251865387
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,LoRA,0.012688615,0.0,0.01268861535936594
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,LoRA 와 QLoRA 의 차이,-0.028344566,0.0,0.028344566002488136
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,Loss Function 예시,0.023679186,0.0,0.023679185658693314
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,Loss Function 정의,-0.00689376,0.0,0.006893760059028864
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,MBTI,-0.016770802,0.0,0.016770802438259125
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,MSE Loss 설명,-0.022149766,0.0,0.022149765864014626
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,MSE Loss 용도,0.021536663,0.0,0.02153666317462921
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,Multi-Label 에서 CE + Softmax 적용 문제점,-0.00454336,0.0,0.004543359857052565
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,PEFT 방법 5가지,0.039082263,0.0,0.03908226266503334
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,거대 언어 모델 정의,0.97656435,0.0,0.976564347743988
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,기본 경험,-0.0063731926,0.0,0.006373192649334669
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,답변 실패,0.007667482,1.0,0.9923325181007385
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,딥러닝,0.0050198846,0.0,0.005019884556531906
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,마지막 할 말,0.0060110763,0.0,0.006011076271533966
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,머신러닝,-0.038391545,0.0,0.03839154541492462
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,면접 시작 인사,0.0011620722,0.0,0.0011620721779763699
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,상세 경험,0.034694687,0.0,0.03469468653202057
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,수식,-0.0145371035,0.0,0.014537103474140167
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,용어 질문,0.04384892,0.0,0.0438489206135273
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,인공지능,-0.0035120232,0.0,0.003512023249641061
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,잠시 휴식,-0.004597276,0.0,0.004597275983542204
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,좋아하는 아이돌,-0.007644194,0.0,0.007644194178283215
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,핵심 아이디어,0.026847018,0.0,0.02684701792895794
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,확률 예측에서 MSE Loss 미 사용 이유,-0.009421285,0.0,0.00942128524184227
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델,BCE 가 좋은 task,-0.0012250592,0.0,0.0012250591535121202
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델,BCE 가 좋은 이유,0.0065210303,0.0,0.006521030329167843
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델,LLM Fine-Tuning 의 PEFT,0.025306424,0.0,0.025306424126029015
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델,LoRA,0.011231396,0.0,0.011231396347284317
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델,LoRA 와 QLoRA 의 차이,-0.012960751,0.0,0.012960750609636307
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델,Loss Function 예시,-0.025561122,0.0,0.025561122223734856
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델,Loss Function 정의,-0.0043213805,0.0,0.004321380518376827
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델,MBTI,-0.016129741,0.0,0.016129741445183754
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델,MSE Loss 설명,-0.04005899,0.0,0.04005898907780647
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델,MSE Loss 용도,0.019993983,0.0,0.019993983209133148
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델,Multi-Label 에서 CE + Softmax 적용 문제점,0.00945832,0.0,0.009458320215344429
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델,PEFT 방법 5가지,0.024725143,0.0,0.024725142866373062
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델,거대 언어 모델 정의,0.9796165,1.0,0.020383477210998535
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델,기본 경험,0.0007180256,0.0,0.0007180256070569158
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델,답변 실패,0.0036440152,0.0,0.003644015174359083
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델,딥러닝,-0.0014900721,0.0,0.001490072114393115
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델,마지막 할 말,-0.0037399281,0.0,0.003739928128197789
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델,머신러닝,-0.0137849245,0.0,0.013784924522042274
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델,면접 시작 인사,0.0016060836,0.0,0.0016060835914686322
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델,상세 경험,0.002905524,0.0,0.0029055241029709578
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델,수식,-0.021447154,0.0,0.021447153761982918
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델,용어 질문,0.01836835,0.0,0.018368350341916084
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델,인공지능,-0.009069951,0.0,0.009069951251149178
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델,잠시 휴식,0.0006012388,0.0,0.0006012388039380312
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델,좋아하는 아이돌,-0.0057716295,0.0,0.005771629512310028
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델,핵심 아이디어,0.03471598,0.0,0.03471598029136658
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델,확률 예측에서 MSE Loss 미 사용 이유,0.004017249,0.0,0.0040172492153942585
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지.,BCE 가 좋은 task,0.0066181477,0.0,0.00661814771592617
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지.,BCE 가 좋은 이유,-0.013381634,0.0,0.013381633907556534
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지.,LLM Fine-Tuning 의 PEFT,0.004414527,0.0,0.004414527211338282
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지.,LoRA,-0.00016665526,0.0,0.0001666552561800927
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지.,LoRA 와 QLoRA 의 차이,-0.0012522445,0.0,0.001252244459465146
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지.,Loss Function 예시,-0.0112788705,0.0,0.011278870515525341
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지.,Loss Function 정의,-0.012649087,0.0,0.012649087235331535
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지.,MBTI,0.0006013205,0.0,0.0006013205274939537
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지.,MSE Loss 설명,0.0026469063,0.0,0.002646906301379204
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지.,MSE Loss 용도,-0.010051214,0.0,0.010051214136183262
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지.,Multi-Label 에서 CE + Softmax 적용 문제점,-0.0024934385,0.0,0.002493438543751836
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지.,PEFT 방법 5가지,-0.011778103,0.0,0.011778103187680244
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지.,거대 언어 모델 정의,0.014380209,0.0,0.014380209147930145
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지.,기본 경험,-0.0032911731,0.0,0.0032911731395870447
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지.,답변 실패,0.98994386,1.0,0.010056138038635254
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지.,딥러닝,-0.0013843015,0.0,0.0013843014603480697
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지.,마지막 할 말,0.0028155711,0.0,0.0028155711479485035
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지.,머신러닝,-0.0041166786,0.0,0.004116678610444069
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지.,면접 시작 인사,0.0020917794,0.0,0.002091779373586178
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지.,상세 경험,-0.001291424,0.0,0.001291424036026001
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지.,수식,-0.0051516783,0.0,0.005151678342372179
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지.,용어 질문,0.0068830964,0.0,0.006883096415549517
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지.,인공지능,-0.020030597,0.0,0.02003059722483158
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지.,잠시 휴식,-0.008608736,0.0,0.008608736097812653
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지.,좋아하는 아이돌,0.0013941617,0.0,0.0013941617216914892
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지.,핵심 아이디어,0.0099407965,0.0,0.00994079653173685
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지.,확률 예측에서 MSE Loss 미 사용 이유,-0.0039902874,0.0,0.00399028742685914
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,BCE 가 좋은 task,0.013940007,0.0,0.01394000742584467
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,BCE 가 좋은 이유,-0.03294539,0.0,0.03294539079070091
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,LLM Fine-Tuning 의 PEFT,0.007547437,0.0,0.007547437213361263
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,LoRA,-0.0024794145,0.0,0.0024794144555926323
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,LoRA 와 QLoRA 의 차이,0.0062445137,0.0,0.006244513671845198
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,Loss Function 예시,-0.002479484,0.0,0.002479484071955085
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,Loss Function 정의,0.073059216,0.0,0.07305921614170074
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,MBTI,0.0116790915,0.0,0.011679091490805149
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,MSE Loss 설명,0.005354306,0.0,0.005354306194931269
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,MSE Loss 용도,-0.011995498,0.0,0.011995498090982437
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,Multi-Label 에서 CE + Softmax 적용 문제점,-0.010254193,0.0,0.010254193097352982
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,PEFT 방법 5가지,-0.0005182287,0.0,0.0005182286840863526
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,거대 언어 모델 정의,-0.0029983355,0.0,0.0029983355198055506
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,기본 경험,-0.011428248,0.0,0.011428248137235641
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,답변 실패,0.95028275,1.0,0.0497172474861145
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,딥러닝,0.019995663,0.0,0.019995663315057755
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,마지막 할 말,0.018396981,0.0,0.018396981060504913
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,머신러닝,0.0013077449,0.0,0.001307744882069528
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,면접 시작 인사,-0.012534791,0.0,0.01253479067236185
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,상세 경험,-0.011300639,0.0,0.011300639249384403
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,수식,-0.018684333,0.0,0.018684333190321922
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,용어 질문,0.009147884,0.0,0.009147884324193
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,인공지능,-0.020484518,0.0,0.020484518259763718
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,잠시 휴식,-0.016785186,0.0,0.016785185784101486
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,좋아하는 아이돌,-0.010022353,0.0,0.010022353380918503
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,핵심 아이디어,0.011665918,0.0,0.011665917932987213
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,확률 예측에서 MSE Loss 미 사용 이유,-0.019585961,0.0,0.01958596147596836
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",BCE 가 좋은 task,-0.00088120083,0.0,0.0008812008309178054
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",BCE 가 좋은 이유,0.0057048015,0.0,0.00570480152964592
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",LLM Fine-Tuning 의 PEFT,-0.002450993,0.0,0.002450993051752448
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",LoRA,-0.005294524,0.0,0.005294524133205414
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",LoRA 와 QLoRA 의 차이,-0.0038255746,0.0,0.0038255746476352215
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",Loss Function 예시,0.0017004394,0.0,0.0017004393739625812
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",Loss Function 정의,0.9759092,1.0,0.024090826511383057
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",MBTI,-0.016717425,0.0,0.016717424616217613
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",MSE Loss 설명,-0.028150508,0.0,0.028150508180260658
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",MSE Loss 용도,-0.024532631,0.0,0.024532631039619446
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",Multi-Label 에서 CE + Softmax 적용 문제점,-0.0050747753,0.0,0.0050747753120958805
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",PEFT 방법 5가지,-0.023514254,0.0,0.02351425401866436
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",거대 언어 모델 정의,-0.032041155,0.0,0.03204115480184555
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",기본 경험,0.0050835744,0.0,0.0050835744477808475
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",답변 실패,0.0047497777,0.0,0.004749777726829052
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",딥러닝,-0.030124294,0.0,0.030124293640255928
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",마지막 할 말,-0.012485916,0.0,0.012485915794968605
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",머신러닝,-0.0094508575,0.0,0.009450857527554035
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",면접 시작 인사,0.022470316,0.0,0.022470315918326378
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",상세 경험,-0.013219757,0.0,0.013219757005572319
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",수식,-0.026924947,0.0,0.026924947276711464
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",용어 질문,0.014980738,0.0,0.014980738051235676
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",인공지능,0.014669954,0.0,0.014669953845441341
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",잠시 휴식,0.005035985,0.0,0.005035984795540571
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",좋아하는 아이돌,-0.005431166,0.0,0.005431165918707848
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",핵심 아이디어,-0.0052687023,0.0,0.005268702283501625
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",확률 예측에서 MSE Loss 미 사용 이유,-0.017422639,0.0,0.017422638833522797
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야,BCE 가 좋은 task,0.00952811,0.0,0.009528109803795815
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야,BCE 가 좋은 이유,-0.01037286,0.0,0.010372860357165337
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야,LLM Fine-Tuning 의 PEFT,0.003914991,0.0,0.003914990928024054
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야,LoRA,-0.0015908498,0.0,0.001590849831700325
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야,LoRA 와 QLoRA 의 차이,0.0016113435,0.0,0.0016113434685394168
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야,Loss Function 예시,-0.005120573,0.0,0.005120573099702597
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야,Loss Function 정의,-0.0007194111,0.0,0.000719411124009639
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야,MBTI,0.0022411915,0.0,0.002241191454231739
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야,MSE Loss 설명,0.0034754546,0.0,0.0034754546359181404
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야,MSE Loss 용도,-0.0026094515,0.0,0.0026094515342265368
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야,Multi-Label 에서 CE + Softmax 적용 문제점,-0.0026841923,0.0,0.002684192266315222
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야,PEFT 방법 5가지,-0.0013011275,0.0,0.0013011274859309196
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야,거대 언어 모델 정의,-0.0011647512,0.0,0.0011647512437775731
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야,기본 경험,-0.00710795,0.0,0.007107949815690517
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야,답변 실패,0.9925455,1.0,0.007454514503479004
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야,딥러닝,0.0012729369,0.0,0.0012729369336739182
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야,마지막 할 말,-0.0028220338,0.0,0.002822033828124404
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야,머신러닝,-0.0014231626,0.0,0.001423162641003728
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야,면접 시작 인사,-0.0017639356,0.0,0.0017639355501160026
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야,상세 경험,-0.00953463,0.0,0.009534629993140697
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야,수식,-0.01241215,0.0,0.012412150390446186
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야,용어 질문,-0.009899775,0.0,0.009899774566292763
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야,인공지능,-0.020952234,0.0,0.02095223404467106
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야,잠시 휴식,-0.009344386,0.0,0.00934438593685627
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야,좋아하는 아이돌,0.00224949,0.0,0.0022494900040328503
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야,핵심 아이디어,0.0065516816,0.0,0.0065516815520823
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야,확률 예측에서 MSE Loss 미 사용 이유,-0.002490647,0.0,0.002490646904334426
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지,BCE 가 좋은 task,0.00603674,0.0,0.00603673979640007
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지,BCE 가 좋은 이유,0.0021277824,0.0,0.002127782441675663
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지,LLM Fine-Tuning 의 PEFT,-0.013369951,0.0,0.013369951397180557
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지,LoRA,-0.010055935,0.0,0.010055935010313988
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지,LoRA 와 QLoRA 의 차이,-0.016638912,0.0,0.01663891226053238
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지,Loss Function 예시,-0.0013157938,0.0,0.0013157938374206424
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지,Loss Function 정의,0.9778347,1.0,0.022165298461914062
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지,MBTI,-0.016330069,0.0,0.016330068930983543
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지,MSE Loss 설명,-0.02069684,0.0,0.020696839317679405
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지,MSE Loss 용도,-0.02678261,0.0,0.026782609522342682
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지,Multi-Label 에서 CE + Softmax 적용 문제점,-0.013517047,0.0,0.01351704727858305
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지,PEFT 방법 5가지,-0.03138941,0.0,0.03138941153883934
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지,거대 언어 모델 정의,-0.011774725,0.0,0.011774725280702114
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지,기본 경험,0.005853487,0.0,0.005853487178683281
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지,답변 실패,-0.008529991,0.0,0.008529990911483765
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지,딥러닝,-0.018474942,0.0,0.018474942073225975
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지,마지막 할 말,-0.008444768,0.0,0.008444768376648426
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지,머신러닝,0.005551176,0.0,0.005551176145672798
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지,면접 시작 인사,0.027816,0.0,0.02781599946320057
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지,상세 경험,-0.0150449155,0.0,0.015044915489852428
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지,수식,-0.017396336,0.0,0.017396336421370506
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지,용어 질문,-0.0017821938,0.0,0.0017821937799453735
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지,인공지능,0.013453252,0.0,0.01345325168222189
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지,잠시 휴식,0.013494558,0.0,0.013494557701051235
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지,좋아하는 아이돌,-0.001966388,0.0,0.001966387964785099
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지,핵심 아이디어,0.0049658855,0.0,0.004965885542333126
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지,확률 예측에서 MSE Loss 미 사용 이유,-0.015091429,0.0,0.015091429464519024
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,BCE 가 좋은 task,0.0048030405,0.0,0.0048030405305325985
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,BCE 가 좋은 이유,-0.012488822,0.0,0.01248882245272398
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,LLM Fine-Tuning 의 PEFT,0.0017541354,0.0,0.0017541353590786457
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,LoRA,0.0019351414,0.0,0.0019351413939148188
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,LoRA 와 QLoRA 의 차이,-0.005150924,0.0,0.0051509239710867405
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,Loss Function 예시,-0.0042975876,0.0,0.004297587554901838
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,Loss Function 정의,-0.0068591335,0.0,0.00685913348570466
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,MBTI,-0.0018052239,0.0,0.0018052238738164306
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,MSE Loss 설명,0.0030448388,0.0,0.003044838784262538
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,MSE Loss 용도,-0.0047051706,0.0,0.004705170635133982
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,Multi-Label 에서 CE + Softmax 적용 문제점,0.0028753215,0.0,0.0028753215447068214
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,PEFT 방법 5가지,-0.0017205814,0.0,0.0017205814365297556
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,거대 언어 모델 정의,-0.0039062724,0.0,0.003906272351741791
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,기본 경험,-0.006226132,0.0,0.006226132158190012
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,답변 실패,0.99227905,1.0,0.007720947265625
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,딥러닝,-0.0033940428,0.0,0.0033940428402274847
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,마지막 할 말,-0.0041318983,0.0,0.004131898283958435
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,머신러닝,0.000799634,0.0,0.0007996340282261372
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,면접 시작 인사,-0.0011891059,0.0,0.001189105911180377
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,상세 경험,-0.007458962,0.0,0.00745896203443408
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,수식,-0.016150173,0.0,0.01615017279982567
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,용어 질문,0.0018872672,0.0,0.0018872672226279974
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,인공지능,-0.01758603,0.0,0.017586030066013336
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,잠시 휴식,0.001251685,0.0,0.0012516849674284458
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,좋아하는 아이돌,-0.0011105391,0.0,0.001110539073124528
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,핵심 아이디어,0.004614391,0.0,0.0046143908984959126
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,확률 예측에서 MSE Loss 미 사용 이유,-0.005040392,0.0,0.005040391813963652
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",BCE 가 좋은 task,-0.007579273,0.0,0.0075792730785906315
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",BCE 가 좋은 이유,-0.0027389748,0.0,0.002738974755629897
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",LLM Fine-Tuning 의 PEFT,-0.045857962,0.0,0.04585796222090721
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",LoRA,0.0032076102,0.0,0.003207610221579671
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",LoRA 와 QLoRA 의 차이,-0.0033590584,0.0,0.003359058406203985
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",Loss Function 예시,0.9864319,1.0,0.013568103313446045
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",Loss Function 정의,-0.027651148,0.0,0.027651147916913033
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",MBTI,0.007285797,0.0,0.0072857970371842384
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",MSE Loss 설명,0.004763969,0.0,0.004763969220221043
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",MSE Loss 용도,0.0021380347,0.0,0.002138034673407674
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",Multi-Label 에서 CE + Softmax 적용 문제점,-0.019388786,0.0,0.01938878558576107
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",PEFT 방법 5가지,-0.0009913897,0.0,0.0009913897374644876
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",거대 언어 모델 정의,-0.012563465,0.0,0.012563465163111687
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",기본 경험,-0.0050820443,0.0,0.005082044284790754
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",답변 실패,-0.006023176,0.0,0.00602317601442337
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",딥러닝,-0.019594103,0.0,0.01959410309791565
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",마지막 할 말,-0.020712625,0.0,0.020712625235319138
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",머신러닝,-0.015244964,0.0,0.015244963578879833
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",면접 시작 인사,0.0054724393,0.0,0.005472439341247082
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",상세 경험,-0.028035281,0.0,0.028035281226038933
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",수식,-0.020695034,0.0,0.0206950344145298
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",용어 질문,-0.0070118713,0.0,0.0070118713192641735
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",인공지능,0.0019869963,0.0,0.0019869962707161903
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",잠시 휴식,-0.016463201,0.0,0.016463201493024826
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",좋아하는 아이돌,-0.0034569113,0.0,0.0034569113049656153
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",핵심 아이디어,-0.0020882937,0.0,0.0020882936660200357
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",확률 예측에서 MSE Loss 미 사용 이유,-0.0051244395,0.0,0.005124439485371113
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어",BCE 가 좋은 task,0.0007464733,0.0,0.0007464733207598329
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어",BCE 가 좋은 이유,0.0036733036,0.0,0.0036733036395162344
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어",LLM Fine-Tuning 의 PEFT,-0.03849781,0.0,0.03849780932068825
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어",LoRA,0.005447684,0.0,0.005447683855891228
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어",LoRA 와 QLoRA 의 차이,-0.009220218,0.0,0.009220218285918236
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어",Loss Function 예시,0.98392785,1.0,0.01607215404510498
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어",Loss Function 정의,-0.030750787,0.0,0.030750786885619164
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어",MBTI,0.00691695,0.0,0.006916949991136789
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어",MSE Loss 설명,-0.032001298,0.0,0.032001297920942307
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어",MSE Loss 용도,-0.0063440553,0.0,0.006344055291265249
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어",Multi-Label 에서 CE + Softmax 적용 문제점,-0.012362928,0.0,0.012362928129732609
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어",PEFT 방법 5가지,0.01020837,0.0,0.01020837016403675
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어",거대 언어 모델 정의,0.004600811,0.0,0.004600810818374157
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어",기본 경험,-0.009888711,0.0,0.009888711385428905
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어",답변 실패,-0.011538892,0.0,0.011538892053067684
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어",딥러닝,-0.020943418,0.0,0.02094341814517975
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어",마지막 할 말,-0.016425926,0.0,0.016425926238298416
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어",머신러닝,-0.023722608,0.0,0.023722607642412186
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어",면접 시작 인사,0.006804312,0.0,0.006804312113672495
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어",상세 경험,-0.006830216,0.0,0.00683021591976285
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어",수식,-0.010057796,0.0,0.01005779579281807
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어",용어 질문,-0.0007379827,0.0,0.0007379826856777072
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어",인공지능,0.0035208422,0.0,0.0035208421759307384
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어",잠시 휴식,-0.005868946,0.0,0.005868946202099323
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어",좋아하는 아이돌,-0.0035469537,0.0,0.0035469536669552326
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어",핵심 아이디어,0.009314298,0.0,0.009314297698438168
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어",확률 예측에서 MSE Loss 미 사용 이유,-0.007852894,0.0,0.007852894254028797
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야?",BCE 가 좋은 task,0.008554765,0.0,0.008554765023291111
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야?",BCE 가 좋은 이유,-0.013850007,0.0,0.013850007206201553
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야?",LLM Fine-Tuning 의 PEFT,-0.00029233162,0.0,0.00029233162058517337
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야?",LoRA,-0.0008851782,0.0,0.0008851782185956836
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야?",LoRA 와 QLoRA 의 차이,-0.0029110042,0.0,0.002911004237830639
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야?",Loss Function 예시,-0.010396035,0.0,0.010396035388112068
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야?",Loss Function 정의,-0.00861701,0.0,0.008617009967565536
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야?",MBTI,-0.0039854106,0.0,0.0039854105561971664
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야?",MSE Loss 설명,0.0054839165,0.0,0.005483916494995356
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야?",MSE Loss 용도,-0.005973686,0.0,0.005973685998469591
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야?",Multi-Label 에서 CE + Softmax 적용 문제점,-0.0015226756,0.0,0.00152267562225461
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야?",PEFT 방법 5가지,0.0040830397,0.0,0.004083039704710245
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야?",거대 언어 모델 정의,-0.00063522166,0.0,0.0006352216587401927
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야?",기본 경험,-0.0029357597,0.0,0.002935759723186493
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야?",답변 실패,0.99112195,1.0,0.008878052234649658
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야?",딥러닝,-0.0016793649,0.0,0.0016793649410828948
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야?",마지막 할 말,-0.007865502,0.0,0.007865501567721367
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야?",머신러닝,-0.00033473084,0.0,0.0003347308374941349
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야?",면접 시작 인사,-0.00079566526,0.0,0.0007956652552820742
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야?",상세 경험,-0.0029301709,0.0,0.0029301708564162254
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야?",수식,-0.0096331285,0.0,0.00963312853127718
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야?",용어 질문,-0.006091759,0.0,0.0060917590744793415
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야?",인공지능,-0.018292103,0.0,0.018292102962732315
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야?",잠시 휴식,-0.00031193445,0.0,0.00031193444738164544
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야?",좋아하는 아이돌,-0.0016658523,0.0,0.001665852265432477
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야?",핵심 아이디어,0.0038146882,0.0,0.0038146881852298975
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야?",확률 예측에서 MSE Loss 미 사용 이유,-0.003363589,0.0,0.003363589057698846
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어",BCE 가 좋은 task,0.0017286814,0.0,0.00172868138179183
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어",BCE 가 좋은 이유,0.0062464946,0.0,0.006246494594961405
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어",LLM Fine-Tuning 의 PEFT,-0.03280458,0.0,0.03280457854270935
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어",LoRA,-0.007310901,0.0,0.007310900837182999
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어",LoRA 와 QLoRA 의 차이,-0.0063189804,0.0,0.006318980362266302
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어",Loss Function 예시,0.9863351,1.0,0.01366490125656128
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어",Loss Function 정의,-0.008474195,0.0,0.008474195376038551
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어",MBTI,0.006852741,0.0,0.0068527408875525
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어",MSE Loss 설명,-0.014760834,0.0,0.014760834164917469
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어",MSE Loss 용도,0.009957621,0.0,0.00995762087404728
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어",Multi-Label 에서 CE + Softmax 적용 문제점,-0.014916248,0.0,0.014916247688233852
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어",PEFT 방법 5가지,-0.004208321,0.0,0.004208321217447519
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어",거대 언어 모델 정의,-0.0104864435,0.0,0.010486443527042866
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어",기본 경험,-0.013033998,0.0,0.01303399819880724
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어",답변 실패,-0.011783746,0.0,0.01178374607115984
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어",딥러닝,-0.03652614,0.0,0.036526139825582504
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어",마지막 할 말,-0.017993445,0.0,0.017993444576859474
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어",머신러닝,-0.01645197,0.0,0.016451969742774963
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어",면접 시작 인사,-0.007177225,0.0,0.007177224848419428
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어",상세 경험,-0.019917237,0.0,0.019917236641049385
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어",수식,-0.01498382,0.0,0.014983819797635078
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어",용어 질문,0.004206689,0.0,0.004206689074635506
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어",인공지능,0.005034198,0.0,0.005034198053181171
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어",잠시 휴식,-0.016244823,0.0,0.01624482311308384
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어",좋아하는 아이돌,-0.002367017,0.0,0.0023670170921832323
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어",핵심 아이디어,0.0050321356,0.0,0.0050321356393396854
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어",확률 예측에서 MSE Loss 미 사용 이유,-0.011732692,0.0,0.011732691898941994
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아",BCE 가 좋은 task,0.0006619644,0.0,0.0006619644118472934
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아",BCE 가 좋은 이유,0.005420485,0.0,0.00542048504576087
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아",LLM Fine-Tuning 의 PEFT,-0.05286118,0.0,0.052861180156469345
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아",LoRA,-0.005259002,0.0,0.0052590020932257175
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아",LoRA 와 QLoRA 의 차이,-0.0031033913,0.0,0.0031033912673592567
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아",Loss Function 예시,0.98689395,1.0,0.013106048107147217
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아",Loss Function 정의,-0.01781509,0.0,0.017815090715885162
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아",MBTI,0.0019198165,0.0,0.001919816480949521
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아",MSE Loss 설명,-0.024841717,0.0,0.02484171651303768
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아",MSE Loss 용도,-0.0016423353,0.0,0.0016423353226855397
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아",Multi-Label 에서 CE + Softmax 적용 문제점,-0.005254766,0.0,0.005254765972495079
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아",PEFT 방법 5가지,0.0023342217,0.0,0.0023342217318713665
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아",거대 언어 모델 정의,-0.010406227,0.0,0.010406226851046085
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아",기본 경험,-0.016934868,0.0,0.016934867948293686
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아",답변 실패,-0.009370023,0.0,0.009370023384690285
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아",딥러닝,-0.01959427,0.0,0.01959427073597908
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아",마지막 할 말,-0.018816859,0.0,0.018816858530044556
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아",머신러닝,-0.018278213,0.0,0.0182782132178545
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아",면접 시작 인사,0.005353649,0.0,0.0053536491468548775
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아",상세 경험,-0.018043404,0.0,0.018043404445052147
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아",수식,-0.011133496,0.0,0.011133495718240738
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아",용어 질문,-0.0027725904,0.0,0.0027725903782993555
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아",인공지능,6.934181e-05,0.0,6.934181146789342e-05
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아",잠시 휴식,-0.016581073,0.0,0.01658107340335846
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아",좋아하는 아이돌,-0.0025836083,0.0,0.002583608264103532
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아",핵심 아이디어,0.011343796,0.0,0.01134379580616951
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아",확률 예측에서 MSE Loss 미 사용 이유,-0.011584929,0.0,0.011584929190576077
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,BCE 가 좋은 task,0.002416875,0.0,0.0024168749805539846
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,BCE 가 좋은 이유,-0.03423263,0.0,0.03423263132572174
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,LLM Fine-Tuning 의 PEFT,0.016789474,0.0,0.016789473593235016
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,LoRA,0.017233968,0.0,0.017233967781066895
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,LoRA 와 QLoRA 의 차이,-0.005627464,0.0,0.005627464037388563
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,Loss Function 예시,0.006491921,0.0,0.0064919209107756615
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,Loss Function 정의,-0.035060532,0.0,0.03506053239107132
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,MBTI,-0.024604974,0.0,0.024604974314570427
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,MSE Loss 설명,0.9750279,1.0,0.024972081184387207
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,MSE Loss 용도,-0.027752519,0.0,0.02775251865386963
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,Multi-Label 에서 CE + Softmax 적용 문제점,-0.013331344,0.0,0.013331344351172447
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,PEFT 방법 5가지,-0.004612351,0.0,0.004612350836396217
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,거대 언어 모델 정의,-0.04131773,0.0,0.04131773114204407
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,기본 경험,0.0012212495,0.0,0.0012212494621053338
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,답변 실패,0.0061326926,0.0,0.006132692564278841
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,딥러닝,0.006892315,0.0,0.006892315112054348
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,마지막 할 말,-0.012008512,0.0,0.012008512392640114
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,머신러닝,0.0124606425,0.0,0.012460642494261265
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,면접 시작 인사,0.0126270335,0.0,0.01262703351676464
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,상세 경험,0.0011453372,0.0,0.0011453372426331043
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,수식,-0.023067934,0.0,0.023067934438586235
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,용어 질문,-0.010041137,0.0,0.010041137225925922
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,인공지능,0.007799701,0.0,0.0077997008338570595
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,잠시 휴식,-0.012911195,0.0,0.012911194935441017
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,좋아하는 아이돌,0.00017505423,0.0,0.00017505422874819487
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,핵심 아이디어,-0.021029776,0.0,0.021029775962233543
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,확률 예측에서 MSE Loss 미 사용 이유,-0.024524648,0.0,0.024524647742509842
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,BCE 가 좋은 task,0.004424094,0.0,0.004424094222486019
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,BCE 가 좋은 이유,-0.019544423,0.0,0.01954442262649536
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,LLM Fine-Tuning 의 PEFT,0.0072303894,0.0,0.0072303893975913525
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,LoRA,0.005804353,0.0,0.005804352927953005
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,LoRA 와 QLoRA 의 차이,0.013431164,0.0,0.013431164436042309
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,Loss Function 예시,0.021734424,0.0,0.02173442393541336
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,Loss Function 정의,-0.0018828452,0.0,0.0018828451866284013
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,MBTI,-0.0093665775,0.0,0.009366577491164207
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,MSE Loss 설명,0.031946365,0.0,0.03194636479020119
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,MSE Loss 용도,0.00017238267,0.0,0.00017238267173524946
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,Multi-Label 에서 CE + Softmax 적용 문제점,-0.012698634,0.0,0.012698633596301079
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,PEFT 방법 5가지,-0.014842826,0.0,0.014842825941741467
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,거대 언어 모델 정의,-0.018020501,0.0,0.018020501360297203
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,기본 경험,0.010370886,0.0,0.010370885953307152
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,답변 실패,0.9469827,1.0,0.05301731824874878
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,딥러닝,0.0016852521,0.0,0.001685252063907683
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,마지막 할 말,-0.01894378,0.0,0.018943779170513153
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,머신러닝,-0.0078601865,0.0,0.007860186509788036
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,면접 시작 인사,-0.019965002,0.0,0.019965002313256264
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,상세 경험,-0.0028542615,0.0,0.0028542615473270416
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,수식,-0.0038173883,0.0,0.0038173883222043514
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,용어 질문,0.007992336,0.0,0.007992335595190525
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,인공지능,-0.015907835,0.0,0.015907835215330124
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,잠시 휴식,-0.007702163,0.0,0.007702162954956293
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,좋아하는 아이돌,-0.0052788816,0.0,0.005278881639242172
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,핵심 아이디어,-0.012293866,0.0,0.012293865904211998
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,확률 예측에서 MSE Loss 미 사용 이유,-0.016806459,0.0,0.016806459054350853
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",BCE 가 좋은 task,0.008261698,0.0,0.008261698298156261
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",BCE 가 좋은 이유,-0.005162777,0.0,0.005162776913493872
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",LLM Fine-Tuning 의 PEFT,-0.018695755,0.0,0.018695754930377007
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",LoRA,-0.00940591,0.0,0.009405910037457943
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",LoRA 와 QLoRA 의 차이,0.0016057715,0.0,0.0016057714819908142
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",Loss Function 예시,-0.0062589166,0.0,0.006258916575461626
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",Loss Function 정의,-0.006324807,0.0,0.006324807181954384
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",MBTI,-0.0076556243,0.0,0.00765562430024147
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",MSE Loss 설명,-0.013911391,0.0,0.01391139067709446
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",MSE Loss 용도,0.979249,1.0,0.020750999450683594
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",Multi-Label 에서 CE + Softmax 적용 문제점,0.012718119,0.0,0.012718118727207184
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",PEFT 방법 5가지,0.008787666,0.0,0.008787666447460651
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",거대 언어 모델 정의,0.025992071,0.0,0.025992071256041527
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",기본 경험,0.010883961,0.0,0.010883960872888565
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",답변 실패,-0.014956126,0.0,0.014956125989556313
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",딥러닝,0.013899083,0.0,0.013899083249270916
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",마지막 할 말,-0.0047609587,0.0,0.004760958719998598
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",머신러닝,0.01163552,0.0,0.011635519564151764
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",면접 시작 인사,0.008930654,0.0,0.008930654264986515
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",상세 경험,-0.018349105,0.0,0.01834910549223423
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",수식,0.0010517263,0.0,0.0010517262853682041
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",용어 질문,-0.004444741,0.0,0.004444741178303957
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",인공지능,0.00010792561,0.0,0.00010792561079142615
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",잠시 휴식,0.0056088525,0.0,0.005608852487057447
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",좋아하는 아이돌,-0.007907632,0.0,0.007907631807029247
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",핵심 아이디어,-0.021369623,0.0,0.02136962302029133
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",확률 예측에서 MSE Loss 미 사용 이유,-0.007366875,0.0,0.007366875186562538
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,BCE 가 좋은 task,0.004707714,0.0,0.004707714077085257
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,BCE 가 좋은 이유,-0.01012104,0.0,0.010121040046215057
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,LLM Fine-Tuning 의 PEFT,0.0078000035,0.0,0.0078000035136938095
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,LoRA,-0.002956452,0.0,0.0029564520809799433
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,LoRA 와 QLoRA 의 차이,0.00014085772,0.0,0.00014085772272665054
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,Loss Function 예시,-0.011178115,0.0,0.011178115382790565
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,Loss Function 정의,-0.010161192,0.0,0.010161192156374454
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,MBTI,-0.0039078468,0.0,0.003907846752554178
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,MSE Loss 설명,0.0019819231,0.0,0.001981923123821616
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,MSE Loss 용도,0.002240026,0.0,0.0022400259040296078
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,Multi-Label 에서 CE + Softmax 적용 문제점,0.0014368145,0.0,0.0014368145493790507
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,PEFT 방법 5가지,-0.007631833,0.0,0.007631833199411631
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,거대 언어 모델 정의,-0.008365469,0.0,0.008365469053387642
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,기본 경험,-0.008736473,0.0,0.008736472576856613
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,답변 실패,0.99217975,1.0,0.0078202486038208
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,딥러닝,-0.007733082,0.0,0.007733081933110952
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,마지막 할 말,-0.0031661892,0.0,0.00316618918441236
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,머신러닝,-0.0022413055,0.0,0.0022413055412471294
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,면접 시작 인사,0.0019385336,0.0,0.0019385336199775338
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,상세 경험,-0.0026083507,0.0,0.0026083507109433413
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,수식,-0.0108007025,0.0,0.01080070249736309
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,용어 질문,-6.6723645e-05,0.0,6.672364543192089e-05
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,인공지능,-0.016738899,0.0,0.016738899052143097
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,잠시 휴식,-0.0043049795,0.0,0.004304979462176561
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,좋아하는 아이돌,0.0038486605,0.0,0.0038486605044454336
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,핵심 아이디어,0.008903015,0.0,0.008903015404939651
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,확률 예측에서 MSE Loss 미 사용 이유,-0.00030312387,0.0,0.0003031238738913089
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,BCE 가 좋은 task,0.007071623,0.0,0.007071623113006353
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,BCE 가 좋은 이유,-0.011992797,0.0,0.011992797255516052
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,LLM Fine-Tuning 의 PEFT,0.0013335954,0.0,0.0013335953699424863
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,LoRA,0.0020255703,0.0,0.0020255702547729015
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,LoRA 와 QLoRA 의 차이,0.00066063635,0.0,0.0006606363458558917
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,Loss Function 예시,0.00017005503,0.0,0.00017005503468681127
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,Loss Function 정의,-0.008427098,0.0,0.008427098393440247
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,MBTI,-0.00151883,0.0,0.001518829958513379
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,MSE Loss 설명,0.0059815184,0.0,0.005981518421322107
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,MSE Loss 용도,-0.002558924,0.0,0.002558924024924636
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,Multi-Label 에서 CE + Softmax 적용 문제점,-0.0013196045,0.0,0.0013196044601500034
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,PEFT 방법 5가지,-0.005343604,0.0,0.005343603901565075
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,거대 언어 모델 정의,-0.0054450342,0.0,0.005445034243166447
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,기본 경험,-0.0073173973,0.0,0.007317397277802229
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,답변 실패,0.9898443,1.0,0.010155677795410156
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,딥러닝,-0.004966859,0.0,0.004966858774423599
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,마지막 할 말,-0.01481765,0.0,0.014817650429904461
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,머신러닝,-0.006484091,0.0,0.006484090816229582
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,면접 시작 인사,-0.0018698671,0.0,0.0018698670901358128
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,상세 경험,-0.009934326,0.0,0.009934325702488422
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,수식,-0.005943977,0.0,0.005943976808339357
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,용어 질문,-0.005275266,0.0,0.005275265779346228
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,인공지능,-0.014284161,0.0,0.014284160919487476
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,잠시 휴식,-0.0038356704,0.0,0.003835670417174697
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,좋아하는 아이돌,0.009437363,0.0,0.009437362663447857
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,핵심 아이디어,0.008742911,0.0,0.00874291080981493
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,확률 예측에서 MSE Loss 미 사용 이유,0.010093313,0.0,0.010093312710523605
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,BCE 가 좋은 task,0.0015090829,0.0,0.0015090828528627753
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,BCE 가 좋은 이유,-0.03084547,0.0,0.03084547072649002
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,LLM Fine-Tuning 의 PEFT,-0.02170339,0.0,0.021703390404582024
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,LoRA,-0.015217179,0.0,0.01521717943251133
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,LoRA 와 QLoRA 의 차이,0.0046728877,0.0,0.004672887735068798
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,Loss Function 예시,-0.023979854,0.0,0.023979853838682175
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,Loss Function 정의,0.0026672124,0.0,0.0026672123931348324
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,MBTI,-0.008170999,0.0,0.00817099865525961
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,MSE Loss 설명,-0.003369776,0.0,0.00336977606639266
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,MSE Loss 용도,-0.008458316,0.0,0.008458316326141357
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,Multi-Label 에서 CE + Softmax 적용 문제점,0.0075023244,0.0,0.007502324413508177
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,PEFT 방법 5가지,-0.006487749,0.0,0.006487749051302671
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,거대 언어 모델 정의,-0.011000139,0.0,0.011000138707458973
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,기본 경험,-0.01337621,0.0,0.013376209884881973
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,답변 실패,0.015597454,0.0,0.015597454272210598
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,딥러닝,0.002379339,0.0,0.0023793389555066824
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,마지막 할 말,0.0026849632,0.0,0.0026849631685763597
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,머신러닝,-0.0016433607,0.0,0.0016433607088401914
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,면접 시작 인사,0.003019561,0.0,0.0030195610597729683
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,상세 경험,-0.008182509,0.0,0.008182508870959282
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,수식,-0.0061005726,0.0,0.006100572645664215
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,용어 질문,-0.005399852,0.0,0.005399852059781551
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,인공지능,0.001723811,0.0,0.0017238110303878784
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,잠시 휴식,-0.020261053,0.0,0.02026105299592018
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,좋아하는 아이돌,0.0055416822,0.0,0.005541682243347168
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,핵심 아이디어,0.006287903,0.0,0.006287903059273958
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,확률 예측에서 MSE Loss 미 사용 이유,0.96188235,1.0,0.03811764717102051
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,BCE 가 좋은 task,-0.01589108,0.0,0.01589108072221279
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,BCE 가 좋은 이유,-0.04313532,0.0,0.04313531890511513
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,LLM Fine-Tuning 의 PEFT,0.019042226,0.0,0.019042225554585457
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,LoRA,-0.022492487,0.0,0.022492486983537674
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,LoRA 와 QLoRA 의 차이,0.01354643,0.0,0.013546429574489594
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,Loss Function 예시,0.0053603672,0.0,0.005360367242246866
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,Loss Function 정의,-0.033291373,0.0,0.033291373401880264
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,MBTI,-0.017687865,0.0,0.01768786460161209
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,MSE Loss 설명,-0.07410825,0.0,0.07410825043916702
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,MSE Loss 용도,-0.008786022,0.0,0.00878602173179388
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,Multi-Label 에서 CE + Softmax 적용 문제점,-0.011383389,0.0,0.011383389122784138
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,PEFT 방법 5가지,0.017547982,0.0,0.017547981813549995
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,거대 언어 모델 정의,-0.0067667686,0.0,0.006766768638044596
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,기본 경험,-0.0062895133,0.0,0.006289513316005468
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,답변 실패,0.6072537,0.0,0.6072536706924438
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,딥러닝,-0.044577178,0.0,0.04457717761397362
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,마지막 할 말,-0.013680088,0.0,0.013680088333785534
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,머신러닝,-0.033765446,0.0,0.03376544639468193
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,면접 시작 인사,-0.021308232,0.0,0.021308232098817825
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,상세 경험,-0.030348653,0.0,0.030348652973771095
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,수식,0.6623566,1.0,0.3376433849334717
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,용어 질문,-0.00621506,0.0,0.006215060129761696
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,인공지능,-0.015143624,0.0,0.0151436235755682
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,잠시 휴식,-0.021981962,0.0,0.021981962025165558
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,좋아하는 아이돌,-0.0076430677,0.0,0.007643067743629217
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,핵심 아이디어,-0.009711324,0.0,0.009711324237287045
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,확률 예측에서 MSE Loss 미 사용 이유,-0.0054399516,0.0,0.005439951550215483
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",BCE 가 좋은 task,-0.008236801,0.0,0.008236801251769066
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",BCE 가 좋은 이유,-0.007084771,0.0,0.007084771059453487
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",LLM Fine-Tuning 의 PEFT,0.0061373706,0.0,0.006137370597571135
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",LoRA,0.015375089,0.0,0.015375088900327682
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",LoRA 와 QLoRA 의 차이,0.019147122,0.0,0.019147122278809547
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",Loss Function 예시,0.022872329,0.0,0.022872328758239746
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",Loss Function 정의,-0.013085947,0.0,0.013085947372019291
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",MBTI,0.0005148473,0.0,0.0005148472846485674
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",MSE Loss 설명,-0.009460159,0.0,0.00946015864610672
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",MSE Loss 용도,-0.018064816,0.0,0.018064815551042557
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",Multi-Label 에서 CE + Softmax 적용 문제점,0.0057634823,0.0,0.005763482302427292
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",PEFT 방법 5가지,-0.0015836499,0.0,0.0015836498932912946
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",거대 언어 모델 정의,-0.016161593,0.0,0.016161592677235603
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",기본 경험,-0.019021485,0.0,0.01902148500084877
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",답변 실패,0.0073999153,0.0,0.007399915251880884
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",딥러닝,-0.01793902,0.0,0.017939019948244095
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",마지막 할 말,0.020942872,0.0,0.020942872390151024
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",머신러닝,0.015803887,0.0,0.015803886577486992
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",면접 시작 인사,0.00028912316,0.0,0.00028912315610796213
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",상세 경험,0.010064688,0.0,0.010064687579870224
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",수식,-0.009928067,0.0,0.009928067214787006
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",용어 질문,0.013066568,0.0,0.013066568411886692
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",인공지능,0.0062413365,0.0,0.006241336464881897
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",잠시 휴식,0.0047076577,0.0,0.004707657732069492
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",좋아하는 아이돌,-0.002580047,0.0,0.0025800468865782022
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",핵심 아이디어,0.96691585,1.0,0.03308415412902832
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",확률 예측에서 MSE Loss 미 사용 이유,-0.03013754,0.0,0.03013754077255726
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,BCE 가 좋은 task,0.058205016,0.0,0.0582050159573555
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,BCE 가 좋은 이유,0.00786829,0.0,0.007868289947509766
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,LLM Fine-Tuning 의 PEFT,-0.02693162,0.0,0.02693161927163601
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,LoRA,-0.0032385408,0.0,0.003238540841266513
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,LoRA 와 QLoRA 의 차이,-0.06622078,0.0,0.06622078269720078
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,Loss Function 예시,0.0691848,0.0,0.0691848024725914
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,Loss Function 정의,-0.05659251,0.0,0.056592509150505066
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,MBTI,0.0016375515,0.0,0.0016375514678657055
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,MSE Loss 설명,0.07857581,0.0,0.07857581228017807
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,MSE Loss 용도,0.013038362,0.0,0.013038362376391888
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,Multi-Label 에서 CE + Softmax 적용 문제점,0.12478834,0.0,0.12478833645582199
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,PEFT 방법 5가지,0.054600764,0.0,0.05460076406598091
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,거대 언어 모델 정의,-0.007594586,0.0,0.007594585884362459
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,기본 경험,-0.03636952,0.0,0.03636952117085457
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,답변 실패,0.16009827,1.0,0.8399017304182053
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,딥러닝,0.03648879,0.0,0.036488790065050125
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,마지막 할 말,-0.050673414,0.0,0.05067341402173042
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,머신러닝,-0.046176128,0.0,0.04617612808942795
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,면접 시작 인사,-0.019024903,0.0,0.01902490295469761
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,상세 경험,-0.018761225,0.0,0.018761225044727325
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,수식,0.2662155,0.0,0.2662155032157898
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,용어 질문,0.065573215,0.0,0.06557321548461914
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,인공지능,0.0728947,0.0,0.07289469987154007
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,잠시 휴식,-0.058618095,0.0,0.05861809477210045
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,좋아하는 아이돌,-0.001021937,0.0,0.0010219370014965534
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,핵심 아이디어,0.47916007,0.0,0.4791600704193115
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,확률 예측에서 MSE Loss 미 사용 이유,-0.08464163,0.0,0.08464162796735764
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",BCE 가 좋은 task,-0.016931692,0.0,0.016931692138314247
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",BCE 가 좋은 이유,0.0036485225,0.0,0.0036485225427895784
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",LLM Fine-Tuning 의 PEFT,-0.017352551,0.0,0.017352551221847534
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",LoRA,0.0011555203,0.0,0.00115552032366395
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",LoRA 와 QLoRA 의 차이,0.035378937,0.0,0.03537893667817116
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",Loss Function 예시,0.024875905,0.0,0.024875905364751816
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",Loss Function 정의,-0.010392132,0.0,0.010392132215201855
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",MBTI,-0.010054966,0.0,0.010054966434836388
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",MSE Loss 설명,-0.0065064277,0.0,0.0065064276568591595
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",MSE Loss 용도,-0.008759404,0.0,0.008759403601288795
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",Multi-Label 에서 CE + Softmax 적용 문제점,0.016942963,0.0,0.016942963004112244
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",PEFT 방법 5가지,-0.0061708866,0.0,0.006170886568725109
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",거대 언어 모델 정의,-0.029986719,0.0,0.02998671866953373
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",기본 경험,-0.023150727,0.0,0.023150727152824402
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",답변 실패,0.0131492475,0.0,0.013149247504770756
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",딥러닝,-0.014598878,0.0,0.014598878100514412
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",마지막 할 말,0.010624617,0.0,0.010624617338180542
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",머신러닝,0.011661886,0.0,0.011661886237561703
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",면접 시작 인사,-0.00579203,0.0,0.00579203013330698
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",상세 경험,0.02016691,0.0,0.0201669093221426
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",수식,0.030389948,0.0,0.030389947816729546
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",용어 질문,0.0072864336,0.0,0.007286433596163988
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",인공지능,0.027019009,0.0,0.027019008994102478
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",잠시 휴식,-0.0005261029,0.0,0.0005261029000394046
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",좋아하는 아이돌,-0.0052565513,0.0,0.005256551317870617
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",핵심 아이디어,0.9597163,1.0,0.0402836799621582
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",확률 예측에서 MSE Loss 미 사용 이유,-0.04226979,0.0,0.042269788682460785
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')]",BCE 가 좋은 task,-0.026681071,0.0,0.026681071147322655
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')]",BCE 가 좋은 이유,-0.022970896,0.0,0.02297089621424675
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')]",LLM Fine-Tuning 의 PEFT,0.0067889146,0.0,0.006788914557546377
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')]",LoRA,-0.0021211891,0.0,0.0021211891435086727
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')]",LoRA 와 QLoRA 의 차이,-0.0093429405,0.0,0.009342940524220467
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')]",Loss Function 예시,-0.00039212973,0.0,0.00039212973206304014
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')]",Loss Function 정의,-0.040019993,0.0,0.04001999273896217
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')]",MBTI,-0.0146325175,0.0,0.014632517471909523
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')]",MSE Loss 설명,-0.03143326,0.0,0.03143325820565224
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')]",MSE Loss 용도,-0.0037578088,0.0,0.0037578088231384754
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')]",Multi-Label 에서 CE + Softmax 적용 문제점,-0.0070456215,0.0,0.007045621518045664
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')]",PEFT 방법 5가지,0.0023660667,0.0,0.002366066677495837
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')]",거대 언어 모델 정의,-0.02848987,0.0,0.028489869087934494
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')]",기본 경험,0.008515105,0.0,0.00851510465145111
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')]",답변 실패,-0.0047131516,0.0,0.004713151603937149
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')]",딥러닝,-0.062108275,0.0,0.062108274549245834
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')]",마지막 할 말,0.014448864,0.0,0.014448864385485649
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')]",머신러닝,-0.012030022,0.0,0.012030022218823433
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')]",면접 시작 인사,-0.014998379,0.0,0.014998379163444042
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')]",상세 경험,-0.023100458,0.0,0.023100458085536957
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')]",수식,0.9380599,1.0,0.06194007396697998
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')]",용어 질문,0.008926008,0.0,0.008926007896661758
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')]",인공지능,-0.020303998,0.0,0.02030399814248085
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')]",잠시 휴식,-0.020833936,0.0,0.02083393558859825
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')]",좋아하는 아이돌,0.00347792,0.0,0.0034779200796037912
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')]",핵심 아이디어,-0.01366926,0.0,0.01366925984621048
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')]",확률 예측에서 MSE Loss 미 사용 이유,-0.0118689025,0.0,0.01186890248209238
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다",BCE 가 좋은 task,-0.008935824,0.0,0.008935824036598206
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다",BCE 가 좋은 이유,-0.0002905012,0.0,0.000290501193376258
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다",LLM Fine-Tuning 의 PEFT,0.00597584,0.0,0.005975840147584677
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다",LoRA,0.010146881,0.0,0.010146881453692913
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다",LoRA 와 QLoRA 의 차이,0.020079518,0.0,0.020079517737030983
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다",Loss Function 예시,0.012106499,0.0,0.012106498703360558
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다",Loss Function 정의,-0.004215614,0.0,0.004215613938868046
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다",MBTI,-0.0073606665,0.0,0.007360666524618864
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다",MSE Loss 설명,-0.019815039,0.0,0.01981503888964653
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다",MSE Loss 용도,-0.012073337,0.0,0.012073337100446224
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다",Multi-Label 에서 CE + Softmax 적용 문제점,0.003867135,0.0,0.003867134917527437
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다",PEFT 방법 5가지,-0.0053131897,0.0,0.005313189700245857
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다",거대 언어 모델 정의,0.001110327,0.0,0.0011103269644081593
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다",기본 경험,-0.015095202,0.0,0.015095202252268791
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다",답변 실패,0.008288989,0.0,0.008288988843560219
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다",딥러닝,-0.031324647,0.0,0.03132464736700058
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다",마지막 할 말,0.012836336,0.0,0.012836336158216
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다",머신러닝,0.01529425,0.0,0.015294250100851059
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다",면접 시작 인사,0.008360364,0.0,0.0083603635430336
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다",상세 경험,0.019163588,0.0,0.01916358806192875
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다",수식,-0.018169543,0.0,0.018169542774558067
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다",용어 질문,0.020126576,0.0,0.020126575604081154
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다",인공지능,0.003459452,0.0,0.0034594519529491663
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다",잠시 휴식,0.0040893336,0.0,0.0040893335826694965
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다",좋아하는 아이돌,-0.0050602327,0.0,0.00506023271009326
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다",핵심 아이디어,0.9681954,1.0,0.03180462121963501
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다",확률 예측에서 MSE Loss 미 사용 이유,-0.008553464,0.0,0.008553463965654373
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거,BCE 가 좋은 task,0.004549991,0.0,0.004549990873783827
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거,BCE 가 좋은 이유,-0.006566325,0.0,0.006566325202584267
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거,LLM Fine-Tuning 의 PEFT,-0.0007632286,0.0,0.0007632286287844181
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거,LoRA,0.0018803568,0.0,0.0018803568091243505
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거,LoRA 와 QLoRA 의 차이,-0.0030658857,0.0,0.003065885743126273
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거,Loss Function 예시,-0.0039388146,0.0,0.003938814625144005
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거,Loss Function 정의,-0.0088516185,0.0,0.008851618506014347
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거,MBTI,0.0035822468,0.0,0.003582246834412217
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거,MSE Loss 설명,0.006253628,0.0,0.006253628060221672
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거,MSE Loss 용도,-0.014855806,0.0,0.014855805784463882
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거,Multi-Label 에서 CE + Softmax 적용 문제점,0.00063650415,0.0,0.0006365041481330991
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거,PEFT 방법 5가지,-0.0010841095,0.0,0.0010841095354408026
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거,거대 언어 모델 정의,-0.008773279,0.0,0.008773279376327991
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거,기본 경험,-0.004268567,0.0,0.004268567077815533
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거,답변 실패,0.99140054,1.0,0.008599460124969482
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거,딥러닝,-0.0052865674,0.0,0.0052865673787891865
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거,마지막 할 말,-0.006369993,0.0,0.006369993090629578
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거,머신러닝,-0.0019665803,0.0,0.001966580282896757
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거,면접 시작 인사,0.00020721946,0.0,0.00020721946202684194
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거,상세 경험,-0.0074577914,0.0,0.0074577913619577885
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거,수식,-0.008127531,0.0,0.008127531036734581
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거,용어 질문,0.0023431587,0.0,0.0023431587032973766
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거,인공지능,-0.0127719175,0.0,0.012771917507052422
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거,잠시 휴식,-0.012732759,0.0,0.01273275911808014
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거,좋아하는 아이돌,0.008035242,0.0,0.00803524162620306
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거,핵심 아이디어,0.0149431825,0.0,0.014943182468414307
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거,확률 예측에서 MSE Loss 미 사용 이유,-0.0027544748,0.0,0.0027544747572392225
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지",BCE 가 좋은 task,-0.013965038,0.0,0.013965037651360035
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지",BCE 가 좋은 이유,-0.0053248387,0.0,0.005324838683009148
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지",LLM Fine-Tuning 의 PEFT,-0.008481386,0.0,0.008481386117637157
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지",LoRA,0.005324127,0.0,0.005324127152562141
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지",LoRA 와 QLoRA 의 차이,0.022771565,0.0,0.0227715652436018
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지",Loss Function 예시,0.010983168,0.0,0.01098316814750433
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지",Loss Function 정의,-0.011477598,0.0,0.011477597989141941
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지",MBTI,-0.02664359,0.0,0.02664358913898468
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지",MSE Loss 설명,0.004384429,0.0,0.004384429194033146
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지",MSE Loss 용도,-0.020716872,0.0,0.020716872066259384
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지",Multi-Label 에서 CE + Softmax 적용 문제점,0.011041246,0.0,0.011041246354579926
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지",PEFT 방법 5가지,-0.020400308,0.0,0.020400308072566986
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지",거대 언어 모델 정의,0.014862826,0.0,0.014862826094031334
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지",기본 경험,-0.033401947,0.0,0.03340194746851921
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지",답변 실패,0.01236344,0.0,0.012363440357148647
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지",딥러닝,-0.014614693,0.0,0.014614692889153957
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지",마지막 할 말,0.012686428,0.0,0.012686427682638168
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지",머신러닝,-0.002747406,0.0,0.002747406018897891
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지",면접 시작 인사,0.0063850423,0.0,0.006385042332112789
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지",상세 경험,0.003560229,0.0,0.0035602289717644453
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지",수식,0.046800025,0.0,0.046800024807453156
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지",용어 질문,0.018268293,0.0,0.018268292769789696
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지",인공지능,0.015559599,0.0,0.015559598803520203
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지",잠시 휴식,-0.0059481193,0.0,0.005948119331151247
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지",좋아하는 아이돌,0.0014441371,0.0,0.001444137073121965
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지",핵심 아이디어,0.9633015,1.0,0.03669852018356323
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지",확률 예측에서 MSE Loss 미 사용 이유,-0.070722036,0.0,0.07072203606367111
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야?",BCE 가 좋은 task,0.08597382,0.0,0.08597382158041
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야?",BCE 가 좋은 이유,-0.061642677,0.0,0.06164267659187317
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야?",LLM Fine-Tuning 의 PEFT,-0.019845428,0.0,0.019845427945256233
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야?",LoRA,0.01760077,0.0,0.0176007691770792
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야?",LoRA 와 QLoRA 의 차이,0.0196133,0.0,0.019613299518823624
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야?",Loss Function 예시,-0.029582325,0.0,0.029582325369119644
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야?",Loss Function 정의,0.0136411125,0.0,0.013641112484037876
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야?",MBTI,-0.020384638,0.0,0.020384637638926506
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야?",MSE Loss 설명,-0.03322798,0.0,0.03322798013687134
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야?",MSE Loss 용도,-0.020561878,0.0,0.020561877638101578
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야?",Multi-Label 에서 CE + Softmax 적용 문제점,-0.0038648602,0.0,0.003864860162138939
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야?",PEFT 방법 5가지,-0.017185455,0.0,0.017185455188155174
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야?",거대 언어 모델 정의,0.030141193,0.0,0.0301411934196949
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야?",기본 경험,-0.0010064456,0.0,0.0010064456146210432
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야?",답변 실패,0.1308302,0.0,0.13083019852638245
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야?",딥러닝,-0.029344825,0.0,0.029344825074076653
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야?",마지막 할 말,-0.027818343,0.0,0.0278183426707983
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야?",머신러닝,0.003710424,0.0,0.0037104240618646145
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야?",면접 시작 인사,-0.018756116,0.0,0.018756115809082985
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야?",상세 경험,0.0046183905,0.0,0.004618390463292599
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야?",수식,-0.054988593,0.0,0.054988592863082886
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야?",용어 질문,0.81908584,1.0,0.18091416358947754
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야?",인공지능,0.0070604216,0.0,0.007060421630740166
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야?",잠시 휴식,0.0004066652,0.0,0.00040666520362719893
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야?",좋아하는 아이돌,-0.057346523,0.0,0.05734652280807495
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야?",핵심 아이디어,0.0039738542,0.0,0.00397385424003005
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야?",확률 예측에서 MSE Loss 미 사용 이유,-0.0034979144,0.0,0.0034979144111275673
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",BCE 가 좋은 task,0.06255272,0.0,0.06255272030830383
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",BCE 가 좋은 이유,-0.011312995,0.0,0.011312995105981827
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",LLM Fine-Tuning 의 PEFT,0.005310115,0.0,0.005310114938765764
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",LoRA,0.0045749373,0.0,0.004574937280267477
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",LoRA 와 QLoRA 의 차이,-0.0052495184,0.0,0.005249518435448408
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",Loss Function 예시,-0.011154196,0.0,0.011154196225106716
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",Loss Function 정의,-0.010829361,0.0,0.010829361155629158
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",MBTI,-0.006613648,0.0,0.006613648030906916
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",MSE Loss 설명,-0.010526709,0.0,0.010526709258556366
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",MSE Loss 용도,-0.012624888,0.0,0.012624887749552727
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",Multi-Label 에서 CE + Softmax 적용 문제점,0.0015350597,0.0,0.0015350596513599157
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",PEFT 방법 5가지,-0.011861288,0.0,0.011861287988722324
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",거대 언어 모델 정의,-0.0063640177,0.0,0.006364017724990845
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",기본 경험,-0.006371267,0.0,0.006371267139911652
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",답변 실패,0.9560414,1.0,0.04395860433578491
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",딥러닝,-0.0014175549,0.0,0.0014175549149513245
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",마지막 할 말,-0.015165124,0.0,0.015165124088525772
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",머신러닝,-0.00895516,0.0,0.008955160155892372
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",면접 시작 인사,-0.017145665,0.0,0.017145665362477303
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",상세 경험,0.0038481208,0.0,0.003848120803013444
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",수식,-0.027287513,0.0,0.02728751301765442
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",용어 질문,0.085522644,0.0,0.08552264422178268
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",인공지능,-0.021973081,0.0,0.021973080933094025
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",잠시 휴식,-0.01363084,0.0,0.013630839996039867
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",좋아하는 아이돌,-0.011472677,0.0,0.011472676880657673
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",핵심 아이디어,0.013833949,0.0,0.013833949342370033
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",확률 예측에서 MSE Loss 미 사용 이유,-0.0014639628,0.0,0.0014639628352597356
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",BCE 가 좋은 task,0.008018408,1.0,0.9919815920293331
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",BCE 가 좋은 이유,-0.012496101,0.0,0.0124961007386446
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",LLM Fine-Tuning 의 PEFT,-0.00043141068,0.0,0.00043141067726537585
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",LoRA,0.0008742753,0.0,0.0008742752834223211
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",LoRA 와 QLoRA 의 차이,0.0003044893,0.0,0.000304489309201017
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",Loss Function 예시,-0.010510944,0.0,0.010510943830013275
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",Loss Function 정의,-0.0067886286,0.0,0.00678862864151597
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",MBTI,-0.0007041222,0.0,0.0007041221833787858
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",MSE Loss 설명,-2.5508787e-05,0.0,2.5508787075523287e-05
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",MSE Loss 용도,-0.00547401,0.0,0.005474010016769171
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",Multi-Label 에서 CE + Softmax 적용 문제점,-0.0019334302,0.0,0.0019334302050992846
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",PEFT 방법 5가지,-0.0053757657,0.0,0.0053757657296955585
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",거대 언어 모델 정의,-0.001710528,0.0,0.0017105280421674252
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",기본 경험,-0.009382599,0.0,0.009382599033415318
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",답변 실패,0.9930134,0.0,0.9930133819580078
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",딥러닝,-0.005086732,0.0,0.005086732096970081
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",마지막 할 말,-0.0062508206,0.0,0.006250820588320494
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",머신러닝,-0.003850695,0.0,0.003850694978609681
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",면접 시작 인사,0.00056711095,0.0,0.0005671109538525343
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",상세 경험,-0.0067928494,0.0,0.006792849395424128
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",수식,-0.009191251,0.0,0.00919125135987997
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",용어 질문,0.008105929,0.0,0.008105929009616375
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",인공지능,-0.017958855,0.0,0.017958855256438255
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",잠시 휴식,-0.0051777842,0.0,0.0051777842454612255
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",좋아하는 아이돌,0.0028226904,0.0,0.002822690410539508
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",핵심 아이디어,0.0062916554,0.0,0.006291655357927084
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",확률 예측에서 MSE Loss 미 사용 이유,-0.0036016698,0.0,0.0036016697995364666
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",BCE 가 좋은 task,0.1729613,0.0,0.1729612946510315
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",BCE 가 좋은 이유,0.90619284,1.0,0.0938071608543396
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",LLM Fine-Tuning 의 PEFT,-0.0628576,0.0,0.06285759806632996
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",LoRA,-0.017398572,0.0,0.017398571595549583
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",LoRA 와 QLoRA 의 차이,-0.006752379,0.0,0.006752378772944212
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",Loss Function 예시,-0.0061490634,0.0,0.006149063352495432
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",Loss Function 정의,-0.0069450806,0.0,0.00694508058950305
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",MBTI,-0.008531971,0.0,0.008531970903277397
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",MSE Loss 설명,-0.026919527,0.0,0.026919526979327202
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",MSE Loss 용도,-0.039097466,0.0,0.039097465574741364
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",Multi-Label 에서 CE + Softmax 적용 문제점,0.048016936,0.0,0.04801693558692932
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",PEFT 방법 5가지,0.0004631563,0.0,0.00046315629151649773
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",거대 언어 모델 정의,0.011262881,0.0,0.011262880638241768
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",기본 경험,-0.0068395133,0.0,0.006839513313025236
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",답변 실패,0.04767985,0.0,0.0476798489689827
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",딥러닝,0.013512744,0.0,0.013512743636965752
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",마지막 할 말,-0.027708802,0.0,0.02770880237221718
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",머신러닝,-0.045075987,0.0,0.04507598653435707
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",면접 시작 인사,-0.025526227,0.0,0.025526227429509163
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",상세 경험,0.0058283308,0.0,0.005828330758959055
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",수식,-0.026113728,0.0,0.026113728061318398
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",용어 질문,-0.01317982,0.0,0.013179820030927658
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",인공지능,0.0030448087,0.0,0.0030448087491095066
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",잠시 휴식,-0.0067850896,0.0,0.006785089615732431
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",좋아하는 아이돌,-0.027733158,0.0,0.027733158320188522
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",핵심 아이디어,-0.023080241,0.0,0.023080240935087204
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",확률 예측에서 MSE Loss 미 사용 이유,-0.0142836515,0.0,0.014283651486039162
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,BCE 가 좋은 task,-0.013542616,0.0,0.013542615808546543
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,BCE 가 좋은 이유,-0.007218428,0.0,0.0072184279561042786
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,LLM Fine-Tuning 의 PEFT,-0.01772078,0.0,0.01772077940404415
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,LoRA,0.0035290027,0.0,0.003529002657160163
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,LoRA 와 QLoRA 의 차이,-0.0037959,0.0,0.0037958999164402485
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,Loss Function 예시,-0.0029860425,0.0,0.0029860425274819136
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,Loss Function 정의,-0.012389355,0.0,0.012389355339109898
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,MBTI,-0.0036085432,0.0,0.0036085431929677725
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,MSE Loss 설명,-0.01580777,0.0,0.01580777019262314
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,MSE Loss 용도,0.0036994729,0.0,0.0036994728725403547
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,Multi-Label 에서 CE + Softmax 적용 문제점,0.9818828,1.0,0.018117189407348633
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,PEFT 방법 5가지,0.027628414,0.0,0.02762841433286667
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,거대 언어 모델 정의,0.00034326946,0.0,0.0003432694647926837
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,기본 경험,-0.013877678,0.0,0.013877677731215954
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,답변 실패,0.004775124,0.0,0.0047751241363584995
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,딥러닝,-0.00573666,0.0,0.005736660212278366
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,마지막 할 말,0.0076942327,0.0,0.007694232743233442
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,머신러닝,0.02051084,0.0,0.0205108392983675
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,면접 시작 인사,0.0045678224,0.0,0.004567822441458702
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,상세 경험,0.006492142,0.0,0.006492142099887133
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,수식,-0.034151457,0.0,0.034151457250118256
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,용어 질문,-0.0077335364,0.0,0.007733536418527365
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,인공지능,0.004757642,0.0,0.004757641814649105
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,잠시 휴식,-0.0065428037,0.0,0.006542803719639778
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,좋아하는 아이돌,-0.0034771196,0.0,0.0034771196078509092
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,핵심 아이디어,0.00035688127,0.0,0.00035688126808963716
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,확률 예측에서 MSE Loss 미 사용 이유,-0.018591542,0.0,0.018591541796922684
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,BCE 가 좋은 task,0.007185298,0.0,0.007185298018157482
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,BCE 가 좋은 이유,-0.010412804,0.0,0.01041280385106802
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,LLM Fine-Tuning 의 PEFT,-0.00016855971,0.0,0.00016855970898177475
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,LoRA,0.00079022686,0.0,0.000790226855315268
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,LoRA 와 QLoRA 의 차이,0.00080838345,0.0,0.0008083834545686841
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,Loss Function 예시,-0.009157529,0.0,0.009157529100775719
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,Loss Function 정의,-0.011765373,0.0,0.011765372939407825
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,MBTI,-0.00047726967,0.0,0.0004772696702275425
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,MSE Loss 설명,0.006650932,0.0,0.006650932133197784
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,MSE Loss 용도,-0.007825657,0.0,0.007825656794011593
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,Multi-Label 에서 CE + Softmax 적용 문제점,0.0030656592,0.0,0.003065659198909998
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,PEFT 방법 5가지,0.0010654853,0.0,0.0010654852958396077
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,거대 언어 모델 정의,-0.002330791,0.0,0.0023307909723371267
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,기본 경험,-0.0044163167,0.0,0.004416316747665405
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,답변 실패,0.99208736,1.0,0.007912635803222656
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,딥러닝,-0.0048994943,0.0,0.0048994943499565125
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,마지막 할 말,-0.009190768,0.0,0.009190768003463745
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,머신러닝,-0.0007633035,0.0,0.0007633034838363528
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,면접 시작 인사,0.005656739,0.0,0.005656739231199026
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,상세 경험,-0.004747032,0.0,0.0047470321878790855
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,수식,-0.011767013,0.0,0.011767012998461723
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,용어 질문,-0.0079703145,0.0,0.007970314472913742
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,인공지능,-0.016225094,0.0,0.016225093975663185
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,잠시 휴식,-0.0073407637,0.0,0.007340763695538044
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,좋아하는 아이돌,0.0062385355,0.0,0.006238535512238741
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,핵심 아이디어,0.008404792,0.0,0.008404792286455631
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,확률 예측에서 MSE Loss 미 사용 이유,-0.0014092502,0.0,0.0014092501951381564
Loss Function 관련 실무 경험 -> [기본 경험] 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,BCE 가 좋은 task,0.0017967977,0.0,0.001796797732822597
Loss Function 관련 실무 경험 -> [기본 경험] 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,BCE 가 좋은 이유,0.008503006,0.0,0.008503005839884281
Loss Function 관련 실무 경험 -> [기본 경험] 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,LLM Fine-Tuning 의 PEFT,0.009411353,0.0,0.009411352686583996
Loss Function 관련 실무 경험 -> [기본 경험] 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,LoRA,-0.0036835894,0.0,0.0036835893988609314
Loss Function 관련 실무 경험 -> [기본 경험] 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,LoRA 와 QLoRA 의 차이,0.0042734626,0.0,0.004273462574928999
Loss Function 관련 실무 경험 -> [기본 경험] 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,Loss Function 예시,-0.009704628,0.0,0.00970462802797556
Loss Function 관련 실무 경험 -> [기본 경험] 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,Loss Function 정의,0.009254739,0.0,0.009254738688468933
Loss Function 관련 실무 경험 -> [기본 경험] 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,MBTI,0.0032915357,0.0,0.003291535656899214
Loss Function 관련 실무 경험 -> [기본 경험] 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,MSE Loss 설명,0.006544674,0.0,0.006544673815369606
Loss Function 관련 실무 경험 -> [기본 경험] 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,MSE Loss 용도,0.015265173,0.0,0.015265173278748989
Loss Function 관련 실무 경험 -> [기본 경험] 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,Multi-Label 에서 CE + Softmax 적용 문제점,-0.011986741,0.0,0.011986740864813328
Loss Function 관련 실무 경험 -> [기본 경험] 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,PEFT 방법 5가지,-0.009360312,0.0,0.009360311552882195
Loss Function 관련 실무 경험 -> [기본 경험] 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,거대 언어 모델 정의,0.017720306,0.0,0.017720306292176247
Loss Function 관련 실무 경험 -> [기본 경험] 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,기본 경험,0.97601044,1.0,0.023989558219909668
Loss Function 관련 실무 경험 -> [기본 경험] 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,답변 실패,0.0011466488,0.0,0.0011466487776488066
Loss Function 관련 실무 경험 -> [기본 경험] 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,딥러닝,0.0022331215,0.0,0.002233121544122696
Loss Function 관련 실무 경험 -> [기본 경험] 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,마지막 할 말,-0.01271599,0.0,0.012715989723801613
Loss Function 관련 실무 경험 -> [기본 경험] 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,머신러닝,0.005759836,0.0,0.005759836174547672
Loss Function 관련 실무 경험 -> [기본 경험] 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,면접 시작 인사,-0.0014058453,0.0,0.0014058452798053622
Loss Function 관련 실무 경험 -> [기본 경험] 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,상세 경험,-0.034970693,0.0,0.03497069329023361
Loss Function 관련 실무 경험 -> [기본 경험] 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,수식,-0.019147865,0.0,0.01914786547422409
Loss Function 관련 실무 경험 -> [기본 경험] 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,용어 질문,0.010633956,0.0,0.010633955709636211
Loss Function 관련 실무 경험 -> [기본 경험] 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,인공지능,1.1738027e-05,0.0,1.1738026842067484e-05
Loss Function 관련 실무 경험 -> [기본 경험] 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,잠시 휴식,-0.006568683,0.0,0.006568682845681906
Loss Function 관련 실무 경험 -> [기본 경험] 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,좋아하는 아이돌,-0.0065498403,0.0,0.006549840327352285
Loss Function 관련 실무 경험 -> [기본 경험] 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,핵심 아이디어,-0.009838181,0.0,0.009838180616497993
Loss Function 관련 실무 경험 -> [기본 경험] 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,확률 예측에서 MSE Loss 미 사용 이유,-0.012124013,0.0,0.012124013155698776
Loss Function 관련 실무 경험 -> [상세 경험] 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,BCE 가 좋은 task,0.0021606968,0.0,0.002160696778446436
Loss Function 관련 실무 경험 -> [상세 경험] 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,BCE 가 좋은 이유,0.0064462414,0.0,0.0064462414011359215
Loss Function 관련 실무 경험 -> [상세 경험] 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,LLM Fine-Tuning 의 PEFT,0.0071161427,0.0,0.00711614266037941
Loss Function 관련 실무 경험 -> [상세 경험] 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,LoRA,0.0058284956,0.0,0.005828495603054762
Loss Function 관련 실무 경험 -> [상세 경험] 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,LoRA 와 QLoRA 의 차이,-0.000667105,0.0,0.0006671050214208663
Loss Function 관련 실무 경험 -> [상세 경험] 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,Loss Function 예시,-0.012876122,0.0,0.012876122258603573
Loss Function 관련 실무 경험 -> [상세 경험] 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,Loss Function 정의,-0.016860172,0.0,0.016860172152519226
Loss Function 관련 실무 경험 -> [상세 경험] 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,MBTI,-0.0022833324,0.0,0.0022833324037492275
Loss Function 관련 실무 경험 -> [상세 경험] 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,MSE Loss 설명,-0.011398421,0.0,0.011398420669138432
Loss Function 관련 실무 경험 -> [상세 경험] 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,MSE Loss 용도,-0.013549852,0.0,0.013549852184951305
Loss Function 관련 실무 경험 -> [상세 경험] 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,Multi-Label 에서 CE + Softmax 적용 문제점,0.0064862184,0.0,0.006486218422651291
Loss Function 관련 실무 경험 -> [상세 경험] 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,PEFT 방법 5가지,-0.0380877,0.0,0.03808769956231117
Loss Function 관련 실무 경험 -> [상세 경험] 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,거대 언어 모델 정의,-0.0061813276,0.0,0.006181327626109123
Loss Function 관련 실무 경험 -> [상세 경험] 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,기본 경험,0.00936708,0.0,0.0093670804053545
Loss Function 관련 실무 경험 -> [상세 경험] 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,답변 실패,0.0018213458,0.0,0.0018213457660749555
Loss Function 관련 실무 경험 -> [상세 경험] 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,딥러닝,-0.022072049,0.0,0.022072048857808113
Loss Function 관련 실무 경험 -> [상세 경험] 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,마지막 할 말,-0.0060456633,0.0,0.006045663263648748
Loss Function 관련 실무 경험 -> [상세 경험] 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,머신러닝,-0.00847055,0.0,0.008470550179481506
Loss Function 관련 실무 경험 -> [상세 경험] 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,면접 시작 인사,0.0019168023,0.0,0.001916802255436778
Loss Function 관련 실무 경험 -> [상세 경험] 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,상세 경험,0.97065216,1.0,0.02934783697128296
Loss Function 관련 실무 경험 -> [상세 경험] 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,수식,0.0043556555,0.0,0.004355655517429113
Loss Function 관련 실무 경험 -> [상세 경험] 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,용어 질문,0.0030588368,0.0,0.0030588367953896523
Loss Function 관련 실무 경험 -> [상세 경험] 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,인공지능,-0.0028761297,0.0,0.002876129699870944
Loss Function 관련 실무 경험 -> [상세 경험] 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,잠시 휴식,0.0012272132,0.0,0.001227213186211884
Loss Function 관련 실무 경험 -> [상세 경험] 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,좋아하는 아이돌,0.006494593,0.0,0.006494592875242233
Loss Function 관련 실무 경험 -> [상세 경험] 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,핵심 아이디어,0.006143399,0.0,0.0061433990485966206
Loss Function 관련 실무 경험 -> [상세 경험] 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,확률 예측에서 MSE Loss 미 사용 이유,0.0013903149,0.0,0.00139031489379704
Loss Function 관련 실무 경험 -> [상세 경험] 근데 기억 진짜 안나는데 어떡하지?,BCE 가 좋은 task,0.0055203396,0.0,0.005520339589565992
Loss Function 관련 실무 경험 -> [상세 경험] 근데 기억 진짜 안나는데 어떡하지?,BCE 가 좋은 이유,-0.011007617,0.0,0.011007617227733135
Loss Function 관련 실무 경험 -> [상세 경험] 근데 기억 진짜 안나는데 어떡하지?,LLM Fine-Tuning 의 PEFT,-0.0014566052,0.0,0.0014566051540896297
Loss Function 관련 실무 경험 -> [상세 경험] 근데 기억 진짜 안나는데 어떡하지?,LoRA,0.00048590236,0.0,0.0004859023611061275
Loss Function 관련 실무 경험 -> [상세 경험] 근데 기억 진짜 안나는데 어떡하지?,LoRA 와 QLoRA 의 차이,-0.0010922689,0.0,0.0010922688525170088
Loss Function 관련 실무 경험 -> [상세 경험] 근데 기억 진짜 안나는데 어떡하지?,Loss Function 예시,-0.009764277,0.0,0.009764277376234531
Loss Function 관련 실무 경험 -> [상세 경험] 근데 기억 진짜 안나는데 어떡하지?,Loss Function 정의,-0.007077145,0.0,0.0070771449245512486
Loss Function 관련 실무 경험 -> [상세 경험] 근데 기억 진짜 안나는데 어떡하지?,MBTI,-0.0010304655,0.0,0.0010304654715582728
Loss Function 관련 실무 경험 -> [상세 경험] 근데 기억 진짜 안나는데 어떡하지?,MSE Loss 설명,0.0018171562,0.0,0.0018171562114730477
Loss Function 관련 실무 경험 -> [상세 경험] 근데 기억 진짜 안나는데 어떡하지?,MSE Loss 용도,-0.0052862586,0.0,0.0052862586453557014
Loss Function 관련 실무 경험 -> [상세 경험] 근데 기억 진짜 안나는데 어떡하지?,Multi-Label 에서 CE + Softmax 적용 문제점,-0.0022280854,0.0,0.0022280854173004627
Loss Function 관련 실무 경험 -> [상세 경험] 근데 기억 진짜 안나는데 어떡하지?,PEFT 방법 5가지,-0.0017861589,0.0,0.001786158885806799
Loss Function 관련 실무 경험 -> [상세 경험] 근데 기억 진짜 안나는데 어떡하지?,거대 언어 모델 정의,-0.0013559349,0.0,0.001355934888124466
Loss Function 관련 실무 경험 -> [상세 경험] 근데 기억 진짜 안나는데 어떡하지?,기본 경험,-0.001714705,0.0,0.0017147050239145756
Loss Function 관련 실무 경험 -> [상세 경험] 근데 기억 진짜 안나는데 어떡하지?,답변 실패,0.99342984,1.0,0.006570160388946533
Loss Function 관련 실무 경험 -> [상세 경험] 근데 기억 진짜 안나는데 어떡하지?,딥러닝,-0.00446952,0.0,0.004469519946724176
Loss Function 관련 실무 경험 -> [상세 경험] 근데 기억 진짜 안나는데 어떡하지?,마지막 할 말,-0.005316165,0.0,0.005316164810210466
Loss Function 관련 실무 경험 -> [상세 경험] 근데 기억 진짜 안나는데 어떡하지?,머신러닝,-0.0049975053,0.0,0.0049975053407251835
Loss Function 관련 실무 경험 -> [상세 경험] 근데 기억 진짜 안나는데 어떡하지?,면접 시작 인사,0.00023525955,0.0,0.00023525954748038203
Loss Function 관련 실무 경험 -> [상세 경험] 근데 기억 진짜 안나는데 어떡하지?,상세 경험,0.00020090221,0.0,0.00020090221369173378
Loss Function 관련 실무 경험 -> [상세 경험] 근데 기억 진짜 안나는데 어떡하지?,수식,-0.007439614,0.0,0.0074396138079464436
Loss Function 관련 실무 경험 -> [상세 경험] 근데 기억 진짜 안나는데 어떡하지?,용어 질문,-0.008014244,0.0,0.00801424402743578
Loss Function 관련 실무 경험 -> [상세 경험] 근데 기억 진짜 안나는데 어떡하지?,인공지능,-0.017133664,0.0,0.017133664339780807
Loss Function 관련 실무 경험 -> [상세 경험] 근데 기억 진짜 안나는데 어떡하지?,잠시 휴식,-0.0084339,0.0,0.008433899842202663
Loss Function 관련 실무 경험 -> [상세 경험] 근데 기억 진짜 안나는데 어떡하지?,좋아하는 아이돌,0.007332364,0.0,0.007332364097237587
Loss Function 관련 실무 경험 -> [상세 경험] 근데 기억 진짜 안나는데 어떡하지?,핵심 아이디어,0.0053292965,0.0,0.005329296458512545
Loss Function 관련 실무 경험 -> [상세 경험] 근데 기억 진짜 안나는데 어떡하지?,확률 예측에서 MSE Loss 미 사용 이유,-0.002812224,0.0,0.0028122239746153355
Loss Function 관련 실무 경험 -> [기본 경험] 나 아직 실무 경험 없는데 ㅠㅠ,BCE 가 좋은 task,-0.0014438814,0.0,0.001443881425075233
Loss Function 관련 실무 경험 -> [기본 경험] 나 아직 실무 경험 없는데 ㅠㅠ,BCE 가 좋은 이유,-0.008536606,0.0,0.008536606095731258
Loss Function 관련 실무 경험 -> [기본 경험] 나 아직 실무 경험 없는데 ㅠㅠ,LLM Fine-Tuning 의 PEFT,0.0049227485,0.0,0.004922748543322086
Loss Function 관련 실무 경험 -> [기본 경험] 나 아직 실무 경험 없는데 ㅠㅠ,LoRA,0.0071270363,0.0,0.007127036340534687
Loss Function 관련 실무 경험 -> [기본 경험] 나 아직 실무 경험 없는데 ㅠㅠ,LoRA 와 QLoRA 의 차이,0.00976413,0.0,0.009764130227267742
Loss Function 관련 실무 경험 -> [기본 경험] 나 아직 실무 경험 없는데 ㅠㅠ,Loss Function 예시,-0.011082612,0.0,0.011082611978054047
Loss Function 관련 실무 경험 -> [기본 경험] 나 아직 실무 경험 없는데 ㅠㅠ,Loss Function 정의,-0.002688902,0.0,0.0026889019645750523
Loss Function 관련 실무 경험 -> [기본 경험] 나 아직 실무 경험 없는데 ㅠㅠ,MBTI,0.011788148,0.0,0.011788148432970047
Loss Function 관련 실무 경험 -> [기본 경험] 나 아직 실무 경험 없는데 ㅠㅠ,MSE Loss 설명,0.0052690334,0.0,0.005269033368676901
Loss Function 관련 실무 경험 -> [기본 경험] 나 아직 실무 경험 없는데 ㅠㅠ,MSE Loss 용도,0.011082142,0.0,0.011082141660153866
Loss Function 관련 실무 경험 -> [기본 경험] 나 아직 실무 경험 없는데 ㅠㅠ,Multi-Label 에서 CE + Softmax 적용 문제점,-0.016165284,0.0,0.01616528443992138
Loss Function 관련 실무 경험 -> [기본 경험] 나 아직 실무 경험 없는데 ㅠㅠ,PEFT 방법 5가지,-0.013755617,0.0,0.0137556167319417
Loss Function 관련 실무 경험 -> [기본 경험] 나 아직 실무 경험 없는데 ㅠㅠ,거대 언어 모델 정의,0.014887112,0.0,0.014887112192809582
Loss Function 관련 실무 경험 -> [기본 경험] 나 아직 실무 경험 없는데 ㅠㅠ,기본 경험,0.96904457,1.0,0.03095543384552002
Loss Function 관련 실무 경험 -> [기본 경험] 나 아직 실무 경험 없는데 ㅠㅠ,답변 실패,0.0068703895,0.0,0.006870389450341463
Loss Function 관련 실무 경험 -> [기본 경험] 나 아직 실무 경험 없는데 ㅠㅠ,딥러닝,0.0010350471,0.0,0.0010350471129640937
Loss Function 관련 실무 경험 -> [기본 경험] 나 아직 실무 경험 없는데 ㅠㅠ,마지막 할 말,-0.0014153485,0.0,0.0014153484953567386
Loss Function 관련 실무 경험 -> [기본 경험] 나 아직 실무 경험 없는데 ㅠㅠ,머신러닝,-0.0139304595,0.0,0.013930459506809711
Loss Function 관련 실무 경험 -> [기본 경험] 나 아직 실무 경험 없는데 ㅠㅠ,면접 시작 인사,-0.0051611,0.0,0.0051611000671982765
Loss Function 관련 실무 경험 -> [기본 경험] 나 아직 실무 경험 없는데 ㅠㅠ,상세 경험,-0.016718373,0.0,0.016718372702598572
Loss Function 관련 실무 경험 -> [기본 경험] 나 아직 실무 경험 없는데 ㅠㅠ,수식,-0.006539991,0.0,0.006539991125464439
Loss Function 관련 실무 경험 -> [기본 경험] 나 아직 실무 경험 없는데 ㅠㅠ,용어 질문,0.0129312035,0.0,0.012931203469634056
Loss Function 관련 실무 경험 -> [기본 경험] 나 아직 실무 경험 없는데 ㅠㅠ,인공지능,0.0071293353,0.0,0.007129335310310125
Loss Function 관련 실무 경험 -> [기본 경험] 나 아직 실무 경험 없는데 ㅠㅠ,잠시 휴식,-0.002062031,0.0,0.00206203106790781
Loss Function 관련 실무 경험 -> [기본 경험] 나 아직 실무 경험 없는데 ㅠㅠ,좋아하는 아이돌,-0.014203451,0.0,0.01420345064252615
Loss Function 관련 실무 경험 -> [기본 경험] 나 아직 실무 경험 없는데 ㅠㅠ,핵심 아이디어,-0.020072617,0.0,0.020072616636753082
Loss Function 관련 실무 경험 -> [기본 경험] 나 아직 실무 경험 없는데 ㅠㅠ,확률 예측에서 MSE Loss 미 사용 이유,-0.013673934,0.0,0.013673934154212475
Loss Function 관련 실무 경험 -> [기본 경험] 대답하기 싫은데 힝,BCE 가 좋은 task,0.005457871,0.0,0.005457871127873659
Loss Function 관련 실무 경험 -> [기본 경험] 대답하기 싫은데 힝,BCE 가 좋은 이유,-0.011797979,0.0,0.011797978542745113
Loss Function 관련 실무 경험 -> [기본 경험] 대답하기 싫은데 힝,LLM Fine-Tuning 의 PEFT,-0.0050049857,0.0,0.005004985723644495
Loss Function 관련 실무 경험 -> [기본 경험] 대답하기 싫은데 힝,LoRA,0.0006379763,0.0,0.0006379762780852616
Loss Function 관련 실무 경험 -> [기본 경험] 대답하기 싫은데 힝,LoRA 와 QLoRA 의 차이,0.0027037528,0.0,0.0027037528343498707
Loss Function 관련 실무 경험 -> [기본 경험] 대답하기 싫은데 힝,Loss Function 예시,-0.013350311,0.0,0.013350310735404491
Loss Function 관련 실무 경험 -> [기본 경험] 대답하기 싫은데 힝,Loss Function 정의,-0.013044666,0.0,0.013044665567576885
Loss Function 관련 실무 경험 -> [기본 경험] 대답하기 싫은데 힝,MBTI,-0.0026801785,0.0,0.0026801784988492727
Loss Function 관련 실무 경험 -> [기본 경험] 대답하기 싫은데 힝,MSE Loss 설명,-0.00076453167,0.0,0.000764531665481627
Loss Function 관련 실무 경험 -> [기본 경험] 대답하기 싫은데 힝,MSE Loss 용도,-0.0042453664,0.0,0.004245366435497999
Loss Function 관련 실무 경험 -> [기본 경험] 대답하기 싫은데 힝,Multi-Label 에서 CE + Softmax 적용 문제점,-0.0012219639,0.0,0.0012219639029353857
Loss Function 관련 실무 경험 -> [기본 경험] 대답하기 싫은데 힝,PEFT 방법 5가지,-0.0032416603,0.0,0.0032416603062301874
Loss Function 관련 실무 경험 -> [기본 경험] 대답하기 싫은데 힝,거대 언어 모델 정의,-0.0013731831,0.0,0.0013731830986216664
Loss Function 관련 실무 경험 -> [기본 경험] 대답하기 싫은데 힝,기본 경험,0.01791273,0.0,0.01791273057460785
Loss Function 관련 실무 경험 -> [기본 경험] 대답하기 싫은데 힝,답변 실패,0.992565,1.0,0.007435023784637451
Loss Function 관련 실무 경험 -> [기본 경험] 대답하기 싫은데 힝,딥러닝,-0.0026689162,0.0,0.0026689162477850914
Loss Function 관련 실무 경험 -> [기본 경험] 대답하기 싫은데 힝,마지막 할 말,-0.007894006,0.0,0.007894005626440048
Loss Function 관련 실무 경험 -> [기본 경험] 대답하기 싫은데 힝,머신러닝,-0.0050880103,0.0,0.005088010337203741
Loss Function 관련 실무 경험 -> [기본 경험] 대답하기 싫은데 힝,면접 시작 인사,0.0039569465,0.0,0.003956946544349194
Loss Function 관련 실무 경험 -> [기본 경험] 대답하기 싫은데 힝,상세 경험,-0.0021281946,0.0,0.0021281945519149303
Loss Function 관련 실무 경험 -> [기본 경험] 대답하기 싫은데 힝,수식,-0.008332486,0.0,0.008332486264407635
Loss Function 관련 실무 경험 -> [기본 경험] 대답하기 싫은데 힝,용어 질문,-0.0036888563,0.0,0.0036888562608510256
Loss Function 관련 실무 경험 -> [기본 경험] 대답하기 싫은데 힝,인공지능,-0.016345376,0.0,0.016345376148819923
Loss Function 관련 실무 경험 -> [기본 경험] 대답하기 싫은데 힝,잠시 휴식,-0.00461965,0.0,0.004619650077074766
Loss Function 관련 실무 경험 -> [기본 경험] 대답하기 싫은데 힝,좋아하는 아이돌,0.006813644,0.0,0.006813643965870142
Loss Function 관련 실무 경험 -> [기본 경험] 대답하기 싫은데 힝,핵심 아이디어,0.005306109,0.0,0.005306108854711056
Loss Function 관련 실무 경험 -> [기본 경험] 대답하기 싫은데 힝,확률 예측에서 MSE Loss 미 사용 이유,-0.0043944446,0.0,0.004394444637000561
Loss Function 관련 실무 경험 -> [기본 경험] 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ,BCE 가 좋은 task,-0.0010377966,0.0,0.0010377966100350022
Loss Function 관련 실무 경험 -> [기본 경험] 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ,BCE 가 좋은 이유,0.0021409907,0.0,0.0021409906912595034
Loss Function 관련 실무 경험 -> [기본 경험] 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ,LLM Fine-Tuning 의 PEFT,0.009453783,0.0,0.009453782811760902
Loss Function 관련 실무 경험 -> [기본 경험] 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ,LoRA,-0.00023535512,0.0,0.00023535512445960194
Loss Function 관련 실무 경험 -> [기본 경험] 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ,LoRA 와 QLoRA 의 차이,0.0036581017,0.0,0.003658101661130786
Loss Function 관련 실무 경험 -> [기본 경험] 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ,Loss Function 예시,-0.004911926,0.0,0.004911926109343767
Loss Function 관련 실무 경험 -> [기본 경험] 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ,Loss Function 정의,0.0009956831,0.0,0.000995683134533465
Loss Function 관련 실무 경험 -> [기본 경험] 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ,MBTI,0.0009241801,0.0,0.0009241800871677697
Loss Function 관련 실무 경험 -> [기본 경험] 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ,MSE Loss 설명,0.0073981443,0.0,0.007398144342005253
Loss Function 관련 실무 경험 -> [기본 경험] 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ,MSE Loss 용도,0.016132614,0.0,0.016132613644003868
Loss Function 관련 실무 경험 -> [기본 경험] 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ,Multi-Label 에서 CE + Softmax 적용 문제점,-0.011291553,0.0,0.011291553266346455
Loss Function 관련 실무 경험 -> [기본 경험] 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ,PEFT 방법 5가지,-0.002336057,0.0,0.0023360569030046463
Loss Function 관련 실무 경험 -> [기본 경험] 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ,거대 언어 모델 정의,0.0208215,0.0,0.020821500569581985
Loss Function 관련 실무 경험 -> [기본 경험] 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ,기본 경험,0.97740793,1.0,0.02259206771850586
Loss Function 관련 실무 경험 -> [기본 경험] 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ,답변 실패,-0.00071474316,0.0,0.0007147431606426835
Loss Function 관련 실무 경험 -> [기본 경험] 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ,딥러닝,0.0018848748,0.0,0.001884874771349132
Loss Function 관련 실무 경험 -> [기본 경험] 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ,마지막 할 말,-0.0056447717,0.0,0.005644771736115217
Loss Function 관련 실무 경험 -> [기본 경험] 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ,머신러닝,-0.00648049,0.0,0.006480489857494831
Loss Function 관련 실무 경험 -> [기본 경험] 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ,면접 시작 인사,-0.005904611,0.0,0.005904611200094223
Loss Function 관련 실무 경험 -> [기본 경험] 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ,상세 경험,-0.03425231,0.0,0.03425230830907822
Loss Function 관련 실무 경험 -> [기본 경험] 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ,수식,-0.010915399,0.0,0.010915398597717285
Loss Function 관련 실무 경험 -> [기본 경험] 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ,용어 질문,0.015636118,0.0,0.01563611812889576
Loss Function 관련 실무 경험 -> [기본 경험] 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ,인공지능,0.00693672,0.0,0.006936720106750727
Loss Function 관련 실무 경험 -> [기본 경험] 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ,잠시 휴식,-0.0021327094,0.0,0.0021327093709260225
Loss Function 관련 실무 경험 -> [기본 경험] 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ,좋아하는 아이돌,-0.0070670457,0.0,0.007067045662552118
Loss Function 관련 실무 경험 -> [기본 경험] 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ,핵심 아이디어,-0.016591826,0.0,0.01659182645380497
Loss Function 관련 실무 경험 -> [기본 경험] 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ,확률 예측에서 MSE Loss 미 사용 이유,-0.008759038,0.0,0.008759037591516972
Loss Function 관련 실무 경험 -> [상세 경험] 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어,BCE 가 좋은 task,0.006459151,0.0,0.006459150929003954
Loss Function 관련 실무 경험 -> [상세 경험] 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어,BCE 가 좋은 이유,0.021047207,0.0,0.021047206595540047
Loss Function 관련 실무 경험 -> [상세 경험] 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어,LLM Fine-Tuning 의 PEFT,0.0018950468,0.0,0.0018950467929244041
Loss Function 관련 실무 경험 -> [상세 경험] 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어,LoRA,-0.00013918338,0.0,0.00013918337936047465
Loss Function 관련 실무 경험 -> [상세 경험] 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어,LoRA 와 QLoRA 의 차이,-0.0011566974,0.0,0.001156697398982942
Loss Function 관련 실무 경험 -> [상세 경험] 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어,Loss Function 예시,-0.008926831,0.0,0.008926831185817719
Loss Function 관련 실무 경험 -> [상세 경험] 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어,Loss Function 정의,0.00017971835,0.0,0.0001797183504095301
Loss Function 관련 실무 경험 -> [상세 경험] 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어,MBTI,-0.0071980674,0.0,0.007198067381978035
Loss Function 관련 실무 경험 -> [상세 경험] 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어,MSE Loss 설명,-0.005655849,0.0,0.005655848886817694
Loss Function 관련 실무 경험 -> [상세 경험] 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어,MSE Loss 용도,-0.0152681,0.0,0.015268100425601006
Loss Function 관련 실무 경험 -> [상세 경험] 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어,Multi-Label 에서 CE + Softmax 적용 문제점,0.0155436415,0.0,0.015543641522526741
Loss Function 관련 실무 경험 -> [상세 경험] 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어,PEFT 방법 5가지,-0.04536473,0.0,0.045364730060100555
Loss Function 관련 실무 경험 -> [상세 경험] 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어,거대 언어 모델 정의,0.0041660657,0.0,0.004166065715253353
Loss Function 관련 실무 경험 -> [상세 경험] 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어,기본 경험,-0.028503498,0.0,0.028503498062491417
Loss Function 관련 실무 경험 -> [상세 경험] 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어,답변 실패,-0.0041168733,0.0,0.0041168732568621635
Loss Function 관련 실무 경험 -> [상세 경험] 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어,딥러닝,-0.007647944,0.0,0.007647944148629904
Loss Function 관련 실무 경험 -> [상세 경험] 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어,마지막 할 말,-0.0066137053,0.0,0.0066137053072452545
Loss Function 관련 실무 경험 -> [상세 경험] 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어,머신러닝,-0.000118675984,0.0,0.00011867598368553445
Loss Function 관련 실무 경험 -> [상세 경험] 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어,면접 시작 인사,-0.0026786795,0.0,0.002678679535165429
Loss Function 관련 실무 경험 -> [상세 경험] 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어,상세 경험,0.9743299,1.0,0.025670111179351807
Loss Function 관련 실무 경험 -> [상세 경험] 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어,수식,-0.010998979,0.0,0.010998979210853577
Loss Function 관련 실무 경험 -> [상세 경험] 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어,용어 질문,0.0028201698,0.0,0.002820169785991311
Loss Function 관련 실무 경험 -> [상세 경험] 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어,인공지능,-0.003826428,0.0,0.003826427971944213
Loss Function 관련 실무 경험 -> [상세 경험] 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어,잠시 휴식,-0.005416226,0.0,0.005416226107627153
Loss Function 관련 실무 경험 -> [상세 경험] 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어,좋아하는 아이돌,0.0048079477,0.0,0.0048079476691782475
Loss Function 관련 실무 경험 -> [상세 경험] 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어,핵심 아이디어,0.009414924,0.0,0.009414924308657646
Loss Function 관련 실무 경험 -> [상세 경험] 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어,확률 예측에서 MSE Loss 미 사용 이유,-0.005767841,0.0,0.005767840892076492
Loss Function 관련 실무 경험 -> [상세 경험] 기억 진짜 안 나는데 가물가물하다,BCE 가 좋은 task,0.003766691,0.0,0.003766691079363227
Loss Function 관련 실무 경험 -> [상세 경험] 기억 진짜 안 나는데 가물가물하다,BCE 가 좋은 이유,-0.009014793,0.0,0.009014792740345001
Loss Function 관련 실무 경험 -> [상세 경험] 기억 진짜 안 나는데 가물가물하다,LLM Fine-Tuning 의 PEFT,-0.002658848,0.0,0.002658847952261567
Loss Function 관련 실무 경험 -> [상세 경험] 기억 진짜 안 나는데 가물가물하다,LoRA,-0.0022479182,0.0,0.002247918164357543
Loss Function 관련 실무 경험 -> [상세 경험] 기억 진짜 안 나는데 가물가물하다,LoRA 와 QLoRA 의 차이,0.0019512172,0.0,0.0019512171857059002
Loss Function 관련 실무 경험 -> [상세 경험] 기억 진짜 안 나는데 가물가물하다,Loss Function 예시,-0.013751264,0.0,0.013751263730227947
Loss Function 관련 실무 경험 -> [상세 경험] 기억 진짜 안 나는데 가물가물하다,Loss Function 정의,-0.0109639885,0.0,0.010963988490402699
Loss Function 관련 실무 경험 -> [상세 경험] 기억 진짜 안 나는데 가물가물하다,MBTI,-0.0014992133,0.0,0.0014992132782936096
Loss Function 관련 실무 경험 -> [상세 경험] 기억 진짜 안 나는데 가물가물하다,MSE Loss 설명,0.0032490997,0.0,0.003249099710956216
Loss Function 관련 실무 경험 -> [상세 경험] 기억 진짜 안 나는데 가물가물하다,MSE Loss 용도,-0.0008598813,0.0,0.0008598812855780125
Loss Function 관련 실무 경험 -> [상세 경험] 기억 진짜 안 나는데 가물가물하다,Multi-Label 에서 CE + Softmax 적용 문제점,-0.0010756175,0.0,0.001075617503374815
Loss Function 관련 실무 경험 -> [상세 경험] 기억 진짜 안 나는데 가물가물하다,PEFT 방법 5가지,-0.00029742814,0.0,0.0002974281378556043
Loss Function 관련 실무 경험 -> [상세 경험] 기억 진짜 안 나는데 가물가물하다,거대 언어 모델 정의,0.002409078,0.0,0.002409077947959304
Loss Function 관련 실무 경험 -> [상세 경험] 기억 진짜 안 나는데 가물가물하다,기본 경험,0.001479541,0.0,0.0014795409515500069
Loss Function 관련 실무 경험 -> [상세 경험] 기억 진짜 안 나는데 가물가물하다,답변 실패,0.99275005,1.0,0.007249951362609863
Loss Function 관련 실무 경험 -> [상세 경험] 기억 진짜 안 나는데 가물가물하다,딥러닝,-0.0036228078,0.0,0.0036228077951818705
Loss Function 관련 실무 경험 -> [상세 경험] 기억 진짜 안 나는데 가물가물하다,마지막 할 말,-0.0070328955,0.0,0.007032895460724831
Loss Function 관련 실무 경험 -> [상세 경험] 기억 진짜 안 나는데 가물가물하다,머신러닝,-0.002626267,0.0,0.0026262670289725065
Loss Function 관련 실무 경험 -> [상세 경험] 기억 진짜 안 나는데 가물가물하다,면접 시작 인사,-0.0026055642,0.0,0.0026055641938000917
Loss Function 관련 실무 경험 -> [상세 경험] 기억 진짜 안 나는데 가물가물하다,상세 경험,0.002838932,0.0,0.002838931977748871
Loss Function 관련 실무 경험 -> [상세 경험] 기억 진짜 안 나는데 가물가물하다,수식,-0.009846142,0.0,0.009846141561865807
Loss Function 관련 실무 경험 -> [상세 경험] 기억 진짜 안 나는데 가물가물하다,용어 질문,-0.011270117,0.0,0.01127011701464653
Loss Function 관련 실무 경험 -> [상세 경험] 기억 진짜 안 나는데 가물가물하다,인공지능,-0.019769466,0.0,0.019769465550780296
Loss Function 관련 실무 경험 -> [상세 경험] 기억 진짜 안 나는데 가물가물하다,잠시 휴식,-0.0046992744,0.0,0.004699274431914091
Loss Function 관련 실무 경험 -> [상세 경험] 기억 진짜 안 나는데 가물가물하다,좋아하는 아이돌,0.0093939295,0.0,0.00939392950385809
Loss Function 관련 실무 경험 -> [상세 경험] 기억 진짜 안 나는데 가물가물하다,핵심 아이디어,0.004566746,0.0,0.004566745832562447
Loss Function 관련 실무 경험 -> [상세 경험] 기억 진짜 안 나는데 가물가물하다,확률 예측에서 MSE Loss 미 사용 이유,-0.0025394557,0.0,0.002539455657824874
Loss Function 관련 실무 경험 -> [기본 경험] 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ,BCE 가 좋은 task,-0.0072205877,0.0,0.007220587693154812
Loss Function 관련 실무 경험 -> [기본 경험] 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ,BCE 가 좋은 이유,-0.0047291964,0.0,0.0047291964292526245
Loss Function 관련 실무 경험 -> [기본 경험] 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ,LLM Fine-Tuning 의 PEFT,0.009807497,0.0,0.009807497262954712
Loss Function 관련 실무 경험 -> [기본 경험] 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ,LoRA,0.0035740123,0.0,0.0035740123130381107
Loss Function 관련 실무 경험 -> [기본 경험] 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ,LoRA 와 QLoRA 의 차이,0.010072839,0.0,0.010072839446365833
Loss Function 관련 실무 경험 -> [기본 경험] 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ,Loss Function 예시,-0.008568229,0.0,0.008568229153752327
Loss Function 관련 실무 경험 -> [기본 경험] 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ,Loss Function 정의,-0.0033867606,0.0,0.0033867605961859226
Loss Function 관련 실무 경험 -> [기본 경험] 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ,MBTI,0.008968231,0.0,0.0089682312682271
Loss Function 관련 실무 경험 -> [기본 경험] 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ,MSE Loss 설명,0.0042469725,0.0,0.004246972501277924
Loss Function 관련 실무 경험 -> [기본 경험] 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ,MSE Loss 용도,0.008668462,0.0,0.008668461814522743
Loss Function 관련 실무 경험 -> [기본 경험] 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ,Multi-Label 에서 CE + Softmax 적용 문제점,-0.014079192,0.0,0.014079191721975803
Loss Function 관련 실무 경험 -> [기본 경험] 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ,PEFT 방법 5가지,-0.010547712,0.0,0.010547712445259094
Loss Function 관련 실무 경험 -> [기본 경험] 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ,거대 언어 모델 정의,0.009213014,0.0,0.00921301357448101
Loss Function 관련 실무 경험 -> [기본 경험] 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ,기본 경험,0.97262007,1.0,0.027379930019378662
Loss Function 관련 실무 경험 -> [기본 경험] 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ,답변 실패,0.0054222276,0.0,0.0054222275502979755
Loss Function 관련 실무 경험 -> [기본 경험] 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ,딥러닝,0.0072328127,0.0,0.007232812698930502
Loss Function 관련 실무 경험 -> [기본 경험] 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ,마지막 할 말,0.0008116248,0.0,0.0008116248063743114
Loss Function 관련 실무 경험 -> [기본 경험] 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ,머신러닝,-0.012629542,0.0,0.01262954156845808
Loss Function 관련 실무 경험 -> [기본 경험] 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ,면접 시작 인사,-0.001942407,0.0,0.0019424069905653596
Loss Function 관련 실무 경험 -> [기본 경험] 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ,상세 경험,-0.024137449,0.0,0.024137448519468307
Loss Function 관련 실무 경험 -> [기본 경험] 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ,수식,0.00040314626,0.0,0.00040314625948667526
Loss Function 관련 실무 경험 -> [기본 경험] 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ,용어 질문,0.013375204,0.0,0.013375204056501389
Loss Function 관련 실무 경험 -> [기본 경험] 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ,인공지능,0.010223037,0.0,0.010223036631941795
Loss Function 관련 실무 경험 -> [기본 경험] 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ,잠시 휴식,-0.00086192734,0.0,0.0008619273430667818
Loss Function 관련 실무 경험 -> [기본 경험] 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ,좋아하는 아이돌,-0.00900748,0.0,0.00900747999548912
Loss Function 관련 실무 경험 -> [기본 경험] 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ,핵심 아이디어,-0.021676,0.0,0.021676000207662582
Loss Function 관련 실무 경험 -> [기본 경험] 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ,확률 예측에서 MSE Loss 미 사용 이유,-0.01669684,0.0,0.016696840524673462
Loss Function 관련 실무 경험 -> [기본 경험] 이걸 왜 알려줘야 하지,BCE 가 좋은 task,-0.0004198667,0.0,0.00041986670112237334
Loss Function 관련 실무 경험 -> [기본 경험] 이걸 왜 알려줘야 하지,BCE 가 좋은 이유,-0.015310695,0.0,0.015310695394873619
Loss Function 관련 실무 경험 -> [기본 경험] 이걸 왜 알려줘야 하지,LLM Fine-Tuning 의 PEFT,-0.0056563853,0.0,0.005656385328620672
Loss Function 관련 실무 경험 -> [기본 경험] 이걸 왜 알려줘야 하지,LoRA,-0.00021746253,0.0,0.0002174625260522589
Loss Function 관련 실무 경험 -> [기본 경험] 이걸 왜 알려줘야 하지,LoRA 와 QLoRA 의 차이,0.0042168787,0.0,0.004216878674924374
Loss Function 관련 실무 경험 -> [기본 경험] 이걸 왜 알려줘야 하지,Loss Function 예시,-0.014038228,0.0,0.014038228429853916
Loss Function 관련 실무 경험 -> [기본 경험] 이걸 왜 알려줘야 하지,Loss Function 정의,-0.015576322,0.0,0.015576321631669998
Loss Function 관련 실무 경험 -> [기본 경험] 이걸 왜 알려줘야 하지,MBTI,3.7495207e-05,0.0,3.749520692508668e-05
Loss Function 관련 실무 경험 -> [기본 경험] 이걸 왜 알려줘야 하지,MSE Loss 설명,-0.007864194,0.0,0.007864193990826607
Loss Function 관련 실무 경험 -> [기본 경험] 이걸 왜 알려줘야 하지,MSE Loss 용도,-0.0052709742,0.0,0.0052709742449223995
Loss Function 관련 실무 경험 -> [기본 경험] 이걸 왜 알려줘야 하지,Multi-Label 에서 CE + Softmax 적용 문제점,-0.00019584873,0.0,0.00019584872643463314
Loss Function 관련 실무 경험 -> [기본 경험] 이걸 왜 알려줘야 하지,PEFT 방법 5가지,-0.0019743538,0.0,0.001974353799596429
Loss Function 관련 실무 경험 -> [기본 경험] 이걸 왜 알려줘야 하지,거대 언어 모델 정의,0.00043476245,0.0,0.00043476244900375605
Loss Function 관련 실무 경험 -> [기본 경험] 이걸 왜 알려줘야 하지,기본 경험,0.07166778,0.0,0.07166778296232224
Loss Function 관련 실무 경험 -> [기본 경험] 이걸 왜 알려줘야 하지,답변 실패,0.98370177,1.0,0.016298234462738037
Loss Function 관련 실무 경험 -> [기본 경험] 이걸 왜 알려줘야 하지,딥러닝,-0.0040052226,0.0,0.0040052225813269615
Loss Function 관련 실무 경험 -> [기본 경험] 이걸 왜 알려줘야 하지,마지막 할 말,-0.0066189035,0.0,0.006618903484195471
Loss Function 관련 실무 경험 -> [기본 경험] 이걸 왜 알려줘야 하지,머신러닝,-0.010255709,0.0,0.010255709290504456
Loss Function 관련 실무 경험 -> [기본 경험] 이걸 왜 알려줘야 하지,면접 시작 인사,0.003127023,0.0,0.003127023112028837
Loss Function 관련 실무 경험 -> [기본 경험] 이걸 왜 알려줘야 하지,상세 경험,0.002283218,0.0,0.0022832180839031935
Loss Function 관련 실무 경험 -> [기본 경험] 이걸 왜 알려줘야 하지,수식,-0.006972817,0.0,0.006972816772758961
Loss Function 관련 실무 경험 -> [기본 경험] 이걸 왜 알려줘야 하지,용어 질문,-0.00043157724,0.0,0.00043157723848707974
Loss Function 관련 실무 경험 -> [기본 경험] 이걸 왜 알려줘야 하지,인공지능,-0.015876494,0.0,0.015876494348049164
Loss Function 관련 실무 경험 -> [기본 경험] 이걸 왜 알려줘야 하지,잠시 휴식,-0.009361413,0.0,0.009361413307487965
Loss Function 관련 실무 경험 -> [기본 경험] 이걸 왜 알려줘야 하지,좋아하는 아이돌,0.0020619354,0.0,0.0020619353745132685
Loss Function 관련 실무 경험 -> [기본 경험] 이걸 왜 알려줘야 하지,핵심 아이디어,-0.0020430633,0.0,0.002043063286691904
Loss Function 관련 실무 경험 -> [기본 경험] 이걸 왜 알려줘야 하지,확률 예측에서 MSE Loss 미 사용 이유,-0.005393578,0.0,0.005393578205257654
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,BCE 가 좋은 task,-0.008368588,0.0,0.00836858805269003
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,BCE 가 좋은 이유,0.009253203,0.0,0.009253202937543392
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,LLM Fine-Tuning 의 PEFT,0.01425081,0.0,0.014250810258090496
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,LoRA,-0.022493528,0.0,0.022493528202176094
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,LoRA 와 QLoRA 의 차이,0.0024534282,0.0,0.002453428227454424
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,Loss Function 예시,0.004711642,0.0,0.004711641930043697
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,Loss Function 정의,-0.027080197,0.0,0.027080196887254715
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,MBTI,0.9850947,1.0,0.014905273914337158
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,MSE Loss 설명,-0.011479591,0.0,0.011479591019451618
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,MSE Loss 용도,-0.0022570912,0.0,0.002257091226056218
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,Multi-Label 에서 CE + Softmax 적용 문제점,-0.0010294818,0.0,0.0010294817620888352
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,PEFT 방법 5가지,0.01914614,0.0,0.019146140664815903
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,거대 언어 모델 정의,-0.013927641,0.0,0.013927641324698925
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,기본 경험,0.0065390198,0.0,0.0065390197560191154
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,답변 실패,-0.004844225,0.0,0.004844225011765957
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,딥러닝,-0.019808492,0.0,0.019808491691946983
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,마지막 할 말,-0.007929825,0.0,0.007929825223982334
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,머신러닝,-0.028738955,0.0,0.028738955035805702
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,면접 시작 인사,0.0020290322,0.0,0.002029032213613391
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,상세 경험,-0.00038371052,0.0,0.0003837105177808553
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,수식,-0.01763798,0.0,0.017637979239225388
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,용어 질문,-0.004347723,0.0,0.004347722977399826
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,인공지능,-0.009001499,0.0,0.00900149904191494
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,잠시 휴식,-0.014515686,0.0,0.014515685848891735
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,좋아하는 아이돌,-0.008489854,0.0,0.008489853702485561
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,핵심 아이디어,0.0011884884,0.0,0.001188488444313407
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,확률 예측에서 MSE Loss 미 사용 이유,-0.00036792047,0.0,0.0003679204673971981
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,BCE 가 좋은 task,0.013128976,0.0,0.013128976337611675
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,BCE 가 좋은 이유,-0.005120785,0.0,0.005120784975588322
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,LLM Fine-Tuning 의 PEFT,0.0036896886,0.0,0.003689688630402088
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,LoRA,-0.0035670379,0.0,0.003567037871107459
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,LoRA 와 QLoRA 의 차이,0.0019788404,0.0,0.001978840446099639
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,Loss Function 예시,-0.0050338083,0.0,0.005033808294683695
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,Loss Function 정의,-0.0009498981,0.0,0.0009498980944044888
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,MBTI,0.0058668065,0.0,0.005866806488484144
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,MSE Loss 설명,-0.0097873155,0.0,0.009787315502762794
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,MSE Loss 용도,-0.009446057,0.0,0.009446056559681892
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,Multi-Label 에서 CE + Softmax 적용 문제점,0.0031947296,0.0,0.0031947295647114515
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,PEFT 방법 5가지,-0.018513776,0.0,0.01851377636194229
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,거대 언어 모델 정의,-0.0072099166,0.0,0.007209916599094868
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,기본 경험,0.0024184822,0.0,0.0024184822104871273
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,답변 실패,-0.0022182637,0.0,0.002218263689428568
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,딥러닝,-0.004491121,0.0,0.004491121042519808
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,마지막 할 말,0.0035105916,0.0,0.003510591574013233
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,머신러닝,0.0009125819,0.0,0.0009125819196924567
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,면접 시작 인사,-0.011908673,0.0,0.011908672749996185
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,상세 경험,0.0032890795,0.0,0.003289079526439309
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,수식,0.0068000806,0.0,0.0068000806495547295
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,용어 질문,-0.017680844,0.0,0.01768084429204464
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,인공지능,-0.008537302,0.0,0.008537301793694496
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,잠시 휴식,0.0026879536,0.0,0.00268795364536345
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,좋아하는 아이돌,0.9845087,1.0,0.015491306781768799
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,핵심 아이디어,-0.010728829,0.0,0.010728828608989716
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,확률 예측에서 MSE Loss 미 사용 이유,-0.0022521988,0.0,0.0022521987557411194
잠시 휴식 -> 재미있는 이야기 해줄래?,BCE 가 좋은 task,-0.002641808,0.0,0.002641808008775115
잠시 휴식 -> 재미있는 이야기 해줄래?,BCE 가 좋은 이유,0.006618169,0.0,0.0066181691363453865
잠시 휴식 -> 재미있는 이야기 해줄래?,LLM Fine-Tuning 의 PEFT,-0.011287511,0.0,0.011287511326372623
잠시 휴식 -> 재미있는 이야기 해줄래?,LoRA,-0.0057634586,0.0,0.005763458553701639
잠시 휴식 -> 재미있는 이야기 해줄래?,LoRA 와 QLoRA 의 차이,0.0072169327,0.0,0.007216932717710733
잠시 휴식 -> 재미있는 이야기 해줄래?,Loss Function 예시,-0.017378556,0.0,0.017378555610775948
잠시 휴식 -> 재미있는 이야기 해줄래?,Loss Function 정의,0.002447258,0.0,0.0024472579825669527
잠시 휴식 -> 재미있는 이야기 해줄래?,MBTI,-0.02327752,0.0,0.023277519270777702
잠시 휴식 -> 재미있는 이야기 해줄래?,MSE Loss 설명,-0.009458798,0.0,0.009458797983825207
잠시 휴식 -> 재미있는 이야기 해줄래?,MSE Loss 용도,-0.0091818655,0.0,0.009181865490972996
잠시 휴식 -> 재미있는 이야기 해줄래?,Multi-Label 에서 CE + Softmax 적용 문제점,-0.0023342934,0.0,0.002334293443709612
잠시 휴식 -> 재미있는 이야기 해줄래?,PEFT 방법 5가지,0.010863653,0.0,0.010863653384149075
잠시 휴식 -> 재미있는 이야기 해줄래?,거대 언어 모델 정의,-0.0029437444,0.0,0.0029437444172799587
잠시 휴식 -> 재미있는 이야기 해줄래?,기본 경험,-0.014376829,0.0,0.014376829378306866
잠시 휴식 -> 재미있는 이야기 해줄래?,답변 실패,-0.009685463,0.0,0.009685463272035122
잠시 휴식 -> 재미있는 이야기 해줄래?,딥러닝,-0.008673436,0.0,0.008673436008393764
잠시 휴식 -> 재미있는 이야기 해줄래?,마지막 할 말,0.014281659,0.0,0.01428165938705206
잠시 휴식 -> 재미있는 이야기 해줄래?,머신러닝,-0.00909895,0.0,0.00909894984215498
잠시 휴식 -> 재미있는 이야기 해줄래?,면접 시작 인사,-0.0043454394,0.0,0.004345439374446869
잠시 휴식 -> 재미있는 이야기 해줄래?,상세 경험,-0.00602928,0.0,0.0060292799025774
잠시 휴식 -> 재미있는 이야기 해줄래?,수식,-0.021113338,0.0,0.021113337948918343
잠시 휴식 -> 재미있는 이야기 해줄래?,용어 질문,0.011039235,0.0,0.011039234697818756
잠시 휴식 -> 재미있는 이야기 해줄래?,인공지능,0.023366334,0.0,0.023366333916783333
잠시 휴식 -> 재미있는 이야기 해줄래?,잠시 휴식,0.99178493,1.0,0.008215069770812988
잠시 휴식 -> 재미있는 이야기 해줄래?,좋아하는 아이돌,-0.001385075,0.0,0.0013850750401616096
잠시 휴식 -> 재미있는 이야기 해줄래?,핵심 아이디어,0.008084201,0.0,0.008084201253950596
잠시 휴식 -> 재미있는 이야기 해줄래?,확률 예측에서 MSE Loss 미 사용 이유,-0.012327645,0.0,0.012327644973993301
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",BCE 가 좋은 task,-0.007530033,0.0,0.0075300331227481365
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",BCE 가 좋은 이유,-0.05637087,0.0,0.056370869278907776
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",LLM Fine-Tuning 의 PEFT,0.97663015,1.0,0.02336984872817993
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",LoRA,-0.030694718,0.0,0.030694717541337013
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",LoRA 와 QLoRA 의 차이,-0.01610896,0.0,0.016108959913253784
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",Loss Function 예시,-0.038967766,0.0,0.038967765867710114
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",Loss Function 정의,-0.026252683,0.0,0.026252683252096176
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",MBTI,0.0045695254,0.0,0.0045695253647863865
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",MSE Loss 설명,0.02188931,0.0,0.02188931033015251
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",MSE Loss 용도,-0.01920989,0.0,0.019209889695048332
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",Multi-Label 에서 CE + Softmax 적용 문제점,0.02177139,0.0,0.021771389991044998
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",PEFT 방법 5가지,-0.0072371173,0.0,0.007237117271870375
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",거대 언어 모델 정의,0.01256949,0.0,0.012569489888846874
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",기본 경험,0.0014583758,0.0,0.0014583758311346173
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",답변 실패,0.021332374,0.0,0.021332373842597008
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",딥러닝,0.010291616,0.0,0.010291616432368755
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",마지막 할 말,0.0052914675,0.0,0.005291467532515526
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",머신러닝,-0.04346638,0.0,0.04346638172864914
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",면접 시작 인사,-0.0074198525,0.0,0.007419852539896965
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",상세 경험,0.009270588,0.0,0.00927058793604374
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",수식,0.024862703,0.0,0.024862702935934067
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",용어 질문,0.03419861,0.0,0.03419860824942589
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",인공지능,-0.012668062,0.0,0.012668062001466751
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",잠시 휴식,-0.006982011,0.0,0.006982010789215565
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",좋아하는 아이돌,-0.0017086256,0.0,0.0017086255829781294
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",핵심 아이디어,0.021640524,0.0,0.02164052426815033
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",확률 예측에서 MSE Loss 미 사용 이유,-0.009692051,0.0,0.009692051447927952
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,BCE 가 좋은 task,0.004555707,0.0,0.004555706866085529
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,BCE 가 좋은 이유,-0.01237813,0.0,0.012378130108118057
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,LLM Fine-Tuning 의 PEFT,0.0067448867,0.0,0.006744886748492718
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,LoRA,0.001147088,0.0,0.0011470880126580596
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,LoRA 와 QLoRA 의 차이,-0.0020525723,0.0,0.0020525723230093718
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,Loss Function 예시,-0.012964465,0.0,0.012964464724063873
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,Loss Function 정의,-0.0062717507,0.0,0.006271750666201115
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,MBTI,-0.00021655178,0.0,0.00021655177988577634
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,MSE Loss 설명,-0.00043380505,0.0,0.00043380504939705133
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,MSE Loss 용도,-0.005921302,0.0,0.005921301897615194
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,Multi-Label 에서 CE + Softmax 적용 문제점,2.3279541e-05,0.0,2.3279540982912295e-05
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,PEFT 방법 5가지,-0.007012168,0.0,0.0070121679455041885
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,거대 언어 모델 정의,-0.0031889463,0.0,0.003188946284353733
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,기본 경험,-0.0034235339,0.0,0.003423533868044615
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,답변 실패,0.99247307,1.0,0.0075269341468811035
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,딥러닝,-0.0056115594,0.0,0.005611559376120567
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,마지막 할 말,-0.004493575,0.0,0.00449357507750392
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,머신러닝,-0.0016067097,0.0,0.0016067096730694175
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,면접 시작 인사,0.002463195,0.0,0.0024631950072944164
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,상세 경험,-0.0036237608,0.0,0.0036237607710063457
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,수식,-0.008793618,0.0,0.008793617598712444
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,용어 질문,0.015198812,0.0,0.015198811888694763
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,인공지능,-0.018626789,0.0,0.01862678863108158
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,잠시 휴식,-0.006112803,0.0,0.0061128027737140656
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,좋아하는 아이돌,-0.0020360495,0.0,0.002036049496382475
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,핵심 아이디어,0.010864183,0.0,0.01086418330669403
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,확률 예측에서 MSE Loss 미 사용 이유,-0.0031660805,0.0,0.003166080452501774
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야!,BCE 가 좋은 task,0.005483573,0.0,0.0054835728369653225
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야!,BCE 가 좋은 이유,-0.03566853,0.0,0.03566852957010269
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야!,LLM Fine-Tuning 의 PEFT,0.9788048,1.0,0.021195173263549805
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야!,LoRA,-0.034879383,0.0,0.03487938269972801
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야!,LoRA 와 QLoRA 의 차이,-0.017686421,0.0,0.017686421051621437
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야!,Loss Function 예시,-0.03456353,0.0,0.03456353023648262
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야!,Loss Function 정의,-0.026278999,0.0,0.02627899870276451
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야!,MBTI,0.0048987865,0.0,0.004898786544799805
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야!,MSE Loss 설명,0.01592615,0.0,0.015926150605082512
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야!,MSE Loss 용도,-0.008930641,0.0,0.00893064122647047
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야!,Multi-Label 에서 CE + Softmax 적용 문제점,-0.009977996,0.0,0.009977996349334717
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야!,PEFT 방법 5가지,-0.0023491739,0.0,0.0023491738829761744
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야!,거대 언어 모델 정의,0.018413289,0.0,0.01841328851878643
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야!,기본 경험,0.00993173,0.0,0.009931730106472969
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야!,답변 실패,0.015429524,0.0,0.015429523773491383
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야!,딥러닝,0.016021434,0.0,0.01602143421769142
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야!,마지막 할 말,0.0037106243,0.0,0.003710624296218157
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야!,머신러닝,-0.04021068,0.0,0.04021067917346954
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야!,면접 시작 인사,-0.008398522,0.0,0.008398521691560745
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야!,상세 경험,0.01770088,0.0,0.017700880765914917
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야!,수식,0.031527914,0.0,0.031527914106845856
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야!,용어 질문,0.012248617,0.0,0.01224861666560173
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야!,인공지능,0.008511768,0.0,0.008511767722666264
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야!,잠시 휴식,-0.009222936,0.0,0.009222935885190964
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야!,좋아하는 아이돌,0.0039025492,0.0,0.0039025491569191217
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야!,핵심 아이디어,0.026109619,0.0,0.026109619066119194
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야!,확률 예측에서 MSE Loss 미 사용 이유,-0.0034997468,0.0,0.0034997467882931232
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다,BCE 가 좋은 task,0.0043687383,0.0,0.004368738271296024
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다,BCE 가 좋은 이유,-0.010057036,0.0,0.010057035833597183
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다,LLM Fine-Tuning 의 PEFT,0.0024455152,0.0,0.0024455152451992035
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다,LoRA,-0.00043802973,0.0,0.00043802973232232034
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다,LoRA 와 QLoRA 의 차이,0.0019361854,0.0,0.0019361854065209627
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다,Loss Function 예시,-0.009911912,0.0,0.009911911562085152
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다,Loss Function 정의,-0.0071496363,0.0,0.0071496362797915936
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다,MBTI,-0.0015584629,0.0,0.001558462856337428
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다,MSE Loss 설명,5.8435762e-05,0.0,5.843576218467206e-05
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다,MSE Loss 용도,-0.0066579874,0.0,0.006657987367361784
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다,Multi-Label 에서 CE + Softmax 적용 문제점,-0.0003392381,0.0,0.00033923808950930834
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다,PEFT 방법 5가지,-0.0021924241,0.0,0.0021924241445958614
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다,거대 언어 모델 정의,-0.00014642644,0.0,0.00014642643509432673
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다,기본 경험,-0.00096231606,0.0,0.000962316058576107
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다,답변 실패,0.99355096,1.0,0.0064490437507629395
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다,딥러닝,-0.0050858166,0.0,0.005085816606879234
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다,마지막 할 말,-0.006535183,0.0,0.006535183172672987
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다,머신러닝,-0.0012558587,0.0,0.001255858689546585
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다,면접 시작 인사,0.001370053,0.0,0.0013700530398637056
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다,상세 경험,-0.0059332047,0.0,0.0059332046657800674
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다,수식,-0.010480204,0.0,0.010480203665792942
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다,용어 질문,-0.00051383383,0.0,0.0005138338310644031
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다,인공지능,-0.020278277,0.0,0.02027827687561512
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다,잠시 휴식,-0.0051928633,0.0,0.005192863289266825
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다,좋아하는 아이돌,0.0022085558,0.0,0.00220855581574142
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다,핵심 아이디어,0.0038290035,0.0,0.003829003544524312
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다,확률 예측에서 MSE Loss 미 사용 이유,-0.0024614155,0.0,0.00246141548268497
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",BCE 가 좋은 task,-0.0038997128,0.0,0.0038997128140181303
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",BCE 가 좋은 이유,0.025872799,0.0,0.02587279863655567
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",LLM Fine-Tuning 의 PEFT,-0.0025440466,0.0,0.002544046612456441
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",LoRA,0.0024563742,0.0,0.0024563742335885763
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",LoRA 와 QLoRA 의 차이,0.010678625,0.0,0.010678624734282494
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",Loss Function 예시,0.0059421393,0.0,0.005942139308899641
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",Loss Function 정의,-0.0012549203,0.0,0.0012549202656373382
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",MBTI,0.0019433381,0.0,0.0019433380803093314
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",MSE Loss 설명,-0.02735814,0.0,0.02735814079642296
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",MSE Loss 용도,0.019515252,0.0,0.019515251740813255
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",Multi-Label 에서 CE + Softmax 적용 문제점,0.044312984,0.0,0.044312983751297
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",PEFT 방법 5가지,0.9764434,1.0,0.02355659008026123
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",거대 언어 모델 정의,0.024152251,0.0,0.024152250960469246
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",기본 경험,0.009210146,0.0,0.00921014603227377
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",답변 실패,-0.0038853497,0.0,0.0038853497244417667
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",딥러닝,-0.018860333,0.0,0.018860332667827606
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",마지막 할 말,0.0036924866,0.0,0.0036924865562468767
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",머신러닝,0.021865902,0.0,0.021865902468562126
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",면접 시작 인사,-0.0038219641,0.0,0.003821964142844081
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",상세 경험,-0.020688603,0.0,0.020688602700829506
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",수식,0.0040422054,0.0,0.004042205400764942
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",용어 질문,-0.028937029,0.0,0.028937028720974922
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",인공지능,-0.011619233,0.0,0.011619232594966888
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",잠시 휴식,0.01526673,0.0,0.015266730450093746
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",좋아하는 아이돌,-0.0072850566,0.0,0.007285056635737419
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",핵심 아이디어,-0.016320266,0.0,0.01632026582956314
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",확률 예측에서 MSE Loss 미 사용 이유,-0.018719593,0.0,0.018719593062996864
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,BCE 가 좋은 task,0.004619716,0.0,0.004619716200977564
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,BCE 가 좋은 이유,-0.012200823,0.0,0.012200823053717613
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,LLM Fine-Tuning 의 PEFT,0.0013942295,0.0,0.0013942294754087925
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,LoRA,0.0019689633,0.0,0.0019689633045345545
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,LoRA 와 QLoRA 의 차이,-0.0034956385,0.0,0.0034956384915858507
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,Loss Function 예시,-0.013629232,0.0,0.013629231601953506
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,Loss Function 정의,-0.006872857,0.0,0.006872856989502907
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,MBTI,0.0011365907,0.0,0.001136590726673603
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,MSE Loss 설명,0.0006505913,0.0,0.0006505912751890719
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,MSE Loss 용도,-0.0060687107,0.0,0.006068710703402758
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,Multi-Label 에서 CE + Softmax 적용 문제점,0.0029625269,0.0,0.0029625268653035164
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,PEFT 방법 5가지,0.0049961493,0.0,0.004996149335056543
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,거대 언어 모델 정의,-7.329141e-05,0.0,7.3291412263643e-05
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,기본 경험,-0.0010343402,0.0,0.0010343402391299605
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,답변 실패,0.9922562,1.0,0.007743775844573975
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,딥러닝,-0.0017228596,0.0,0.0017228595679625869
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,마지막 할 말,-0.006247221,0.0,0.006247221026569605
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,머신러닝,0.0007419042,0.0,0.0007419041940011084
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,면접 시작 인사,0.0028772017,0.0,0.0028772016521543264
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,상세 경험,-0.0075868363,0.0,0.007586836349219084
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,수식,-0.010904498,0.0,0.010904498398303986
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,용어 질문,-0.006243546,0.0,0.006243546027690172
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,인공지능,-0.017591167,0.0,0.017591167241334915
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,잠시 휴식,-0.004402548,0.0,0.00440254807472229
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,좋아하는 아이돌,-3.924624e-05,0.0,3.924623888451606e-05
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,핵심 아이디어,0.005426895,0.0,0.005426894873380661
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,확률 예측에서 MSE Loss 미 사용 이유,-0.0005201478,0.0,0.0005201477906666696
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,BCE 가 좋은 task,0.0018690116,0.0,0.0018690115539357066
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,BCE 가 좋은 이유,-0.0020957382,0.0,0.0020957381930202246
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,LLM Fine-Tuning 의 PEFT,-0.025410872,0.0,0.02541087195277214
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,LoRA,0.9796872,1.0,0.020312786102294922
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,LoRA 와 QLoRA 의 차이,-0.021044027,0.0,0.02104402706027031
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,Loss Function 예시,0.0070990995,0.0,0.0070990994572639465
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,Loss Function 정의,0.014708438,0.0,0.014708437956869602
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,MBTI,-0.033309367,0.0,0.033309366554021835
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,MSE Loss 설명,-0.008665656,0.0,0.008665655739605427
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,MSE Loss 용도,-0.006768926,0.0,0.0067689260467886925
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,Multi-Label 에서 CE + Softmax 적용 문제점,0.0067534344,0.0,0.006753434427082539
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,PEFT 방법 5가지,-0.004561281,0.0,0.004561280831694603
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,거대 언어 모델 정의,0.0012854306,0.0,0.0012854306260123849
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,기본 경험,0.012102814,0.0,0.012102814391255379
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,답변 실패,0.00375617,0.0,0.003756169928237796
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,딥러닝,-0.019063208,0.0,0.019063208252191544
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,마지막 할 말,0.0075302664,0.0,0.007530266419053078
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,머신러닝,-0.009055646,0.0,0.009055646136403084
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,면접 시작 인사,-0.004875928,0.0,0.004875928163528442
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,상세 경험,-0.02398526,0.0,0.023985259234905243
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,수식,0.015777094,0.0,0.015777094289660454
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,용어 질문,-0.006086946,0.0,0.006086945999413729
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,인공지능,-0.012785294,0.0,0.012785294093191624
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,잠시 휴식,-0.0063022566,0.0,0.006302256602793932
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,좋아하는 아이돌,-0.01503989,0.0,0.015039890073239803
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,핵심 아이디어,-0.009761671,0.0,0.009761670604348183
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,확률 예측에서 MSE Loss 미 사용 이유,-0.016693272,0.0,0.016693271696567535
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,BCE 가 좋은 task,-0.0018895011,0.0,0.0018895011162385345
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,BCE 가 좋은 이유,-0.01098622,0.0,0.010986220091581345
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,LLM Fine-Tuning 의 PEFT,0.0037420462,0.0,0.0037420461885631084
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,LoRA,0.004991705,0.0,0.004991705063730478
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,LoRA 와 QLoRA 의 차이,-0.0012866668,0.0,0.0012866668403148651
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,Loss Function 예시,-0.0044166273,0.0,0.0044166273437440395
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,Loss Function 정의,-0.007341373,0.0,0.0073413727805018425
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,MBTI,-0.0068777413,0.0,0.006877741310745478
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,MSE Loss 설명,-0.0008740008,0.0,0.0008740007760934532
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,MSE Loss 용도,-0.01163297,0.0,0.011632969602942467
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,Multi-Label 에서 CE + Softmax 적용 문제점,-0.0009322121,0.0,0.0009322121040895581
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,PEFT 방법 5가지,-0.0028236783,0.0,0.0028236783109605312
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,거대 언어 모델 정의,-0.0026598016,0.0,0.0026598016265779734
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,기본 경험,-0.0014586629,0.0,0.0014586629113182425
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,답변 실패,0.99098325,1.0,0.009016752243041992
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,딥러닝,-0.0044221175,0.0,0.004422117490321398
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,마지막 할 말,0.00042371266,0.0,0.0004237126559019089
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,머신러닝,-0.0044828663,0.0,0.0044828662648797035
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,면접 시작 인사,0.0011978557,0.0,0.0011978556867688894
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,상세 경험,-0.00088916236,0.0,0.0008891623583622277
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,수식,-0.009692481,0.0,0.00969248078763485
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,용어 질문,-0.0062050037,0.0,0.006205003708600998
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,인공지능,-0.014276437,0.0,0.01427643746137619
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,잠시 휴식,-0.002539628,0.0,0.002539627952501178
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,좋아하는 아이돌,0.0029493102,0.0,0.0029493102338165045
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,핵심 아이디어,0.009524923,0.0,0.00952492281794548
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,확률 예측에서 MSE Loss 미 사용 이유,-0.004205873,0.0,0.004205872770398855
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야!,BCE 가 좋은 task,0.006645843,0.0,0.006645842920988798
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야!,BCE 가 좋은 이유,-0.0009447909,0.0,0.0009447908960282803
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야!,LLM Fine-Tuning 의 PEFT,-0.024971677,0.0,0.024971676990389824
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야!,LoRA,0.9828401,1.0,0.017159879207611084
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야!,LoRA 와 QLoRA 의 차이,-0.024810411,0.0,0.024810411036014557
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야!,Loss Function 예시,0.008081573,0.0,0.008081573061645031
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야!,Loss Function 정의,0.0066419034,0.0,0.006641903426498175
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야!,MBTI,-0.010939884,0.0,0.0109398839995265
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야!,MSE Loss 설명,-0.0028433048,0.0,0.002843304770067334
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야!,MSE Loss 용도,-0.005881707,0.0,0.0058817071840167046
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야!,Multi-Label 에서 CE + Softmax 적용 문제점,-0.002786029,0.0,0.0027860288973897696
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야!,PEFT 방법 5가지,0.00020464917,0.0,0.00020464917179197073
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야!,거대 언어 모델 정의,0.017229369,0.0,0.017229368910193443
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야!,기본 경험,0.00097410305,0.0,0.0009741030517034233
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야!,답변 실패,-0.0058717495,0.0,0.005871749483048916
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야!,딥러닝,-0.013448178,0.0,0.013448177836835384
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야!,마지막 할 말,0.0065465034,0.0,0.006546503398567438
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야!,머신러닝,-0.005598604,0.0,0.005598604213446379
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야!,면접 시작 인사,-0.009866231,0.0,0.009866231121122837
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야!,상세 경험,-0.013863863,0.0,0.013863863423466682
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야!,수식,0.0045725754,0.0,0.004572575446218252
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야!,용어 질문,-0.010402258,0.0,0.010402257554233074
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야!,인공지능,-0.005074109,0.0,0.005074108950793743
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야!,잠시 휴식,-0.009415358,0.0,0.009415358304977417
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야!,좋아하는 아이돌,-0.008755477,0.0,0.008755477145314217
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야!,핵심 아이디어,0.00445874,0.0,0.004458739887923002
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야!,확률 예측에서 MSE Loss 미 사용 이유,-0.021196261,0.0,0.021196261048316956
LoRA -> 무슨 OOM 없앤다는 것 같은데,BCE 가 좋은 task,0.0013436312,0.0,0.0013436311855912209
LoRA -> 무슨 OOM 없앤다는 것 같은데,BCE 가 좋은 이유,-0.010699057,0.0,0.010699057020246983
LoRA -> 무슨 OOM 없앤다는 것 같은데,LLM Fine-Tuning 의 PEFT,0.0012512818,0.0,0.0012512818211689591
LoRA -> 무슨 OOM 없앤다는 것 같은데,LoRA,0.0019276753,0.0,0.001927675330080092
LoRA -> 무슨 OOM 없앤다는 것 같은데,LoRA 와 QLoRA 의 차이,-0.00017224887,0.0,0.00017224886687472463
LoRA -> 무슨 OOM 없앤다는 것 같은데,Loss Function 예시,-0.009924298,0.0,0.009924298152327538
LoRA -> 무슨 OOM 없앤다는 것 같은데,Loss Function 정의,-0.0074864323,0.0,0.007486432325094938
LoRA -> 무슨 OOM 없앤다는 것 같은데,MBTI,-0.0017885775,0.0,0.0017885775305330753
LoRA -> 무슨 OOM 없앤다는 것 같은데,MSE Loss 설명,0.0025704852,0.0,0.002570485230535269
LoRA -> 무슨 OOM 없앤다는 것 같은데,MSE Loss 용도,-0.00830406,0.0,0.008304060436785221
LoRA -> 무슨 OOM 없앤다는 것 같은데,Multi-Label 에서 CE + Softmax 적용 문제점,0.0024472624,0.0,0.002447262406349182
LoRA -> 무슨 OOM 없앤다는 것 같은데,PEFT 방법 5가지,-0.0028041156,0.0,0.0028041156474500895
LoRA -> 무슨 OOM 없앤다는 것 같은데,거대 언어 모델 정의,-0.0052573667,0.0,0.005257366690784693
LoRA -> 무슨 OOM 없앤다는 것 같은데,기본 경험,-0.0018962591,0.0,0.0018962591420859098
LoRA -> 무슨 OOM 없앤다는 것 같은데,답변 실패,0.9941669,1.0,0.005833089351654053
LoRA -> 무슨 OOM 없앤다는 것 같은데,딥러닝,-0.004373505,0.0,0.004373504780232906
LoRA -> 무슨 OOM 없앤다는 것 같은데,마지막 할 말,-0.00527855,0.0,0.005278550088405609
LoRA -> 무슨 OOM 없앤다는 것 같은데,머신러닝,-0.001009427,0.0,0.001009427011013031
LoRA -> 무슨 OOM 없앤다는 것 같은데,면접 시작 인사,-0.0016428437,0.0,0.001642843708395958
LoRA -> 무슨 OOM 없앤다는 것 같은데,상세 경험,-0.0009395987,0.0,0.000939598714467138
LoRA -> 무슨 OOM 없앤다는 것 같은데,수식,-0.010009763,0.0,0.010009762831032276
LoRA -> 무슨 OOM 없앤다는 것 같은데,용어 질문,0.0016523516,0.0,0.0016523515805602074
LoRA -> 무슨 OOM 없앤다는 것 같은데,인공지능,-0.014197654,0.0,0.014197654090821743
LoRA -> 무슨 OOM 없앤다는 것 같은데,잠시 휴식,-0.0045172884,0.0,0.004517288412898779
LoRA -> 무슨 OOM 없앤다는 것 같은데,좋아하는 아이돌,0.0048543955,0.0,0.004854395519942045
LoRA -> 무슨 OOM 없앤다는 것 같은데,핵심 아이디어,0.007979565,0.0,0.007979565300047398
LoRA -> 무슨 OOM 없앤다는 것 같은데,확률 예측에서 MSE Loss 미 사용 이유,0.00038636188,0.0,0.00038636187673546374
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,BCE 가 좋은 task,-0.025001388,0.0,0.025001388043165207
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,BCE 가 좋은 이유,0.037386227,0.0,0.037386227399110794
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,LLM Fine-Tuning 의 PEFT,-0.012326389,0.0,0.012326388619840145
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,LoRA,-0.0310306,0.0,0.031030600890517235
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,LoRA 와 QLoRA 의 차이,0.9823363,1.0,0.01766371726989746
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,Loss Function 예시,0.0049306764,0.0,0.004930676426738501
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,Loss Function 정의,-0.009329874,0.0,0.009329874068498611
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,MBTI,-0.018028494,0.0,0.018028493970632553
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,MSE Loss 설명,-0.024431005,0.0,0.024431005120277405
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,MSE Loss 용도,-0.0049920897,0.0,0.0049920896999537945
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,Multi-Label 에서 CE + Softmax 적용 문제점,0.004325945,0.0,0.004325944930315018
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,PEFT 방법 5가지,0.026021292,0.0,0.026021292433142662
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,거대 언어 모델 정의,-0.020306993,0.0,0.020306993275880814
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,기본 경험,0.0056979675,0.0,0.005697967484593391
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,답변 실패,-0.009144743,0.0,0.009144742973148823
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,딥러닝,-0.01953176,0.0,0.01953176036477089
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,마지막 할 말,0.00933263,0.0,0.009332629851996899
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,머신러닝,-0.0036167072,0.0,0.0036167071666568518
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,면접 시작 인사,-0.010142691,0.0,0.010142691433429718
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,상세 경험,-0.0030989088,0.0,0.0030989088118076324
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,수식,0.006864391,0.0,0.006864390801638365
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,용어 질문,-0.017890979,0.0,0.01789097860455513
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,인공지능,0.013070464,0.0,0.013070464134216309
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,잠시 휴식,0.0036459246,0.0,0.0036459246184676886
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,좋아하는 아이돌,-0.001050259,0.0,0.0010502589866518974
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,핵심 아이디어,-0.0023184048,0.0,0.002318404847756028
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,확률 예측에서 MSE Loss 미 사용 이유,-0.007934552,0.0,0.007934551686048508
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,BCE 가 좋은 task,-0.001057746,0.0,0.001057746005244553
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,BCE 가 좋은 이유,-0.010445032,0.0,0.010445032268762589
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,LLM Fine-Tuning 의 PEFT,0.0005571549,0.0,0.0005571548826992512
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,LoRA,0.00074077805,0.0,0.0007407780503854156
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,LoRA 와 QLoRA 의 차이,0.0075416663,0.0,0.0075416662730276585
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,Loss Function 예시,-0.004329037,0.0,0.004329036921262741
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,Loss Function 정의,-0.00997704,0.0,0.009977039881050587
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,MBTI,-0.00073833764,0.0,0.0007383376359939575
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,MSE Loss 설명,0.0009594858,0.0,0.0009594858274795115
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,MSE Loss 용도,-0.0070457417,0.0,0.007045741658657789
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,Multi-Label 에서 CE + Softmax 적용 문제점,0.00050247926,0.0,0.000502479262650013
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,PEFT 방법 5가지,-0.005114087,0.0,0.005114086903631687
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,거대 언어 모델 정의,-0.005917291,0.0,0.005917291156947613
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,기본 경험,-0.003962698,0.0,0.0039626979269087315
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,답변 실패,0.99308157,1.0,0.006918430328369141
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,딥러닝,-0.0015609215,0.0,0.001560921547934413
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,마지막 할 말,-0.003329275,0.0,0.003329274943098426
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,머신러닝,-0.001992333,0.0,0.0019923329818993807
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,면접 시작 인사,-0.0020565847,0.0,0.002056584693491459
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,상세 경험,-0.008695396,0.0,0.008695395663380623
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,수식,-0.010831939,0.0,0.010831939056515694
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,용어 질문,-0.0023646064,0.0,0.00236460636369884
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,인공지능,-0.01658401,0.0,0.016584010794758797
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,잠시 휴식,-0.0057360977,0.0,0.005736097693443298
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,좋아하는 아이돌,0.0027849816,0.0,0.0027849816251546144
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,핵심 아이디어,0.009360157,0.0,0.009360156953334808
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,확률 예측에서 MSE Loss 미 사용 이유,-0.0026353635,0.0,0.0026353634893894196
마지막 할 말 -> 로라야 정말 고마워!,BCE 가 좋은 task,-5.6526733e-05,0.0,5.652673280565068e-05
마지막 할 말 -> 로라야 정말 고마워!,BCE 가 좋은 이유,-0.0063393535,0.0,0.006339353509247303
마지막 할 말 -> 로라야 정말 고마워!,LLM Fine-Tuning 의 PEFT,0.0021271943,0.0,0.0021271943114697933
마지막 할 말 -> 로라야 정말 고마워!,LoRA,0.006905831,0.0,0.006905830930918455
마지막 할 말 -> 로라야 정말 고마워!,LoRA 와 QLoRA 의 차이,0.0077505833,0.0,0.0077505833469331264
마지막 할 말 -> 로라야 정말 고마워!,Loss Function 예시,-0.011703322,0.0,0.01170332171022892
마지막 할 말 -> 로라야 정말 고마워!,Loss Function 정의,-0.0007959571,0.0,0.0007959571084938943
마지막 할 말 -> 로라야 정말 고마워!,MBTI,-0.0019744353,0.0,0.0019744352903217077
마지막 할 말 -> 로라야 정말 고마워!,MSE Loss 설명,-0.008499027,0.0,0.008499027229845524
마지막 할 말 -> 로라야 정말 고마워!,MSE Loss 용도,0.0050514685,0.0,0.005051468499004841
마지막 할 말 -> 로라야 정말 고마워!,Multi-Label 에서 CE + Softmax 적용 문제점,-0.0026024918,0.0,0.0026024917606264353
마지막 할 말 -> 로라야 정말 고마워!,PEFT 방법 5가지,0.015017538,0.0,0.015017538331449032
마지막 할 말 -> 로라야 정말 고마워!,거대 언어 모델 정의,0.0031176095,0.0,0.0031176095362752676
마지막 할 말 -> 로라야 정말 고마워!,기본 경험,-0.014010109,0.0,0.014010109007358551
마지막 할 말 -> 로라야 정말 고마워!,답변 실패,-0.0042243954,0.0,0.004224395379424095
마지막 할 말 -> 로라야 정말 고마워!,딥러닝,-0.002791097,0.0,0.0027910969220101833
마지막 할 말 -> 로라야 정말 고마워!,마지막 할 말,0.9897488,1.0,0.010251224040985107
마지막 할 말 -> 로라야 정말 고마워!,머신러닝,0.013443193,0.0,0.013443193398416042
마지막 할 말 -> 로라야 정말 고마워!,면접 시작 인사,-0.0031456298,0.0,0.003145629772916436
마지막 할 말 -> 로라야 정말 고마워!,상세 경험,-0.003281384,0.0,0.0032813840080052614
마지막 할 말 -> 로라야 정말 고마워!,수식,0.007586583,0.0,0.007586583029478788
마지막 할 말 -> 로라야 정말 고마워!,용어 질문,-0.013836621,0.0,0.013836621306836605
마지막 할 말 -> 로라야 정말 고마워!,인공지능,0.0169915,0.0,0.016991499811410904
마지막 할 말 -> 로라야 정말 고마워!,잠시 휴식,0.0026588368,0.0,0.0026588367763906717
마지막 할 말 -> 로라야 정말 고마워!,좋아하는 아이돌,0.013455463,0.0,0.013455462642014027
마지막 할 말 -> 로라야 정말 고마워!,핵심 아이디어,0.010999218,0.0,0.010999217629432678
마지막 할 말 -> 로라야 정말 고마워!,확률 예측에서 MSE Loss 미 사용 이유,0.013130975,0.0,0.0131309749558568
마지막 할 말 -> 로라야 사랑해,BCE 가 좋은 task,0.0012796585,0.0,0.0012796585215255618
마지막 할 말 -> 로라야 사랑해,BCE 가 좋은 이유,-0.010313711,0.0,0.010313711129128933
마지막 할 말 -> 로라야 사랑해,LLM Fine-Tuning 의 PEFT,-0.0003362426,0.0,0.0003362426068633795
마지막 할 말 -> 로라야 사랑해,LoRA,0.008337783,0.0,0.008337782695889473
마지막 할 말 -> 로라야 사랑해,LoRA 와 QLoRA 의 차이,0.00536753,0.0,0.005367530044168234
마지막 할 말 -> 로라야 사랑해,Loss Function 예시,-0.012681635,0.0,0.012681635096669197
마지막 할 말 -> 로라야 사랑해,Loss Function 정의,-0.00048781492,0.0,0.00048781491932459176
마지막 할 말 -> 로라야 사랑해,MBTI,-0.00020554372,0.0,0.00020554372167680413
마지막 할 말 -> 로라야 사랑해,MSE Loss 설명,-0.009482225,0.0,0.009482225403189659
마지막 할 말 -> 로라야 사랑해,MSE Loss 용도,0.0054272423,0.0,0.005427242256700993
마지막 할 말 -> 로라야 사랑해,Multi-Label 에서 CE + Softmax 적용 문제점,-0.0036968663,0.0,0.0036968663334846497
마지막 할 말 -> 로라야 사랑해,PEFT 방법 5가지,0.013531873,0.0,0.013531873002648354
마지막 할 말 -> 로라야 사랑해,거대 언어 모델 정의,0.0013319885,0.0,0.001331988489255309
마지막 할 말 -> 로라야 사랑해,기본 경험,-0.014132659,0.0,0.014132658950984478
마지막 할 말 -> 로라야 사랑해,답변 실패,-0.004124267,0.0,0.004124267026782036
마지막 할 말 -> 로라야 사랑해,딥러닝,-0.0011107014,0.0,0.0011107013560831547
마지막 할 말 -> 로라야 사랑해,마지막 할 말,0.9916097,1.0,0.008390307426452637
마지막 할 말 -> 로라야 사랑해,머신러닝,0.013988703,0.0,0.013988702557981014
마지막 할 말 -> 로라야 사랑해,면접 시작 인사,-0.0041335598,0.0,0.004133559763431549
마지막 할 말 -> 로라야 사랑해,상세 경험,-0.007771495,0.0,0.007771494798362255
마지막 할 말 -> 로라야 사랑해,수식,0.0077076186,0.0,0.007707618642598391
마지막 할 말 -> 로라야 사랑해,용어 질문,-0.010964869,0.0,0.01096486859023571
마지막 할 말 -> 로라야 사랑해,인공지능,0.014462853,0.0,0.014462852850556374
마지막 할 말 -> 로라야 사랑해,잠시 휴식,0.0055327546,0.0,0.005532754585146904
마지막 할 말 -> 로라야 사랑해,좋아하는 아이돌,0.018886477,0.0,0.018886476755142212
마지막 할 말 -> 로라야 사랑해,핵심 아이디어,0.0150546925,0.0,0.015054692514240742
마지막 할 말 -> 로라야 사랑해,확률 예측에서 MSE Loss 미 사용 이유,0.012966506,0.0,0.01296650618314743
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,BCE 가 좋은 task,0.0043470217,0.0,0.004347021691501141
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,BCE 가 좋은 이유,-0.0064902506,0.0,0.006490250583738089
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,LLM Fine-Tuning 의 PEFT,-0.00054210704,0.0,0.0005421070381999016
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,LoRA,0.0018697254,0.0,0.0018697254126891494
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,LoRA 와 QLoRA 의 차이,1.24871785e-05,0.0,1.2487178537412547e-05
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,Loss Function 예시,-0.011628183,0.0,0.011628182604908943
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,Loss Function 정의,0.002165522,0.0,0.0021655219607055187
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,MBTI,0.011482579,0.0,0.011482578702270985
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,MSE Loss 설명,-0.010236372,0.0,0.010236372239887714
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,MSE Loss 용도,0.0028376062,0.0,0.0028376062400639057
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,Multi-Label 에서 CE + Softmax 적용 문제점,-0.009119363,0.0,0.009119362570345402
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,PEFT 방법 5가지,0.017718052,0.0,0.017718052491545677
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,거대 언어 모델 정의,0.00061062636,0.0,0.0006106263608671725
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,기본 경험,-0.021678675,0.0,0.021678674966096878
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,답변 실패,-0.006053062,0.0,0.006053062155842781
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,딥러닝,-0.0028469672,0.0,0.0028469671960920095
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,마지막 할 말,0.98723996,1.0,0.012760043144226074
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,머신러닝,0.015494833,0.0,0.015494832769036293
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,면접 시작 인사,-0.010108187,0.0,0.01010818686336279
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,상세 경험,-0.0030983812,0.0,0.0030983812175691128
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,수식,0.008658847,0.0,0.008658846840262413
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,용어 질문,-0.017073952,0.0,0.01707395166158676
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,인공지능,0.017590139,0.0,0.01759013906121254
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,잠시 휴식,0.007011401,0.0,0.007011401001363993
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,좋아하는 아이돌,0.015622091,0.0,0.015622090548276901
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,핵심 아이디어,0.019433478,0.0,0.019433477893471718
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,확률 예측에서 MSE Loss 미 사용 이유,0.010436669,0.0,0.010436668992042542
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워,BCE 가 좋은 task,0.0024951228,0.0,0.002495122840628028
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워,BCE 가 좋은 이유,-0.0003775224,0.0,0.00037752240314148366
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워,LLM Fine-Tuning 의 PEFT,6.570634e-06,0.0,6.57063401376945e-06
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워,LoRA,0.0020145583,0.0,0.002014558296650648
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워,LoRA 와 QLoRA 의 차이,-0.0018977239,0.0,0.0018977238796651363
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워,Loss Function 예시,-0.01319444,0.0,0.013194439932703972
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워,Loss Function 정의,-0.00010403531,0.0,0.00010403530905023217
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워,MBTI,-0.00075882365,0.0,0.0007588236476294696
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워,MSE Loss 설명,-0.0031919482,0.0,0.0031919481698423624
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워,MSE Loss 용도,0.00018911628,0.0,0.0001891162828542292
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워,Multi-Label 에서 CE + Softmax 적용 문제점,-0.0015265258,0.0,0.0015265258261933923
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워,PEFT 방법 5가지,0.009932693,0.0,0.009932693094015121
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워,거대 언어 모델 정의,0.0010659761,0.0,0.00106597610283643
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워,기본 경험,-0.013967748,0.0,0.013967747800052166
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워,답변 실패,-0.010069699,0.0,0.01006969902664423
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워,딥러닝,-0.0025811398,0.0,0.0025811397936195135
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워,마지막 할 말,0.9880944,1.0,0.01190561056137085
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워,머신러닝,0.010186501,0.0,0.01018650084733963
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워,면접 시작 인사,0.00024206954,0.0,0.00024206953821703792
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워,상세 경험,0.0012181472,0.0,0.0012181472266092896
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워,수식,0.0076682596,0.0,0.0076682595536112785
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워,용어 질문,-0.008333972,0.0,0.008333971723914146
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워,인공지능,0.016903257,0.0,0.016903256997466087
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워,잠시 휴식,0.0075675095,0.0,0.007567509543150663
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워,좋아하는 아이돌,0.009827432,0.0,0.009827432222664356
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워,핵심 아이디어,0.012785416,0.0,0.012785416096448898
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워,확률 예측에서 MSE Loss 미 사용 이유,0.009959797,0.0,0.009959797374904156
