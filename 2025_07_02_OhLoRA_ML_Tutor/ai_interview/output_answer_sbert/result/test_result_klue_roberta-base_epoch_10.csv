input_part,output_answer,predicted_score,ground_truth_score,absolute_error
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,BCE 가 좋은 task,-0.025161948,0.0,0.025161948055028915
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,BCE 가 좋은 이유,-0.01465045,0.0,0.014650450088083744
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,LLM Fine-Tuning 의 PEFT,4.903007e-05,0.0,4.903006993117742e-05
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,LoRA,-0.005944265,0.0,0.005944265052676201
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,LoRA 와 QLoRA 의 차이,-0.0024187986,0.0,0.002418798627331853
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,Loss Function 예시,0.002004275,0.0,0.0020042750984430313
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,Loss Function 정의,-0.0013850566,0.0,0.001385056646540761
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,MBTI,0.0038399606,0.0,0.003839960554614663
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,MSE Loss 설명,-0.006613085,0.0,0.006613085046410561
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,MSE Loss 용도,-0.0075095487,0.0,0.007509548682719469
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,Multi-Label 에서 CE + Softmax 적용 문제점,0.0032289254,0.0,0.0032289254013448954
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,PEFT 방법 5가지,-0.00084233604,0.0,0.0008423360413871706
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,거대 언어 모델 정의,-0.0013775702,0.0,0.0013775702100247145
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,기본 경험,-0.027002508,0.0,0.02700250782072544
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,답변 실패,-0.004041026,0.0,0.004041025880724192
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,딥러닝,0.0011447411,0.0,0.0011447410797700286
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,마지막 할 말,-0.017058983,0.0,0.01705898344516754
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,머신러닝,-0.005351617,0.0,0.0053516170009970665
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,면접 시작 인사,0.9884832,1.0,0.011516809463500977
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,상세 경험,-0.004581727,0.0,0.004581727087497711
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,수식,0.001738959,0.0,0.0017389589920639992
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,용어 질문,0.015830332,0.0,0.015830332413315773
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,인공지능,-0.009362633,0.0,0.009362633340060711
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,잠시 휴식,0.0018909122,0.0,0.0018909121863543987
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,좋아하는 아이돌,-0.005230366,0.0,0.005230365786701441
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,핵심 아이디어,-0.010202288,0.0,0.010202287696301937
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,확률 예측에서 MSE Loss 미 사용 이유,-0.0039546564,0.0,0.003954656422138214
면접 시작 인사 -> 로라야 안녕 정말 반가워,BCE 가 좋은 task,-0.01748326,0.0,0.017483260482549667
면접 시작 인사 -> 로라야 안녕 정말 반가워,BCE 가 좋은 이유,-0.01666995,0.0,0.016669949516654015
면접 시작 인사 -> 로라야 안녕 정말 반가워,LLM Fine-Tuning 의 PEFT,-0.00028724855,0.0,0.00028724854928441346
면접 시작 인사 -> 로라야 안녕 정말 반가워,LoRA,0.0020171558,0.0,0.0020171557553112507
면접 시작 인사 -> 로라야 안녕 정말 반가워,LoRA 와 QLoRA 의 차이,0.0013006282,0.0,0.001300628180615604
면접 시작 인사 -> 로라야 안녕 정말 반가워,Loss Function 예시,0.0016955286,0.0,0.0016955286264419556
면접 시작 인사 -> 로라야 안녕 정말 반가워,Loss Function 정의,-0.0011506121,0.0,0.0011506121372804046
면접 시작 인사 -> 로라야 안녕 정말 반가워,MBTI,0.011162107,0.0,0.0111621068790555
면접 시작 인사 -> 로라야 안녕 정말 반가워,MSE Loss 설명,0.00073151983,0.0,0.000731519830878824
면접 시작 인사 -> 로라야 안녕 정말 반가워,MSE Loss 용도,-0.0067262677,0.0,0.006726267747581005
면접 시작 인사 -> 로라야 안녕 정말 반가워,Multi-Label 에서 CE + Softmax 적용 문제점,-0.009735516,0.0,0.009735516272485256
면접 시작 인사 -> 로라야 안녕 정말 반가워,PEFT 방법 5가지,-0.019227078,0.0,0.019227078184485435
면접 시작 인사 -> 로라야 안녕 정말 반가워,거대 언어 모델 정의,0.009998961,0.0,0.009998961351811886
면접 시작 인사 -> 로라야 안녕 정말 반가워,기본 경험,-0.025681484,0.0,0.02568148449063301
면접 시작 인사 -> 로라야 안녕 정말 반가워,답변 실패,-0.0060591274,0.0,0.006059127394109964
면접 시작 인사 -> 로라야 안녕 정말 반가워,딥러닝,-0.0027287924,0.0,0.0027287923730909824
면접 시작 인사 -> 로라야 안녕 정말 반가워,마지막 할 말,-0.009298042,0.0,0.009298042394220829
면접 시작 인사 -> 로라야 안녕 정말 반가워,머신러닝,0.0011031732,0.0,0.0011031732428818941
면접 시작 인사 -> 로라야 안녕 정말 반가워,면접 시작 인사,0.9898045,1.0,0.010195493698120117
면접 시작 인사 -> 로라야 안녕 정말 반가워,상세 경험,-0.010856196,0.0,0.010856196284294128
면접 시작 인사 -> 로라야 안녕 정말 반가워,수식,-0.009045984,0.0,0.009045983664691448
면접 시작 인사 -> 로라야 안녕 정말 반가워,용어 질문,0.0048557585,0.0,0.004855758510529995
면접 시작 인사 -> 로라야 안녕 정말 반가워,인공지능,-0.008055486,0.0,0.008055485785007477
면접 시작 인사 -> 로라야 안녕 정말 반가워,잠시 휴식,-0.0052483887,0.0,0.0052483887411653996
면접 시작 인사 -> 로라야 안녕 정말 반가워,좋아하는 아이돌,-0.0020292858,0.0,0.00202928576618433
면접 시작 인사 -> 로라야 안녕 정말 반가워,핵심 아이디어,-0.015351203,0.0,0.01535120327025652
면접 시작 인사 -> 로라야 안녕 정말 반가워,확률 예측에서 MSE Loss 미 사용 이유,-0.0015417215,0.0,0.001541721518151462
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,BCE 가 좋은 task,-0.019618278,0.0,0.019618278369307518
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,BCE 가 좋은 이유,-0.012096351,0.0,0.012096351012587547
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,LLM Fine-Tuning 의 PEFT,-0.0027324492,0.0,0.00273244921118021
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,LoRA,-0.0059867883,0.0,0.005986788310110569
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,LoRA 와 QLoRA 의 차이,-0.0019382744,0.0,0.0019382743630558252
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,Loss Function 예시,0.009215248,0.0,0.009215247817337513
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,Loss Function 정의,-0.005475552,0.0,0.005475551821291447
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,MBTI,-0.003564136,0.0,0.003564136102795601
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,MSE Loss 설명,-0.0008603734,0.0,0.0008603733731433749
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,MSE Loss 용도,-0.006721974,0.0,0.00672197388485074
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,Multi-Label 에서 CE + Softmax 적용 문제점,0.0014742562,0.0,0.0014742561616003513
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,PEFT 방법 5가지,-0.01749876,0.0,0.017498759552836418
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,거대 언어 모델 정의,-0.00068693736,0.0,0.0006869373610243201
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,기본 경험,-0.021162096,0.0,0.02116209641098976
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,답변 실패,0.0015119185,0.0,0.0015119184972718358
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,딥러닝,-4.1908694e-05,0.0,4.190869367448613e-05
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,마지막 할 말,-0.017193923,0.0,0.017193922773003578
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,머신러닝,-0.00507611,0.0,0.0050761098973453045
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,면접 시작 인사,0.9854866,1.0,0.014513373374938965
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,상세 경험,-0.005354606,0.0,0.005354606080800295
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,수식,-0.0064317696,0.0,0.0064317695796489716
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,용어 질문,0.002274555,0.0,0.0022745549213141203
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,인공지능,-0.01421563,0.0,0.014215629547834396
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,잠시 휴식,-0.014022882,0.0,0.014022882096469402
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,좋아하는 아이돌,-0.005051353,0.0,0.0050513530150055885
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,핵심 아이디어,-0.0075992392,0.0,0.007599239237606525
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,확률 예측에서 MSE Loss 미 사용 이유,0.006697188,0.0,0.006697188131511211
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,BCE 가 좋은 task,-0.013956591,0.0,0.013956590555608273
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,BCE 가 좋은 이유,-0.0049199825,0.0,0.004919982515275478
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,LLM Fine-Tuning 의 PEFT,0.001321599,0.0,0.0013215990038588643
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,LoRA,-0.0048225434,0.0,0.004822543356567621
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,LoRA 와 QLoRA 의 차이,0.0037831592,0.0,0.003783159190788865
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,Loss Function 예시,0.004967599,0.0,0.0049675991758704185
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,Loss Function 정의,-0.00083603564,0.0,0.0008360356441698968
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,MBTI,0.0023484544,0.0,0.002348454436287284
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,MSE Loss 설명,-0.0009983975,0.0,0.0009983974741771817
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,MSE Loss 용도,-0.0075588534,0.0,0.0075588533654809
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,Multi-Label 에서 CE + Softmax 적용 문제점,-0.00062653655,0.0,0.0006265365518629551
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,PEFT 방법 5가지,-0.011169716,0.0,0.011169715784490108
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,거대 언어 모델 정의,0.004753549,0.0,0.004753549117594957
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,기본 경험,-0.027393434,0.0,0.027393434196710587
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,답변 실패,-0.010557152,0.0,0.010557152330875397
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,딥러닝,-0.0028798836,0.0,0.0028798836283385754
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,마지막 할 말,-0.0053894892,0.0,0.005389489233493805
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,머신러닝,-0.0031642725,0.0,0.0031642725225538015
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,면접 시작 인사,0.9843017,1.0,0.01569831371307373
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,상세 경험,-0.008932426,0.0,0.008932425640523434
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,수식,-0.01283224,0.0,0.01283224020153284
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,용어 질문,-0.0007000326,0.0,0.0007000325713306665
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,인공지능,-0.012389578,0.0,0.01238957792520523
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,잠시 휴식,-0.0052055153,0.0,0.005205515306442976
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,좋아하는 아이돌,-0.008370834,0.0,0.008370834402740002
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,핵심 아이디어,-0.011181644,0.0,0.011181644164025784
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,확률 예측에서 MSE Loss 미 사용 이유,0.005195832,0.0,0.0051958318799734116
면접 시작 인사 -> 파이팅! 시작하자,BCE 가 좋은 task,-0.015266423,0.0,0.015266423113644123
면접 시작 인사 -> 파이팅! 시작하자,BCE 가 좋은 이유,-0.011485857,0.0,0.011485856957733631
면접 시작 인사 -> 파이팅! 시작하자,LLM Fine-Tuning 의 PEFT,0.0022054669,0.0,0.002205466851592064
면접 시작 인사 -> 파이팅! 시작하자,LoRA,-0.0062980223,0.0,0.006298022344708443
면접 시작 인사 -> 파이팅! 시작하자,LoRA 와 QLoRA 의 차이,-0.008492348,0.0,0.008492347784340382
면접 시작 인사 -> 파이팅! 시작하자,Loss Function 예시,0.0071931933,0.0,0.007193193305283785
면접 시작 인사 -> 파이팅! 시작하자,Loss Function 정의,-0.0018079581,0.0,0.0018079581204801798
면접 시작 인사 -> 파이팅! 시작하자,MBTI,0.010729634,0.0,0.010729634203016758
면접 시작 인사 -> 파이팅! 시작하자,MSE Loss 설명,-0.00056002534,0.0,0.0005600253352895379
면접 시작 인사 -> 파이팅! 시작하자,MSE Loss 용도,-0.0044051316,0.0,0.004405131563544273
면접 시작 인사 -> 파이팅! 시작하자,Multi-Label 에서 CE + Softmax 적용 문제점,-0.0040339106,0.0,0.004033910576254129
면접 시작 인사 -> 파이팅! 시작하자,PEFT 방법 5가지,-0.0074632573,0.0,0.007463257294148207
면접 시작 인사 -> 파이팅! 시작하자,거대 언어 모델 정의,0.0037284477,0.0,0.003728447714820504
면접 시작 인사 -> 파이팅! 시작하자,기본 경험,-0.035087537,0.0,0.03508753702044487
면접 시작 인사 -> 파이팅! 시작하자,답변 실패,-0.007507639,0.0,0.00750763900578022
면접 시작 인사 -> 파이팅! 시작하자,딥러닝,-0.0027486759,0.0,0.0027486758772283792
면접 시작 인사 -> 파이팅! 시작하자,마지막 할 말,-0.016124401,0.0,0.01612440124154091
면접 시작 인사 -> 파이팅! 시작하자,머신러닝,0.002593159,0.0,0.0025931589771062136
면접 시작 인사 -> 파이팅! 시작하자,면접 시작 인사,0.9915688,1.0,0.008431196212768555
면접 시작 인사 -> 파이팅! 시작하자,상세 경험,-0.014358148,0.0,0.014358147978782654
면접 시작 인사 -> 파이팅! 시작하자,수식,-0.0104724625,0.0,0.010472462512552738
면접 시작 인사 -> 파이팅! 시작하자,용어 질문,0.0075787725,0.0,0.007578772492706776
면접 시작 인사 -> 파이팅! 시작하자,인공지능,-0.01087382,0.0,0.010873819701373577
면접 시작 인사 -> 파이팅! 시작하자,잠시 휴식,0.0025959834,0.0,0.0025959834456443787
면접 시작 인사 -> 파이팅! 시작하자,좋아하는 아이돌,-0.005476644,0.0,0.005476643797010183
면접 시작 인사 -> 파이팅! 시작하자,핵심 아이디어,0.00017397974,0.0,0.00017397974443156272
면접 시작 인사 -> 파이팅! 시작하자,확률 예측에서 MSE Loss 미 사용 이유,-0.005028745,0.0,0.005028745159506798
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",BCE 가 좋은 task,-0.009448899,0.0,0.009448898956179619
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",BCE 가 좋은 이유,-0.0009437657,0.0,0.0009437656844966114
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",LLM Fine-Tuning 의 PEFT,-0.008962041,0.0,0.008962040767073631
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",LoRA,0.0024475357,0.0,0.0024475357495248318
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",LoRA 와 QLoRA 의 차이,0.0016683191,0.0,0.0016683191061019897
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",Loss Function 예시,-0.0007466261,0.0,0.0007466261158697307
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",Loss Function 정의,-0.0067077484,0.0,0.006707748398184776
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",MBTI,-0.0076066754,0.0,0.007606675382703543
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",MSE Loss 설명,0.0032915394,0.0,0.0032915393821895123
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",MSE Loss 용도,-0.0055297897,0.0,0.005529789719730616
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",Multi-Label 에서 CE + Softmax 적용 문제점,-0.014238374,0.0,0.014238374307751656
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",PEFT 방법 5가지,-0.0003456297,0.0,0.00034562969813123345
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",거대 언어 모델 정의,-0.020856246,0.0,0.02085624635219574
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",기본 경험,0.028614372,0.0,0.02861437201499939
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",답변 실패,0.984253,1.0,0.015747010707855225
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",딥러닝,-0.004704243,0.0,0.004704243037849665
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",마지막 할 말,-0.0024483562,0.0,0.002448356244713068
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",머신러닝,-0.0042652953,0.0,0.0042652953416109085
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",면접 시작 인사,-0.0037224016,0.0,0.0037224015686661005
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",상세 경험,-0.0062285527,0.0,0.006228552665561438
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",수식,-0.0044641066,0.0,0.004464106634259224
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",용어 질문,-0.020757182,0.0,0.02075718156993389
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",인공지능,-0.0064841,0.0,0.006484100129455328
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",잠시 휴식,-0.011377289,0.0,0.011377288959920406
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",좋아하는 아이돌,0.0042328443,0.0,0.004232844337821007
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",핵심 아이디어,-0.008985287,0.0,0.008985286578536034
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",확률 예측에서 MSE Loss 미 사용 이유,-0.004473977,0.0,0.004473976790904999
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",BCE 가 좋은 task,-0.0031495797,0.0,0.003149579744786024
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",BCE 가 좋은 이유,-0.00056494045,0.0,0.000564940448384732
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",LLM Fine-Tuning 의 PEFT,0.0036431288,0.0,0.003643128788098693
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",LoRA,-0.002837992,0.0,0.00283799204044044
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",LoRA 와 QLoRA 의 차이,-0.0059491275,0.0,0.005949127487838268
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",Loss Function 예시,-0.0070774993,0.0,0.00707749929279089
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",Loss Function 정의,-0.022181759,0.0,0.022181758657097816
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",MBTI,0.00403035,0.0,0.0040303501300513744
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",MSE Loss 설명,0.009513545,0.0,0.009513544850051403
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",MSE Loss 용도,0.006349128,0.0,0.00634912820532918
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",Multi-Label 에서 CE + Softmax 적용 문제점,-0.0047166296,0.0,0.0047166296280920506
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",PEFT 방법 5가지,-0.0015300274,0.0,0.0015300273662433028
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",거대 언어 모델 정의,-0.009527746,0.0,0.00952774565666914
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",기본 경험,-0.00041706915,0.0,0.00041706915362738073
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",답변 실패,-0.0027600424,0.0,0.0027600424364209175
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",딥러닝,-0.016214695,0.0,0.01621469482779503
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",마지막 할 말,0.0066607757,0.0,0.006660775747150183
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",머신러닝,-0.027424071,0.0,0.027424070984125137
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",면접 시작 인사,-0.00074437726,0.0,0.0007443772628903389
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",상세 경험,-0.0011066061,0.0,0.0011066060978919268
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",수식,-0.007864487,0.0,0.00786448735743761
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",용어 질문,0.0025836013,0.0,0.0025836012791842222
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",인공지능,0.9801926,1.0,0.019807398319244385
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",잠시 휴식,0.003865685,0.0,0.0038656850811094046
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",좋아하는 아이돌,0.0081798015,0.0,0.008179801516234875
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",핵심 아이디어,0.00342895,0.0,0.003428949974477291
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",확률 예측에서 MSE Loss 미 사용 이유,0.0063645253,0.0,0.00636452529579401
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",BCE 가 좋은 task,0.0036211389,0.0,0.0036211388651281595
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",BCE 가 좋은 이유,-0.006713447,0.0,0.006713447161018848
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",LLM Fine-Tuning 의 PEFT,-0.0129513,0.0,0.012951299548149109
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",LoRA,-0.013557849,0.0,0.013557849451899529
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",LoRA 와 QLoRA 의 차이,0.00066294207,0.0,0.000662942067719996
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",Loss Function 예시,0.0032553347,0.0,0.003255334682762623
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",Loss Function 정의,-0.025278904,0.0,0.025278903543949127
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",MBTI,-0.024075903,0.0,0.024075902998447418
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",MSE Loss 설명,-0.0024955322,0.0,0.0024955321568995714
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",MSE Loss 용도,0.015947543,0.0,0.01594754308462143
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",Multi-Label 에서 CE + Softmax 적용 문제점,0.01005581,0.0,0.01005581021308899
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",PEFT 방법 5가지,0.00864043,0.0,0.008640429936349392
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",거대 언어 모델 정의,0.005174702,0.0,0.005174702033400536
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",기본 경험,0.004341942,0.0,0.0043419417925179005
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",답변 실패,-0.009041346,0.0,0.009041345678269863
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",딥러닝,-0.00966281,0.0,0.009662809781730175
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",마지막 할 말,-0.008167722,0.0,0.008167722262442112
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",머신러닝,0.9843534,1.0,0.01564657688140869
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",면접 시작 인사,-0.0020273537,0.0,0.00202735373750329
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",상세 경험,0.004600505,0.0,0.004600504878908396
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",수식,0.011877574,0.0,0.011877574026584625
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",용어 질문,0.0006200243,0.0,0.0006200242787599564
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",인공지능,-0.035667617,0.0,0.03566761687397957
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",잠시 휴식,0.0048158863,0.0,0.00481588626280427
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",좋아하는 아이돌,-0.0026822223,0.0,0.0026822222862392664
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",핵심 아이디어,-0.0006689208,0.0,0.0006689208094030619
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",확률 예측에서 MSE Loss 미 사용 이유,-0.008015696,0.0,0.008015695959329605
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",BCE 가 좋은 task,-0.00026004753,0.0,0.000260047527262941
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",BCE 가 좋은 이유,0.0012524079,0.0,0.001252407906576991
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",LLM Fine-Tuning 의 PEFT,0.0059510944,0.0,0.005951094441115856
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",LoRA,-0.028473267,0.0,0.0284732673317194
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",LoRA 와 QLoRA 의 차이,0.006079978,0.0,0.006079977843910456
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",Loss Function 예시,-0.007757226,0.0,0.007757226005196571
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",Loss Function 정의,-0.032005735,0.0,0.032005734741687775
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",MBTI,0.0050061517,0.0,0.005006151739507914
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",MSE Loss 설명,0.0039550858,0.0,0.003955085761845112
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",MSE Loss 용도,-0.009116172,0.0,0.00911617185920477
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",Multi-Label 에서 CE + Softmax 적용 문제점,0.0032096587,0.0,0.0032096586655825377
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",PEFT 방법 5가지,-0.017712833,0.0,0.017712833359837532
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",거대 언어 모델 정의,-0.005106942,0.0,0.005106941796839237
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",기본 경험,-0.008087057,0.0,0.008087056688964367
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",답변 실패,-0.0009918136,0.0,0.0009918136056512594
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",딥러닝,0.9809232,1.0,0.019076824188232422
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",마지막 할 말,0.0156741,0.0,0.015674099326133728
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",머신러닝,-0.033156686,0.0,0.033156685531139374
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",면접 시작 인사,-0.010833169,0.0,0.01083316933363676
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",상세 경험,-0.0032557936,0.0,0.0032557935919612646
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",수식,-0.0016750154,0.0,0.0016750154318287969
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",용어 질문,-0.014565456,0.0,0.014565455727279186
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",인공지능,-0.035033163,0.0,0.03503316268324852
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",잠시 휴식,-0.0033704524,0.0,0.0033704524394124746
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",좋아하는 아이돌,-0.014583705,0.0,0.014583704993128777
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",핵심 아이디어,0.0028972912,0.0,0.002897291211411357
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",확률 예측에서 MSE Loss 미 사용 이유,-0.0034875511,0.0,0.0034875511191785336
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",BCE 가 좋은 task,-0.0056766025,0.0,0.005676602479070425
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",BCE 가 좋은 이유,-0.00787769,0.0,0.00787768978625536
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",LLM Fine-Tuning 의 PEFT,0.003038465,0.0,0.0030384650453925133
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",LoRA,-0.022998806,0.0,0.022998806089162827
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",LoRA 와 QLoRA 의 차이,0.008081921,0.0,0.008081921376287937
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",Loss Function 예시,-0.008084923,0.0,0.008084923028945923
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",Loss Function 정의,-0.024210773,0.0,0.024210773408412933
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",MBTI,0.002281669,0.0,0.0022816690616309643
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",MSE Loss 설명,-0.0036641916,0.0,0.0036641915794461966
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",MSE Loss 용도,-0.0130015565,0.0,0.013001556508243084
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",Multi-Label 에서 CE + Softmax 적용 문제점,0.00068487076,0.0,0.0006848707562312484
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",PEFT 방법 5가지,-0.015247612,0.0,0.01524761226028204
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",거대 언어 모델 정의,-0.0068673235,0.0,0.006867323536425829
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",기본 경험,-0.010938976,0.0,0.01093897596001625
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",답변 실패,0.0035046209,0.0,0.0035046208649873734
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",딥러닝,0.9825958,1.0,0.01740419864654541
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",마지막 할 말,0.013700217,0.0,0.013700217008590698
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",머신러닝,-0.017742146,0.0,0.01774214580655098
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",면접 시작 인사,-0.007670967,0.0,0.007670966908335686
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",상세 경험,-0.005214784,0.0,0.005214783828705549
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",수식,-0.0035005964,0.0,0.003500596387311816
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",용어 질문,-0.013885228,0.0,0.013885227963328362
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",인공지능,-0.02412306,0.0,0.0241230595856905
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",잠시 휴식,-0.0056842305,0.0,0.005684230476617813
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",좋아하는 아이돌,-0.009998873,0.0,0.009998872876167297
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",핵심 아이디어,0.0026963395,0.0,0.0026963395066559315
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",확률 예측에서 MSE Loss 미 사용 이유,0.00010332896,0.0,0.00010332895908504725
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",BCE 가 좋은 task,-0.02588715,0.0,0.025887150317430496
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",BCE 가 좋은 이유,0.019143624,0.0,0.01914362423121929
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",LLM Fine-Tuning 의 PEFT,-0.030235024,0.0,0.03023502416908741
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",LoRA,-0.0030155955,0.0,0.0030155954882502556
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",LoRA 와 QLoRA 의 차이,0.009351118,0.0,0.00935111753642559
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",Loss Function 예시,-0.00094001106,0.0,0.0009400110575370491
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",Loss Function 정의,-0.017971486,0.0,0.01797148585319519
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",MBTI,-0.011203561,0.0,0.01120356097817421
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",MSE Loss 설명,0.0037739119,0.0,0.0037739118561148643
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",MSE Loss 용도,0.017024497,0.0,0.01702449657022953
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",Multi-Label 에서 CE + Softmax 적용 문제점,0.011448878,0.0,0.011448877863585949
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",PEFT 방법 5가지,-0.012611262,0.0,0.012611261568963528
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",거대 언어 모델 정의,-0.031413198,0.0,0.03141319751739502
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",기본 경험,0.0031606536,0.0,0.0031606536358594894
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",답변 실패,0.25013012,1.0,0.7498698830604553
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",딥러닝,-0.036297087,0.0,0.036297086626291275
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",마지막 할 말,-0.028962033,0.0,0.0289620328694582
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",머신러닝,0.8654272,0.0,0.8654271960258484
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",면접 시작 인사,-0.040377013,0.0,0.04037701338529587
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",상세 경험,-0.02617569,0.0,0.026175690814852715
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",수식,0.016956082,0.0,0.016956081613898277
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",용어 질문,-0.0084416615,0.0,0.008441661484539509
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",인공지능,0.053430907,0.0,0.05343090742826462
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",잠시 휴식,0.018184887,0.0,0.018184887245297432
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",좋아하는 아이돌,-0.02364754,0.0,0.02364753931760788
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",핵심 아이디어,0.012951433,0.0,0.012951432727277279
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",확률 예측에서 MSE Loss 미 사용 이유,-0.016558653,0.0,0.016558652743697166
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아",BCE 가 좋은 task,-0.00204782,0.0,0.0020478200167417526
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아",BCE 가 좋은 이유,0.0043087564,0.0,0.004308756440877914
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아",LLM Fine-Tuning 의 PEFT,-0.0031863833,0.0,0.0031863832846283913
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아",LoRA,0.003265619,0.0,0.003265619045123458
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아",LoRA 와 QLoRA 의 차이,-0.0026597315,0.0,0.0026597315445542336
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아",Loss Function 예시,-0.0024557358,0.0,0.0024557358119636774
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아",Loss Function 정의,-0.0077672834,0.0,0.007767283357679844
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아",MBTI,-0.011731113,0.0,0.01173111330717802
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아",MSE Loss 설명,0.0012321024,0.0,0.0012321023968979716
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아",MSE Loss 용도,-0.006839608,0.0,0.00683960784226656
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아",Multi-Label 에서 CE + Softmax 적용 문제점,-0.0077991323,0.0,0.007799132261425257
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아",PEFT 방법 5가지,-0.0037055502,0.0,0.003705550218001008
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아",거대 언어 모델 정의,-0.017355265,0.0,0.017355265095829964
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아",기본 경험,0.0008575013,0.0,0.0008575012907385826
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아",답변 실패,0.99223787,1.0,0.007762134075164795
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아",딥러닝,-0.0027985713,0.0,0.00279857125133276
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아",마지막 할 말,-0.0059509072,0.0,0.0059509072452783585
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아",머신러닝,0.00028336814,0.0,0.00028336813556961715
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아",면접 시작 인사,-0.0029239513,0.0,0.0029239512514322996
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아",상세 경험,-0.007641801,0.0,0.00764180114492774
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아",수식,-0.00719992,0.0,0.007199919782578945
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아",용어 질문,-0.016733447,0.0,0.016733447089791298
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아",인공지능,-0.0075431243,0.0,0.007543124258518219
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아",잠시 휴식,0.0044380343,0.0,0.004438034258782864
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아",좋아하는 아이돌,-0.00046165535,0.0,0.00046165534877218306
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아",핵심 아이디어,-0.0038510084,0.0,0.0038510083686560392
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아",확률 예측에서 MSE Loss 미 사용 이유,-0.004011727,0.0,0.004011726938188076
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지?",BCE 가 좋은 task,-0.0043849484,0.0,0.004384948406368494
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지?",BCE 가 좋은 이유,0.00019746754,0.0,0.00019746753969229758
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지?",LLM Fine-Tuning 의 PEFT,0.0019518445,0.0,0.0019518445478752255
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지?",LoRA,-0.0019183261,0.0,0.0019183261319994926
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지?",LoRA 와 QLoRA 의 차이,-0.00793637,0.0,0.007936369627714157
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지?",Loss Function 예시,-0.011481063,0.0,0.011481063440442085
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지?",Loss Function 정의,-0.02131778,0.0,0.021317780017852783
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지?",MBTI,-0.00057437416,0.0,0.0005743741639889777
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지?",MSE Loss 설명,0.011462753,0.0,0.01146275270730257
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지?",MSE Loss 용도,0.007166614,0.0,0.007166613824665546
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지?",Multi-Label 에서 CE + Softmax 적용 문제점,-0.0055829044,0.0,0.005582904443144798
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지?",PEFT 방법 5가지,-0.00585848,0.0,0.0058584799990057945
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지?",거대 언어 모델 정의,-0.0063121826,0.0,0.006312182638794184
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지?",기본 경험,-0.003258006,0.0,0.0032580059487372637
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지?",답변 실패,0.00082409434,0.0,0.000824094342533499
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지?",딥러닝,-0.0094145425,0.0,0.009414542466402054
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지?",마지막 할 말,0.008365739,0.0,0.00836573913693428
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지?",머신러닝,-0.026569115,0.0,0.02656911499798298
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지?",면접 시작 인사,-0.0055862167,0.0,0.005586216691881418
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지?",상세 경험,-0.001561468,0.0,0.0015614680014550686
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지?",수식,-0.0036197184,0.0,0.0036197183653712273
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지?",용어 질문,0.0049698437,0.0,0.004969843663275242
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지?",인공지능,0.9776546,1.0,0.022345423698425293
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지?",잠시 휴식,0.0024186026,0.0,0.0024186025839298964
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지?",좋아하는 아이돌,0.0057446575,0.0,0.005744657479226589
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지?",핵심 아이디어,0.002823647,0.0,0.0028236471116542816
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지?",확률 예측에서 MSE Loss 미 사용 이유,0.0050771474,0.0,0.005077147390693426
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야!",BCE 가 좋은 task,0.006173445,0.0,0.006173444911837578
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야!",BCE 가 좋은 이유,-0.006210752,0.0,0.006210751831531525
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야!",LLM Fine-Tuning 의 PEFT,-0.008227003,0.0,0.008227002806961536
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야!",LoRA,-0.0050978456,0.0,0.005097845569252968
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야!",LoRA 와 QLoRA 의 차이,0.0062415944,0.0,0.0062415944412350655
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야!",Loss Function 예시,0.0005280571,0.0,0.0005280571058392525
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야!",Loss Function 정의,-0.027365912,0.0,0.02736591175198555
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야!",MBTI,-0.021915894,0.0,0.021915894001722336
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야!",MSE Loss 설명,-0.009771568,0.0,0.009771567769348621
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야!",MSE Loss 용도,0.018516337,0.0,0.018516337499022484
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야!",Multi-Label 에서 CE + Softmax 적용 문제점,0.015227241,0.0,0.015227241441607475
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야!",PEFT 방법 5가지,0.011459429,0.0,0.011459428817033768
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야!",거대 언어 모델 정의,0.0050352607,0.0,0.005035260692238808
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야!",기본 경험,-0.001503173,0.0,0.001503173029050231
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야!",답변 실패,-0.009074474,0.0,0.00907447375357151
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야!",딥러닝,-0.014339245,0.0,0.014339244924485683
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야!",마지막 할 말,-0.011654785,0.0,0.01165478490293026
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야!",머신러닝,0.98408407,1.0,0.01591593027114868
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야!",면접 시작 인사,-0.0018557739,0.0,0.001855773851275444
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야!",상세 경험,0.01483987,0.0,0.014839869923889637
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야!",수식,0.011636697,0.0,0.011636696755886078
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야!",용어 질문,0.0041642804,0.0,0.004164280369877815
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야!",인공지능,-0.041526288,0.0,0.04152628779411316
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야!",잠시 휴식,0.004055366,0.0,0.0040553659200668335
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야!",좋아하는 아이돌,-0.0040652263,0.0,0.004065226297825575
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야!",핵심 아이디어,-0.0009412206,0.0,0.0009412206127308309
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야!",확률 예측에서 MSE Loss 미 사용 이유,-0.0068863668,0.0,0.006886366754770279
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지",BCE 가 좋은 task,-0.003628716,0.0,0.003628716105595231
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지",BCE 가 좋은 이유,-0.007987422,0.0,0.007987421937286854
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지",LLM Fine-Tuning 의 PEFT,0.0025671818,0.0,0.0025671818293631077
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지",LoRA,-0.025631836,0.0,0.02563183568418026
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지",LoRA 와 QLoRA 의 차이,0.010047402,0.0,0.01004740223288536
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지",Loss Function 예시,-0.004714532,0.0,0.004714531823992729
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지",Loss Function 정의,-0.026881773,0.0,0.02688177302479744
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지",MBTI,0.0064990907,0.0,0.006499090697616339
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지",MSE Loss 설명,-0.0023865602,0.0,0.002386560197919607
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지",MSE Loss 용도,-0.009296085,0.0,0.009296084754168987
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지",Multi-Label 에서 CE + Softmax 적용 문제점,0.0011562428,0.0,0.001156242797151208
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지",PEFT 방법 5가지,-0.019031916,0.0,0.019031915813684464
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지",거대 언어 모델 정의,-0.0036765686,0.0,0.0036765686236321926
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지",기본 경험,-0.008960851,0.0,0.008960850536823273
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지",답변 실패,-0.001386571,0.0,0.0013865709770470858
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지",딥러닝,0.9813078,1.0,0.018692195415496826
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지",마지막 할 말,0.018765524,0.0,0.01876552402973175
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지",머신러닝,-0.03370409,0.0,0.03370409086346626
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지",면접 시작 인사,-0.0071977857,0.0,0.007197785656899214
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지",상세 경험,-0.00080947287,0.0,0.0008094728691503406
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지",수식,-0.0016746,0.0,0.0016745999455451965
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지",용어 질문,-0.015262617,0.0,0.01526261679828167
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지",인공지능,-0.029542701,0.0,0.029542701318860054
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지",잠시 휴식,-0.0018913562,0.0,0.0018913561943918467
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지",좋아하는 아이돌,-0.0140343215,0.0,0.014034321531653404
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지",핵심 아이디어,0.0038628925,0.0,0.00386289251036942
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지",확률 예측에서 MSE Loss 미 사용 이유,-0.005059025,0.0,0.0050590247847139835
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야.",BCE 가 좋은 task,-0.003485985,0.0,0.0034859851002693176
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야.",BCE 가 좋은 이유,-0.011864764,0.0,0.01186476368457079
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야.",LLM Fine-Tuning 의 PEFT,0.005786706,0.0,0.005786706227809191
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야.",LoRA,-0.02994121,0.0,0.02994121052324772
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야.",LoRA 와 QLoRA 의 차이,0.010384457,0.0,0.010384457185864449
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야.",Loss Function 예시,-0.0062740585,0.0,0.006274058483541012
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야.",Loss Function 정의,-0.026666934,0.0,0.026666933670639992
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야.",MBTI,0.007236013,0.0,0.007236013188958168
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야.",MSE Loss 설명,0.0002204685,0.0,0.00022046850062906742
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야.",MSE Loss 용도,-0.011594683,0.0,0.011594682931900024
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야.",Multi-Label 에서 CE + Softmax 적용 문제점,0.0039624996,0.0,0.003962499555200338
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야.",PEFT 방법 5가지,-0.018353086,0.0,0.018353085964918137
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야.",거대 언어 모델 정의,-0.005716948,0.0,0.005716947838664055
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야.",기본 경험,-0.006204804,0.0,0.006204803939908743
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야.",답변 실패,-0.0022386906,0.0,0.002238690620288253
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야.",딥러닝,0.9812039,1.0,0.018796086311340332
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야.",마지막 할 말,0.0145709785,0.0,0.014570978470146656
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야.",머신러닝,-0.02933583,0.0,0.029335830360651016
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야.",면접 시작 인사,-0.010800889,0.0,0.010800888761878014
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야.",상세 경험,-0.004412496,0.0,0.004412495996803045
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야.",수식,-0.0027980178,0.0,0.0027980178128927946
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야.",용어 질문,-0.009411198,0.0,0.00941119808703661
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야.",인공지능,-0.02764251,0.0,0.02764251083135605
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야.",잠시 휴식,-0.0028586613,0.0,0.002858661348000169
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야.",좋아하는 아이돌,-0.0111751035,0.0,0.011175103485584259
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야.",핵심 아이디어,-0.0052482407,0.0,0.005248240660876036
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야.",확률 예측에서 MSE Loss 미 사용 이유,-0.004780677,0.0,0.004780677147209644
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로",BCE 가 좋은 task,-0.0073942565,0.0,0.0073942565359175205
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로",BCE 가 좋은 이유,0.004208844,0.0,0.004208844155073166
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로",LLM Fine-Tuning 의 PEFT,-0.0047376896,0.0,0.00473768962547183
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로",LoRA,-0.0025098172,0.0,0.002509817248210311
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로",LoRA 와 QLoRA 의 차이,-0.003704884,0.0,0.0037048840895295143
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로",Loss Function 예시,-0.007426845,0.0,0.007426844909787178
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로",Loss Function 정의,-0.009781835,0.0,0.009781834669411182
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로",MBTI,-0.0028853815,0.0,0.002885381458327174
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로",MSE Loss 설명,-0.0019414026,0.0,0.0019414025591686368
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로",MSE Loss 용도,-0.012830574,0.0,0.012830574065446854
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로",Multi-Label 에서 CE + Softmax 적용 문제점,-0.005896858,0.0,0.005896857939660549
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로",PEFT 방법 5가지,-0.0018746215,0.0,0.0018746214918792248
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로",거대 언어 모델 정의,-0.011877179,0.0,0.011877179145812988
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로",기본 경험,-0.00059281976,0.0,0.0005928197642788291
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로",답변 실패,0.9935992,1.0,0.0064008235931396484
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로",딥러닝,-0.009102672,0.0,0.009102672338485718
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로",마지막 할 말,-0.0016452699,0.0,0.001645269920118153
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로",머신러닝,-0.00014991795,0.0,0.00014991794887464494
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로",면접 시작 인사,-0.0016526803,0.0,0.0016526803374290466
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로",상세 경험,-0.0028284162,0.0,0.002828416181728244
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로",수식,0.0022864891,0.0,0.0022864891216158867
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로",용어 질문,-0.0051111206,0.0,0.005111120641231537
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로",인공지능,-0.00656579,0.0,0.00656579015776515
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로",잠시 휴식,0.000365158,0.0,0.00036515799001790583
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로",좋아하는 아이돌,-0.0072132433,0.0,0.007213243283331394
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로",핵심 아이디어,-0.0075761066,0.0,0.007576106581836939
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로",확률 예측에서 MSE Loss 미 사용 이유,-0.002402517,0.0,0.0024025170132517815
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,BCE 가 좋은 task,0.0077426247,0.0,0.00774262472987175
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,BCE 가 좋은 이유,0.005480247,0.0,0.005480247084051371
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,LLM Fine-Tuning 의 PEFT,0.00088164484,0.0,0.0008816448389552534
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,LoRA,-0.01559279,0.0,0.015592790208756924
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,LoRA 와 QLoRA 의 차이,0.0046173097,0.0,0.0046173096634447575
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,Loss Function 예시,-0.004472546,0.0,0.004472545813769102
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,Loss Function 정의,-0.012918899,0.0,0.012918898835778236
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,MBTI,-0.00070594443,0.0,0.0007059444324113429
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,MSE Loss 설명,-0.023930253,0.0,0.023930253461003304
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,MSE Loss 용도,0.007548979,0.0,0.007548979017883539
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,Multi-Label 에서 CE + Softmax 적용 문제점,-0.005987304,0.0,0.005987303797155619
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,PEFT 방법 5가지,-0.0046815546,0.0,0.00468155462294817
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,거대 언어 모델 정의,0.99143684,1.0,0.00856316089630127
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,기본 경험,0.00477976,0.0,0.004779759794473648
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,답변 실패,-0.011184414,0.0,0.01118441391736269
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,딥러닝,-0.010512546,0.0,0.010512545704841614
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,마지막 할 말,-0.010803586,0.0,0.0108035858720541
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,머신러닝,0.012957408,0.0,0.012957408092916012
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,면접 시작 인사,-0.0017827931,0.0,0.0017827930860221386
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,상세 경험,-0.009893502,0.0,0.009893502108752728
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,수식,-0.015556948,0.0,0.015556948259472847
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,용어 질문,-0.024768837,0.0,0.024768836796283722
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,인공지능,-0.010917663,0.0,0.010917662642896175
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,잠시 휴식,0.0026921628,0.0,0.0026921627577394247
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,좋아하는 아이돌,0.0077810735,0.0,0.007781073451042175
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,핵심 아이디어,-0.001617895,0.0,0.0016178949736058712
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,확률 예측에서 MSE Loss 미 사용 이유,0.003952825,0.0,0.003952824976295233
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,BCE 가 좋은 task,0.007752651,0.0,0.007752650883048773
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,BCE 가 좋은 이유,-0.01606158,0.0,0.016061579808592796
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,LLM Fine-Tuning 의 PEFT,-0.0006758849,0.0,0.0006758848903700709
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,LoRA,0.00376008,0.0,0.003760080086067319
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,LoRA 와 QLoRA 의 차이,-0.0049657547,0.0,0.004965754691511393
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,Loss Function 예시,-0.0011683345,0.0,0.0011683345073834062
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,Loss Function 정의,-0.02364166,0.0,0.023641660809516907
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,MBTI,-0.0124199595,0.0,0.012419959530234337
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,MSE Loss 설명,0.0035191188,0.0,0.0035191187635064125
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,MSE Loss 용도,-0.0039506652,0.0,0.0039506652392446995
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,Multi-Label 에서 CE + Softmax 적용 문제점,0.0012505093,0.0,0.0012505092890933156
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,PEFT 방법 5가지,-0.00073628145,0.0,0.0007362814503721893
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,거대 언어 모델 정의,0.9789295,0.0,0.9789295196533203
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,기본 경험,0.01272542,0.0,0.01272542029619217
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,답변 실패,0.014223504,1.0,0.9857764961197972
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,딥러닝,0.009303635,0.0,0.009303634986281395
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,마지막 할 말,-0.01788195,0.0,0.017881950363516808
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,머신러닝,-0.00703727,0.0,0.0070372698828577995
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,면접 시작 인사,-0.012370854,0.0,0.012370853684842587
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,상세 경험,8.188142e-06,0.0,8.188141691789497e-06
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,수식,-0.021110492,0.0,0.021110491827130318
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,용어 질문,-0.015361473,0.0,0.015361472964286804
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,인공지능,-0.006117061,0.0,0.0061170607805252075
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,잠시 휴식,0.0032894928,0.0,0.0032894928008317947
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,좋아하는 아이돌,0.012752085,0.0,0.012752084992825985
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,핵심 아이디어,-0.00689035,0.0,0.006890350021421909
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,확률 예측에서 MSE Loss 미 사용 이유,-0.011011322,0.0,0.011011322028934956
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델,BCE 가 좋은 task,0.014531659,0.0,0.014531658962368965
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델,BCE 가 좋은 이유,-0.0016101932,0.0,0.0016101931687444448
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델,LLM Fine-Tuning 의 PEFT,-0.0029027618,0.0,0.0029027618002146482
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델,LoRA,-0.006518058,0.0,0.0065180580131709576
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델,LoRA 와 QLoRA 의 차이,-0.0048168493,0.0,0.004816849250346422
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델,Loss Function 예시,-0.012763748,0.0,0.012763747945427895
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델,Loss Function 정의,0.009272269,0.0,0.00927226897329092
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델,MBTI,-0.004524906,0.0,0.004524906165897846
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델,MSE Loss 설명,-0.015657445,0.0,0.015657445415854454
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델,MSE Loss 용도,0.014029232,0.0,0.01402923185378313
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델,Multi-Label 에서 CE + Softmax 적용 문제점,-5.1810897e-05,0.0,5.181089727557264e-05
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델,PEFT 방법 5가지,-0.010050617,0.0,0.010050617158412933
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델,거대 언어 모델 정의,0.98563594,1.0,0.014364063739776611
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델,기본 경험,0.0064960322,0.0,0.0064960322342813015
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델,답변 실패,-0.016789498,0.0,0.016789497807621956
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델,딥러닝,-0.0130810365,0.0,0.013081036508083344
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델,마지막 할 말,-0.014547246,0.0,0.01454724557697773
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델,머신러닝,0.0024211612,0.0,0.0024211611598730087
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델,면접 시작 인사,-0.008214358,0.0,0.008214358240365982
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델,상세 경험,-0.002996363,0.0,0.002996362978592515
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델,수식,-0.014421541,0.0,0.01442154124379158
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델,용어 질문,-0.031680156,0.0,0.0316801555454731
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델,인공지능,-0.008856431,0.0,0.008856430649757385
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델,잠시 휴식,0.002286821,0.0,0.0022868209052830935
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델,좋아하는 아이돌,0.0067916573,0.0,0.00679165730252862
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델,핵심 아이디어,0.0022396715,0.0,0.0022396715357899666
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델,확률 예측에서 MSE Loss 미 사용 이유,0.0082248645,0.0,0.00822486449033022
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지.,BCE 가 좋은 task,-0.01595535,0.0,0.015955349430441856
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지.,BCE 가 좋은 이유,-0.0014876862,0.0,0.001487686182372272
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지.,LLM Fine-Tuning 의 PEFT,-0.012250736,0.0,0.012250736355781555
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지.,LoRA,0.009809694,0.0,0.00980969425290823
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지.,LoRA 와 QLoRA 의 차이,-0.0106149595,0.0,0.01061495952308178
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지.,Loss Function 예시,-0.01736911,0.0,0.017369110137224197
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지.,Loss Function 정의,-0.012254186,0.0,0.012254185974597931
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지.,MBTI,-0.00033202747,0.0,0.0003320274699945003
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지.,MSE Loss 설명,-0.0054120864,0.0,0.005412086378782988
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지.,MSE Loss 용도,-0.008595328,0.0,0.008595327846705914
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지.,Multi-Label 에서 CE + Softmax 적용 문제점,-0.012694801,0.0,0.012694801203906536
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지.,PEFT 방법 5가지,-0.012505825,0.0,0.01250582467764616
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지.,거대 언어 모델 정의,0.047452815,0.0,0.047452814877033234
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지.,기본 경험,-0.008180354,0.0,0.008180353790521622
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지.,답변 실패,0.98421025,1.0,0.01578974723815918
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지.,딥러닝,-0.0004735732,0.0,0.00047357319272123277
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지.,마지막 할 말,3.4573444e-05,0.0,3.4573444281704724e-05
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지.,머신러닝,-0.014164652,0.0,0.014164651744067669
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지.,면접 시작 인사,0.0014319402,0.0,0.001431940239854157
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지.,상세 경험,0.0045634783,0.0,0.004563478287309408
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지.,수식,-0.0026542374,0.0,0.002654237439855933
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지.,용어 질문,0.0013902882,0.0,0.0013902882346883416
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지.,인공지능,-0.016026568,0.0,0.016026567667722702
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지.,잠시 휴식,0.0036747865,0.0,0.003674786537885666
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지.,좋아하는 아이돌,-0.006087918,0.0,0.00608791783452034
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지.,핵심 아이디어,-0.012705632,0.0,0.01270563155412674
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지.,확률 예측에서 MSE Loss 미 사용 이유,-0.004004572,0.0,0.004004572052508593
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,BCE 가 좋은 task,-0.020218082,0.0,0.020218081772327423
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,BCE 가 좋은 이유,-0.05343937,0.0,0.05343937128782272
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,LLM Fine-Tuning 의 PEFT,-0.019794453,0.0,0.01979445293545723
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,LoRA,-0.0147671625,0.0,0.014767162501811981
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,LoRA 와 QLoRA 의 차이,-0.017818218,0.0,0.01781821809709072
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,Loss Function 예시,-0.032471497,0.0,0.03247149661183357
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,Loss Function 정의,0.9088869,0.0,0.9088869094848633
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,MBTI,0.010586726,0.0,0.010586725547909737
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,MSE Loss 설명,-0.00022822083,0.0,0.00022822082974016666
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,MSE Loss 용도,0.026480686,0.0,0.02648068591952324
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,Multi-Label 에서 CE + Softmax 적용 문제점,-0.014158606,0.0,0.014158605597913265
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,PEFT 방법 5가지,-0.02511205,0.0,0.025112049654126167
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,거대 언어 모델 정의,-0.027764462,0.0,0.027764461934566498
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,기본 경험,0.006059709,0.0,0.006059709005057812
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,답변 실패,0.107868895,1.0,0.8921311050653458
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,딥러닝,0.007729093,0.0,0.007729093078523874
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,마지막 할 말,-0.026372733,0.0,0.02637273259460926
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,머신러닝,0.0007711104,0.0,0.0007711104117333889
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,면접 시작 인사,-0.030043496,0.0,0.030043495818972588
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,상세 경험,0.022699015,0.0,0.022699015215039253
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,수식,-0.003119735,0.0,0.0031197350472211838
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,용어 질문,-0.0008802201,0.0,0.0008802200900390744
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,인공지능,-0.029877277,0.0,0.029877277091145515
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,잠시 휴식,-0.020507611,0.0,0.020507611334323883
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,좋아하는 아이돌,-0.0034266547,0.0,0.0034266547299921513
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,핵심 아이디어,0.061130874,0.0,0.06113087385892868
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,확률 예측에서 MSE Loss 미 사용 이유,-0.044773508,0.0,0.04477350786328316
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",BCE 가 좋은 task,-0.0013182785,0.0,0.0013182784896343946
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",BCE 가 좋은 이유,0.0060974336,0.0,0.006097433622926474
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",LLM Fine-Tuning 의 PEFT,-0.015849832,0.0,0.015849832445383072
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",LoRA,-0.017704818,0.0,0.01770481839776039
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",LoRA 와 QLoRA 의 차이,-0.023302067,0.0,0.023302067071199417
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",Loss Function 예시,-0.0018197342,0.0,0.0018197342287749052
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",Loss Function 정의,0.98553336,1.0,0.014466643333435059
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",MBTI,0.0060696513,0.0,0.006069651339203119
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",MSE Loss 설명,-0.03451527,0.0,0.034515269100666046
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",MSE Loss 용도,0.0044949623,0.0,0.004494962282478809
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",Multi-Label 에서 CE + Softmax 적용 문제점,0.00015162831,0.0,0.00015162830823101103
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",PEFT 방법 5가지,-0.008404059,0.0,0.008404059335589409
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",거대 언어 모델 정의,-0.015837353,0.0,0.015837352722883224
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",기본 경험,0.0003680068,0.0,0.00036800678935833275
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",답변 실패,0.0039074398,0.0,0.003907439764589071
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",딥러닝,-0.027012108,0.0,0.027012107893824577
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",마지막 할 말,-0.009386372,0.0,0.009386371821165085
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",머신러닝,-0.01870126,0.0,0.018701260909438133
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",면접 시작 인사,-3.0341369e-06,0.0,3.0341368528752355e-06
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",상세 경험,-0.0076005817,0.0,0.007600581739097834
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",수식,0.00477022,0.0,0.004770219791680574
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",용어 질문,-0.007914543,0.0,0.007914543151855469
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",인공지능,-0.022965005,0.0,0.022965004667639732
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",잠시 휴식,0.0026551338,0.0,0.0026551338378340006
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",좋아하는 아이돌,0.0038992397,0.0,0.0038992397021502256
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",핵심 아이디어,-0.025298413,0.0,0.025298412889242172
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",확률 예측에서 MSE Loss 미 사용 이유,-0.024561772,0.0,0.024561772122979164
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야,BCE 가 좋은 task,-0.0030161235,0.0,0.0030161235481500626
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야,BCE 가 좋은 이유,-0.0021796138,0.0,0.0021796138025820255
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야,LLM Fine-Tuning 의 PEFT,-0.007833617,0.0,0.007833616808056831
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야,LoRA,-0.0029350405,0.0,0.002935040509328246
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야,LoRA 와 QLoRA 의 차이,-0.01027673,0.0,0.010276730172336102
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야,Loss Function 예시,-0.0036499277,0.0,0.0036499276757240295
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야,Loss Function 정의,0.023432478,0.0,0.023432478308677673
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야,MBTI,-0.0003916326,0.0,0.00039163260953500867
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야,MSE Loss 설명,-0.008921127,0.0,0.008921126835048199
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야,MSE Loss 용도,0.0061902613,0.0,0.006190261337906122
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야,Multi-Label 에서 CE + Softmax 적용 문제점,-0.0067190267,0.0,0.00671902671456337
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야,PEFT 방법 5가지,-0.011728688,0.0,0.011728688143193722
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야,거대 언어 모델 정의,-0.01403653,0.0,0.014036529697477818
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야,기본 경험,-0.008811481,0.0,0.008811481297016144
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야,답변 실패,0.9825894,1.0,0.017410576343536377
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야,딥러닝,-0.0009856854,0.0,0.0009856853866949677
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야,마지막 할 말,-0.0071021807,0.0,0.007102180738002062
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야,머신러닝,-0.014160492,0.0,0.014160492457449436
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야,면접 시작 인사,-0.010367083,0.0,0.010367083363234997
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야,상세 경험,0.004304477,0.0,0.004304477013647556
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야,수식,-0.0048921998,0.0,0.004892199765890837
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야,용어 질문,-0.00875254,0.0,0.00875253975391388
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야,인공지능,-0.017218372,0.0,0.017218371853232384
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야,잠시 휴식,-0.003255474,0.0,0.003255473915487528
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야,좋아하는 아이돌,-0.014889956,0.0,0.014889956451952457
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야,핵심 아이디어,0.0046860417,0.0,0.004686041735112667
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야,확률 예측에서 MSE Loss 미 사용 이유,-0.01247726,0.0,0.012477260082960129
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지,BCE 가 좋은 task,0.0029690454,0.0,0.0029690454248338938
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지,BCE 가 좋은 이유,-0.008932435,0.0,0.00893243495374918
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지,LLM Fine-Tuning 의 PEFT,-0.021805048,0.0,0.0218050479888916
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지,LoRA,-0.022797778,0.0,0.022797778248786926
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지,LoRA 와 QLoRA 의 차이,-0.020447683,0.0,0.020447682589292526
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지,Loss Function 예시,-0.009332044,0.0,0.009332044050097466
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지,Loss Function 정의,0.98694146,1.0,0.01305854320526123
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지,MBTI,0.0093608815,0.0,0.00936088152229786
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지,MSE Loss 설명,-0.022786735,0.0,0.022786734625697136
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지,MSE Loss 용도,-0.00030458096,0.0,0.0003045809571631253
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지,Multi-Label 에서 CE + Softmax 적용 문제점,-0.0069462974,0.0,0.006946297362446785
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지,PEFT 방법 5가지,-0.0062320246,0.0,0.006232024636119604
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지,거대 언어 모델 정의,-0.017921276,0.0,0.01792127639055252
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지,기본 경험,0.0023171182,0.0,0.002317118225619197
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지,답변 실패,-0.0063693584,0.0,0.006369358394294977
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지,딥러닝,-0.016423836,0.0,0.01642383635044098
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지,마지막 할 말,-0.009910143,0.0,0.009910142980515957
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지,머신러닝,-0.01808221,0.0,0.018082210794091225
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지,면접 시작 인사,0.003967393,0.0,0.003967393189668655
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지,상세 경험,-0.014069971,0.0,0.014069970697164536
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지,수식,-0.011073113,0.0,0.011073113419115543
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지,용어 질문,-0.00105258,0.0,0.001052579958923161
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지,인공지능,-0.018141996,0.0,0.01814199611544609
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지,잠시 휴식,0.0068845414,0.0,0.006884541362524033
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지,좋아하는 아이돌,0.0037293837,0.0,0.0037293836940079927
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지,핵심 아이디어,-0.019507777,0.0,0.01950777694582939
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지,확률 예측에서 MSE Loss 미 사용 이유,-0.018054916,0.0,0.018054915592074394
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,BCE 가 좋은 task,-0.004259092,0.0,0.004259091801941395
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,BCE 가 좋은 이유,-0.0027813506,0.0,0.0027813506312668324
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,LLM Fine-Tuning 의 PEFT,-0.0016444891,0.0,0.0016444891225546598
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,LoRA,-0.0016723566,0.0,0.0016723566222935915
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,LoRA 와 QLoRA 의 차이,-0.004309109,0.0,0.004309108946472406
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,Loss Function 예시,0.009062696,0.0,0.009062696248292923
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,Loss Function 정의,-0.0037594668,0.0,0.0037594668101519346
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,MBTI,-0.002462766,0.0,0.0024627659004181623
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,MSE Loss 설명,-0.0041137147,0.0,0.004113714676350355
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,MSE Loss 용도,-0.0045943395,0.0,0.004594339523464441
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,Multi-Label 에서 CE + Softmax 적용 문제점,-0.008082369,0.0,0.008082369342446327
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,PEFT 방법 5가지,-0.002789601,0.0,0.0027896009851247072
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,거대 언어 모델 정의,-0.0133521445,0.0,0.01335214450955391
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,기본 경험,-0.0032541065,0.0,0.0032541065011173487
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,답변 실패,0.9940396,1.0,0.005960404872894287
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,딥러닝,-0.006856335,0.0,0.006856334861367941
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,마지막 할 말,-0.0010816255,0.0,0.0010816254653036594
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,머신러닝,-0.0067249746,0.0,0.0067249746061861515
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,면접 시작 인사,-0.005484345,0.0,0.005484344903379679
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,상세 경험,-0.00026848895,0.0,0.00026848894776776433
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,수식,-0.0069900597,0.0,0.006990059744566679
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,용어 질문,-0.015829608,0.0,0.015829607844352722
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,인공지능,-0.012555471,0.0,0.012555470690131187
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,잠시 휴식,0.0035326518,0.0,0.00353265181183815
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,좋아하는 아이돌,-0.005700746,0.0,0.005700746085494757
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,핵심 아이디어,-0.013132592,0.0,0.013132591731846333
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,확률 예측에서 MSE Loss 미 사용 이유,-0.008195839,0.0,0.008195838890969753
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",BCE 가 좋은 task,-0.007307336,0.0,0.007307336200028658
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",BCE 가 좋은 이유,-0.006477575,0.0,0.006477574817836285
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",LLM Fine-Tuning 의 PEFT,-0.012431103,0.0,0.012431102804839611
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",LoRA,-0.02971725,0.0,0.029717249795794487
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",LoRA 와 QLoRA 의 차이,-0.018000819,0.0,0.01800081878900528
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",Loss Function 예시,0.9919573,1.0,0.008042693138122559
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",Loss Function 정의,-0.01063688,0.0,0.010636880062520504
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",MBTI,-0.008970827,0.0,0.008970826864242554
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",MSE Loss 설명,0.0049315034,0.0,0.004931503441184759
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",MSE Loss 용도,-0.007561201,0.0,0.007561201229691505
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",Multi-Label 에서 CE + Softmax 적용 문제점,0.009503966,0.0,0.009503966197371483
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",PEFT 방법 5가지,-0.007596634,0.0,0.007596633862704039
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",거대 언어 모델 정의,-0.0049171518,0.0,0.004917151760309935
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",기본 경험,-0.007849759,0.0,0.007849759422242641
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",답변 실패,-0.0024284336,0.0,0.0024284336250275373
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",딥러닝,-0.0051039006,0.0,0.00510390056297183
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",마지막 할 말,-0.0057899845,0.0,0.005789984483271837
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",머신러닝,-0.0033749773,0.0,0.003374977270141244
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",면접 시작 인사,0.0063719414,0.0,0.006371941417455673
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",상세 경험,-0.00959672,0.0,0.009596720337867737
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",수식,-0.013747844,0.0,0.01374784391373396
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",용어 질문,0.008838678,0.0,0.008838677778840065
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",인공지능,-0.011066914,0.0,0.011066913604736328
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",잠시 휴식,-0.0033373996,0.0,0.0033373995684087276
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",좋아하는 아이돌,-0.020043064,0.0,0.020043063908815384
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",핵심 아이디어,0.011875723,0.0,0.01187572255730629
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",확률 예측에서 MSE Loss 미 사용 이유,0.013904214,0.0,0.013904213905334473
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어",BCE 가 좋은 task,-0.0053384486,0.0,0.005338448565453291
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어",BCE 가 좋은 이유,-0.008726606,0.0,0.008726606145501137
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어",LLM Fine-Tuning 의 PEFT,-0.008622571,0.0,0.008622570894658566
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어",LoRA,-0.02327976,0.0,0.023279760032892227
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어",LoRA 와 QLoRA 의 차이,-0.0149562,0.0,0.014956199564039707
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어",Loss Function 예시,0.98803914,1.0,0.011960864067077637
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어",Loss Function 정의,-0.021190628,0.0,0.02119062840938568
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어",MBTI,-0.00024992897,0.0,0.00024992896942421794
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어",MSE Loss 설명,-0.007329427,0.0,0.007329427171498537
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어",MSE Loss 용도,-0.01285216,0.0,0.012852160260081291
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어",Multi-Label 에서 CE + Softmax 적용 문제점,0.005097893,0.0,0.005097893066704273
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어",PEFT 방법 5가지,0.001040218,0.0,0.0010402180487290025
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어",거대 언어 모델 정의,0.004208338,0.0,0.004208337981253862
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어",기본 경험,-0.013541672,0.0,0.013541672378778458
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어",답변 실패,-0.0010259319,0.0,0.0010259319096803665
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어",딥러닝,-0.00521309,0.0,0.005213090218603611
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어",마지막 할 말,-0.011495603,0.0,0.011495603248476982
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어",머신러닝,-0.0031593235,0.0,0.003159323474392295
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어",면접 시작 인사,0.0076149073,0.0,0.007614907342940569
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어",상세 경험,-0.003603221,0.0,0.0036032209172844887
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어",수식,-0.020829428,0.0,0.020829427987337112
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어",용어 질문,0.010377145,0.0,0.010377145372331142
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어",인공지능,-0.007842176,0.0,0.007842175662517548
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어",잠시 휴식,0.0056466246,0.0,0.005646624602377415
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어",좋아하는 아이돌,-0.014238559,0.0,0.01423855870962143
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어",핵심 아이디어,0.0071425233,0.0,0.007142523303627968
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어",확률 예측에서 MSE Loss 미 사용 이유,0.011072698,0.0,0.011072698049247265
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야?",BCE 가 좋은 task,-0.0013145783,0.0,0.0013145783450454473
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야?",BCE 가 좋은 이유,0.0039771036,0.0,0.003977103624492884
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야?",LLM Fine-Tuning 의 PEFT,-0.0028606185,0.0,0.0028606185223907232
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야?",LoRA,-0.0028689834,0.0,0.002868983428925276
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야?",LoRA 와 QLoRA 의 차이,-0.005189444,0.0,0.005189443938434124
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야?",Loss Function 예시,0.0048340415,0.0,0.004834041465073824
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야?",Loss Function 정의,-0.006725956,0.0,0.006725956220179796
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야?",MBTI,0.0011389419,0.0,0.0011389418505132198
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야?",MSE Loss 설명,-0.0023189944,0.0,0.0023189943749457598
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야?",MSE Loss 용도,-0.011897033,0.0,0.011897033080458641
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야?",Multi-Label 에서 CE + Softmax 적용 문제점,-0.00718224,0.0,0.007182240020483732
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야?",PEFT 방법 5가지,-0.0026420474,0.0,0.002642047358676791
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야?",거대 언어 모델 정의,-0.016504556,0.0,0.016504555940628052
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야?",기본 경험,-0.0082804905,0.0,0.008280490525066853
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야?",답변 실패,0.9942282,1.0,0.005771815776824951
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야?",딥러닝,-0.004726579,0.0,0.004726578947156668
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야?",마지막 할 말,-0.0055525554,0.0,0.005552555434405804
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야?",머신러닝,-0.010662402,0.0,0.010662402026355267
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야?",면접 시작 인사,-0.0035031282,0.0,0.0035031281877309084
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야?",상세 경험,-0.0011704249,0.0,0.0011704248609021306
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야?",수식,-0.0039523933,0.0,0.0039523933082818985
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야?",용어 질문,-0.019067038,0.0,0.019067037850618362
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야?",인공지능,-0.0109583605,0.0,0.010958360508084297
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야?",잠시 휴식,0.0022928314,0.0,0.002292831428349018
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야?",좋아하는 아이돌,-0.0065175593,0.0,0.006517559289932251
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야?",핵심 아이디어,-0.009236251,0.0,0.009236251004040241
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야?",확률 예측에서 MSE Loss 미 사용 이유,-0.0026196253,0.0,0.0026196253020316362
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어",BCE 가 좋은 task,-0.008338378,0.0,0.008338377811014652
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어",BCE 가 좋은 이유,0.004067591,0.0,0.004067590925842524
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어",LLM Fine-Tuning 의 PEFT,-0.0026847392,0.0,0.0026847391854971647
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어",LoRA,-0.03025717,0.0,0.030257169157266617
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어",LoRA 와 QLoRA 의 차이,-0.019099887,0.0,0.0190998874604702
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어",Loss Function 예시,0.988033,1.0,0.011967003345489502
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어",Loss Function 정의,-0.015606779,0.0,0.015606778673827648
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어",MBTI,0.002071815,0.0,0.002071815077215433
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어",MSE Loss 설명,0.00094554736,0.0,0.0009455473627895117
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어",MSE Loss 용도,-0.005717567,0.0,0.005717567168176174
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어",Multi-Label 에서 CE + Softmax 적용 문제점,0.0048559294,0.0,0.004855929408222437
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어",PEFT 방법 5가지,-0.0034207902,0.0,0.0034207901917397976
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어",거대 언어 모델 정의,-0.005612563,0.0,0.0056125628761947155
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어",기본 경험,-0.01619776,0.0,0.01619775965809822
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어",답변 실패,0.0004776851,0.0,0.00047768509830348194
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어",딥러닝,-0.004043393,0.0,0.004043392837047577
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어",마지막 할 말,-0.0083490135,0.0,0.008349013514816761
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어",머신러닝,-0.009264713,0.0,0.009264713153243065
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어",면접 시작 인사,0.0023992436,0.0,0.0023992436472326517
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어",상세 경험,-0.0031015342,0.0,0.0031015342101454735
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어",수식,-0.017267944,0.0,0.017267944291234016
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어",용어 질문,0.00086910295,0.0,0.0008691029506735504
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어",인공지능,-0.013731228,0.0,0.013731228187680244
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어",잠시 휴식,-0.0010656322,0.0,0.0010656322119757533
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어",좋아하는 아이돌,-0.019021481,0.0,0.01902148127555847
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어",핵심 아이디어,0.005379727,0.0,0.0053797271102666855
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어",확률 예측에서 MSE Loss 미 사용 이유,0.006240092,0.0,0.006240092217922211
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아",BCE 가 좋은 task,-0.0065346565,0.0,0.006534656509757042
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아",BCE 가 좋은 이유,-0.0008385836,0.0,0.000838583626318723
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아",LLM Fine-Tuning 의 PEFT,-0.010362287,0.0,0.010362287051975727
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아",LoRA,-0.028814942,0.0,0.02881494164466858
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아",LoRA 와 QLoRA 의 차이,-0.017948432,0.0,0.01794843189418316
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아",Loss Function 예시,0.9895491,1.0,0.010450899600982666
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아",Loss Function 정의,-0.017012274,0.0,0.017012273892760277
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아",MBTI,-0.0058850087,0.0,0.005885008722543716
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아",MSE Loss 설명,-0.010192359,0.0,0.010192358866333961
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아",MSE Loss 용도,-0.0101278825,0.0,0.010127882473170757
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아",Multi-Label 에서 CE + Softmax 적용 문제점,0.0074282256,0.0,0.0074282255955040455
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아",PEFT 방법 5가지,-0.0057545677,0.0,0.0057545676827430725
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아",거대 언어 모델 정의,-0.009793583,0.0,0.009793583303689957
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아",기본 경험,-0.013505317,0.0,0.013505317270755768
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아",답변 실패,0.0019839308,0.0,0.0019839308224618435
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아",딥러닝,-0.0034282117,0.0,0.0034282116685062647
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아",마지막 할 말,-0.006543507,0.0,0.006543506868183613
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아",머신러닝,-0.0071103876,0.0,0.0071103875525295734
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아",면접 시작 인사,0.008103929,0.0,0.008103929460048676
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아",상세 경험,-0.0041855094,0.0,0.004185509402304888
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아",수식,-0.008348075,0.0,0.008348074741661549
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아",용어 질문,0.013683939,0.0,0.013683939352631569
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아",인공지능,-0.009978767,0.0,0.009978766553103924
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아",잠시 휴식,-0.001408826,0.0,0.001408825977705419
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아",좋아하는 아이돌,-0.016683657,0.0,0.016683656722307205
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아",핵심 아이디어,0.014303537,0.0,0.014303537085652351
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아",확률 예측에서 MSE Loss 미 사용 이유,0.010625551,0.0,0.010625551454722881
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,BCE 가 좋은 task,-0.005409237,0.0,0.0054092369973659515
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,BCE 가 좋은 이유,0.009407227,0.0,0.00940722692757845
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,LLM Fine-Tuning 의 PEFT,-0.026962651,0.0,0.026962650939822197
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,LoRA,-0.03498149,0.0,0.034981489181518555
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,LoRA 와 QLoRA 의 차이,-0.01976123,0.0,0.019761230796575546
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,Loss Function 예시,-0.0045174896,0.0,0.004517489578574896
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,Loss Function 정의,-0.03416105,0.0,0.034161049872636795
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,MBTI,-0.009012412,0.0,0.009012412279844284
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,MSE Loss 설명,0.98418045,1.0,0.015819549560546875
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,MSE Loss 용도,0.006760212,0.0,0.006760212127119303
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,Multi-Label 에서 CE + Softmax 적용 문제점,0.044100996,0.0,0.04410099610686302
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,PEFT 방법 5가지,-0.0042508976,0.0,0.0042508975602686405
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,거대 언어 모델 정의,-0.021498406,0.0,0.021498406305909157
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,기본 경험,0.004322105,0.0,0.004322105087339878
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,답변 실패,-0.0015781851,0.0,0.0015781851252540946
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,딥러닝,-0.003153273,0.0,0.0031532729044556618
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,마지막 할 말,0.0023031116,0.0,0.0023031115997582674
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,머신러닝,-0.0023991412,0.0,0.002399141201749444
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,면접 시작 인사,-0.0047751036,0.0,0.004775103647261858
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,상세 경험,-0.0075582014,0.0,0.007558201439678669
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,수식,-0.027439306,0.0,0.027439305558800697
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,용어 질문,-0.0064708996,0.0,0.006470899563282728
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,인공지능,0.0030477175,0.0,0.0030477175023406744
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,잠시 휴식,-0.016436921,0.0,0.016436921432614326
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,좋아하는 아이돌,-0.005250832,0.0,0.005250832065939903
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,핵심 아이디어,0.018944886,0.0,0.018944885581731796
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,확률 예측에서 MSE Loss 미 사용 이유,0.0013900663,0.0,0.0013900663470849395
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,BCE 가 좋은 task,-0.0022361341,0.0,0.0022361341398209333
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,BCE 가 좋은 이유,0.027877921,0.0,0.027877921238541603
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,LLM Fine-Tuning 의 PEFT,-0.033754736,0.0,0.03375473618507385
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,LoRA,-0.037461676,0.0,0.03746167570352554
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,LoRA 와 QLoRA 의 차이,-0.017682102,0.0,0.01768210157752037
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,Loss Function 예시,-0.011212465,0.0,0.011212465353310108
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,Loss Function 정의,-0.01172119,0.0,0.011721190065145493
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,MBTI,-0.022001902,0.0,0.022001901641488075
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,MSE Loss 설명,0.96123296,0.0,0.9612329602241516
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,MSE Loss 용도,-0.0008978572,0.0,0.0008978571859188378
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,Multi-Label 에서 CE + Softmax 적용 문제점,0.017557714,0.0,0.017557714134454727
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,PEFT 방법 5가지,-0.020593403,0.0,0.02059340290725231
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,거대 언어 모델 정의,-0.037262913,0.0,0.03726291283965111
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,기본 경험,-0.007342644,0.0,0.007342644035816193
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,답변 실패,0.031484514,1.0,0.9685154855251312
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,딥러닝,-0.0010488697,0.0,0.0010488696862012148
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,마지막 할 말,-0.025793985,0.0,0.025793984532356262
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,머신러닝,-0.010269414,0.0,0.010269413702189922
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,면접 시작 인사,-0.015117863,0.0,0.015117863193154335
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,상세 경험,-1.3425213e-05,0.0,1.342521318292711e-05
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,수식,-0.013891228,0.0,0.013891227543354034
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,용어 질문,-0.008527318,0.0,0.008527318015694618
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,인공지능,0.004396805,0.0,0.004396805074065924
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,잠시 휴식,-0.025459927,0.0,0.025459926575422287
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,좋아하는 아이돌,-0.004831681,0.0,0.004831681028008461
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,핵심 아이디어,0.05011667,0.0,0.05011666938662529
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,확률 예측에서 MSE Loss 미 사용 이유,-0.011476763,0.0,0.011476762592792511
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",BCE 가 좋은 task,-0.0068970523,0.0,0.00689705228433013
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",BCE 가 좋은 이유,-0.0052361987,0.0,0.0052361986599862576
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",LLM Fine-Tuning 의 PEFT,-0.012862352,0.0,0.012862351723015308
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",LoRA,-0.019334909,0.0,0.019334908574819565
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",LoRA 와 QLoRA 의 차이,-0.011442702,0.0,0.011442702263593674
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",Loss Function 예시,-0.008614322,0.0,0.008614322170615196
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",Loss Function 정의,0.0046881475,0.0,0.004688147455453873
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",MBTI,0.008199821,0.0,0.008199821226298809
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",MSE Loss 설명,0.0042379466,0.0,0.004237946588546038
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",MSE Loss 용도,0.9803698,1.0,0.01963019371032715
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",Multi-Label 에서 CE + Softmax 적용 문제점,0.006209928,0.0,0.006209928076714277
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",PEFT 방법 5가지,0.005185843,0.0,0.005185842979699373
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",거대 언어 모델 정의,0.007594934,0.0,0.007594934199005365
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",기본 경험,0.0049621803,0.0,0.004962180275470018
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",답변 실패,-0.004318124,0.0,0.004318124148994684
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",딥러닝,-0.007723472,0.0,0.0077234720811247826
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",마지막 할 말,-0.014439589,0.0,0.014439589343965054
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",머신러닝,-0.011697906,0.0,0.011697906069457531
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",면접 시작 인사,-0.0070817815,0.0,0.007081781513988972
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",상세 경험,-0.031346023,0.0,0.031346023082733154
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",수식,-0.0058935606,0.0,0.005893560592085123
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",용어 질문,-0.011304313,0.0,0.011304313316941261
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",인공지능,-0.0127074355,0.0,0.01270743552595377
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",잠시 휴식,-0.0036725868,0.0,0.003672586753964424
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",좋아하는 아이돌,-0.00020234607,0.0,0.00020234606927260756
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",핵심 아이디어,-0.007150154,0.0,0.00715015409514308
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",확률 예측에서 MSE Loss 미 사용 이유,-0.010816101,0.0,0.010816100984811783
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,BCE 가 좋은 task,-0.008651502,0.0,0.008651502430438995
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,BCE 가 좋은 이유,-0.0073729795,0.0,0.007372979540377855
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,LLM Fine-Tuning 의 PEFT,-0.00040881117,0.0,0.0004088111745659262
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,LoRA,-0.0057080057,0.0,0.005708005744963884
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,LoRA 와 QLoRA 의 차이,-0.0051588137,0.0,0.0051588136702775955
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,Loss Function 예시,-0.0066274256,0.0,0.00662742555141449
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,Loss Function 정의,-0.011314705,0.0,0.01131470501422882
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,MBTI,-0.017389404,0.0,0.01738940365612507
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,MSE Loss 설명,-0.0023514384,0.0,0.002351438393816352
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,MSE Loss 용도,0.052001357,0.0,0.052001357078552246
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,Multi-Label 에서 CE + Softmax 적용 문제점,-0.004983721,0.0,0.0049837208352983
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,PEFT 방법 5가지,-0.010864603,0.0,0.010864603333175182
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,거대 언어 모델 정의,0.0025767274,0.0,0.002576727420091629
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,기본 경험,-0.0038017556,0.0,0.0038017556071281433
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,답변 실패,0.9844404,1.0,0.015559613704681396
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,딥러닝,-0.0176756,0.0,0.017675599083304405
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,마지막 할 말,-0.0061612185,0.0,0.006161218509078026
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,머신러닝,-0.004838023,0.0,0.004838022869080305
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,면접 시작 인사,-0.00048718692,0.0,0.00048718691687099636
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,상세 경험,-0.0057471944,0.0,0.005747194401919842
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,수식,-0.013653249,0.0,0.01365324854850769
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,용어 질문,-0.012067901,0.0,0.012067900970578194
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,인공지능,-0.023606036,0.0,0.023606035858392715
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,잠시 휴식,0.0019012982,0.0,0.0019012981792911887
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,좋아하는 아이돌,-0.007108888,0.0,0.0071088881231844425
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,핵심 아이디어,-0.010433059,0.0,0.010433059185743332
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,확률 예측에서 MSE Loss 미 사용 이유,-0.008618696,0.0,0.00861869566142559
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,BCE 가 좋은 task,-0.0050881193,0.0,0.005088119301944971
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,BCE 가 좋은 이유,0.008129659,0.0,0.008129659108817577
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,LLM Fine-Tuning 의 PEFT,-0.003553132,0.0,0.0035531320609152317
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,LoRA,0.0013916014,0.0,0.0013916013995185494
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,LoRA 와 QLoRA 의 차이,-0.015885923,0.0,0.01588592305779457
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,Loss Function 예시,0.0017901089,0.0,0.0017901088576763868
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,Loss Function 정의,-0.005633119,0.0,0.005633119028061628
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,MBTI,-0.0061267954,0.0,0.006126795429736376
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,MSE Loss 설명,-0.0014966156,0.0,0.0014966155868023634
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,MSE Loss 용도,0.002932629,0.0,0.002932629082351923
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,Multi-Label 에서 CE + Softmax 적용 문제점,-0.012495896,0.0,0.012495895847678185
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,PEFT 방법 5가지,-0.013722812,0.0,0.013722811825573444
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,거대 언어 모델 정의,-0.013339955,0.0,0.013339955359697342
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,기본 경험,-0.008322565,0.0,0.008322564885020256
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,답변 실패,0.9890085,1.0,0.010991513729095459
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,딥러닝,-0.0056370064,0.0,0.005637006368488073
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,마지막 할 말,-0.008679358,0.0,0.008679358288645744
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,머신러닝,-0.014053215,0.0,0.014053215272724628
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,면접 시작 인사,-0.007885974,0.0,0.007885973900556564
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,상세 경험,0.0007485259,0.0,0.0007485258975066245
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,수식,0.0067663016,0.0,0.006766301579773426
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,용어 질문,-0.014091562,0.0,0.014091561548411846
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,인공지능,-0.0035644977,0.0,0.0035644976887851954
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,잠시 휴식,0.00652317,0.0,0.006523170042783022
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,좋아하는 아이돌,-0.0046226103,0.0,0.0046226102858781815
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,핵심 아이디어,-0.016246064,0.0,0.016246063634753227
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,확률 예측에서 MSE Loss 미 사용 이유,0.009643905,0.0,0.009643904864788055
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,BCE 가 좋은 task,-0.0013933674,0.0,0.001393367419950664
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,BCE 가 좋은 이유,-0.017267106,0.0,0.017267106100916862
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,LLM Fine-Tuning 의 PEFT,0.001111836,0.0,0.0011118360562250018
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,LoRA,-0.017942572,0.0,0.01794257201254368
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,LoRA 와 QLoRA 의 차이,0.006067206,0.0,0.006067206151783466
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,Loss Function 예시,0.0053845425,0.0,0.005384542513638735
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,Loss Function 정의,-0.027567856,0.0,0.027567856013774872
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,MBTI,0.0021589776,0.0,0.0021589775569736958
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,MSE Loss 설명,0.013291465,0.0,0.013291465118527412
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,MSE Loss 용도,0.000710596,0.0,0.0007105959812179208
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,Multi-Label 에서 CE + Softmax 적용 문제점,0.003642258,0.0,0.0036422580014914274
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,PEFT 방법 5가지,-0.014945652,0.0,0.014945652335882187
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,거대 언어 모델 정의,-0.0114628,0.0,0.011462800204753876
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,기본 경험,0.011295231,0.0,0.011295231059193611
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,답변 실패,-0.009717479,0.0,0.009717479348182678
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,딥러닝,-0.0007483872,0.0,0.0007483871886506677
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,마지막 할 말,-0.0034312876,0.0,0.003431287594139576
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,머신러닝,-0.012029474,0.0,0.012029473669826984
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,면접 시작 인사,-0.005452211,0.0,0.0054522110149264336
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,상세 경험,-0.009606375,0.0,0.009606375358998775
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,수식,-0.011321438,0.0,0.011321437545120716
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,용어 질문,-0.0038715939,0.0,0.0038715938571840525
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,인공지능,0.0016080912,0.0,0.0016080911736935377
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,잠시 휴식,-0.0088325115,0.0,0.008832511492073536
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,좋아하는 아이돌,-0.0018964519,0.0,0.0018964519258588552
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,핵심 아이디어,-0.022540053,0.0,0.022540053352713585
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,확률 예측에서 MSE Loss 미 사용 이유,0.9833638,1.0,0.016636192798614502
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,BCE 가 좋은 task,-0.0062214467,0.0,0.0062214466743171215
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,BCE 가 좋은 이유,0.0015832107,0.0,0.0015832106582820415
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,LLM Fine-Tuning 의 PEFT,0.00050452584,0.0,0.0005045258440077305
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,LoRA,-0.00042975138,0.0,0.0004297513805795461
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,LoRA 와 QLoRA 의 차이,0.0009093243,0.0,0.0009093243279494345
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,Loss Function 예시,-0.011563686,0.0,0.011563685722649097
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,Loss Function 정의,0.005045272,0.0,0.005045271944254637
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,MBTI,0.011584081,0.0,0.011584080755710602
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,MSE Loss 설명,-0.024697084,0.0,0.024697083979845047
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,MSE Loss 용도,0.00036141046,0.0,0.00036141046439297497
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,Multi-Label 에서 CE + Softmax 적용 문제점,-0.00021741123,0.0,0.00021741123055107892
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,PEFT 방법 5가지,0.00199993,0.0,0.001999930012971163
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,거대 언어 모델 정의,-0.009099554,0.0,0.009099554270505905
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,기본 경험,0.010978494,0.0,0.010978493839502335
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,답변 실패,-0.009391344,0.0,0.009391344152390957
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,딥러닝,0.0016080388,0.0,0.0016080387867987156
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,마지막 할 말,0.0056320997,0.0,0.005632099695503712
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,머신러닝,-0.0014407714,0.0,0.0014407713897526264
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,면접 시작 인사,-0.0007911503,0.0,0.0007911503198556602
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,상세 경험,-0.029456718,0.0,0.029456717893481255
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,수식,0.97328043,1.0,0.02671957015991211
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,용어 질문,0.011834302,0.0,0.011834301985800266
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,인공지능,-0.0065511614,0.0,0.0065511614084243774
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,잠시 휴식,0.009446842,0.0,0.009446841664612293
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,좋아하는 아이돌,0.0156141,0.0,0.0156140998005867
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,핵심 아이디어,-0.0049678693,0.0,0.004967869259417057
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,확률 예측에서 MSE Loss 미 사용 이유,-0.0015942085,0.0,0.001594208530150354
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",BCE 가 좋은 task,0.0060074735,0.0,0.006007473450154066
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",BCE 가 좋은 이유,-0.029658733,0.0,0.029658732935786247
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",LLM Fine-Tuning 의 PEFT,0.006263692,0.0,0.006263691931962967
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",LoRA,0.0053505353,0.0,0.005350535269826651
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",LoRA 와 QLoRA 의 차이,0.013002998,0.0,0.013002998195588589
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",Loss Function 예시,0.008215595,0.0,0.008215595036745071
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",Loss Function 정의,-0.019058466,0.0,0.0190584659576416
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",MBTI,-0.00088456756,0.0,0.0008845675620250404
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",MSE Loss 설명,-0.005165866,0.0,0.005165866110473871
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",MSE Loss 용도,0.017011229,0.0,0.017011228948831558
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",Multi-Label 에서 CE + Softmax 적용 문제점,0.0029827808,0.0,0.0029827808029949665
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",PEFT 방법 5가지,-0.007267976,0.0,0.007267976179718971
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",거대 언어 모델 정의,-0.0057926937,0.0,0.005792693700641394
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",기본 경험,0.0022785694,0.0,0.0022785693872720003
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",답변 실패,-0.0017076163,0.0,0.0017076162621378899
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",딥러닝,0.016004246,0.0,0.016004245728254318
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",마지막 할 말,-0.0035550552,0.0,0.0035550552420318127
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",머신러닝,0.0115963,0.0,0.011596299707889557
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",면접 시작 인사,-0.010011542,0.0,0.010011541657149792
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",상세 경험,-0.008327209,0.0,0.008327209390699863
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",수식,0.023574147,0.0,0.02357414737343788
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",용어 질문,-0.0018779171,0.0,0.0018779170932248235
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",인공지능,-0.004974275,0.0,0.004974274896085262
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",잠시 휴식,-0.0045869676,0.0,0.0045869676396250725
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",좋아하는 아이돌,-0.008582855,0.0,0.008582854643464088
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",핵심 아이디어,0.9779458,1.0,0.022054195404052734
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",확률 예측에서 MSE Loss 미 사용 이유,-0.053293988,0.0,0.05329398810863495
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,BCE 가 좋은 task,0.033348635,0.0,0.03334863483905792
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,BCE 가 좋은 이유,0.15187752,0.0,0.1518775224685669
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,LLM Fine-Tuning 의 PEFT,0.009223495,0.0,0.009223494678735733
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,LoRA,0.018421099,0.0,0.018421098589897156
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,LoRA 와 QLoRA 의 차이,-0.050591215,0.0,0.05059121549129486
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,Loss Function 예시,-0.019122226,0.0,0.019122226163744926
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,Loss Function 정의,-0.00638136,0.0,0.0063813598826527596
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,MBTI,-0.02037371,0.0,0.020373709499835968
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,MSE Loss 설명,0.11650474,0.0,0.1165047436952591
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,MSE Loss 용도,0.04387035,0.0,0.04387034848332405
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,Multi-Label 에서 CE + Softmax 적용 문제점,0.107565366,0.0,0.10756536573171616
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,PEFT 방법 5가지,-0.06041098,0.0,0.06041098013520241
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,거대 언어 모델 정의,0.048597824,0.0,0.048597823828458786
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,기본 경험,-0.024386382,0.0,0.02438638173043728
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,답변 실패,0.07649454,1.0,0.923505462706089
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,딥러닝,0.014869217,0.0,0.014869216829538345
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,마지막 할 말,-0.0047777854,0.0,0.004777785390615463
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,머신러닝,-0.039989088,0.0,0.03998908773064613
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,면접 시작 인사,-0.02850721,0.0,0.028507210314273834
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,상세 경험,0.02295819,0.0,0.022958189249038696
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,수식,0.03816548,0.0,0.03816547989845276
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,용어 질문,-0.050880693,0.0,0.05088069289922714
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,인공지능,-0.007748524,0.0,0.007748524192720652
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,잠시 휴식,-0.0109436065,0.0,0.010943606495857239
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,좋아하는 아이돌,0.007528487,0.0,0.007528487127274275
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,핵심 아이디어,0.7732653,0.0,0.7732653021812439
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,확률 예측에서 MSE Loss 미 사용 이유,-0.12357025,0.0,0.12357024848461151
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",BCE 가 좋은 task,-0.0004237163,0.0,0.00042371629388071597
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",BCE 가 좋은 이유,-0.023275042,0.0,0.023275041952729225
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",LLM Fine-Tuning 의 PEFT,0.00715369,0.0,0.007153689861297607
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",LoRA,0.011883678,0.0,0.011883677914738655
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",LoRA 와 QLoRA 의 차이,0.018659616,0.0,0.018659615889191628
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",Loss Function 예시,0.015582996,0.0,0.015582996420562267
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",Loss Function 정의,-0.022453481,0.0,0.02245348133146763
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",MBTI,0.0001507438,0.0,0.0001507437991676852
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",MSE Loss 설명,-0.007939974,0.0,0.007939973846077919
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",MSE Loss 용도,0.0113434,0.0,0.011343399994075298
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",Multi-Label 에서 CE + Softmax 적용 문제점,0.0068243127,0.0,0.00682431273162365
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",PEFT 방법 5가지,-0.012260734,0.0,0.012260734103620052
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",거대 언어 모델 정의,-0.0035956341,0.0,0.0035956341307610273
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",기본 경험,-0.002342098,0.0,0.0023420979268848896
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",답변 실패,0.00074318453,0.0,0.000743184529710561
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",딥러닝,0.025892599,0.0,0.025892598554491997
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",마지막 할 말,-0.006102092,0.0,0.0061020920984447
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",머신러닝,0.0011086889,0.0,0.0011086888844147325
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",면접 시작 인사,-0.012987405,0.0,0.012987405061721802
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",상세 경험,-0.014093987,0.0,0.014093986712396145
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",수식,0.01782059,0.0,0.017820589244365692
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",용어 질문,-0.00684979,0.0,0.006849789991974831
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",인공지능,0.0021768615,0.0,0.002176861511543393
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",잠시 휴식,-0.010313205,0.0,0.010313205420970917
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",좋아하는 아이돌,8.377086e-05,0.0,8.377085760002956e-05
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",핵심 아이디어,0.976256,1.0,0.02374398708343506
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",확률 예측에서 MSE Loss 미 사용 이유,-0.05091143,0.0,0.05091143026947975
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')]",BCE 가 좋은 task,-0.012027257,0.0,0.0120272571220994
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')]",BCE 가 좋은 이유,-0.01681807,0.0,0.01681807078421116
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')]",LLM Fine-Tuning 의 PEFT,0.0033459526,0.0,0.0033459526021033525
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')]",LoRA,-0.0026484365,0.0,0.002648436464369297
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')]",LoRA 와 QLoRA 의 차이,0.013541804,0.0,0.013541803695261478
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')]",Loss Function 예시,-0.003943523,0.0,0.003943522926419973
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')]",Loss Function 정의,0.008898607,0.0,0.008898607455193996
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')]",MBTI,0.003555149,0.0,0.003555149072781205
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')]",MSE Loss 설명,-0.030830063,0.0,0.030830062925815582
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')]",MSE Loss 용도,-0.0057099005,0.0,0.0057099005207419395
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')]",Multi-Label 에서 CE + Softmax 적용 문제점,0.006399548,0.0,0.0063995481468737125
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')]",PEFT 방법 5가지,0.009679967,0.0,0.009679966606199741
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')]",거대 언어 모델 정의,-0.011485275,0.0,0.011485274881124496
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')]",기본 경험,0.003977067,0.0,0.003977066837251186
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')]",답변 실패,-0.007388877,0.0,0.0073888772167265415
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')]",딥러닝,-0.006412477,0.0,0.006412477232515812
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')]",마지막 할 말,0.016998649,0.0,0.016998648643493652
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')]",머신러닝,-0.013998198,0.0,0.013998198322951794
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')]",면접 시작 인사,-0.0023732278,0.0,0.0023732278496026993
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')]",상세 경험,-0.034829024,0.0,0.034829024225473404
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')]",수식,0.9570928,1.0,0.04290717840194702
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')]",용어 질문,0.0120063275,0.0,0.012006327509880066
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')]",인공지능,-0.0035123224,0.0,0.003512322437018156
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')]",잠시 휴식,0.01192834,0.0,0.011928340420126915
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')]",좋아하는 아이돌,0.013199687,0.0,0.013199687004089355
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')]",핵심 아이디어,0.018737646,0.0,0.01873764581978321
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')]",확률 예측에서 MSE Loss 미 사용 이유,-0.019625634,0.0,0.01962563395500183
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다",BCE 가 좋은 task,0.0006580156,0.0,0.0006580156041309237
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다",BCE 가 좋은 이유,-0.030889986,0.0,0.03088998608291149
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다",LLM Fine-Tuning 의 PEFT,0.0060011754,0.0,0.006001175381243229
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다",LoRA,0.0005958406,0.0,0.0005958406254649162
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다",LoRA 와 QLoRA 의 차이,0.0020758414,0.0,0.0020758414175361395
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다",Loss Function 예시,0.0108135315,0.0,0.010813531465828419
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다",Loss Function 정의,-0.022660304,0.0,0.022660303860902786
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다",MBTI,-0.00023100672,0.0,0.00023100672115106136
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다",MSE Loss 설명,-0.005567426,0.0,0.005567425861954689
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다",MSE Loss 용도,0.018054275,0.0,0.01805427484214306
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다",Multi-Label 에서 CE + Softmax 적용 문제점,0.013603141,0.0,0.013603140600025654
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다",PEFT 방법 5가지,-0.006493253,0.0,0.006493253167718649
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다",거대 언어 모델 정의,-0.003951894,0.0,0.003951894119381905
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다",기본 경험,0.001644705,0.0,0.001644704956561327
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다",답변 실패,0.0004826165,0.0,0.0004826165095437318
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다",딥러닝,0.017624756,0.0,0.017624756321310997
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다",마지막 할 말,-0.0053454014,0.0,0.005345401354134083
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다",머신러닝,0.0154154105,0.0,0.01541541051119566
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다",면접 시작 인사,-0.010399149,0.0,0.010399148799479008
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다",상세 경험,-0.006204192,0.0,0.0062041920609772205
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다",수식,0.019433677,0.0,0.019433677196502686
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다",용어 질문,0.0001792381,0.0,0.0001792380935512483
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다",인공지능,-0.0021272302,0.0,0.002127230167388916
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다",잠시 휴식,-0.008594097,0.0,0.008594096638262272
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다",좋아하는 아이돌,-0.004456609,0.0,0.004456609021872282
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다",핵심 아이디어,0.97816706,1.0,0.021832942962646484
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다",확률 예측에서 MSE Loss 미 사용 이유,-0.057109743,0.0,0.05710974335670471
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거,BCE 가 좋은 task,-0.0073105996,0.0,0.0073105995543301105
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거,BCE 가 좋은 이유,0.0028219577,0.0,0.002821957692503929
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거,LLM Fine-Tuning 의 PEFT,-0.0030935076,0.0,0.00309350760653615
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거,LoRA,0.0054782024,0.0,0.005478202365338802
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거,LoRA 와 QLoRA 의 차이,0.004050722,0.0,0.004050721880048513
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거,Loss Function 예시,-0.0069702277,0.0,0.00697022769600153
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거,Loss Function 정의,-0.013534753,0.0,0.013534752652049065
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거,MBTI,-0.0009449389,0.0,0.0009449389181099832
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거,MSE Loss 설명,0.0021989204,0.0,0.002198920352384448
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거,MSE Loss 용도,-0.011240321,0.0,0.011240321211516857
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거,Multi-Label 에서 CE + Softmax 적용 문제점,-0.009322198,0.0,0.00932219810783863
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거,PEFT 방법 5가지,-0.005730264,0.0,0.005730263888835907
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거,거대 언어 모델 정의,-0.016035182,0.0,0.016035182401537895
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거,기본 경험,-0.000618299,0.0,0.0006182990036904812
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거,답변 실패,0.9946158,1.0,0.005384206771850586
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거,딥러닝,-0.0059326054,0.0,0.005932605359703302
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거,마지막 할 말,-0.0029567746,0.0,0.002956774551421404
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거,머신러닝,-0.014601524,0.0,0.014601523987948895
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거,면접 시작 인사,0.0024815702,0.0,0.0024815702345222235
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거,상세 경험,0.0010966649,0.0,0.0010966649278998375
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거,수식,-0.0011771915,0.0,0.0011771915014833212
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거,용어 질문,-0.011675005,0.0,0.011675004847347736
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거,인공지능,-0.008656018,0.0,0.008656018413603306
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거,잠시 휴식,-8.642113e-05,0.0,8.642113243695349e-05
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거,좋아하는 아이돌,-0.0026732264,0.0,0.002673226408660412
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거,핵심 아이디어,-0.0024645366,0.0,0.00246453657746315
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거,확률 예측에서 MSE Loss 미 사용 이유,-0.008604085,0.0,0.008604085072875023
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지",BCE 가 좋은 task,0.006223001,0.0,0.006223001051694155
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지",BCE 가 좋은 이유,0.009325671,0.0,0.009325671009719372
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지",LLM Fine-Tuning 의 PEFT,0.010255406,0.0,0.010255405679345131
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지",LoRA,0.0029407705,0.0,0.002940770471468568
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지",LoRA 와 QLoRA 의 차이,-0.00090953254,0.0,0.000909532536752522
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지",Loss Function 예시,0.0080204,0.0,0.008020400069653988
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지",Loss Function 정의,-0.027786326,0.0,0.02778632566332817
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지",MBTI,0.0030066245,0.0,0.003006624523550272
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지",MSE Loss 설명,0.0106010055,0.0,0.010601005516946316
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지",MSE Loss 용도,0.02077572,0.0,0.020775720477104187
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지",Multi-Label 에서 CE + Softmax 적용 문제점,0.0045331856,0.0,0.004533185623586178
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지",PEFT 방법 5가지,-0.022595527,0.0,0.02259552665054798
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지",거대 언어 모델 정의,0.0018401626,0.0,0.001840162556618452
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지",기본 경험,-0.003406981,0.0,0.0034069810062646866
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지",답변 실패,0.014255098,0.0,0.014255098067224026
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지",딥러닝,0.029426329,0.0,0.02942632883787155
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지",마지막 할 말,-0.002054724,0.0,0.002054723910987377
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지",머신러닝,-0.01017299,0.0,0.010172990150749683
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지",면접 시작 인사,-0.011031554,0.0,0.011031554080545902
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지",상세 경험,-0.008771836,0.0,0.008771835826337337
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지",수식,0.044180587,0.0,0.04418058693408966
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지",용어 질문,-0.008607496,0.0,0.008607495576143265
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지",인공지능,-0.01886587,0.0,0.01886587031185627
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지",잠시 휴식,-0.013427292,0.0,0.013427291996777058
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지",좋아하는 아이돌,0.0045651537,0.0,0.0045651537366211414
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지",핵심 아이디어,0.966214,1.0,0.033785998821258545
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지",확률 예측에서 MSE Loss 미 사용 이유,-0.080943026,0.0,0.0809430256485939
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야?",BCE 가 좋은 task,0.0049008555,0.0,0.004900855477899313
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야?",BCE 가 좋은 이유,-0.023850298,0.0,0.023850297555327415
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야?",LLM Fine-Tuning 의 PEFT,-0.023754066,0.0,0.023754065856337547
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야?",LoRA,0.038123045,0.0,0.03812304511666298
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야?",LoRA 와 QLoRA 의 차이,-0.008539838,0.0,0.008539837785065174
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야?",Loss Function 예시,-0.004780316,0.0,0.0047803157940506935
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야?",Loss Function 정의,0.009838068,0.0,0.009838067926466465
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야?",MBTI,-0.001915718,0.0,0.001915717963129282
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야?",MSE Loss 설명,-0.0022824346,0.0,0.002282434608787298
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야?",MSE Loss 용도,0.0029889797,0.0,0.002988979686051607
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야?",Multi-Label 에서 CE + Softmax 적용 문제점,0.0137594035,0.0,0.013759403489530087
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야?",PEFT 방법 5가지,-0.053818353,0.0,0.05381835252046585
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야?",거대 언어 모델 정의,0.021494918,0.0,0.021494917571544647
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야?",기본 경험,0.0038865563,0.0,0.003886556252837181
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야?",답변 실패,-0.010799472,0.0,0.010799472220242023
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야?",딥러닝,-0.02196587,0.0,0.02196587063372135
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야?",마지막 할 말,0.007876664,0.0,0.007876664400100708
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야?",머신러닝,0.023399608,0.0,0.023399608209729195
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야?",면접 시작 인사,-0.0016011811,0.0,0.0016011811094358563
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야?",상세 경험,-0.016107839,0.0,0.016107838600873947
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야?",수식,-0.010454755,0.0,0.010454755276441574
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야?",용어 질문,0.9520677,1.0,0.047932326793670654
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야?",인공지능,-0.011812839,0.0,0.011812838725745678
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야?",잠시 휴식,-0.0025827198,0.0,0.0025827197823673487
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야?",좋아하는 아이돌,-0.016702063,0.0,0.016702063381671906
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야?",핵심 아이디어,-0.012766647,0.0,0.012766647152602673
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야?",확률 예측에서 MSE Loss 미 사용 이유,-0.015678328,0.0,0.015678327530622482
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",BCE 가 좋은 task,-0.008533698,0.0,0.008533697575330734
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",BCE 가 좋은 이유,-0.0013857065,0.0,0.001385706476867199
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",LLM Fine-Tuning 의 PEFT,-0.0098472135,0.0,0.009847213514149189
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",LoRA,0.0028666598,0.0,0.00286665977910161
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",LoRA 와 QLoRA 의 차이,-0.01208741,0.0,0.012087410315871239
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",Loss Function 예시,-0.008341197,0.0,0.008341196924448013
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",Loss Function 정의,-0.0011444641,0.0,0.0011444641277194023
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",MBTI,-0.002033133,0.0,0.0020331330597400665
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",MSE Loss 설명,0.00016555865,0.0,0.00016555865295231342
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",MSE Loss 용도,-0.0015155991,0.0,0.0015155990840867162
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",Multi-Label 에서 CE + Softmax 적용 문제점,-0.009977469,0.0,0.009977469220757484
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",PEFT 방법 5가지,-0.013437232,0.0,0.013437232002615929
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",거대 언어 모델 정의,-0.013789793,0.0,0.01378979254513979
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",기본 경험,-0.0052918107,0.0,0.005291810724884272
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",답변 실패,0.9729017,1.0,0.02709829807281494
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",딥러닝,-0.0127738165,0.0,0.012773816473782063
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",마지막 할 말,-0.008080103,0.0,0.008080103434622288
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",머신러닝,-0.010642651,0.0,0.010642650537192822
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",면접 시작 인사,-0.0029703632,0.0,0.0029703632462769747
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",상세 경험,-0.0015841164,0.0,0.001584116369485855
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",수식,-0.015740102,0.0,0.015740102156996727
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",용어 질문,0.06910926,0.0,0.06910926103591919
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",인공지능,-0.022248436,0.0,0.022248435765504837
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",잠시 휴식,-0.00695512,0.0,0.006955119781196117
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",좋아하는 아이돌,-0.015726503,0.0,0.015726502984762192
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",핵심 아이디어,-0.0080024265,0.0,0.008002426475286484
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",확률 예측에서 MSE Loss 미 사용 이유,-0.011416711,0.0,0.011416710913181305
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",BCE 가 좋은 task,-0.009390133,1.0,1.009390133433044
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",BCE 가 좋은 이유,-0.0026240349,0.0,0.002624034881591797
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",LLM Fine-Tuning 의 PEFT,-0.0035136556,0.0,0.003513655625283718
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",LoRA,0.0007042253,0.0,0.0007042253273539245
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",LoRA 와 QLoRA 의 차이,-0.0014024376,0.0,0.0014024375705048442
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",Loss Function 예시,-0.007062833,0.0,0.007062832824885845
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",Loss Function 정의,-0.005771365,0.0,0.005771365016698837
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",MBTI,0.0004930589,0.0,0.0004930588766001165
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",MSE Loss 설명,-0.008330595,0.0,0.00833059474825859
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",MSE Loss 용도,-0.006738642,0.0,0.006738642230629921
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",Multi-Label 에서 CE + Softmax 적용 문제점,-0.01103829,0.0,0.011038290336728096
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",PEFT 방법 5가지,-0.0049731624,0.0,0.004973162431269884
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",거대 언어 모델 정의,-0.008342313,0.0,0.008342312648892403
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",기본 경험,0.00034153962,0.0,0.00034153962042182684
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",답변 실패,0.993434,0.0,0.9934340119361877
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",딥러닝,-0.010549892,0.0,0.010549891740083694
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",마지막 할 말,-0.002408903,0.0,0.00240890309214592
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",머신러닝,-0.009673191,0.0,0.009673191234469414
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",면접 시작 인사,-0.0021781037,0.0,0.0021781036630272865
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",상세 경험,0.00042865454,0.0,0.00042865454452112317
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",수식,-0.010176165,0.0,0.010176165029406548
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",용어 질문,0.007456897,0.0,0.00745689682662487
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",인공지능,-0.009167857,0.0,0.00916785653680563
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",잠시 휴식,-0.001242783,0.0,0.0012427830370143056
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",좋아하는 아이돌,-0.0061703543,0.0,0.006170354317873716
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",핵심 아이디어,-0.009126455,0.0,0.009126454591751099
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",확률 예측에서 MSE Loss 미 사용 이유,-0.00370339,0.0,0.0037033900152891874
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",BCE 가 좋은 task,0.06209268,0.0,0.06209268048405647
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",BCE 가 좋은 이유,0.93801713,1.0,0.06198287010192871
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",LLM Fine-Tuning 의 PEFT,0.0006774782,0.0,0.0006774782086722553
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",LoRA,-0.02026612,0.0,0.02026611939072609
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",LoRA 와 QLoRA 의 차이,0.0053462773,0.0,0.005346277263015509
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",Loss Function 예시,-0.019555315,0.0,0.019555315375328064
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",Loss Function 정의,0.00021896276,0.0,0.0002189627557527274
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",MBTI,0.0057727676,0.0,0.005772767588496208
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",MSE Loss 설명,0.009092562,0.0,0.009092561900615692
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",MSE Loss 용도,-0.0009330787,0.0,0.0009330786997452378
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",Multi-Label 에서 CE + Softmax 적용 문제점,-0.007042745,0.0,0.007042745128273964
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",PEFT 방법 5가지,-0.0508522,0.0,0.05085219815373421
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",거대 언어 모델 정의,-0.0074312333,0.0,0.007431233301758766
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",기본 경험,0.0049771713,0.0,0.0049771713092923164
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",답변 실패,0.05670245,0.0,0.056702449917793274
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",딥러닝,-0.01505874,0.0,0.01505874004215002
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",마지막 할 말,-0.005200316,0.0,0.005200316198170185
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",머신러닝,0.01633088,0.0,0.01633087918162346
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",면접 시작 인사,0.0005530886,0.0,0.0005530886119231582
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",상세 경험,-0.0010111282,0.0,0.001011128188110888
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",수식,-0.01019749,0.0,0.010197490453720093
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",용어 질문,0.0019889697,0.0,0.0019889697432518005
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",인공지능,-0.0016116004,0.0,0.0016116003971546888
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",잠시 휴식,-0.015067894,0.0,0.015067894011735916
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",좋아하는 아이돌,-0.017567825,0.0,0.017567824572324753
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",핵심 아이디어,-0.0140277725,0.0,0.014027772471308708
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",확률 예측에서 MSE Loss 미 사용 이유,-0.032177415,0.0,0.03217741474509239
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,BCE 가 좋은 task,-0.007924776,0.0,0.007924775592982769
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,BCE 가 좋은 이유,-0.01502224,0.0,0.01502223964780569
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,LLM Fine-Tuning 의 PEFT,-0.006030123,0.0,0.0060301232151687145
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,LoRA,-0.0018430481,0.0,0.0018430481432005763
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,LoRA 와 QLoRA 의 차이,0.005550344,0.0,0.005550344008952379
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,Loss Function 예시,-0.007216498,0.0,0.007216497790068388
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,Loss Function 정의,-0.004619301,0.0,0.004619300831109285
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,MBTI,0.022029845,0.0,0.022029845044016838
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,MSE Loss 설명,0.00609296,0.0,0.006092960014939308
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,MSE Loss 용도,-0.005636292,0.0,0.005636292044073343
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,Multi-Label 에서 CE + Softmax 적용 문제점,0.98833495,1.0,0.011665046215057373
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,PEFT 방법 5가지,0.00458461,0.0,0.004584609996527433
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,거대 언어 모델 정의,-0.010063196,0.0,0.01006319560110569
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,기본 경험,0.00051232433,0.0,0.0005123243317939341
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,답변 실패,0.00910222,0.0,0.009102219715714455
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,딥러닝,0.01467685,0.0,0.014676850289106369
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,마지막 할 말,0.0106238155,0.0,0.010623815469443798
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,머신러닝,0.009690045,0.0,0.00969004537910223
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,면접 시작 인사,0.00086449407,0.0,0.000864494068082422
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,상세 경험,-0.0010942424,0.0,0.0010942424414679408
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,수식,0.006990746,0.0,0.006990746129304171
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,용어 질문,0.019800102,0.0,0.019800102338194847
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,인공지능,-0.008445097,0.0,0.008445097133517265
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,잠시 휴식,0.0009373822,0.0,0.0009373822249472141
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,좋아하는 아이돌,-0.0011511556,0.0,0.0011511555640026927
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,핵심 아이디어,-0.00026097224,0.0,0.0002609722432680428
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,확률 예측에서 MSE Loss 미 사용 이유,0.0025714936,0.0,0.0025714936200529337
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,BCE 가 좋은 task,0.0015832956,0.0,0.0015832956414669752
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,BCE 가 좋은 이유,0.001225009,0.0,0.0012250089785084128
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,LLM Fine-Tuning 의 PEFT,-0.0059231,0.0,0.0059230998158454895
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,LoRA,-0.00077646965,0.0,0.0007764696492813528
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,LoRA 와 QLoRA 의 차이,-0.008608181,0.0,0.008608181029558182
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,Loss Function 예시,-0.0043709683,0.0,0.004370968323200941
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,Loss Function 정의,-0.005031543,0.0,0.005031542852520943
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,MBTI,-0.0023562508,0.0,0.0023562507703900337
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,MSE Loss 설명,-0.00010069921,0.0,0.00010069920972455293
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,MSE Loss 용도,-0.004278655,0.0,0.0042786551639437675
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,Multi-Label 에서 CE + Softmax 적용 문제점,0.008299252,0.0,0.008299252018332481
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,PEFT 방법 5가지,-0.005735537,0.0,0.00573553703725338
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,거대 언어 모델 정의,-0.016856087,0.0,0.016856087371706963
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,기본 경험,-0.0114739295,0.0,0.01147392950952053
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,답변 실패,0.99124724,1.0,0.008752763271331787
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,딥러닝,-0.010006274,0.0,0.010006274096667767
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,마지막 할 말,-0.005680497,0.0,0.00568049680441618
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,머신러닝,-0.008416763,0.0,0.008416762575507164
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,면접 시작 인사,0.0015456891,0.0,0.0015456890687346458
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,상세 경험,-0.002776352,0.0,0.0027763519901782274
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,수식,-0.008852272,0.0,0.008852272294461727
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,용어 질문,-0.015465005,0.0,0.015465005300939083
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,인공지능,-0.012734472,0.0,0.012734471820294857
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,잠시 휴식,0.0029788665,0.0,0.0029788664542138577
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,좋아하는 아이돌,-0.0023159964,0.0,0.0023159964475780725
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,핵심 아이디어,-0.012165112,0.0,0.012165112420916557
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,확률 예측에서 MSE Loss 미 사용 이유,-0.00859216,0.0,0.008592160418629646
Loss Function 관련 실무 경험 -> 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,BCE 가 좋은 task,-0.015602418,0.0,0.015602418221533298
Loss Function 관련 실무 경험 -> 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,BCE 가 좋은 이유,0.010519128,0.0,0.010519128292798996
Loss Function 관련 실무 경험 -> 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,LLM Fine-Tuning 의 PEFT,-0.002421033,0.0,0.002421033103018999
Loss Function 관련 실무 경험 -> 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,LoRA,0.017810773,0.0,0.017810773104429245
Loss Function 관련 실무 경험 -> 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,LoRA 와 QLoRA 의 차이,0.010088128,0.0,0.010088128037750721
Loss Function 관련 실무 경험 -> 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,Loss Function 예시,-0.010206428,0.0,0.010206428356468678
Loss Function 관련 실무 경험 -> 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,Loss Function 정의,0.005264018,0.0,0.0052640181966125965
Loss Function 관련 실무 경험 -> 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,MBTI,0.0020658358,0.0,0.002065835753455758
Loss Function 관련 실무 경험 -> 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,MSE Loss 설명,0.014650897,0.0,0.01465089712291956
Loss Function 관련 실무 경험 -> 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,MSE Loss 용도,-0.016869513,0.0,0.01686951331794262
Loss Function 관련 실무 경험 -> 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,Multi-Label 에서 CE + Softmax 적용 문제점,0.0025766927,0.0,0.0025766927283257246
Loss Function 관련 실무 경험 -> 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,PEFT 방법 5가지,-0.017524755,0.0,0.017524754628539085
Loss Function 관련 실무 경험 -> 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,거대 언어 모델 정의,0.0008674525,0.0,0.000867452472448349
Loss Function 관련 실무 경험 -> 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,기본 경험,0.023765774,1.0,0.9762342255562544
Loss Function 관련 실무 경험 -> 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,답변 실패,0.0016066427,0.0,0.001606642734259367
Loss Function 관련 실무 경험 -> 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,딥러닝,-0.006501855,0.0,0.0065018548630177975
Loss Function 관련 실무 경험 -> 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,마지막 할 말,-0.01341014,0.0,0.013410139828920364
Loss Function 관련 실무 경험 -> 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,머신러닝,0.019733258,0.0,0.019733257591724396
Loss Function 관련 실무 경험 -> 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,면접 시작 인사,-0.017174272,0.0,0.01717427186667919
Loss Function 관련 실무 경험 -> 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,상세 경험,0.9712676,0.0,0.971267580986023
Loss Function 관련 실무 경험 -> 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,수식,-0.04155473,0.0,0.041554730385541916
Loss Function 관련 실무 경험 -> 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,용어 질문,-0.019961799,0.0,0.019961798563599586
Loss Function 관련 실무 경험 -> 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,인공지능,-0.0143711185,0.0,0.014371118508279324
Loss Function 관련 실무 경험 -> 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,잠시 휴식,-0.008536324,0.0,0.00853632390499115
Loss Function 관련 실무 경험 -> 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,좋아하는 아이돌,-0.0046330127,0.0,0.004633012693375349
Loss Function 관련 실무 경험 -> 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,핵심 아이디어,-0.01357933,0.0,0.013579330407083035
Loss Function 관련 실무 경험 -> 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,확률 예측에서 MSE Loss 미 사용 이유,-0.005064814,0.0,0.005064813885837793
Loss Function 관련 실무 경험 -> 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,BCE 가 좋은 task,0.01845388,0.0,0.01845387928187847
Loss Function 관련 실무 경험 -> 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,BCE 가 좋은 이유,0.015567942,0.0,0.015567941591143608
Loss Function 관련 실무 경험 -> 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,LLM Fine-Tuning 의 PEFT,0.004126588,0.0,0.004126587882637978
Loss Function 관련 실무 경험 -> 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,LoRA,-0.0039433544,0.0,0.003943354357033968
Loss Function 관련 실무 경험 -> 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,LoRA 와 QLoRA 의 차이,0.008868796,0.0,0.008868795819580555
Loss Function 관련 실무 경험 -> 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,Loss Function 예시,-0.003908366,0.0,0.003908365964889526
Loss Function 관련 실무 경험 -> 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,Loss Function 정의,-0.0038165438,0.0,0.003816543845459819
Loss Function 관련 실무 경험 -> 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,MBTI,-0.00050506496,0.0,0.000505064963363111
Loss Function 관련 실무 경험 -> 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,MSE Loss 설명,0.0031763415,0.0,0.0031763415317982435
Loss Function 관련 실무 경험 -> 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,MSE Loss 용도,-0.018548155,0.0,0.018548155203461647
Loss Function 관련 실무 경험 -> 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,Multi-Label 에서 CE + Softmax 적용 문제점,0.006944563,0.0,0.006944562774151564
Loss Function 관련 실무 경험 -> 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,PEFT 방법 5가지,-0.0002896446,0.0,0.00028964460943825543
Loss Function 관련 실무 경험 -> 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,거대 언어 모델 정의,-0.0013251852,0.0,0.001325185177847743
Loss Function 관련 실무 경험 -> 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,기본 경험,0.9626165,0.0,0.962616503238678
Loss Function 관련 실무 경험 -> 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,답변 실패,-0.005130923,0.0,0.0051309228874742985
Loss Function 관련 실무 경험 -> 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,딥러닝,-0.0016560346,0.0,0.001656034612096846
Loss Function 관련 실무 경험 -> 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,마지막 할 말,0.0016579034,0.0,0.0016579034272581339
Loss Function 관련 실무 경험 -> 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,머신러닝,0.009152118,0.0,0.009152118116617203
Loss Function 관련 실무 경험 -> 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,면접 시작 인사,-0.020515407,0.0,0.020515406504273415
Loss Function 관련 실무 경험 -> 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,상세 경험,0.029964022,1.0,0.9700359776616096
Loss Function 관련 실무 경험 -> 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,수식,0.0005258802,0.0,0.0005258801975287497
Loss Function 관련 실무 경험 -> 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,용어 질문,-0.013443141,0.0,0.013443141244351864
Loss Function 관련 실무 경험 -> 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,인공지능,0.003304945,0.0,0.003304945072159171
Loss Function 관련 실무 경험 -> 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,잠시 휴식,-0.009696399,0.0,0.009696398861706257
Loss Function 관련 실무 경험 -> 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,좋아하는 아이돌,0.012510644,0.0,0.012510644271969795
Loss Function 관련 실무 경험 -> 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,핵심 아이디어,-0.009719219,0.0,0.00971921905875206
Loss Function 관련 실무 경험 -> 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,확률 예측에서 MSE Loss 미 사용 이유,-0.0069022244,0.0,0.006902224384248257
Loss Function 관련 실무 경험 -> 근데 기억 진짜 안나는데 어떡하지?,BCE 가 좋은 task,-0.0071305423,0.0,0.007130542304366827
Loss Function 관련 실무 경험 -> 근데 기억 진짜 안나는데 어떡하지?,BCE 가 좋은 이유,-0.002732172,0.0,0.0027321719098836184
Loss Function 관련 실무 경험 -> 근데 기억 진짜 안나는데 어떡하지?,LLM Fine-Tuning 의 PEFT,-0.0028220275,0.0,0.0028220275416970253
Loss Function 관련 실무 경험 -> 근데 기억 진짜 안나는데 어떡하지?,LoRA,0.0017598068,0.0,0.0017598067643120885
Loss Function 관련 실무 경험 -> 근데 기억 진짜 안나는데 어떡하지?,LoRA 와 QLoRA 의 차이,-0.0048256894,0.0,0.004825689364224672
Loss Function 관련 실무 경험 -> 근데 기억 진짜 안나는데 어떡하지?,Loss Function 예시,-0.0034774356,0.0,0.0034774355590343475
Loss Function 관련 실무 경험 -> 근데 기억 진짜 안나는데 어떡하지?,Loss Function 정의,-0.0063636964,0.0,0.006363696418702602
Loss Function 관련 실무 경험 -> 근데 기억 진짜 안나는데 어떡하지?,MBTI,-0.0020796175,0.0,0.002079617464914918
Loss Function 관련 실무 경험 -> 근데 기억 진짜 안나는데 어떡하지?,MSE Loss 설명,-0.0027246834,0.0,0.002724683377891779
Loss Function 관련 실무 경험 -> 근데 기억 진짜 안나는데 어떡하지?,MSE Loss 용도,-0.0049779764,0.0,0.0049779764376580715
Loss Function 관련 실무 경험 -> 근데 기억 진짜 안나는데 어떡하지?,Multi-Label 에서 CE + Softmax 적용 문제점,-0.008405742,0.0,0.008405742235481739
Loss Function 관련 실무 경험 -> 근데 기억 진짜 안나는데 어떡하지?,PEFT 방법 5가지,-0.006163931,0.0,0.006163930986076593
Loss Function 관련 실무 경험 -> 근데 기억 진짜 안나는데 어떡하지?,거대 언어 모델 정의,-0.007858989,0.0,0.00785898882895708
Loss Function 관련 실무 경험 -> 근데 기억 진짜 안나는데 어떡하지?,기본 경험,0.010468223,0.0,0.010468223132193089
Loss Function 관련 실무 경험 -> 근데 기억 진짜 안나는데 어떡하지?,답변 실패,0.99395144,1.0,0.00604856014251709
Loss Function 관련 실무 경험 -> 근데 기억 진짜 안나는데 어떡하지?,딥러닝,-0.010307186,0.0,0.010307186283171177
Loss Function 관련 실무 경험 -> 근데 기억 진짜 안나는데 어떡하지?,마지막 할 말,-0.0019211705,0.0,0.0019211705075576901
Loss Function 관련 실무 경험 -> 근데 기억 진짜 안나는데 어떡하지?,머신러닝,-0.009753916,0.0,0.009753916412591934
Loss Function 관련 실무 경험 -> 근데 기억 진짜 안나는데 어떡하지?,면접 시작 인사,0.0027588948,0.0,0.0027588948141783476
Loss Function 관련 실무 경험 -> 근데 기억 진짜 안나는데 어떡하지?,상세 경험,0.001964169,0.0,0.0019641690887510777
Loss Function 관련 실무 경험 -> 근데 기억 진짜 안나는데 어떡하지?,수식,-0.0022133484,0.0,0.002213348401710391
Loss Function 관련 실무 경험 -> 근데 기억 진짜 안나는데 어떡하지?,용어 질문,-0.016095644,0.0,0.016095643863081932
Loss Function 관련 실무 경험 -> 근데 기억 진짜 안나는데 어떡하지?,인공지능,-0.01422892,0.0,0.014228920452296734
Loss Function 관련 실무 경험 -> 근데 기억 진짜 안나는데 어떡하지?,잠시 휴식,-0.002300845,0.0,0.002300844993442297
Loss Function 관련 실무 경험 -> 근데 기억 진짜 안나는데 어떡하지?,좋아하는 아이돌,-0.0037740401,0.0,0.0037740401457995176
Loss Function 관련 실무 경험 -> 근데 기억 진짜 안나는데 어떡하지?,핵심 아이디어,-0.007596311,0.0,0.007596311159431934
Loss Function 관련 실무 경험 -> 근데 기억 진짜 안나는데 어떡하지?,확률 예측에서 MSE Loss 미 사용 이유,-0.007925366,0.0,0.007925366051495075
Loss Function 관련 실무 경험 -> 나 아직 실무 경험 없는데 ㅠㅠ,BCE 가 좋은 task,-3.6553365e-06,0.0,3.6553365134750493e-06
Loss Function 관련 실무 경험 -> 나 아직 실무 경험 없는데 ㅠㅠ,BCE 가 좋은 이유,0.01820162,0.0,0.018201619386672974
Loss Function 관련 실무 경험 -> 나 아직 실무 경험 없는데 ㅠㅠ,LLM Fine-Tuning 의 PEFT,-0.00219103,0.0,0.002191029954701662
Loss Function 관련 실무 경험 -> 나 아직 실무 경험 없는데 ㅠㅠ,LoRA,-0.0066805575,0.0,0.006680557504296303
Loss Function 관련 실무 경험 -> 나 아직 실무 경험 없는데 ㅠㅠ,LoRA 와 QLoRA 의 차이,0.014709259,0.0,0.014709259383380413
Loss Function 관련 실무 경험 -> 나 아직 실무 경험 없는데 ㅠㅠ,Loss Function 예시,-0.0043359753,0.0,0.004335975274443626
Loss Function 관련 실무 경험 -> 나 아직 실무 경험 없는데 ㅠㅠ,Loss Function 정의,0.0027308608,0.0,0.0027308608405292034
Loss Function 관련 실무 경험 -> 나 아직 실무 경험 없는데 ㅠㅠ,MBTI,0.0011100697,0.0,0.0011100696865469217
Loss Function 관련 실무 경험 -> 나 아직 실무 경험 없는데 ㅠㅠ,MSE Loss 설명,0.002192049,0.0,0.002192049054428935
Loss Function 관련 실무 경험 -> 나 아직 실무 경험 없는데 ㅠㅠ,MSE Loss 용도,-0.009318855,0.0,0.009318854659795761
Loss Function 관련 실무 경험 -> 나 아직 실무 경험 없는데 ㅠㅠ,Multi-Label 에서 CE + Softmax 적용 문제점,-0.0008735252,0.0,0.0008735252195037901
Loss Function 관련 실무 경험 -> 나 아직 실무 경험 없는데 ㅠㅠ,PEFT 방법 5가지,-0.0040979725,0.0,0.00409797253087163
Loss Function 관련 실무 경험 -> 나 아직 실무 경험 없는데 ㅠㅠ,거대 언어 모델 정의,-0.0015282503,0.0,0.0015282502863556147
Loss Function 관련 실무 경험 -> 나 아직 실무 경험 없는데 ㅠㅠ,기본 경험,0.9754526,1.0,0.02454739809036255
Loss Function 관련 실무 경험 -> 나 아직 실무 경험 없는데 ㅠㅠ,답변 실패,-0.002305172,0.0,0.0023051719181239605
Loss Function 관련 실무 경험 -> 나 아직 실무 경험 없는데 ㅠㅠ,딥러닝,-0.00930346,0.0,0.009303459897637367
Loss Function 관련 실무 경험 -> 나 아직 실무 경험 없는데 ㅠㅠ,마지막 할 말,-0.012187944,0.0,0.012187943793833256
Loss Function 관련 실무 경험 -> 나 아직 실무 경험 없는데 ㅠㅠ,머신러닝,0.004201061,0.0,0.004201061092317104
Loss Function 관련 실무 경험 -> 나 아직 실무 경험 없는데 ㅠㅠ,면접 시작 인사,-0.01897223,0.0,0.018972229212522507
Loss Function 관련 실무 경험 -> 나 아직 실무 경험 없는데 ㅠㅠ,상세 경험,0.0034981135,0.0,0.0034981134813278913
Loss Function 관련 실무 경험 -> 나 아직 실무 경험 없는데 ㅠㅠ,수식,1.4424465e-05,0.0,1.4424465007323306e-05
Loss Function 관련 실무 경험 -> 나 아직 실무 경험 없는데 ㅠㅠ,용어 질문,-0.0048339935,0.0,0.004833993501961231
Loss Function 관련 실무 경험 -> 나 아직 실무 경험 없는데 ㅠㅠ,인공지능,-0.0015510495,0.0,0.0015510495286434889
Loss Function 관련 실무 경험 -> 나 아직 실무 경험 없는데 ㅠㅠ,잠시 휴식,-0.005305917,0.0,0.005305917002260685
Loss Function 관련 실무 경험 -> 나 아직 실무 경험 없는데 ㅠㅠ,좋아하는 아이돌,0.0001396145,0.0,0.00013961449440103024
Loss Function 관련 실무 경험 -> 나 아직 실무 경험 없는데 ㅠㅠ,핵심 아이디어,-0.0056512114,0.0,0.005651211366057396
Loss Function 관련 실무 경험 -> 나 아직 실무 경험 없는데 ㅠㅠ,확률 예측에서 MSE Loss 미 사용 이유,-0.0031582816,0.0,0.003158281557261944
Loss Function 관련 실무 경험 -> 대답하기 싫은데 힝,BCE 가 좋은 task,-0.0059719305,0.0,0.005971930455416441
Loss Function 관련 실무 경험 -> 대답하기 싫은데 힝,BCE 가 좋은 이유,0.00072543626,0.0,0.000725436257198453
Loss Function 관련 실무 경험 -> 대답하기 싫은데 힝,LLM Fine-Tuning 의 PEFT,-0.0046240482,0.0,0.004624048247933388
Loss Function 관련 실무 경험 -> 대답하기 싫은데 힝,LoRA,0.003379579,0.0,0.003379578934982419
Loss Function 관련 실무 경험 -> 대답하기 싫은데 힝,LoRA 와 QLoRA 의 차이,-0.0010208356,0.0,0.0010208355961367488
Loss Function 관련 실무 경험 -> 대답하기 싫은데 힝,Loss Function 예시,-0.004103872,0.0,0.0041038719937205315
Loss Function 관련 실무 경험 -> 대답하기 싫은데 힝,Loss Function 정의,-0.010507261,0.0,0.010507261380553246
Loss Function 관련 실무 경험 -> 대답하기 싫은데 힝,MBTI,-0.0018562659,0.0,0.0018562659388408065
Loss Function 관련 실무 경험 -> 대답하기 싫은데 힝,MSE Loss 설명,-0.001328651,0.0,0.0013286509783938527
Loss Function 관련 실무 경험 -> 대답하기 싫은데 힝,MSE Loss 용도,-0.007561107,0.0,0.007561107166111469
Loss Function 관련 실무 경험 -> 대답하기 싫은데 힝,Multi-Label 에서 CE + Softmax 적용 문제점,-0.007997912,0.0,0.007997912354767323
Loss Function 관련 실무 경험 -> 대답하기 싫은데 힝,PEFT 방법 5가지,-0.009130504,0.0,0.009130503982305527
Loss Function 관련 실무 경험 -> 대답하기 싫은데 힝,거대 언어 모델 정의,-0.010392353,0.0,0.010392352938652039
Loss Function 관련 실무 경험 -> 대답하기 싫은데 힝,기본 경험,0.006528057,0.0,0.006528057157993317
Loss Function 관련 실무 경험 -> 대답하기 싫은데 힝,답변 실패,0.9926863,1.0,0.007313728332519531
Loss Function 관련 실무 경험 -> 대답하기 싫은데 힝,딥러닝,-0.0087462645,0.0,0.00874626450240612
Loss Function 관련 실무 경험 -> 대답하기 싫은데 힝,마지막 할 말,-0.0019023274,0.0,0.0019023274071514606
Loss Function 관련 실무 경험 -> 대답하기 싫은데 힝,머신러닝,-0.0093893055,0.0,0.009389305487275124
Loss Function 관련 실무 경험 -> 대답하기 싫은데 힝,면접 시작 인사,0.0013627098,0.0,0.0013627097941935062
Loss Function 관련 실무 경험 -> 대답하기 싫은데 힝,상세 경험,0.001618729,0.0,0.0016187289729714394
Loss Function 관련 실무 경험 -> 대답하기 싫은데 힝,수식,-0.0058427127,0.0,0.0058427127078175545
Loss Function 관련 실무 경험 -> 대답하기 싫은데 힝,용어 질문,-0.012022133,0.0,0.012022132985293865
Loss Function 관련 실무 경험 -> 대답하기 싫은데 힝,인공지능,-0.011543295,0.0,0.011543295346200466
Loss Function 관련 실무 경험 -> 대답하기 싫은데 힝,잠시 휴식,0.00428412,0.0,0.004284120164811611
Loss Function 관련 실무 경험 -> 대답하기 싫은데 힝,좋아하는 아이돌,-0.0038070339,0.0,0.0038070338778197765
Loss Function 관련 실무 경험 -> 대답하기 싫은데 힝,핵심 아이디어,-0.010548748,0.0,0.010548748075962067
Loss Function 관련 실무 경험 -> 대답하기 싫은데 힝,확률 예측에서 MSE Loss 미 사용 이유,-0.004635914,0.0,0.004635914228856564
Loss Function 관련 실무 경험 -> 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ,BCE 가 좋은 task,-0.0003114115,0.0,0.00031141150975599885
Loss Function 관련 실무 경험 -> 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ,BCE 가 좋은 이유,0.009554818,0.0,0.009554818272590637
Loss Function 관련 실무 경험 -> 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ,LLM Fine-Tuning 의 PEFT,0.0033243506,0.0,0.0033243505749851465
Loss Function 관련 실무 경험 -> 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ,LoRA,-0.011983647,0.0,0.011983647011220455
Loss Function 관련 실무 경험 -> 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ,LoRA 와 QLoRA 의 차이,-0.0025519216,0.0,0.0025519216433167458
Loss Function 관련 실무 경험 -> 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ,Loss Function 예시,-0.0036034603,0.0,0.003603460267186165
Loss Function 관련 실무 경험 -> 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ,Loss Function 정의,-0.0031602806,0.0,0.003160280641168356
Loss Function 관련 실무 경험 -> 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ,MBTI,-0.0104216775,0.0,0.010421677492558956
Loss Function 관련 실무 경험 -> 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ,MSE Loss 설명,0.0067020105,0.0,0.00670201051980257
Loss Function 관련 실무 경험 -> 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ,MSE Loss 용도,0.0021446105,0.0,0.00214461050927639
Loss Function 관련 실무 경험 -> 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ,Multi-Label 에서 CE + Softmax 적용 문제점,0.0047259703,0.0,0.0047259703278541565
Loss Function 관련 실무 경험 -> 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ,PEFT 방법 5가지,-9.320283e-06,0.0,9.320283425040543e-06
Loss Function 관련 실무 경험 -> 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ,거대 언어 모델 정의,0.005360282,0.0,0.005360282026231289
Loss Function 관련 실무 경험 -> 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ,기본 경험,0.96497524,1.0,0.03502476215362549
Loss Function 관련 실무 경험 -> 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ,답변 실패,0.045503452,0.0,0.04550345242023468
Loss Function 관련 실무 경험 -> 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ,딥러닝,-0.0033503824,0.0,0.003350382437929511
Loss Function 관련 실무 경험 -> 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ,마지막 할 말,-0.0072563225,0.0,0.007256322540342808
Loss Function 관련 실무 경험 -> 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ,머신러닝,-0.007039521,0.0,0.007039520889520645
Loss Function 관련 실무 경험 -> 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ,면접 시작 인사,-0.021131966,0.0,0.0211319662630558
Loss Function 관련 실무 경험 -> 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ,상세 경험,-0.015252291,0.0,0.015252291224896908
Loss Function 관련 실무 경험 -> 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ,수식,-0.0035811812,0.0,0.0035811811685562134
Loss Function 관련 실무 경험 -> 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ,용어 질문,-0.012795656,0.0,0.012795655988156796
Loss Function 관련 실무 경험 -> 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ,인공지능,-0.004298862,0.0,0.0042988620698452
Loss Function 관련 실무 경험 -> 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ,잠시 휴식,-0.010524427,0.0,0.010524426586925983
Loss Function 관련 실무 경험 -> 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ,좋아하는 아이돌,-0.009085546,0.0,0.009085546247661114
Loss Function 관련 실무 경험 -> 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ,핵심 아이디어,-0.015069372,0.0,0.015069372020661831
Loss Function 관련 실무 경험 -> 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ,확률 예측에서 MSE Loss 미 사용 이유,-0.004004715,0.0,0.004004715010523796
Loss Function 관련 실무 경험 -> 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어,BCE 가 좋은 task,-0.0115131065,0.0,0.011513106524944305
Loss Function 관련 실무 경험 -> 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어,BCE 가 좋은 이유,0.009224726,0.0,0.009224725887179375
Loss Function 관련 실무 경험 -> 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어,LLM Fine-Tuning 의 PEFT,-0.0014496177,0.0,0.0014496176736429334
Loss Function 관련 실무 경험 -> 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어,LoRA,0.017483478,0.0,0.017483478412032127
Loss Function 관련 실무 경험 -> 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어,LoRA 와 QLoRA 의 차이,0.018185763,0.0,0.01818576268851757
Loss Function 관련 실무 경험 -> 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어,Loss Function 예시,-0.008228266,0.0,0.008228265680372715
Loss Function 관련 실무 경험 -> 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어,Loss Function 정의,-0.002466343,0.0,0.0024663431104272604
Loss Function 관련 실무 경험 -> 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어,MBTI,0.010852027,0.0,0.010852026753127575
Loss Function 관련 실무 경험 -> 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어,MSE Loss 설명,0.010015822,0.0,0.010015822015702724
Loss Function 관련 실무 경험 -> 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어,MSE Loss 용도,-0.017880723,0.0,0.017880722880363464
Loss Function 관련 실무 경험 -> 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어,Multi-Label 에서 CE + Softmax 적용 문제점,0.008817881,0.0,0.008817881345748901
Loss Function 관련 실무 경험 -> 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어,PEFT 방법 5가지,-0.025492083,0.0,0.02549208328127861
Loss Function 관련 실무 경험 -> 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어,거대 언어 모델 정의,-0.009512323,0.0,0.009512322954833508
Loss Function 관련 실무 경험 -> 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어,기본 경험,0.009734013,0.0,0.009734013117849827
Loss Function 관련 실무 경험 -> 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어,답변 실패,0.0027721354,0.0,0.002772135427221656
Loss Function 관련 실무 경험 -> 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어,딥러닝,-0.009405408,0.0,0.009405408054590225
Loss Function 관련 실무 경험 -> 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어,마지막 할 말,-0.019805128,0.0,0.019805127754807472
Loss Function 관련 실무 경험 -> 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어,머신러닝,0.010577169,0.0,0.010577169246971607
Loss Function 관련 실무 경험 -> 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어,면접 시작 인사,-0.014949116,0.0,0.014949115924537182
Loss Function 관련 실무 경험 -> 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어,상세 경험,0.97007686,1.0,0.02992314100265503
Loss Function 관련 실무 경험 -> 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어,수식,-0.034075435,0.0,0.03407543525099754
Loss Function 관련 실무 경험 -> 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어,용어 질문,-0.016281296,0.0,0.01628129556775093
Loss Function 관련 실무 경험 -> 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어,인공지능,-0.011832597,0.0,0.011832596734166145
Loss Function 관련 실무 경험 -> 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어,잠시 휴식,-0.009050724,0.0,0.009050724096596241
Loss Function 관련 실무 경험 -> 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어,좋아하는 아이돌,-0.011322157,0.0,0.011322157457470894
Loss Function 관련 실무 경험 -> 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어,핵심 아이디어,-0.011715257,0.0,0.011715256609022617
Loss Function 관련 실무 경험 -> 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어,확률 예측에서 MSE Loss 미 사용 이유,-0.013305188,0.0,0.013305188156664371
Loss Function 관련 실무 경험 -> 기억 진짜 안 나는데 가물가물하다,BCE 가 좋은 task,-0.010653767,0.0,0.010653766803443432
Loss Function 관련 실무 경험 -> 기억 진짜 안 나는데 가물가물하다,BCE 가 좋은 이유,-0.005012987,0.0,0.005012987181544304
Loss Function 관련 실무 경험 -> 기억 진짜 안 나는데 가물가물하다,LLM Fine-Tuning 의 PEFT,-0.0018363551,0.0,0.0018363550771027803
Loss Function 관련 실무 경험 -> 기억 진짜 안 나는데 가물가물하다,LoRA,0.00031111762,0.0,0.00031111761927604675
Loss Function 관련 실무 경험 -> 기억 진짜 안 나는데 가물가물하다,LoRA 와 QLoRA 의 차이,-0.0030624971,0.0,0.0030624971259385347
Loss Function 관련 실무 경험 -> 기억 진짜 안 나는데 가물가물하다,Loss Function 예시,-0.0030019486,0.0,0.0030019485857337713
Loss Function 관련 실무 경험 -> 기억 진짜 안 나는데 가물가물하다,Loss Function 정의,-0.006104843,0.0,0.006104843225330114
Loss Function 관련 실무 경험 -> 기억 진짜 안 나는데 가물가물하다,MBTI,-0.0036425432,0.0,0.0036425432190299034
Loss Function 관련 실무 경험 -> 기억 진짜 안 나는데 가물가물하다,MSE Loss 설명,-0.0037923134,0.0,0.0037923133932054043
Loss Function 관련 실무 경험 -> 기억 진짜 안 나는데 가물가물하다,MSE Loss 용도,-0.004666371,0.0,0.004666370805352926
Loss Function 관련 실무 경험 -> 기억 진짜 안 나는데 가물가물하다,Multi-Label 에서 CE + Softmax 적용 문제점,-0.0062449826,0.0,0.006244982592761517
Loss Function 관련 실무 경험 -> 기억 진짜 안 나는데 가물가물하다,PEFT 방법 5가지,-0.0058467025,0.0,0.005846702493727207
Loss Function 관련 실무 경험 -> 기억 진짜 안 나는데 가물가물하다,거대 언어 모델 정의,-0.008174464,0.0,0.008174464106559753
Loss Function 관련 실무 경험 -> 기억 진짜 안 나는데 가물가물하다,기본 경험,0.008582907,0.0,0.008582906797528267
Loss Function 관련 실무 경험 -> 기억 진짜 안 나는데 가물가물하다,답변 실패,0.99340856,1.0,0.006591439247131348
Loss Function 관련 실무 경험 -> 기억 진짜 안 나는데 가물가물하다,딥러닝,-0.0071498444,0.0,0.00714984443038702
Loss Function 관련 실무 경험 -> 기억 진짜 안 나는데 가물가물하다,마지막 할 말,-0.007346772,0.0,0.007346772123128176
Loss Function 관련 실무 경험 -> 기억 진짜 안 나는데 가물가물하다,머신러닝,-0.007507011,0.0,0.007507010828703642
Loss Function 관련 실무 경험 -> 기억 진짜 안 나는데 가물가물하다,면접 시작 인사,-0.00055790227,0.0,0.0005579022690653801
Loss Function 관련 실무 경험 -> 기억 진짜 안 나는데 가물가물하다,상세 경험,0.0025923415,0.0,0.002592341508716345
Loss Function 관련 실무 경험 -> 기억 진짜 안 나는데 가물가물하다,수식,-0.007448992,0.0,0.007448992226272821
Loss Function 관련 실무 경험 -> 기억 진짜 안 나는데 가물가물하다,용어 질문,-0.011868054,0.0,0.011868054047226906
Loss Function 관련 실무 경험 -> 기억 진짜 안 나는데 가물가물하다,인공지능,-0.014410549,0.0,0.014410548843443394
Loss Function 관련 실무 경험 -> 기억 진짜 안 나는데 가물가물하다,잠시 휴식,0.0021440205,0.0,0.002144020516425371
Loss Function 관련 실무 경험 -> 기억 진짜 안 나는데 가물가물하다,좋아하는 아이돌,-0.0059759924,0.0,0.005975992418825626
Loss Function 관련 실무 경험 -> 기억 진짜 안 나는데 가물가물하다,핵심 아이디어,-0.011410833,0.0,0.011410833336412907
Loss Function 관련 실무 경험 -> 기억 진짜 안 나는데 가물가물하다,확률 예측에서 MSE Loss 미 사용 이유,-0.0062140864,0.0,0.006214086432009935
Loss Function 관련 실무 경험 -> 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ,BCE 가 좋은 task,0.0019495443,0.0,0.0019495442975312471
Loss Function 관련 실무 경험 -> 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ,BCE 가 좋은 이유,0.015778804,0.0,0.015778804197907448
Loss Function 관련 실무 경험 -> 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ,LLM Fine-Tuning 의 PEFT,-0.0009768619,0.0,0.0009768619202077389
Loss Function 관련 실무 경험 -> 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ,LoRA,-0.0042254166,0.0,0.004225416574627161
Loss Function 관련 실무 경험 -> 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ,LoRA 와 QLoRA 의 차이,0.013722962,0.0,0.013722961768507957
Loss Function 관련 실무 경험 -> 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ,Loss Function 예시,-0.007115007,0.0,0.007115006912499666
Loss Function 관련 실무 경험 -> 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ,Loss Function 정의,0.0008532165,0.0,0.0008532165084034204
Loss Function 관련 실무 경험 -> 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ,MBTI,0.0005771043,0.0,0.000577104277908802
Loss Function 관련 실무 경험 -> 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ,MSE Loss 설명,0.0013631034,0.0,0.001363103394396603
Loss Function 관련 실무 경험 -> 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ,MSE Loss 용도,-0.0076631345,0.0,0.0076631344854831696
Loss Function 관련 실무 경험 -> 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ,Multi-Label 에서 CE + Softmax 적용 문제점,-0.001360728,0.0,0.0013607280561700463
Loss Function 관련 실무 경험 -> 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ,PEFT 방법 5가지,-0.00304609,0.0,0.003046090016141534
Loss Function 관련 실무 경험 -> 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ,거대 언어 모델 정의,0.0034543069,0.0,0.003454306861385703
Loss Function 관련 실무 경험 -> 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ,기본 경험,0.9771638,1.0,0.02283620834350586
Loss Function 관련 실무 경험 -> 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ,답변 실패,-0.005003103,0.0,0.00500310305505991
Loss Function 관련 실무 경험 -> 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ,딥러닝,-0.008996658,0.0,0.008996658027172089
Loss Function 관련 실무 경험 -> 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ,마지막 할 말,-0.007549428,0.0,0.007549427915364504
Loss Function 관련 실무 경험 -> 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ,머신러닝,0.008714022,0.0,0.008714022114872932
Loss Function 관련 실무 경험 -> 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ,면접 시작 인사,-0.01577328,0.0,0.01577327959239483
Loss Function 관련 실무 경험 -> 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ,상세 경험,-0.0061367205,0.0,0.006136720534414053
Loss Function 관련 실무 경험 -> 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ,수식,0.005071894,0.0,0.00507189380005002
Loss Function 관련 실무 경험 -> 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ,용어 질문,-0.0031793774,0.0,0.0031793774105608463
Loss Function 관련 실무 경험 -> 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ,인공지능,-0.0024816934,0.0,0.0024816934019327164
Loss Function 관련 실무 경험 -> 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ,잠시 휴식,0.0018794981,0.0,0.001879498129710555
Loss Function 관련 실무 경험 -> 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ,좋아하는 아이돌,0.0029837182,0.0,0.002983718179166317
Loss Function 관련 실무 경험 -> 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ,핵심 아이디어,-0.011017349,0.0,0.011017348617315292
Loss Function 관련 실무 경험 -> 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ,확률 예측에서 MSE Loss 미 사용 이유,-0.0026010538,0.0,0.002601053798571229
Loss Function 관련 실무 경험 -> 이걸 왜 알려줘야 하지,BCE 가 좋은 task,-0.013032447,0.0,0.01303244661539793
Loss Function 관련 실무 경험 -> 이걸 왜 알려줘야 하지,BCE 가 좋은 이유,-0.0015187582,0.0,0.0015187582466751337
Loss Function 관련 실무 경험 -> 이걸 왜 알려줘야 하지,LLM Fine-Tuning 의 PEFT,-0.0011895834,0.0,0.001189583446830511
Loss Function 관련 실무 경험 -> 이걸 왜 알려줘야 하지,LoRA,0.0019935586,0.0,0.0019935586024075747
Loss Function 관련 실무 경험 -> 이걸 왜 알려줘야 하지,LoRA 와 QLoRA 의 차이,-0.0033775603,0.0,0.00337756029330194
Loss Function 관련 실무 경험 -> 이걸 왜 알려줘야 하지,Loss Function 예시,-0.0049320194,0.0,0.004932019393891096
Loss Function 관련 실무 경험 -> 이걸 왜 알려줘야 하지,Loss Function 정의,-0.007444133,0.0,0.007444133050739765
Loss Function 관련 실무 경험 -> 이걸 왜 알려줘야 하지,MBTI,-0.0031764796,0.0,0.0031764796003699303
Loss Function 관련 실무 경험 -> 이걸 왜 알려줘야 하지,MSE Loss 설명,0.001650759,0.0,0.001650759018957615
Loss Function 관련 실무 경험 -> 이걸 왜 알려줘야 하지,MSE Loss 용도,-0.0073826327,0.0,0.007382632698863745
Loss Function 관련 실무 경험 -> 이걸 왜 알려줘야 하지,Multi-Label 에서 CE + Softmax 적용 문제점,-0.006674977,0.0,0.006674977019429207
Loss Function 관련 실무 경험 -> 이걸 왜 알려줘야 하지,PEFT 방법 5가지,-0.0055530504,0.0,0.005553050432354212
Loss Function 관련 실무 경험 -> 이걸 왜 알려줘야 하지,거대 언어 모델 정의,-0.011391425,0.0,0.01139142457395792
Loss Function 관련 실무 경험 -> 이걸 왜 알려줘야 하지,기본 경험,0.020484354,0.0,0.020484354346990585
Loss Function 관련 실무 경험 -> 이걸 왜 알려줘야 하지,답변 실패,0.9928973,1.0,0.0071027278900146484
Loss Function 관련 실무 경험 -> 이걸 왜 알려줘야 하지,딥러닝,-0.013165468,0.0,0.013165468350052834
Loss Function 관련 실무 경험 -> 이걸 왜 알려줘야 하지,마지막 할 말,-0.0033001509,0.0,0.0033001508563756943
Loss Function 관련 실무 경험 -> 이걸 왜 알려줘야 하지,머신러닝,-0.009948207,0.0,0.009948207065463066
Loss Function 관련 실무 경험 -> 이걸 왜 알려줘야 하지,면접 시작 인사,0.0044186963,0.0,0.004418696276843548
Loss Function 관련 실무 경험 -> 이걸 왜 알려줘야 하지,상세 경험,0.0024823672,0.0,0.0024823672138154507
Loss Function 관련 실무 경험 -> 이걸 왜 알려줘야 하지,수식,-0.011089705,0.0,0.011089704930782318
Loss Function 관련 실무 경험 -> 이걸 왜 알려줘야 하지,용어 질문,0.0014721074,0.0,0.0014721073675900698
Loss Function 관련 실무 경험 -> 이걸 왜 알려줘야 하지,인공지능,-0.013406021,0.0,0.01340602058917284
Loss Function 관련 실무 경험 -> 이걸 왜 알려줘야 하지,잠시 휴식,-0.005367628,0.0,0.0053676278330385685
Loss Function 관련 실무 경험 -> 이걸 왜 알려줘야 하지,좋아하는 아이돌,-0.004809802,0.0,0.004809801932424307
Loss Function 관련 실무 경험 -> 이걸 왜 알려줘야 하지,핵심 아이디어,-0.006220484,0.0,0.006220484152436256
Loss Function 관련 실무 경험 -> 이걸 왜 알려줘야 하지,확률 예측에서 MSE Loss 미 사용 이유,-0.0060280687,0.0,0.006028068717569113
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,BCE 가 좋은 task,0.012034217,0.0,0.012034216895699501
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,BCE 가 좋은 이유,0.027331764,0.0,0.0273317638784647
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,LLM Fine-Tuning 의 PEFT,-0.004420079,0.0,0.0044200788252055645
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,LoRA,-0.013816501,0.0,0.013816501013934612
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,LoRA 와 QLoRA 의 차이,0.006781547,0.0,0.006781546864658594
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,Loss Function 예시,-0.005999303,0.0,0.0059993029572069645
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,Loss Function 정의,0.00621964,0.0,0.0062196399085223675
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,MBTI,0.9839973,1.0,0.01600271463394165
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,MSE Loss 설명,0.001670469,0.0,0.0016704689478501678
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,MSE Loss 용도,-0.0063334024,0.0,0.00633340235799551
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,Multi-Label 에서 CE + Softmax 적용 문제점,0.017062824,0.0,0.017062824219465256
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,PEFT 방법 5가지,0.004960018,0.0,0.0049600182101130486
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,거대 언어 모델 정의,0.006147133,0.0,0.006147133186459541
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,기본 경험,0.004087856,0.0,0.004087856039404869
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,답변 실패,-0.006868404,0.0,0.006868403870612383
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,딥러닝,-0.007720318,0.0,0.007720318157225847
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,마지막 할 말,-0.0132048605,0.0,0.013204860500991344
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,머신러닝,-0.0072663943,0.0,0.007266394328325987
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,면접 시작 인사,0.009108397,0.0,0.009108397178351879
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,상세 경험,-0.005142254,0.0,0.005142253823578358
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,수식,0.00019590528,0.0,0.00019590527517721057
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,용어 질문,-0.008549539,0.0,0.008549539372324944
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,인공지능,0.008209093,0.0,0.008209092542529106
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,잠시 휴식,-0.0017001687,0.0,0.0017001687083393335
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,좋아하는 아이돌,0.0015302957,0.0,0.001530295703560114
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,핵심 아이디어,0.006901244,0.0,0.006901244167238474
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,확률 예측에서 MSE Loss 미 사용 이유,0.008471215,0.0,0.008471215143799782
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,BCE 가 좋은 task,0.0018429802,0.0,0.0018429801566526294
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,BCE 가 좋은 이유,-0.0017586668,0.0,0.0017586668254807591
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,LLM Fine-Tuning 의 PEFT,0.003881355,0.0,0.0038813550490885973
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,LoRA,0.0038068919,0.0,0.0038068918511271477
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,LoRA 와 QLoRA 의 차이,0.008605988,0.0,0.008605987764894962
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,Loss Function 예시,-0.00874299,0.0,0.008742989972233772
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,Loss Function 정의,0.006722812,0.0,0.006722812075167894
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,MBTI,0.003828265,0.0,0.003828265005722642
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,MSE Loss 설명,-0.016950844,0.0,0.01695084385573864
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,MSE Loss 용도,0.002891308,0.0,0.00289130792953074
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,Multi-Label 에서 CE + Softmax 적용 문제점,0.004690905,0.0,0.004690905101597309
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,PEFT 방법 5가지,0.010737689,0.0,0.010737689211964607
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,거대 언어 모델 정의,0.014873385,0.0,0.014873385429382324
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,기본 경험,-0.009147506,0.0,0.009147506207227707
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,답변 실패,-0.010211742,0.0,0.010211741551756859
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,딥러닝,0.000115881,0.0,0.00011588099732762203
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,마지막 할 말,-0.020226164,0.0,0.020226163789629936
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,머신러닝,-0.008797119,0.0,0.008797119371592999
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,면접 시작 인사,0.0013656353,0.0,0.0013656353112310171
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,상세 경험,0.00065003656,0.0,0.0006500365561805665
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,수식,0.015200423,0.0,0.015200423076748848
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,용어 질문,-0.0025094151,0.0,0.0025094151496887207
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,인공지능,0.009969985,0.0,0.009969985112547874
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,잠시 휴식,0.00033052755,0.0,0.0003305275458842516
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,좋아하는 아이돌,0.98800427,1.0,0.01199573278427124
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,핵심 아이디어,-0.021245167,0.0,0.021245166659355164
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,확률 예측에서 MSE Loss 미 사용 이유,0.00446196,0.0,0.004461959935724735
잠시 휴식 -> 재미있는 이야기 해줄래?,BCE 가 좋은 task,0.0008122831,0.0,0.0008122830768115819
잠시 휴식 -> 재미있는 이야기 해줄래?,BCE 가 좋은 이유,0.002306852,0.0,0.002306852024048567
잠시 휴식 -> 재미있는 이야기 해줄래?,LLM Fine-Tuning 의 PEFT,-0.00084432116,0.0,0.0008443211554549634
잠시 휴식 -> 재미있는 이야기 해줄래?,LoRA,0.0018239483,0.0,0.0018239483470097184
잠시 휴식 -> 재미있는 이야기 해줄래?,LoRA 와 QLoRA 의 차이,0.0012245909,0.0,0.0012245909310877323
잠시 휴식 -> 재미있는 이야기 해줄래?,Loss Function 예시,0.0032431716,0.0,0.0032431716099381447
잠시 휴식 -> 재미있는 이야기 해줄래?,Loss Function 정의,-0.0023527192,0.0,0.002352719195187092
잠시 휴식 -> 재미있는 이야기 해줄래?,MBTI,-0.00030911245,0.0,0.00030911245266906917
잠시 휴식 -> 재미있는 이야기 해줄래?,MSE Loss 설명,-0.009478221,0.0,0.009478220716118813
잠시 휴식 -> 재미있는 이야기 해줄래?,MSE Loss 용도,-0.007171708,0.0,0.007171708159148693
잠시 휴식 -> 재미있는 이야기 해줄래?,Multi-Label 에서 CE + Softmax 적용 문제점,0.0050667394,0.0,0.005066739395260811
잠시 휴식 -> 재미있는 이야기 해줄래?,PEFT 방법 5가지,-0.009666611,0.0,0.009666611440479755
잠시 휴식 -> 재미있는 이야기 해줄래?,거대 언어 모델 정의,0.0012554624,0.0,0.0012554624117910862
잠시 휴식 -> 재미있는 이야기 해줄래?,기본 경험,-0.008650049,0.0,0.00865004863590002
잠시 휴식 -> 재미있는 이야기 해줄래?,답변 실패,-0.0029628836,0.0,0.002962883561849594
잠시 휴식 -> 재미있는 이야기 해줄래?,딥러닝,-0.0011186939,0.0,0.0011186938500031829
잠시 휴식 -> 재미있는 이야기 해줄래?,마지막 할 말,0.0062654973,0.0,0.006265497300773859
잠시 휴식 -> 재미있는 이야기 해줄래?,머신러닝,-0.0028840099,0.0,0.0028840098530054092
잠시 휴식 -> 재미있는 이야기 해줄래?,면접 시작 인사,0.0034746458,0.0,0.003474645782262087
잠시 휴식 -> 재미있는 이야기 해줄래?,상세 경험,0.0041310657,0.0,0.004131065681576729
잠시 휴식 -> 재미있는 이야기 해줄래?,수식,-0.00027848955,0.0,0.00027848954778164625
잠시 휴식 -> 재미있는 이야기 해줄래?,용어 질문,-0.0036549573,0.0,0.0036549572832882404
잠시 휴식 -> 재미있는 이야기 해줄래?,인공지능,0.0047903038,0.0,0.004790303763002157
잠시 휴식 -> 재미있는 이야기 해줄래?,잠시 휴식,0.9930616,1.0,0.0069383978843688965
잠시 휴식 -> 재미있는 이야기 해줄래?,좋아하는 아이돌,-0.0025865287,0.0,0.0025865286588668823
잠시 휴식 -> 재미있는 이야기 해줄래?,핵심 아이디어,-0.004630297,0.0,0.00463029695674777
잠시 휴식 -> 재미있는 이야기 해줄래?,확률 예측에서 MSE Loss 미 사용 이유,-0.008682558,0.0,0.008682558313012123
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",BCE 가 좋은 task,0.0016408447,0.0,0.0016408447409048676
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",BCE 가 좋은 이유,-0.010150904,0.0,0.010150903835892677
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",LLM Fine-Tuning 의 PEFT,0.9770487,1.0,0.02295130491256714
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",LoRA,-0.05917679,0.0,0.059176791459321976
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",LoRA 와 QLoRA 의 차이,-0.0015898214,0.0,0.0015898214187473059
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",Loss Function 예시,-0.027765851,0.0,0.027765851467847824
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",Loss Function 정의,-0.02223451,0.0,0.022234510630369186
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",MBTI,-0.0023371917,0.0,0.0023371917195618153
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",MSE Loss 설명,-0.019480348,0.0,0.019480347633361816
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",MSE Loss 용도,-0.013693904,0.0,0.01369390357285738
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",Multi-Label 에서 CE + Softmax 적용 문제점,0.006515598,0.0,0.006515597924590111
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",PEFT 방법 5가지,-0.004674944,0.0,0.004674944095313549
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",거대 언어 모델 정의,-0.005289638,0.0,0.005289637949317694
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",기본 경험,-0.0030568666,0.0,0.003056866582483053
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",답변 실패,0.034855228,0.0,0.034855227917432785
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",딥러닝,0.0011863447,0.0,0.001186344656161964
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",마지막 할 말,0.004768312,0.0,0.004768311977386475
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",머신러닝,-0.029133055,0.0,0.029133055359125137
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",면접 시작 인사,0.010379667,0.0,0.010379667393863201
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",상세 경험,-0.018088702,0.0,0.018088702112436295
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",수식,0.009957845,0.0,0.009957845322787762
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",용어 질문,0.030899117,0.0,0.03089911676943302
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",인공지능,-0.016144643,0.0,0.0161446426063776
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",잠시 휴식,0.005072251,0.0,0.005072250962257385
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",좋아하는 아이돌,-0.0020419,0.0,0.0020419000647962093
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",핵심 아이디어,-0.0044943895,0.0,0.004494389519095421
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",확률 예측에서 MSE Loss 미 사용 이유,0.008518716,0.0,0.00851871632039547
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,BCE 가 좋은 task,-0.015215988,0.0,0.015215988270938396
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,BCE 가 좋은 이유,-0.0044869767,0.0,0.004486976657062769
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,LLM Fine-Tuning 의 PEFT,0.005815938,0.0,0.005815938115119934
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,LoRA,-0.008395353,0.0,0.008395353332161903
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,LoRA 와 QLoRA 의 차이,-0.00526035,0.0,0.0052603501826524734
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,Loss Function 예시,-0.020333637,0.0,0.020333636552095413
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,Loss Function 정의,0.00023347016,0.0,0.00023347015667241067
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,MBTI,0.0064428155,0.0,0.0064428155310451984
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,MSE Loss 설명,-0.006742357,0.0,0.006742356810718775
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,MSE Loss 용도,-0.01350597,0.0,0.013505970127880573
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,Multi-Label 에서 CE + Softmax 적용 문제점,-0.0013218217,0.0,0.0013218217063695192
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,PEFT 방법 5가지,-0.010381654,0.0,0.010381653904914856
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,거대 언어 모델 정의,-0.0067086504,0.0,0.006708650384098291
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,기본 경험,-0.006312565,0.0,0.006312564946711063
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,답변 실패,0.98724693,1.0,0.012753069400787354
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,딥러닝,-0.015497511,0.0,0.015497511252760887
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,마지막 할 말,-0.0033939853,0.0,0.003393985331058502
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,머신러닝,-0.0075988416,0.0,0.007598841562867165
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,면접 시작 인사,-0.0004469643,0.0,0.00044696428813040257
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,상세 경험,-0.005796746,0.0,0.005796745885163546
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,수식,-0.010143844,0.0,0.010143844410777092
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,용어 질문,0.061866283,0.0,0.06186628341674805
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,인공지능,-0.0064215437,0.0,0.006421543657779694
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,잠시 휴식,-0.0030137938,0.0,0.003013793844729662
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,좋아하는 아이돌,-0.0139563875,0.0,0.013956387527287006
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,핵심 아이디어,-0.0007820393,0.0,0.0007820393075235188
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,확률 예측에서 MSE Loss 미 사용 이유,-0.011207101,0.0,0.011207100935280323
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야!,BCE 가 좋은 task,-0.0034893295,0.0,0.003489329479634762
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야!,BCE 가 좋은 이유,-0.0119997,0.0,0.011999700218439102
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야!,LLM Fine-Tuning 의 PEFT,0.9818438,1.0,0.01815617084503174
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야!,LoRA,-0.04544845,0.0,0.04544844850897789
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야!,LoRA 와 QLoRA 의 차이,-0.00733574,0.0,0.007335740141570568
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야!,Loss Function 예시,-0.024538863,0.0,0.024538863450288773
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야!,Loss Function 정의,-0.019885838,0.0,0.0198858380317688
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야!,MBTI,-0.0009253366,0.0,0.0009253366151824594
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야!,MSE Loss 설명,-0.02269026,0.0,0.022690260782837868
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야!,MSE Loss 용도,-0.01548386,0.0,0.015483859926462173
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야!,Multi-Label 에서 CE + Softmax 적용 문제점,-0.0068736444,0.0,0.006873644422739744
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야!,PEFT 방법 5가지,-0.010115021,0.0,0.010115020908415318
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야!,거대 언어 모델 정의,0.0027757403,0.0,0.0027757403440773487
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야!,기본 경험,-0.0035396733,0.0,0.0035396732855588198
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야!,답변 실패,0.013878744,0.0,0.013878744095563889
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야!,딥러닝,0.009371118,0.0,0.009371117688715458
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야!,마지막 할 말,-0.00019181606,0.0,0.00019181605603080243
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야!,머신러닝,-0.016569072,0.0,0.016569072380661964
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야!,면접 시작 인사,0.0070304135,0.0,0.00703041348606348
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야!,상세 경험,-0.00039593532,0.0,0.0003959353198297322
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야!,수식,-0.00095883093,0.0,0.0009588309330865741
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야!,용어 질문,-0.0046605617,0.0,0.004660561680793762
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야!,인공지능,-0.009716457,0.0,0.00971645675599575
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야!,잠시 휴식,-0.0037111717,0.0,0.003711171681061387
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야!,좋아하는 아이돌,0.005814639,0.0,0.0058146389201283455
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야!,핵심 아이디어,-0.014975466,0.0,0.014975465834140778
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야!,확률 예측에서 MSE Loss 미 사용 이유,-0.0077808015,0.0,0.007780801504850388
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다,BCE 가 좋은 task,-0.007681195,0.0,0.0076811951585114
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다,BCE 가 좋은 이유,-0.0017928565,0.0,0.0017928564921021461
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다,LLM Fine-Tuning 의 PEFT,0.0022724383,0.0,0.002272438257932663
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다,LoRA,-0.00031263495,0.0,0.00031263494747690856
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다,LoRA 와 QLoRA 의 차이,-0.0019274099,0.0,0.0019274099031463265
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다,Loss Function 예시,-0.005933803,0.0,0.005933803040534258
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다,Loss Function 정의,-0.0025417348,0.0,0.0025417348369956017
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다,MBTI,-0.003173916,0.0,0.0031739159021526575
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다,MSE Loss 설명,-0.0056155743,0.0,0.005615574307739735
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다,MSE Loss 용도,-0.00957107,0.0,0.009571069851517677
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다,Multi-Label 에서 CE + Softmax 적용 문제점,-0.00638253,0.0,0.006382530089467764
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다,PEFT 방법 5가지,-0.0060746195,0.0,0.0060746194794774055
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다,거대 언어 모델 정의,-0.0058854907,0.0,0.00588549068197608
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다,기본 경험,0.0043100608,0.0,0.004310060758143663
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다,답변 실패,0.9946736,1.0,0.005326390266418457
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다,딥러닝,-0.009589279,0.0,0.00958927907049656
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다,마지막 할 말,-0.0040651113,0.0,0.00406511127948761
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다,머신러닝,-0.011175063,0.0,0.01117506343871355
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다,면접 시작 인사,-0.0011982953,0.0,0.001198295271024108
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다,상세 경험,-0.0025343911,0.0,0.002534391125664115
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다,수식,-0.008134229,0.0,0.008134229108691216
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다,용어 질문,-0.0044035744,0.0,0.004403574392199516
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다,인공지능,-0.01189324,0.0,0.011893239803612232
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다,잠시 휴식,-0.00280801,0.0,0.002808009972795844
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다,좋아하는 아이돌,-0.0051428624,0.0,0.005142862442880869
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다,핵심 아이디어,-0.009707075,0.0,0.009707074612379074
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다,확률 예측에서 MSE Loss 미 사용 이유,-0.0061395303,0.0,0.006139530334621668
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",BCE 가 좋은 task,-0.003409303,0.0,0.0034093030262738466
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",BCE 가 좋은 이유,-0.0104888575,0.0,0.010488857515156269
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",LLM Fine-Tuning 의 PEFT,-0.0018830543,0.0,0.0018830542685464025
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",LoRA,-0.008397831,0.0,0.00839783065021038
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",LoRA 와 QLoRA 의 차이,-0.004829759,0.0,0.004829758778214455
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",Loss Function 예시,0.00046941513,0.0,0.00046941512846387923
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",Loss Function 정의,-0.0063482937,0.0,0.006348293740302324
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",MBTI,-0.0063479925,0.0,0.006347992457449436
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",MSE Loss 설명,0.01913016,0.0,0.0191301591694355
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",MSE Loss 용도,0.010764684,0.0,0.010764683596789837
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",Multi-Label 에서 CE + Softmax 적용 문제점,0.007818217,0.0,0.007818217389285564
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",PEFT 방법 5가지,0.9793997,1.0,0.020600318908691406
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",거대 언어 모델 정의,-0.015974768,0.0,0.01597476750612259
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",기본 경험,0.0052022585,0.0,0.005202258471399546
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",답변 실패,-0.003243654,0.0,0.0032436540350317955
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",딥러닝,-0.009480549,0.0,0.009480549022555351
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",마지막 할 말,-0.013219192,0.0,0.013219191692769527
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",머신러닝,0.00040413055,0.0,0.000404130551032722
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",면접 시작 인사,-0.00019591069,0.0,0.00019591068848967552
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",상세 경험,-0.0041989605,0.0,0.004198960494250059
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",수식,0.02001752,0.0,0.02001751959323883
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",용어 질문,-0.022010386,0.0,0.022010385990142822
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",인공지능,0.0014999192,0.0,0.0014999192208051682
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",잠시 휴식,-0.015161154,0.0,0.015161153860390186
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",좋아하는 아이돌,0.01400815,0.0,0.014008150435984135
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",핵심 아이디어,-0.030832015,0.0,0.030832014977931976
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",확률 예측에서 MSE Loss 미 사용 이유,0.0030586717,0.0,0.0030586717184633017
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,BCE 가 좋은 task,-0.004207416,0.0,0.004207415971904993
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,BCE 가 좋은 이유,0.0030378518,0.0,0.003037851769477129
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,LLM Fine-Tuning 의 PEFT,-0.0027132165,0.0,0.002713216468691826
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,LoRA,0.0012586672,0.0,0.0012586672091856599
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,LoRA 와 QLoRA 의 차이,-0.0010932662,0.0,0.0010932661825791001
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,Loss Function 예시,0.0027073494,0.0,0.002707349369302392
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,Loss Function 정의,-0.010459125,0.0,0.01045912504196167
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,MBTI,-0.0031273952,0.0,0.003127395175397396
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,MSE Loss 설명,-0.0041126767,0.0,0.004112676717340946
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,MSE Loss 용도,-0.007818928,0.0,0.007818927988409996
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,Multi-Label 에서 CE + Softmax 적용 문제점,-0.0086794095,0.0,0.008679409511387348
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,PEFT 방법 5가지,0.005669435,0.0,0.005669435020536184
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,거대 언어 모델 정의,-0.019296777,0.0,0.01929677650332451
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,기본 경험,0.0012798178,0.0,0.001279817777685821
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,답변 실패,0.99325514,1.0,0.006744861602783203
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,딥러닝,-0.0059320466,0.0,0.005932046566158533
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,마지막 할 말,-0.0047851633,0.0,0.004785163328051567
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,머신러닝,-0.007499492,0.0,0.007499491795897484
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,면접 시작 인사,-0.005045824,0.0,0.005045824218541384
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,상세 경험,-0.0042479527,0.0,0.004247952718287706
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,수식,-0.0060739764,0.0,0.0060739764012396336
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,용어 질문,-0.021911075,0.0,0.021911075338721275
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,인공지능,-0.013004076,0.0,0.013004075735807419
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,잠시 휴식,-0.00012112509,0.0,0.00012112509284634143
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,좋아하는 아이돌,0.00021636927,0.0,0.00021636926976498216
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,핵심 아이디어,-0.0109276455,0.0,0.010927645489573479
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,확률 예측에서 MSE Loss 미 사용 이유,-0.0008248391,0.0,0.0008248391095548868
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,BCE 가 좋은 task,-0.0023284063,0.0,0.002328406320884824
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,BCE 가 좋은 이유,-0.0128689,0.0,0.01286889985203743
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,LLM Fine-Tuning 의 PEFT,-0.043440357,0.0,0.043440356850624084
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,LoRA,0.9806646,1.0,0.019335389137268066
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,LoRA 와 QLoRA 의 차이,0.017463388,0.0,0.017463387921452522
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,Loss Function 예시,-0.006114757,0.0,0.006114757154136896
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,Loss Function 정의,-0.02516942,0.0,0.02516941912472248
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,MBTI,-0.02547262,0.0,0.025472620502114296
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,MSE Loss 설명,-0.03636837,0.0,0.036368370056152344
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,MSE Loss 용도,-0.011885812,0.0,0.0118858115747571
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,Multi-Label 에서 CE + Softmax 적용 문제점,0.020325499,0.0,0.020325498655438423
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,PEFT 방법 5가지,0.0060731103,0.0,0.006073110271245241
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,거대 언어 모델 정의,-0.023502542,0.0,0.023502541705965996
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,기본 경험,-0.006756257,0.0,0.006756256800144911
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,답변 실패,0.015479168,0.0,0.01547916792333126
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,딥러닝,-0.020386329,0.0,0.020386328920722008
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,마지막 할 말,-0.014470037,0.0,0.014470037072896957
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,머신러닝,-0.01671006,0.0,0.016710059717297554
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,면접 시작 인사,-0.0035853144,0.0,0.003585314378142357
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,상세 경험,0.010872458,0.0,0.01087245810776949
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,수식,-0.0006527575,0.0,0.0006527574732899666
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,용어 질문,-0.0061450875,0.0,0.006145087536424398
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,인공지능,-0.011567253,0.0,0.011567252688109875
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,잠시 휴식,-0.007241235,0.0,0.007241235114634037
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,좋아하는 아이돌,-0.011533786,0.0,0.011533785611391068
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,핵심 아이디어,-0.0026907555,0.0,0.0026907555293291807
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,확률 예측에서 MSE Loss 미 사용 이유,0.0002151778,0.0,0.00021517780260182917
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,BCE 가 좋은 task,-0.010287574,0.0,0.01028757356107235
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,BCE 가 좋은 이유,0.00062819355,0.0,0.0006281935493461788
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,LLM Fine-Tuning 의 PEFT,-0.003561615,0.0,0.003561615012586117
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,LoRA,0.006530456,0.0,0.006530455779284239
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,LoRA 와 QLoRA 의 차이,-0.004198325,0.0,0.004198324866592884
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,Loss Function 예시,-7.900999e-06,0.0,7.900998753029853e-06
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,Loss Function 정의,-0.009371268,0.0,0.009371267631649971
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,MBTI,-0.0019211788,0.0,0.0019211787730455399
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,MSE Loss 설명,-0.0011573121,0.0,0.0011573120718821883
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,MSE Loss 용도,-0.009554828,0.0,0.009554827585816383
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,Multi-Label 에서 CE + Softmax 적용 문제점,-0.0018419011,0.0,0.0018419011030346155
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,PEFT 방법 5가지,-0.008297885,0.0,0.008297884836792946
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,거대 언어 모델 정의,-0.013025451,0.0,0.013025451451539993
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,기본 경험,0.0063561886,0.0,0.00635618856176734
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,답변 실패,0.9933303,1.0,0.0066697001457214355
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,딥러닝,-0.005989805,0.0,0.0059898048639297485
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,마지막 할 말,-0.002605133,0.0,0.0026051329914480448
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,머신러닝,-0.005087747,0.0,0.005087746772915125
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,면접 시작 인사,-0.0007920084,0.0,0.0007920084171928465
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,상세 경험,-0.0009750245,0.0,0.0009750244789756835
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,수식,-0.0143989315,0.0,0.01439893152564764
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,용어 질문,-0.01833677,0.0,0.018336769193410873
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,인공지능,-0.011003413,0.0,0.011003413237631321
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,잠시 휴식,0.0078546675,0.0,0.007854667492210865
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,좋아하는 아이돌,-0.0068746097,0.0,0.006874609738588333
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,핵심 아이디어,-0.005453062,0.0,0.005453061778098345
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,확률 예측에서 MSE Loss 미 사용 이유,-0.00029842346,0.0,0.0002984234597533941
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야!,BCE 가 좋은 task,0.002325283,0.0,0.002325282897800207
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야!,BCE 가 좋은 이유,-0.01676095,0.0,0.016760950908064842
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야!,LLM Fine-Tuning 의 PEFT,-0.039121214,0.0,0.03912121430039406
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야!,LoRA,0.9825717,1.0,0.017428278923034668
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야!,LoRA 와 QLoRA 의 차이,0.0049994593,0.0,0.004999459255486727
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야!,Loss Function 예시,-0.015027479,0.0,0.015027479268610477
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야!,Loss Function 정의,-0.012190033,0.0,0.012190032750368118
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야!,MBTI,-0.004541291,0.0,0.004541290923953056
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야!,MSE Loss 설명,-0.02220099,0.0,0.022200990468263626
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야!,MSE Loss 용도,-0.00840145,0.0,0.008401449769735336
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야!,Multi-Label 에서 CE + Softmax 적용 문제점,0.015056896,0.0,0.015056896023452282
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야!,PEFT 방법 5가지,-0.004311154,0.0,0.004311154130846262
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야!,거대 언어 모델 정의,-0.0073409854,0.0,0.0073409853503108025
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야!,기본 경험,-0.0035638416,0.0,0.0035638415720313787
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야!,답변 실패,0.0030124444,0.0,0.003012444358319044
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야!,딥러닝,-0.023561087,0.0,0.023561086505651474
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야!,마지막 할 말,-0.010124715,0.0,0.01012471504509449
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야!,머신러닝,-0.011063372,0.0,0.011063371784985065
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야!,면접 시작 인사,-0.0028706095,0.0,0.0028706095181405544
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야!,상세 경험,0.014678341,0.0,0.014678341336548328
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야!,수식,-0.016286746,0.0,0.01628674566745758
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야!,용어 질문,-0.010455006,0.0,0.010455005802214146
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야!,인공지능,0.0009686596,0.0,0.0009686595876701176
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야!,잠시 휴식,-0.0075916345,0.0,0.007591634523123503
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야!,좋아하는 아이돌,-0.005862314,0.0,0.005862313788384199
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야!,핵심 아이디어,0.0049842363,0.0,0.0049842363223433495
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야!,확률 예측에서 MSE Loss 미 사용 이유,-0.008004563,0.0,0.008004562929272652
LoRA -> 무슨 OOM 없앤다는 것 같은데,BCE 가 좋은 task,-0.004743247,0.0,0.004743246827274561
LoRA -> 무슨 OOM 없앤다는 것 같은데,BCE 가 좋은 이유,0.0014319471,0.0,0.0014319471083581448
LoRA -> 무슨 OOM 없앤다는 것 같은데,LLM Fine-Tuning 의 PEFT,-0.003410488,0.0,0.003410487901419401
LoRA -> 무슨 OOM 없앤다는 것 같은데,LoRA,0.005090012,0.0,0.005090012215077877
LoRA -> 무슨 OOM 없앤다는 것 같은데,LoRA 와 QLoRA 의 차이,-0.00089169416,0.0,0.0008916941587813199
LoRA -> 무슨 OOM 없앤다는 것 같은데,Loss Function 예시,-0.0048512686,0.0,0.004851268604397774
LoRA -> 무슨 OOM 없앤다는 것 같은데,Loss Function 정의,-0.0074289087,0.0,0.007428908720612526
LoRA -> 무슨 OOM 없앤다는 것 같은데,MBTI,0.0010503312,0.0,0.0010503311641514301
LoRA -> 무슨 OOM 없앤다는 것 같은데,MSE Loss 설명,-0.0043029245,0.0,0.004302924498915672
LoRA -> 무슨 OOM 없앤다는 것 같은데,MSE Loss 용도,-0.009390773,0.0,0.009390773251652718
LoRA -> 무슨 OOM 없앤다는 것 같은데,Multi-Label 에서 CE + Softmax 적용 문제점,-0.004593033,0.0,0.004593032877892256
LoRA -> 무슨 OOM 없앤다는 것 같은데,PEFT 방법 5가지,-0.0038004545,0.0,0.0038004545494914055
LoRA -> 무슨 OOM 없앤다는 것 같은데,거대 언어 모델 정의,-0.009594511,0.0,0.009594511240720749
LoRA -> 무슨 OOM 없앤다는 것 같은데,기본 경험,0.00025188932,0.0,0.00025188931613229215
LoRA -> 무슨 OOM 없앤다는 것 같은데,답변 실패,0.9959477,1.0,0.004052281379699707
LoRA -> 무슨 OOM 없앤다는 것 같은데,딥러닝,-0.006767338,0.0,0.006767338141798973
LoRA -> 무슨 OOM 없앤다는 것 같은데,마지막 할 말,-0.0017910524,0.0,0.0017910524038597941
LoRA -> 무슨 OOM 없앤다는 것 같은데,머신러닝,-0.008578478,0.0,0.00857847835868597
LoRA -> 무슨 OOM 없앤다는 것 같은데,면접 시작 인사,-0.0017213018,0.0,0.0017213018145412207
LoRA -> 무슨 OOM 없앤다는 것 같은데,상세 경험,-0.0016718131,0.0,0.0016718130791559815
LoRA -> 무슨 OOM 없앤다는 것 같은데,수식,-0.009039514,0.0,0.009039513766765594
LoRA -> 무슨 OOM 없앤다는 것 같은데,용어 질문,-0.0142066525,0.0,0.014206652529537678
LoRA -> 무슨 OOM 없앤다는 것 같은데,인공지능,-0.008741762,0.0,0.008741761557757854
LoRA -> 무슨 OOM 없앤다는 것 같은데,잠시 휴식,-0.00062626455,0.0,0.0006262645474635065
LoRA -> 무슨 OOM 없앤다는 것 같은데,좋아하는 아이돌,-0.0039492925,0.0,0.003949292469769716
LoRA -> 무슨 OOM 없앤다는 것 같은데,핵심 아이디어,-0.007012495,0.0,0.007012494839727879
LoRA -> 무슨 OOM 없앤다는 것 같은데,확률 예측에서 MSE Loss 미 사용 이유,-0.006590364,0.0,0.006590364035218954
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,BCE 가 좋은 task,-0.009757908,0.0,0.009757908061146736
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,BCE 가 좋은 이유,-0.011489764,0.0,0.011489763855934143
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,LLM Fine-Tuning 의 PEFT,-0.009595363,0.0,0.009595363400876522
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,LoRA,-0.00030859935,0.0,0.00030859935213811696
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,LoRA 와 QLoRA 의 차이,0.98465735,1.0,0.015342652797698975
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,Loss Function 예시,-0.004669296,0.0,0.0046692960895597935
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,Loss Function 정의,-0.02065701,0.0,0.0206570103764534
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,MBTI,0.023317501,0.0,0.023317500948905945
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,MSE Loss 설명,-0.026113706,0.0,0.026113705709576607
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,MSE Loss 용도,-0.0014759401,0.0,0.001475940109230578
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,Multi-Label 에서 CE + Softmax 적용 문제점,0.005713752,0.0,0.005713752005249262
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,PEFT 방법 5가지,-0.019361198,0.0,0.01936119794845581
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,거대 언어 모델 정의,-0.0011448961,0.0,0.001144896144978702
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,기본 경험,0.007361199,0.0,0.007361198775470257
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,답변 실패,-0.0036645362,0.0,0.0036645361687988043
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,딥러닝,0.0070426576,0.0,0.00704265758395195
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,마지막 할 말,-0.012269318,0.0,0.012269318103790283
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,머신러닝,-0.0034129724,0.0,0.0034129724372178316
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,면접 시작 인사,-0.0025531119,0.0,0.0025531118735671043
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,상세 경험,-0.013751023,0.0,0.013751023449003696
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,수식,0.005406446,0.0,0.005406445823609829
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,용어 질문,0.005662048,0.0,0.005662047769874334
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,인공지능,-0.01612766,0.0,0.016127660870552063
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,잠시 휴식,-0.002196934,0.0,0.002196934074163437
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,좋아하는 아이돌,0.00043094548,0.0,0.0004309454816393554
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,핵심 아이디어,0.011743424,0.0,0.011743424460291862
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,확률 예측에서 MSE Loss 미 사용 이유,0.011954288,0.0,0.011954287998378277
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,BCE 가 좋은 task,-0.00576852,0.0,0.005768519826233387
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,BCE 가 좋은 이유,0.0030450057,0.0,0.0030450057238340378
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,LLM Fine-Tuning 의 PEFT,-0.0017392017,0.0,0.0017392017180100083
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,LoRA,-0.00019872336,0.0,0.0001987233554245904
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,LoRA 와 QLoRA 의 차이,0.009927167,0.0,0.009927166625857353
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,Loss Function 예시,0.0018995653,0.0,0.0018995653372257948
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,Loss Function 정의,-0.011977081,0.0,0.011977081187069416
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,MBTI,-0.003575567,0.0,0.0035755669232457876
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,MSE Loss 설명,-0.0026533366,0.0,0.0026533366180956364
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,MSE Loss 용도,-0.007322973,0.0,0.007322973106056452
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,Multi-Label 에서 CE + Softmax 적용 문제점,-0.0036538932,0.0,0.0036538932472467422
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,PEFT 방법 5가지,-0.0040760334,0.0,0.004076033364981413
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,거대 언어 모델 정의,-0.014404878,0.0,0.01440487802028656
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,기본 경험,-0.00042647636,0.0,0.0004264763556420803
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,답변 실패,0.9922836,1.0,0.00771641731262207
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,딥러닝,-0.0060089817,0.0,0.006008981727063656
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,마지막 할 말,-0.0005873779,0.0,0.0005873778718523681
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,머신러닝,-0.007394096,0.0,0.007394095882773399
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,면접 시작 인사,-0.004683888,0.0,0.004683888051658869
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,상세 경험,-0.0024111567,0.0,0.0024111566599458456
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,수식,-0.007981356,0.0,0.007981356233358383
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,용어 질문,-0.02271502,0.0,0.022715020924806595
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,인공지능,-0.014320635,0.0,0.014320635236799717
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,잠시 휴식,0.00020969298,0.0,0.0002096929820254445
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,좋아하는 아이돌,-0.005534185,0.0,0.005534185096621513
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,핵심 아이디어,-0.006484145,0.0,0.0064841448329389095
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,확률 예측에서 MSE Loss 미 사용 이유,-0.0056988895,0.0,0.005698889493942261
마지막 할 말 -> 로라야 정말 고마워!,BCE 가 좋은 task,0.0063499906,0.0,0.006349990610033274
마지막 할 말 -> 로라야 정말 고마워!,BCE 가 좋은 이유,0.0047577727,0.0,0.0047577726654708385
마지막 할 말 -> 로라야 정말 고마워!,LLM Fine-Tuning 의 PEFT,-0.0013116897,0.0,0.0013116897316649556
마지막 할 말 -> 로라야 정말 고마워!,LoRA,0.0027664823,0.0,0.00276648229919374
마지막 할 말 -> 로라야 정말 고마워!,LoRA 와 QLoRA 의 차이,-0.004681207,0.0,0.004681206773966551
마지막 할 말 -> 로라야 정말 고마워!,Loss Function 예시,-0.0041268896,0.0,0.004126889631152153
마지막 할 말 -> 로라야 정말 고마워!,Loss Function 정의,0.009107056,0.0,0.009107056073844433
마지막 할 말 -> 로라야 정말 고마워!,MBTI,-0.0078068804,0.0,0.00780688039958477
마지막 할 말 -> 로라야 정말 고마워!,MSE Loss 설명,0.002226525,0.0,0.0022265249863266945
마지막 할 말 -> 로라야 정말 고마워!,MSE Loss 용도,-0.013646771,0.0,0.01364677120000124
마지막 할 말 -> 로라야 정말 고마워!,Multi-Label 에서 CE + Softmax 적용 문제점,0.002151431,0.0,0.0021514310501515865
마지막 할 말 -> 로라야 정말 고마워!,PEFT 방법 5가지,-0.00938959,0.0,0.009389589540660381
마지막 할 말 -> 로라야 정말 고마워!,거대 언어 모델 정의,-0.009438342,0.0,0.009438342414796352
마지막 할 말 -> 로라야 정말 고마워!,기본 경험,-0.010444188,0.0,0.010444187559187412
마지막 할 말 -> 로라야 정말 고마워!,답변 실패,-0.0027016837,0.0,0.0027016836684197187
마지막 할 말 -> 로라야 정말 고마워!,딥러닝,0.015569284,0.0,0.015569283626973629
마지막 할 말 -> 로라야 정말 고마워!,마지막 할 말,0.99309725,1.0,0.006902754306793213
마지막 할 말 -> 로라야 정말 고마워!,머신러닝,-0.0074625257,0.0,0.007462525740265846
마지막 할 말 -> 로라야 정말 고마워!,면접 시작 인사,-0.008461157,0.0,0.008461156859993935
마지막 할 말 -> 로라야 정말 고마워!,상세 경험,-0.00081618916,0.0,0.000816189160104841
마지막 할 말 -> 로라야 정말 고마워!,수식,-0.0017247852,0.0,0.0017247851938009262
마지막 할 말 -> 로라야 정말 고마워!,용어 질문,0.003673024,0.0,0.003673024009913206
마지막 할 말 -> 로라야 정말 고마워!,인공지능,0.0023752402,0.0,0.0023752402048557997
마지막 할 말 -> 로라야 정말 고마워!,잠시 휴식,-0.006202425,0.0,0.006202424876391888
마지막 할 말 -> 로라야 정말 고마워!,좋아하는 아이돌,-0.014462622,0.0,0.014462621882557869
마지막 할 말 -> 로라야 정말 고마워!,핵심 아이디어,-0.011573123,0.0,0.011573122814297676
마지막 할 말 -> 로라야 정말 고마워!,확률 예측에서 MSE Loss 미 사용 이유,0.0033299776,0.0,0.0033299776259809732
마지막 할 말 -> 로라야 사랑해,BCE 가 좋은 task,0.0016782169,0.0,0.0016782168531790376
마지막 할 말 -> 로라야 사랑해,BCE 가 좋은 이유,0.0033393358,0.0,0.0033393357880413532
마지막 할 말 -> 로라야 사랑해,LLM Fine-Tuning 의 PEFT,-0.003220239,0.0,0.003220238955691457
마지막 할 말 -> 로라야 사랑해,LoRA,0.0031812866,0.0,0.003181286621838808
마지막 할 말 -> 로라야 사랑해,LoRA 와 QLoRA 의 차이,-0.0044708415,0.0,0.004470841493457556
마지막 할 말 -> 로라야 사랑해,Loss Function 예시,-0.005042265,0.0,0.005042265169322491
마지막 할 말 -> 로라야 사랑해,Loss Function 정의,0.007981365,0.0,0.007981364615261555
마지막 할 말 -> 로라야 사랑해,MBTI,-0.004914634,0.0,0.004914633929729462
마지막 할 말 -> 로라야 사랑해,MSE Loss 설명,0.0021019515,0.0,0.0021019515115767717
마지막 할 말 -> 로라야 사랑해,MSE Loss 용도,-0.011678599,0.0,0.011678598821163177
마지막 할 말 -> 로라야 사랑해,Multi-Label 에서 CE + Softmax 적용 문제점,-0.0005439275,0.0,0.0005439274827949703
마지막 할 말 -> 로라야 사랑해,PEFT 방법 5가지,-0.011348119,0.0,0.011348119005560875
마지막 할 말 -> 로라야 사랑해,거대 언어 모델 정의,-0.0064075673,0.0,0.006407567299902439
마지막 할 말 -> 로라야 사랑해,기본 경험,-0.010773032,0.0,0.01077303197234869
마지막 할 말 -> 로라야 사랑해,답변 실패,-0.00030619698,0.0,0.00030619697645306587
마지막 할 말 -> 로라야 사랑해,딥러닝,0.014965306,0.0,0.014965306036174297
마지막 할 말 -> 로라야 사랑해,마지막 할 말,0.99350953,1.0,0.006490468978881836
마지막 할 말 -> 로라야 사랑해,머신러닝,-0.006864621,0.0,0.006864620838314295
마지막 할 말 -> 로라야 사랑해,면접 시작 인사,-0.013970018,0.0,0.013970018364489079
마지막 할 말 -> 로라야 사랑해,상세 경험,-0.0067615667,0.0,0.006761566735804081
마지막 할 말 -> 로라야 사랑해,수식,-0.0013713319,0.0,0.0013713318621739745
마지막 할 말 -> 로라야 사랑해,용어 질문,0.0028609568,0.0,0.0028609568253159523
마지막 할 말 -> 로라야 사랑해,인공지능,0.0023660294,0.0,0.0023660294245928526
마지막 할 말 -> 로라야 사랑해,잠시 휴식,-0.0008588729,0.0,0.0008588728960603476
마지막 할 말 -> 로라야 사랑해,좋아하는 아이돌,-0.011075128,0.0,0.011075127869844437
마지막 할 말 -> 로라야 사랑해,핵심 아이디어,-0.010309125,0.0,0.010309125296771526
마지막 할 말 -> 로라야 사랑해,확률 예측에서 MSE Loss 미 사용 이유,0.006268678,0.0,0.006268677767366171
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,BCE 가 좋은 task,0.0009174474,0.0,0.0009174473816528916
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,BCE 가 좋은 이유,0.005875452,0.0,0.0058754519559443
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,LLM Fine-Tuning 의 PEFT,-0.0037766104,0.0,0.0037766103632748127
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,LoRA,-0.0015453154,0.0,0.0015453153755515814
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,LoRA 와 QLoRA 의 차이,-0.0025533105,0.0,0.002553310478106141
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,Loss Function 예시,-0.004667542,0.0,0.004667541943490505
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,Loss Function 정의,0.010641684,0.0,0.01064168382436037
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,MBTI,-0.002931799,0.0,0.002931799041107297
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,MSE Loss 설명,0.0020888317,0.0,0.00208883173763752
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,MSE Loss 용도,-0.012792303,0.0,0.01279230322688818
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,Multi-Label 에서 CE + Softmax 적용 문제점,-0.0026770611,0.0,0.002677061129361391
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,PEFT 방법 5가지,-0.013000426,0.0,0.0130004258826375
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,거대 언어 모델 정의,-0.009772793,0.0,0.009772793389856815
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,기본 경험,-0.010980885,0.0,0.010980884544551373
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,답변 실패,-0.0036374799,0.0,0.0036374798510223627
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,딥러닝,0.011425435,0.0,0.011425434611737728
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,마지막 할 말,0.99108666,1.0,0.00891333818435669
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,머신러닝,-0.003999563,0.0,0.003999562934041023
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,면접 시작 인사,-0.01418842,0.0,0.01418842002749443
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,상세 경험,-0.00027169398,0.0,0.00027169397799298167
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,수식,-0.002701467,0.0,0.002701466903090477
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,용어 질문,0.0018490596,0.0,0.0018490595975890756
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,인공지능,0.004604094,0.0,0.004604094196110964
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,잠시 휴식,-7.674384e-05,0.0,7.67438395996578e-05
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,좋아하는 아이돌,-0.0074230526,0.0,0.007423052564263344
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,핵심 아이디어,-0.008746633,0.0,0.008746633306145668
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,확률 예측에서 MSE Loss 미 사용 이유,0.0047463835,0.0,0.004746383521705866
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워,BCE 가 좋은 task,0.012559372,0.0,0.012559372000396252
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워,BCE 가 좋은 이유,0.0064608064,0.0,0.006460806354880333
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워,LLM Fine-Tuning 의 PEFT,-0.004037358,0.0,0.004037357866764069
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워,LoRA,-0.00084829587,0.0,0.0008482958655804396
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워,LoRA 와 QLoRA 의 차이,-0.004880042,0.0,0.004880041815340519
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워,Loss Function 예시,-0.0044471044,0.0,0.004447104409337044
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워,Loss Function 정의,0.010972091,0.0,0.010972090996801853
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워,MBTI,-0.012197319,0.0,0.01219731941819191
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워,MSE Loss 설명,0.008963607,0.0,0.008963607251644135
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워,MSE Loss 용도,-0.015641505,0.0,0.015641504898667336
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워,Multi-Label 에서 CE + Softmax 적용 문제점,0.005988378,0.0,0.005988378077745438
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워,PEFT 방법 5가지,-0.013538859,0.0,0.013538858853280544
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워,거대 언어 모델 정의,-0.0070731333,0.0,0.007073133252561092
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워,기본 경험,-0.0029068743,0.0,0.0029068742878735065
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워,답변 실패,-0.007042524,0.0,0.007042523939162493
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워,딥러닝,0.015293073,0.0,0.015293072909116745
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워,마지막 할 말,0.9894708,1.0,0.01052922010421753
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워,머신러닝,-0.011629959,0.0,0.011629958637058735
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워,면접 시작 인사,-0.0114330165,0.0,0.011433016508817673
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워,상세 경험,0.0024249493,0.0,0.002424949314445257
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워,수식,-0.0036901236,0.0,0.0036901235580444336
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워,용어 질문,0.0058082147,0.0,0.005808214657008648
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워,인공지능,0.004464414,0.0,0.004464413970708847
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워,잠시 휴식,-0.0033390557,0.0,0.0033390556927770376
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워,좋아하는 아이돌,-0.0140082445,0.0,0.01400824449956417
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워,핵심 아이디어,-0.01250986,0.0,0.012509860098361969
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워,확률 예측에서 MSE Loss 미 사용 이유,0.005654771,0.0,0.005654770880937576
