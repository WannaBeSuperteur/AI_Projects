input_part,output_answer,predicted_score,ground_truth_score,absolute_error
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,BCE 가 좋은 task,0.00242615,0.0,0.00242615002207458
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,"BCE 가 좋은 task, 이유",-0.0072697806,0.0,0.007269780617207289
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,LLM Fine-Tuning 의 PEFT,-0.0064774044,0.0,0.00647740438580513
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,LoRA,-0.01341442,0.0,0.013414420187473297
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,LoRA 와 QLoRA 의 차이,0.014036331,0.0,0.014036331325769424
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,Loss Function 예시,-0.01792908,0.0,0.0179290808737278
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,Loss Function 정의,-0.009654701,0.0,0.009654700756072998
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,MBTI,-0.0072960514,0.0,0.007296051364392042
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,MSE Loss 설명,-0.0023654536,0.0,0.0023654536344110966
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,MSE Loss 용도,0.0020120984,0.0,0.002012098440900445
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,Multi-Label 에서 CE + Softmax 적용 문제점,0.018462807,0.0,0.018462806940078735
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,PEFT 방법 5가지,-0.0019116398,0.0,0.0019116398179903626
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,거대 언어 모델 정의,0.0028318523,0.0,0.0028318522963672876
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,기본 경험,-0.0022886237,0.0,0.0022886237129569054
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,답변 실패,-0.014141549,0.0,0.014141549356281757
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,딥러닝,0.003995381,0.0,0.0039953808300197124
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,마지막 할 말,-0.013228957,0.0,0.01322895660996437
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,머신러닝,0.004421302,0.0,0.004421302117407322
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,면접 시작 인사,0.9820165,1.0,0.01798349618911743
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,상세 경험,0.00509386,0.0,0.005093859974294901
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,수식,0.016940169,0.0,0.016940169036388397
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,인공지능,-0.00156394,0.0,0.0015639399643987417
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,잠시 휴식,-0.009082374,0.0,0.009082374162971973
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,좋아하는 아이돌,-0.0063186083,0.0,0.006318608298897743
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,핵심 아이디어,-0.00071499177,0.0,0.0007149917655624449
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,확률 예측에서 MSE Loss 미 사용 이유,0.004254869,0.0,0.004254869185388088
면접 시작 인사 -> 로라야 안녕 정말 반가워,BCE 가 좋은 task,0.002118019,0.0,0.0021180189214646816
면접 시작 인사 -> 로라야 안녕 정말 반가워,"BCE 가 좋은 task, 이유",-7.456171e-05,0.0,7.456170715158805e-05
면접 시작 인사 -> 로라야 안녕 정말 반가워,LLM Fine-Tuning 의 PEFT,-0.0066102175,0.0,0.0066102175042033195
면접 시작 인사 -> 로라야 안녕 정말 반가워,LoRA,0.006785364,0.0,0.006785363890230656
면접 시작 인사 -> 로라야 안녕 정말 반가워,LoRA 와 QLoRA 의 차이,0.0068205306,0.0,0.006820530630648136
면접 시작 인사 -> 로라야 안녕 정말 반가워,Loss Function 예시,-0.017772324,0.0,0.01777232438325882
면접 시작 인사 -> 로라야 안녕 정말 반가워,Loss Function 정의,-0.0010784378,0.0,0.0010784377809613943
면접 시작 인사 -> 로라야 안녕 정말 반가워,MBTI,0.0020140125,0.0,0.0020140125416219234
면접 시작 인사 -> 로라야 안녕 정말 반가워,MSE Loss 설명,-0.0029849065,0.0,0.0029849065467715263
면접 시작 인사 -> 로라야 안녕 정말 반가워,MSE Loss 용도,-0.007040826,0.0,0.007040826138108969
면접 시작 인사 -> 로라야 안녕 정말 반가워,Multi-Label 에서 CE + Softmax 적용 문제점,0.014564927,0.0,0.014564926736056805
면접 시작 인사 -> 로라야 안녕 정말 반가워,PEFT 방법 5가지,-0.016183056,0.0,0.01618305593729019
면접 시작 인사 -> 로라야 안녕 정말 반가워,거대 언어 모델 정의,0.005956284,0.0,0.005956283770501614
면접 시작 인사 -> 로라야 안녕 정말 반가워,기본 경험,0.0068919966,0.0,0.0068919965997338295
면접 시작 인사 -> 로라야 안녕 정말 반가워,답변 실패,-0.0066250474,0.0,0.006625047419220209
면접 시작 인사 -> 로라야 안녕 정말 반가워,딥러닝,0.0039988495,0.0,0.003998849540948868
면접 시작 인사 -> 로라야 안녕 정말 반가워,마지막 할 말,-0.0038967817,0.0,0.0038967817090451717
면접 시작 인사 -> 로라야 안녕 정말 반가워,머신러닝,0.0041109724,0.0,0.0041109723970294
면접 시작 인사 -> 로라야 안녕 정말 반가워,면접 시작 인사,0.98523945,1.0,0.01476055383682251
면접 시작 인사 -> 로라야 안녕 정말 반가워,상세 경험,0.0051359567,0.0,0.005135956685990095
면접 시작 인사 -> 로라야 안녕 정말 반가워,수식,-0.0045355144,0.0,0.004535514395684004
면접 시작 인사 -> 로라야 안녕 정말 반가워,인공지능,-0.0021591487,0.0,0.0021591486874967813
면접 시작 인사 -> 로라야 안녕 정말 반가워,잠시 휴식,-0.004179266,0.0,0.004179265815764666
면접 시작 인사 -> 로라야 안녕 정말 반가워,좋아하는 아이돌,-0.0032221286,0.0,0.0032221286091953516
면접 시작 인사 -> 로라야 안녕 정말 반가워,핵심 아이디어,-0.01090885,0.0,0.010908849537372589
면접 시작 인사 -> 로라야 안녕 정말 반가워,확률 예측에서 MSE Loss 미 사용 이유,0.0032310586,0.0,0.003231058595702052
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,BCE 가 좋은 task,-0.005385115,0.0,0.005385114811360836
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,"BCE 가 좋은 task, 이유",-0.009597319,0.0,0.009597319178283215
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,LLM Fine-Tuning 의 PEFT,-0.0042209732,0.0,0.004220973234623671
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,LoRA,0.003838182,0.0,0.003838181961327791
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,LoRA 와 QLoRA 의 차이,0.008516739,0.0,0.008516739122569561
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,Loss Function 예시,-0.0018023604,0.0,0.0018023604061454535
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,Loss Function 정의,-0.014567903,0.0,0.014567903243005276
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,MBTI,-0.010213161,0.0,0.010213160887360573
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,MSE Loss 설명,0.0053852485,0.0,0.005385248456150293
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,MSE Loss 용도,0.00076210505,0.0,0.0007621050463058054
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,Multi-Label 에서 CE + Softmax 적용 문제점,0.009052554,0.0,0.00905255414545536
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,PEFT 방법 5가지,-0.022211071,0.0,0.022211071103811264
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,거대 언어 모델 정의,0.0067426963,0.0,0.006742696277797222
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,기본 경험,0.008042823,0.0,0.00804282259196043
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,답변 실패,-0.0021301822,0.0,0.0021301822271198034
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,딥러닝,0.0015087485,0.0,0.0015087485080584884
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,마지막 할 말,-0.00842279,0.0,0.008422790095210075
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,머신러닝,0.0065989974,0.0,0.00659899739548564
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,면접 시작 인사,0.98132646,1.0,0.01867353916168213
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,상세 경험,0.00026308367,0.0,0.0002630836679600179
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,수식,-0.0036168564,0.0,0.003616856411099434
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,인공지능,-0.00732126,0.0,0.007321259938180447
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,잠시 휴식,-0.019827405,0.0,0.019827404990792274
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,좋아하는 아이돌,-0.007976717,0.0,0.007976717315614223
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,핵심 아이디어,-0.0041661295,0.0,0.004166129510849714
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,확률 예측에서 MSE Loss 미 사용 이유,-1.956044e-05,0.0,1.9560440705390647e-05
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,BCE 가 좋은 task,-0.0017042297,0.0,0.0017042297404259443
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,"BCE 가 좋은 task, 이유",0.0126032075,0.0,0.012603207491338253
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,LLM Fine-Tuning 의 PEFT,-0.0024077408,0.0,0.0024077408015727997
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,LoRA,0.007139082,0.0,0.007139082066714764
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,LoRA 와 QLoRA 의 차이,0.017681232,0.0,0.01768123172223568
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,Loss Function 예시,-0.017119674,0.0,0.017119674012064934
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,Loss Function 정의,-0.010875052,0.0,0.010875051841139793
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,MBTI,-0.019914303,0.0,0.019914302974939346
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,MSE Loss 설명,-0.0048016827,0.0,0.004801682662218809
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,MSE Loss 용도,-0.0018739646,0.0,0.0018739645602181554
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,Multi-Label 에서 CE + Softmax 적용 문제점,0.0098356465,0.0,0.009835646487772465
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,PEFT 방법 5가지,-0.0058588237,0.0,0.005858823657035828
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,거대 언어 모델 정의,0.009491324,0.0,0.009491324424743652
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,기본 경험,0.014260596,0.0,0.014260595664381981
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,답변 실패,-0.024410097,0.0,0.024410096928477287
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,딥러닝,0.009151562,0.0,0.009151562117040157
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,마지막 할 말,0.027724162,0.0,0.027724161744117737
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,머신러닝,7.891124e-05,0.0,7.891123823355883e-05
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,면접 시작 인사,0.9715479,1.0,0.02845209836959839
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,상세 경험,0.016249204,0.0,0.01624920405447483
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,수식,0.0013840692,0.0,0.001384069211781025
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,인공지능,-0.0074980264,0.0,0.007498026359826326
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,잠시 휴식,-0.0125202,0.0,0.01252019964158535
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,좋아하는 아이돌,-0.0077936375,0.0,0.007793637458235025
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,핵심 아이디어,-0.009256701,0.0,0.009256700985133648
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,확률 예측에서 MSE Loss 미 사용 이유,0.0020616662,0.0,0.0020616662222892046
면접 시작 인사 -> 파이팅! 시작하자,BCE 가 좋은 task,-0.0013891405,0.0,0.0013891404960304499
면접 시작 인사 -> 파이팅! 시작하자,"BCE 가 좋은 task, 이유",-0.0077298456,0.0,0.007729845587164164
면접 시작 인사 -> 파이팅! 시작하자,LLM Fine-Tuning 의 PEFT,0.002709035,0.0,0.002709035063162446
면접 시작 인사 -> 파이팅! 시작하자,LoRA,-0.005599258,0.0,0.005599258001893759
면접 시작 인사 -> 파이팅! 시작하자,LoRA 와 QLoRA 의 차이,0.005137288,0.0,0.005137288011610508
면접 시작 인사 -> 파이팅! 시작하자,Loss Function 예시,-0.008969197,0.0,0.008969197049736977
면접 시작 인사 -> 파이팅! 시작하자,Loss Function 정의,-0.0051651504,0.0,0.005165150389075279
면접 시작 인사 -> 파이팅! 시작하자,MBTI,0.0028785805,0.0,0.0028785804752260447
면접 시작 인사 -> 파이팅! 시작하자,MSE Loss 설명,0.0021193125,0.0,0.0021193125285208225
면접 시작 인사 -> 파이팅! 시작하자,MSE Loss 용도,0.002735999,0.0,0.002735998947173357
면접 시작 인사 -> 파이팅! 시작하자,Multi-Label 에서 CE + Softmax 적용 문제점,0.016622595,0.0,0.016622595489025116
면접 시작 인사 -> 파이팅! 시작하자,PEFT 방법 5가지,-0.0032839514,0.0,0.0032839514315128326
면접 시작 인사 -> 파이팅! 시작하자,거대 언어 모델 정의,0.0022143442,0.0,0.0022143442183732986
면접 시작 인사 -> 파이팅! 시작하자,기본 경험,-0.005589488,0.0,0.005589487962424755
면접 시작 인사 -> 파이팅! 시작하자,답변 실패,-0.013580548,0.0,0.013580547645688057
면접 시작 인사 -> 파이팅! 시작하자,딥러닝,0.0007005312,0.0,0.0007005311781540513
면접 시작 인사 -> 파이팅! 시작하자,마지막 할 말,-0.006441983,0.0,0.006441982928663492
면접 시작 인사 -> 파이팅! 시작하자,머신러닝,0.0022126988,0.0,0.0022126988042145967
면접 시작 인사 -> 파이팅! 시작하자,면접 시작 인사,0.98622495,1.0,0.013775050640106201
면접 시작 인사 -> 파이팅! 시작하자,상세 경험,0.007579389,0.0,0.007579389028251171
면접 시작 인사 -> 파이팅! 시작하자,수식,0.005362735,0.0,0.005362735129892826
면접 시작 인사 -> 파이팅! 시작하자,인공지능,-0.0059672184,0.0,0.005967218428850174
면접 시작 인사 -> 파이팅! 시작하자,잠시 휴식,-0.009619869,0.0,0.00961986929178238
면접 시작 인사 -> 파이팅! 시작하자,좋아하는 아이돌,-0.0047154976,0.0,0.0047154976055026054
면접 시작 인사 -> 파이팅! 시작하자,핵심 아이디어,0.002686398,0.0,0.0026863981038331985
면접 시작 인사 -> 파이팅! 시작하자,확률 예측에서 MSE Loss 미 사용 이유,0.0040177763,0.0,0.004017776343971491
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",BCE 가 좋은 task,-0.030089704,0.0,0.03008970431983471
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데","BCE 가 좋은 task, 이유",0.04334088,0.0,0.043340880423784256
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",LLM Fine-Tuning 의 PEFT,-0.006948437,0.0,0.006948437076061964
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",LoRA,-0.03055598,0.0,0.030555980280041695
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",LoRA 와 QLoRA 의 차이,-0.0023134751,0.0,0.002313475124537945
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",Loss Function 예시,-0.010990425,0.0,0.010990425013005733
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",Loss Function 정의,-0.05828009,0.0,0.0582800917327404
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",MBTI,-0.008224527,0.0,0.008224527351558208
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",MSE Loss 설명,-0.035757747,0.0,0.035757746547460556
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",MSE Loss 용도,-0.009881694,0.0,0.009881693869829178
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",Multi-Label 에서 CE + Softmax 적용 문제점,-0.0009214446,0.0,0.0009214446181431413
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",PEFT 방법 5가지,-0.006698597,0.0,0.006698597222566605
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",거대 언어 모델 정의,-0.01237192,0.0,0.012371920049190521
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",기본 경험,0.46534014,0.0,0.46534013748168945
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",답변 실패,0.7038383,1.0,0.2961617112159729
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",딥러닝,0.0020509106,0.0,0.002050910610705614
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",마지막 할 말,-0.014965722,0.0,0.01496572233736515
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",머신러닝,-0.0074996683,0.0,0.0074996682815253735
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",면접 시작 인사,-0.011150012,0.0,0.011150011792778969
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",상세 경험,-0.036823872,0.0,0.03682387247681618
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",수식,-0.025603352,0.0,0.02560335211455822
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",인공지능,0.102231316,0.0,0.10223131626844406
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",잠시 휴식,-0.024949396,0.0,0.024949396029114723
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",좋아하는 아이돌,0.032745786,0.0,0.032745786011219025
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",핵심 아이디어,-0.029100316,0.0,0.029100315645337105
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",확률 예측에서 MSE Loss 미 사용 이유,-0.03768509,0.0,0.0376850888133049
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",BCE 가 좋은 task,0.004460156,0.0,0.004460155963897705
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!","BCE 가 좋은 task, 이유",-0.0011973705,0.0,0.0011973704677075148
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",LLM Fine-Tuning 의 PEFT,0.017851165,0.0,0.01785116456449032
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",LoRA,0.0075116255,0.0,0.007511625532060862
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",LoRA 와 QLoRA 의 차이,0.0075387973,0.0,0.0075387973338365555
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",Loss Function 예시,-0.017968236,0.0,0.017968235537409782
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",Loss Function 정의,0.001431395,0.0,0.0014313949504867196
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",MBTI,-0.0049451957,0.0,0.004945195745676756
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",MSE Loss 설명,0.0014204314,0.0,0.0014204314211383462
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",MSE Loss 용도,-0.006901002,0.0,0.006901002023369074
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",Multi-Label 에서 CE + Softmax 적용 문제점,-0.0025631222,0.0,0.002563122194260359
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",PEFT 방법 5가지,0.022213908,0.0,0.022213907912373543
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",거대 언어 모델 정의,0.008607869,0.0,0.008607869036495686
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",기본 경험,-0.017801916,0.0,0.017801916226744652
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",답변 실패,-0.016700203,0.0,0.016700202599167824
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",딥러닝,-0.013376088,0.0,0.013376087881624699
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",마지막 할 말,0.0030796633,0.0,0.0030796632636338472
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",머신러닝,-0.05342363,0.0,0.05342362821102142
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",면접 시작 인사,0.0030236961,0.0,0.003023696132004261
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",상세 경험,0.016696729,0.0,0.016696728765964508
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",수식,-0.006881325,0.0,0.006881325040012598
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",인공지능,0.91916966,1.0,0.08083033561706543
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",잠시 휴식,-0.016041405,0.0,0.016041405498981476
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",좋아하는 아이돌,0.010058778,0.0,0.010058778338134289
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",핵심 아이디어,0.018385975,0.0,0.018385974690318108
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",확률 예측에서 MSE Loss 미 사용 이유,0.012032935,0.0,0.01203293539583683
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",BCE 가 좋은 task,0.014008442,0.0,0.01400844193994999
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지","BCE 가 좋은 task, 이유",-0.006022492,0.0,0.006022491957992315
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",LLM Fine-Tuning 의 PEFT,0.0048056063,0.0,0.004805606324225664
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",LoRA,-0.005525775,0.0,0.005525774788111448
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",LoRA 와 QLoRA 의 차이,0.010399307,0.0,0.010399307124316692
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",Loss Function 예시,0.03860567,0.0,0.038605671375989914
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",Loss Function 정의,0.020109,0.0,0.02010899968445301
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",MBTI,-0.0017815955,0.0,0.001781595521606505
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",MSE Loss 설명,-0.0007671209,0.0,0.0007671209168620408
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",MSE Loss 용도,0.0020725457,0.0,0.002072545699775219
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",Multi-Label 에서 CE + Softmax 적용 문제점,-0.00053353206,0.0,0.0005335320602171123
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",PEFT 방법 5가지,0.013304388,0.0,0.013304388150572777
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",거대 언어 모델 정의,0.017755972,0.0,0.01775597222149372
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",기본 경험,-0.008759141,0.0,0.008759140968322754
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",답변 실패,-0.015547464,0.0,0.015547463670372963
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",딥러닝,-0.012130962,0.0,0.012130961753427982
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",마지막 할 말,-0.00592253,0.0,0.005922529846429825
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",머신러닝,0.9121436,1.0,0.08785641193389893
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",면접 시작 인사,0.0007320956,0.0,0.00073209562106058
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",상세 경험,0.01723754,0.0,0.01723754033446312
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",수식,0.01852215,0.0,0.018522150814533234
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",인공지능,-0.065422624,0.0,0.06542262434959412
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",잠시 휴식,0.008227565,0.0,0.008227565325796604
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",좋아하는 아이돌,0.024723278,0.0,0.024723278358578682
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",핵심 아이디어,-0.020026991,0.0,0.02002699114382267
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",확률 예측에서 MSE Loss 미 사용 이유,0.02500333,0.0,0.025003330782055855
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",BCE 가 좋은 task,0.0065206853,0.0,0.006520685274153948
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?","BCE 가 좋은 task, 이유",-0.0025196753,0.0,0.0025196752976626158
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",LLM Fine-Tuning 의 PEFT,-0.02315064,0.0,0.023150639608502388
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",LoRA,-0.016269706,0.0,0.016269706189632416
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",LoRA 와 QLoRA 의 차이,0.029000886,0.0,0.029000885784626007
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",Loss Function 예시,-0.0059335884,0.0,0.005933588370680809
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",Loss Function 정의,-0.011168194,0.0,0.011168194003403187
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",MBTI,0.015028702,0.0,0.015028702095150948
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",MSE Loss 설명,0.018596506,0.0,0.018596505746245384
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",MSE Loss 용도,0.0014294692,0.0,0.0014294692082330585
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",Multi-Label 에서 CE + Softmax 적용 문제점,-0.014300989,0.0,0.014300988987088203
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",PEFT 방법 5가지,-0.004787689,0.0,0.004787689074873924
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",거대 언어 모델 정의,0.012998101,0.0,0.01299810130149126
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",기본 경험,-0.0031822992,0.0,0.0031822992023080587
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",답변 실패,0.005131291,0.0,0.005131291225552559
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",딥러닝,0.94671124,1.0,0.05328875780105591
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",마지막 할 말,-0.0135684395,0.0,0.013568439520895481
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",머신러닝,-0.043614183,0.0,0.043614182621240616
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",면접 시작 인사,-0.0006123109,0.0,0.0006123108905740082
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",상세 경험,0.026558451,0.0,0.02655845135450363
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",수식,-0.005487037,0.0,0.005487036891281605
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",인공지능,-0.053662278,0.0,0.05366227775812149
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",잠시 휴식,-0.02516964,0.0,0.02516964077949524
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",좋아하는 아이돌,-0.004659151,0.0,0.004659151192754507
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",핵심 아이디어,-0.0031722921,0.0,0.0031722921412438154
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",확률 예측에서 MSE Loss 미 사용 이유,-0.0066518453,0.0,0.006651845294982195
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",BCE 가 좋은 task,-0.0032551913,0.0,0.003255191259086132
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야","BCE 가 좋은 task, 이유",-0.027595073,0.0,0.027595072984695435
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",LLM Fine-Tuning 의 PEFT,-0.022971954,0.0,0.022971954196691513
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",LoRA,-0.021194778,0.0,0.021194778382778168
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",LoRA 와 QLoRA 의 차이,0.040856257,0.0,0.0408562570810318
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",Loss Function 예시,-0.010417531,0.0,0.010417531244456768
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",Loss Function 정의,-0.012375202,0.0,0.012375202029943466
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",MBTI,0.024849571,0.0,0.02484957128763199
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",MSE Loss 설명,0.0106656,0.0,0.010665600188076496
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",MSE Loss 용도,-0.0067290864,0.0,0.006729086395353079
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",Multi-Label 에서 CE + Softmax 적용 문제점,-0.023919895,0.0,0.02391989529132843
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",PEFT 방법 5가지,-0.007716823,0.0,0.007716822903603315
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",거대 언어 모델 정의,0.011333984,0.0,0.011333984322845936
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",기본 경험,0.009432988,0.0,0.009432988241314888
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",답변 실패,0.013339001,0.0,0.013339000754058361
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",딥러닝,0.9398137,1.0,0.06018632650375366
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",마지막 할 말,-0.00799628,0.0,0.007996279746294022
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",머신러닝,-0.008340761,0.0,0.008340761065483093
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",면접 시작 인사,-0.0013437106,0.0,0.0013437105808407068
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",상세 경험,0.022022445,0.0,0.022022444754838943
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",수식,-0.011912155,0.0,0.011912154965102673
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",인공지능,-0.035345115,0.0,0.03534511476755142
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",잠시 휴식,-0.026631176,0.0,0.026631176471710205
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",좋아하는 아이돌,-0.0029019252,0.0,0.002901925239712
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",핵심 아이디어,-0.016707089,0.0,0.01670708879828453
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",확률 예측에서 MSE Loss 미 사용 이유,-0.011927388,0.0,0.011927387677133083
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",BCE 가 좋은 task,0.0034048622,0.0,0.0034048622474074364
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?","BCE 가 좋은 task, 이유",-0.032582775,0.0,0.03258277475833893
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",LLM Fine-Tuning 의 PEFT,0.0011991973,0.0,0.001199197256937623
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",LoRA,-0.023149215,0.0,0.023149214684963226
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",LoRA 와 QLoRA 의 차이,0.007279892,0.0,0.007279891986399889
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",Loss Function 예시,0.0055452264,0.0,0.005545226391404867
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",Loss Function 정의,0.017308278,0.0,0.017308278009295464
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",MBTI,-0.002208653,0.0,0.0022086529061198235
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",MSE Loss 설명,0.027299738,0.0,0.02729973755776882
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",MSE Loss 용도,-0.018609248,0.0,0.018609248101711273
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",Multi-Label 에서 CE + Softmax 적용 문제점,-0.0054234685,0.0,0.005423468537628651
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",PEFT 방법 5가지,0.014528126,0.0,0.014528126455843449
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",거대 언어 모델 정의,-0.00090434117,0.0,0.0009043411700986326
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",기본 경험,0.032117404,0.0,0.03211740404367447
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",답변 실패,-0.013386834,1.0,1.0133868344128132
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",딥러닝,0.010479253,0.0,0.01047925278544426
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",마지막 할 말,0.011050813,0.0,0.011050812900066376
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",머신러닝,0.8951369,0.0,0.8951368927955627
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",면접 시작 인사,-0.01852581,0.0,0.018525810912251472
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",상세 경험,0.0032579298,0.0,0.003257929813116789
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",수식,-0.020364784,0.0,0.020364783704280853
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",인공지능,0.012689041,0.0,0.01268904097378254
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",잠시 휴식,0.016456435,0.0,0.01645643450319767
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",좋아하는 아이돌,0.0067123123,0.0,0.0067123123444616795
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",핵심 아이디어,0.02942228,0.0,0.029422279447317123
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",확률 예측에서 MSE Loss 미 사용 이유,0.005422334,0.0,0.005422334186732769
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,BCE 가 좋은 task,0.001899303,0.0,0.0018993030535057187
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,"BCE 가 좋은 task, 이유",0.0042715743,0.0,0.004271574318408966
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,LLM Fine-Tuning 의 PEFT,-0.0028394554,0.0,0.0028394553810358047
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,LoRA,-0.006582013,0.0,0.006582012865692377
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,LoRA 와 QLoRA 의 차이,0.0072768014,0.0,0.0072768013924360275
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,Loss Function 예시,-0.0050000935,0.0,0.00500009348616004
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,Loss Function 정의,-0.003025152,0.0,0.0030251520220190287
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,MBTI,-0.000911235,0.0,0.0009112349944189191
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,MSE Loss 설명,0.00205068,0.0,0.0020506801083683968
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,MSE Loss 용도,-0.0075510433,0.0,0.007551043294370174
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,Multi-Label 에서 CE + Softmax 적용 문제점,-0.006029058,0.0,0.006029057782143354
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,PEFT 방법 5가지,0.0064983335,0.0,0.006498333532363176
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,거대 언어 모델 정의,0.97370255,1.0,0.026297450065612793
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,기본 경험,-0.0080738235,0.0,0.008073823526501656
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,답변 실패,-0.0061116377,0.0,0.006111637689173222
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,딥러닝,0.0010656839,0.0,0.0010656839003786445
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,마지막 할 말,-0.0026209666,0.0,0.002620966639369726
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,머신러닝,0.003053898,0.0,0.0030538979917764664
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,면접 시작 인사,0.002747777,0.0,0.0027477769181132317
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,상세 경험,0.00040627609,0.0,0.0004062760854139924
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,수식,-0.010111241,0.0,0.010111240670084953
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,인공지능,-0.010381362,0.0,0.010381362400949001
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,잠시 휴식,-0.005889869,0.0,0.005889868829399347
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,좋아하는 아이돌,-0.005475782,0.0,0.005475781857967377
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,핵심 아이디어,-0.0029538197,0.0,0.0029538196977227926
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,확률 예측에서 MSE Loss 미 사용 이유,0.0048245164,0.0,0.004824516363441944
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,BCE 가 좋은 task,0.01481819,0.0,0.014818189665675163
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,"BCE 가 좋은 task, 이유",0.0038139767,0.0,0.0038139766547828913
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,LLM Fine-Tuning 의 PEFT,-0.010818969,0.0,0.010818969458341599
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,LoRA,0.012648117,0.0,0.012648116797208786
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,LoRA 와 QLoRA 의 차이,0.008907851,0.0,0.008907850831747055
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,Loss Function 예시,-0.0051344433,0.0,0.005134443286806345
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,Loss Function 정의,-0.0022599148,0.0,0.0022599147632718086
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,MBTI,-0.0027351358,0.0,0.002735135843977332
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,MSE Loss 설명,0.0053724237,0.0,0.005372423678636551
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,MSE Loss 용도,-0.016160145,0.0,0.01616014540195465
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,Multi-Label 에서 CE + Softmax 적용 문제점,-0.001585413,0.0,0.0015854130033403635
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,PEFT 방법 5가지,-0.01674622,0.0,0.016746219247579575
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,거대 언어 모델 정의,0.03903499,0.0,0.03903498873114586
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,기본 경험,-0.006494797,0.0,0.006494796834886074
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,답변 실패,0.9712477,1.0,0.02875232696533203
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,딥러닝,0.002603767,0.0,0.0026037669740617275
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,마지막 할 말,-0.008127989,0.0,0.008127989247441292
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,머신러닝,-0.005010876,0.0,0.005010875873267651
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,면접 시작 인사,-0.0044439337,0.0,0.004443933721631765
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,상세 경험,0.008825436,0.0,0.008825436234474182
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,수식,-0.025470141,0.0,0.02547014132142067
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,인공지능,-0.016308853,0.0,0.016308853402733803
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,잠시 휴식,-0.014324957,0.0,0.014324956573545933
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,좋아하는 아이돌,-0.009478212,0.0,0.009478212334215641
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,핵심 아이디어,0.009870168,0.0,0.009870167821645737
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,확률 예측에서 MSE Loss 미 사용 이유,-0.014599437,0.0,0.014599436894059181
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,BCE 가 좋은 task,-0.0038418511,0.0,0.0038418511394411325
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,"BCE 가 좋은 task, 이유",-0.025858514,0.0,0.02585851401090622
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,LLM Fine-Tuning 의 PEFT,-0.013712847,0.0,0.013712846674025059
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,LoRA,-0.010027857,0.0,0.010027856566011906
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,LoRA 와 QLoRA 의 차이,-0.019989185,0.0,0.01998918503522873
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,Loss Function 예시,-0.02137513,0.0,0.021375130861997604
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,Loss Function 정의,0.9628346,0.0,0.9628345966339111
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,MBTI,-0.019841319,0.0,0.01984131895005703
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,MSE Loss 설명,-0.04850768,0.0,0.048507679253816605
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,MSE Loss 용도,-0.004573615,0.0,0.004573614802211523
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,Multi-Label 에서 CE + Softmax 적용 문제점,-0.008738527,0.0,0.008738527074456215
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,PEFT 방법 5가지,0.022243662,0.0,0.022243661805987358
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,거대 언어 모델 정의,-0.01729658,0.0,0.017296580597758293
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,기본 경험,-0.011499923,0.0,0.011499922722578049
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,답변 실패,0.0011101986,1.0,0.9988898014416918
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,딥러닝,0.019182004,0.0,0.019182004034519196
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,마지막 할 말,0.022028146,0.0,0.02202814631164074
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,머신러닝,0.0010978198,0.0,0.0010978197678923607
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,면접 시작 인사,-0.015716357,0.0,0.01571635715663433
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,상세 경험,-1.10623605e-05,0.0,1.106236049963627e-05
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,수식,0.003224781,0.0,0.0032247810158878565
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,인공지능,-0.012580621,0.0,0.012580621056258678
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,잠시 휴식,-0.013901253,0.0,0.01390125323086977
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,좋아하는 아이돌,0.0044922754,0.0,0.004492275416851044
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,핵심 아이디어,0.007776075,0.0,0.007776075042784214
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,확률 예측에서 MSE Loss 미 사용 이유,-0.014589179,0.0,0.014589179307222366
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",BCE 가 좋은 task,0.0020363366,0.0,0.0020363365765661
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!","BCE 가 좋은 task, 이유",-0.006463296,0.0,0.00646329578012228
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",LLM Fine-Tuning 의 PEFT,-0.00044213777,0.0,0.00044213776709511876
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",LoRA,-0.009127614,0.0,0.009127614088356495
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",LoRA 와 QLoRA 의 차이,-0.00022987973,0.0,0.00022987973352428526
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",Loss Function 예시,-0.009704999,0.0,0.009704998694360256
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",Loss Function 정의,0.9643975,1.0,0.03560250997543335
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",MBTI,-0.018303387,0.0,0.018303386867046356
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",MSE Loss 설명,-0.02030072,0.0,0.020300719887018204
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",MSE Loss 용도,-0.012607931,0.0,0.012607931159436703
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",Multi-Label 에서 CE + Softmax 적용 문제점,-0.015103619,0.0,0.015103618614375591
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",PEFT 방법 5가지,0.034854997,0.0,0.03485499694943428
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",거대 언어 모델 정의,-0.024794323,0.0,0.02479432336986065
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",기본 경험,-0.0059450297,0.0,0.00594502966850996
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",답변 실패,-0.01958399,0.0,0.019583990797400475
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",딥러닝,-0.029420642,0.0,0.02942064218223095
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",마지막 할 말,-0.009729868,0.0,0.009729867801070213
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",머신러닝,0.012744387,0.0,0.012744386680424213
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",면접 시작 인사,0.0066359313,0.0,0.006635931320488453
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",상세 경험,-0.0032427234,0.0,0.003242723410949111
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",수식,0.03351165,0.0,0.03351164981722832
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",인공지능,-0.008513292,0.0,0.008513292297720909
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",잠시 휴식,-0.0046362164,0.0,0.004636216443032026
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",좋아하는 아이돌,0.0025905706,0.0,0.0025905705988407135
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",핵심 아이디어,-0.017155891,0.0,0.01715589128434658
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",확률 예측에서 MSE Loss 미 사용 이유,-0.0075793336,0.0,0.0075793336145579815
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,BCE 가 좋은 task,-0.005840006,0.0,0.005840005818754435
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,"BCE 가 좋은 task, 이유",0.011609042,0.0,0.011609042063355446
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,LLM Fine-Tuning 의 PEFT,0.0003638272,0.0,0.0003638271882664412
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,LoRA,-0.0055767316,0.0,0.005576731637120247
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,LoRA 와 QLoRA 의 차이,-0.013073668,0.0,0.013073667883872986
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,Loss Function 예시,0.045308188,0.0,0.0453081876039505
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,Loss Function 정의,-0.019101853,0.0,0.019101852551102638
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,MBTI,-0.0063672364,0.0,0.006367236375808716
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,MSE Loss 설명,-0.002815428,0.0,0.0028154279571026564
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,MSE Loss 용도,-0.011849247,0.0,0.01184924691915512
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,Multi-Label 에서 CE + Softmax 적용 문제점,-0.0001307892,0.0,0.0001307891943724826
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,PEFT 방법 5가지,-0.0034263262,0.0,0.0034263262059539557
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,거대 언어 모델 정의,-0.01191186,0.0,0.01191185973584652
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,기본 경험,-0.008000929,0.0,0.008000928908586502
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,답변 실패,0.97622913,1.0,0.02377086877822876
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,딥러닝,-0.012809121,0.0,0.012809121049940586
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,마지막 할 말,0.004329154,0.0,0.004329153802245855
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,머신러닝,-0.015884763,0.0,0.0158847626298666
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,면접 시작 인사,-0.00020267806,0.0,0.00020267805666662753
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,상세 경험,-0.010283802,0.0,0.010283801704645157
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,수식,-0.0065892423,0.0,0.00658924225717783
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,인공지능,-0.013348745,0.0,0.013348745182156563
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,잠시 휴식,-0.011098704,0.0,0.011098704300820827
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,좋아하는 아이돌,0.004040069,0.0,0.004040068946778774
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,핵심 아이디어,-0.0011459566,0.0,0.0011459565721452236
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,확률 예측에서 MSE Loss 미 사용 이유,-0.013734395,0.0,0.013734394684433937
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",BCE 가 좋은 task,0.0023338778,0.0,0.0023338778410106897
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지","BCE 가 좋은 task, 이유",-0.00845856,0.0,0.008458560332655907
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",LLM Fine-Tuning 의 PEFT,-0.004894212,0.0,0.0048942118883132935
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",LoRA,-0.00082103565,0.0,0.0008210356463678181
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",LoRA 와 QLoRA 의 차이,0.0046183816,0.0,0.00461838161572814
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",Loss Function 예시,0.97412777,1.0,0.025872230529785156
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",Loss Function 정의,-0.027368102,0.0,0.027368102222681046
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",MBTI,-0.017775966,0.0,0.017775965854525566
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",MSE Loss 설명,-0.0015637263,0.0,0.0015637263422831893
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",MSE Loss 용도,0.0025785307,0.0,0.0025785306934267282
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",Multi-Label 에서 CE + Softmax 적용 문제점,-0.010827017,0.0,0.01082701701670885
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",PEFT 방법 5가지,-0.011893669,0.0,0.01189366914331913
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",거대 언어 모델 정의,-0.008427848,0.0,0.008427848108112812
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",기본 경험,0.0075087654,0.0,0.0075087654404342175
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",답변 실패,0.008142223,0.0,0.00814222265034914
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",딥러닝,-0.0049514705,0.0,0.004951470531523228
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",마지막 할 말,-0.00016705466,0.0,0.00016705466259736568
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",머신러닝,-0.004204891,0.0,0.0042048911564052105
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",면접 시작 인사,-0.0074402234,0.0,0.007440223358571529
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",상세 경험,-0.010088612,0.0,0.010088612325489521
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",수식,-0.009111228,0.0,0.00911122839897871
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",인공지능,0.011797479,0.0,0.01179747935384512
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",잠시 휴식,-0.00047016618,0.0,0.0004701661819126457
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",좋아하는 아이돌,-0.0152085405,0.0,0.015208540484309196
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",핵심 아이디어,-0.010009578,0.0,0.010009578429162502
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",확률 예측에서 MSE Loss 미 사용 이유,0.003960326,0.0,0.003960325848311186
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,BCE 가 좋은 task,0.008078743,0.0,0.008078742772340775
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,"BCE 가 좋은 task, 이유",-0.030693684,0.0,0.03069368377327919
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,LLM Fine-Tuning 의 PEFT,3.360798e-06,0.0,3.360798018547939e-06
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,LoRA,-0.037186615,0.0,0.03718661516904831
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,LoRA 와 QLoRA 의 차이,0.008263615,0.0,0.00826361496001482
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,Loss Function 예시,-0.013051701,0.0,0.01305170077830553
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,Loss Function 정의,-0.03564487,0.0,0.03564487025141716
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,MBTI,-0.020633249,0.0,0.02063324861228466
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,MSE Loss 설명,0.9711762,1.0,0.028823792934417725
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,MSE Loss 용도,-0.028696617,0.0,0.028696617111563683
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,Multi-Label 에서 CE + Softmax 적용 문제점,-0.02659808,0.0,0.02659807913005352
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,PEFT 방법 5가지,-0.0054944227,0.0,0.005494422744959593
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,거대 언어 모델 정의,0.0023994166,0.0,0.0023994166404008865
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,기본 경험,0.007853117,0.0,0.00785311684012413
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,답변 실패,0.002394652,0.0,0.0023946519941091537
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,딥러닝,0.02314334,0.0,0.023143339902162552
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,마지막 할 말,-0.012934969,0.0,0.012934968806803226
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,머신러닝,-0.025082357,0.0,0.025082357227802277
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,면접 시작 인사,0.007987266,0.0,0.007987266406416893
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,상세 경험,-0.013826107,0.0,0.013826106674969196
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,수식,-0.03452822,0.0,0.0345282182097435
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,인공지능,0.002034339,0.0,0.00203433888964355
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,잠시 휴식,0.006650989,0.0,0.006650988943874836
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,좋아하는 아이돌,-0.0015709434,0.0,0.0015709433937445283
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,핵심 아이디어,-0.01325638,0.0,0.013256380334496498
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,확률 예측에서 MSE Loss 미 사용 이유,0.015762122,0.0,0.015762122347950935
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,BCE 가 좋은 task,0.0046724468,0.0,0.004672446753829718
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,"BCE 가 좋은 task, 이유",-0.033016954,0.0,0.033016953617334366
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,LLM Fine-Tuning 의 PEFT,-0.006084462,0.0,0.006084462162107229
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,LoRA,-0.025996262,0.0,0.025996262207627296
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,LoRA 와 QLoRA 의 차이,-0.0053532873,0.0,0.005353287328034639
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,Loss Function 예시,-0.01417218,0.0,0.014172179624438286
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,Loss Function 정의,-0.036552757,0.0,0.036552757024765015
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,MBTI,-0.033122215,0.0,0.033122215420007706
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,MSE Loss 설명,0.95569676,0.0,0.9556967616081238
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,MSE Loss 용도,-0.026888274,0.0,0.026888273656368256
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,Multi-Label 에서 CE + Softmax 적용 문제점,-0.03492563,0.0,0.03492562845349312
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,PEFT 방법 5가지,-0.0145809185,0.0,0.014580918475985527
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,거대 언어 모델 정의,0.0095529845,0.0,0.00955298449844122
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,기본 경험,0.0032758089,0.0,0.0032758088782429695
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,답변 실패,0.0043453807,1.0,0.9956546192988753
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,딥러닝,0.015372557,0.0,0.015372556634247303
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,마지막 할 말,-0.022451773,0.0,0.022451773285865784
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,머신러닝,-0.023980409,0.0,0.023980408906936646
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,면접 시작 인사,-0.0037429114,0.0,0.0037429113872349262
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,상세 경험,0.0038935067,0.0,0.0038935067132115364
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,수식,-0.01847553,0.0,0.018475530669093132
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,인공지능,0.011427195,0.0,0.011427194811403751
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,잠시 휴식,0.010408112,0.0,0.010408111847937107
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,좋아하는 아이돌,-0.00044467903,0.0,0.0004446790262591094
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,핵심 아이디어,0.015219065,0.0,0.015219065360724926
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,확률 예측에서 MSE Loss 미 사용 이유,0.00091310486,0.0,0.0009131048573181033
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",BCE 가 좋은 task,0.0031057103,0.0,0.0031057102605700493
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지","BCE 가 좋은 task, 이유",-0.012274763,0.0,0.012274762615561485
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",LLM Fine-Tuning 의 PEFT,0.0091936,0.0,0.00919360015541315
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",LoRA,-0.014207814,0.0,0.014207813888788223
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",LoRA 와 QLoRA 의 차이,0.005829327,0.0,0.005829326808452606
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",Loss Function 예시,-0.0019645973,0.0,0.001964597264304757
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",Loss Function 정의,0.011985318,0.0,0.011985317803919315
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",MBTI,0.0022039108,0.0,0.002203910844400525
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",MSE Loss 설명,0.012086943,0.0,0.012086942791938782
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",MSE Loss 용도,0.977576,1.0,0.022423982620239258
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",Multi-Label 에서 CE + Softmax 적용 문제점,0.013513453,0.0,0.013513453304767609
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",PEFT 방법 5가지,0.0014402516,0.0,0.0014402515953406692
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",거대 언어 모델 정의,-0.0057688756,0.0,0.00576887559145689
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",기본 경험,-0.0074364827,0.0,0.007436482701450586
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",답변 실패,-0.021035995,0.0,0.021035995334386826
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",딥러닝,0.008774533,0.0,0.008774532936513424
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",마지막 할 말,-0.011532059,0.0,0.01153205893933773
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",머신러닝,0.000988205,0.0,0.000988204963505268
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",면접 시작 인사,0.0008461411,0.0,0.0008461410761810839
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",상세 경험,0.009400328,0.0,0.009400327689945698
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",수식,-0.027073285,0.0,0.02707328461110592
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",인공지능,-0.018827753,0.0,0.018827753141522408
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",잠시 휴식,-0.0059007998,0.0,0.005900799762457609
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",좋아하는 아이돌,0.0016900467,0.0,0.001690046745352447
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",핵심 아이디어,-0.020573035,0.0,0.02057303488254547
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",확률 예측에서 MSE Loss 미 사용 이유,-0.011012265,0.0,0.011012265458703041
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,BCE 가 좋은 task,-0.0029501144,0.0,0.002950114430859685
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,"BCE 가 좋은 task, 이유",-0.0062265587,0.0,0.006226558703929186
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,LLM Fine-Tuning 의 PEFT,-0.0056578843,0.0,0.005657884292304516
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,LoRA,0.001036973,0.0,0.0010369729716330767
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,LoRA 와 QLoRA 의 차이,-0.0015018146,0.0,0.0015018145786598325
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,Loss Function 예시,-0.0023780295,0.0,0.002378029515966773
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,Loss Function 정의,0.0009843668,0.0,0.000984366750344634
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,MBTI,-0.001321999,0.0,0.0013219990069046617
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,MSE Loss 설명,0.0059739384,0.0,0.005973938386887312
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,MSE Loss 용도,0.017303335,0.0,0.017303334549069405
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,Multi-Label 에서 CE + Softmax 적용 문제점,-0.001577204,0.0,0.0015772039769217372
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,PEFT 방법 5가지,-0.0076795653,0.0,0.007679565344005823
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,거대 언어 모델 정의,-0.007135606,0.0,0.007135605905205011
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,기본 경험,-0.018396283,0.0,0.01839628256857395
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,답변 실패,0.98429775,1.0,0.015702247619628906
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,딥러닝,-0.011279945,0.0,0.011279945261776447
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,마지막 할 말,-0.0020593856,0.0,0.002059385646134615
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,머신러닝,-0.008652939,0.0,0.008652939461171627
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,면접 시작 인사,-0.0028590702,0.0,0.002859070198610425
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,상세 경험,0.00055381266,0.0,0.0005538126570172608
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,수식,-0.010140417,0.0,0.010140417143702507
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,인공지능,-0.017339868,0.0,0.01733986847102642
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,잠시 휴식,-0.0074915187,0.0,0.007491518743336201
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,좋아하는 아이돌,-0.0011771499,0.0,0.001177149941213429
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,핵심 아이디어,-0.015497265,0.0,0.015497265383601189
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,확률 예측에서 MSE Loss 미 사용 이유,-0.005184544,0.0,0.005184543784707785
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,BCE 가 좋은 task,0.0031183572,0.0,0.00311835715547204
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,"BCE 가 좋은 task, 이유",0.009795945,0.0,0.009795945137739182
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,LLM Fine-Tuning 의 PEFT,0.0032678568,0.0,0.0032678567804396152
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,LoRA,0.0084681455,0.0,0.00846814550459385
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,LoRA 와 QLoRA 의 차이,0.014008364,0.0,0.014008363708853722
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,Loss Function 예시,-0.0077941706,0.0,0.007794170640408993
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,Loss Function 정의,-0.002228,0.0,0.0022279999684542418
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,MBTI,-0.010452906,0.0,0.010452905669808388
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,MSE Loss 설명,0.0063449335,0.0,0.006344933528453112
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,MSE Loss 용도,-0.008143585,0.0,0.008143585175275803
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,Multi-Label 에서 CE + Softmax 적용 문제점,-0.00011392328,0.0,0.00011392327724024653
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,PEFT 방법 5가지,-0.013182199,0.0,0.013182198628783226
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,거대 언어 모델 정의,0.0013297864,0.0,0.0013297863770276308
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,기본 경험,-0.028589793,0.0,0.028589792549610138
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,답변 실패,0.9666676,1.0,0.03333240747451782
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,딥러닝,0.0019500961,0.0,0.0019500961061567068
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,마지막 할 말,-0.012408832,0.0,0.012408832088112831
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,머신러닝,-0.015571969,0.0,0.015571968629956245
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,면접 시작 인사,-0.015007468,0.0,0.015007467940449715
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,상세 경험,-0.014887042,0.0,0.014887042343616486
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,수식,-0.008291602,0.0,0.00829160213470459
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,인공지능,-0.0010086822,0.0,0.0010086821857839823
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,잠시 휴식,-0.0077773673,0.0,0.007777367252856493
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,좋아하는 아이돌,-0.0066403816,0.0,0.006640381645411253
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,핵심 아이디어,-0.005062751,0.0,0.00506275100633502
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,확률 예측에서 MSE Loss 미 사용 이유,0.050263118,0.0,0.050263117998838425
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,BCE 가 좋은 task,-0.0019952804,0.0,0.001995280385017395
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,"BCE 가 좋은 task, 이유",-0.007925283,0.0,0.007925283163785934
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,LLM Fine-Tuning 의 PEFT,0.012766602,0.0,0.012766602449119091
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,LoRA,0.012660783,0.0,0.012660782784223557
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,LoRA 와 QLoRA 의 차이,-0.0041099377,0.0,0.004109937697649002
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,Loss Function 예시,-0.005108801,0.0,0.005108801182359457
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,Loss Function 정의,-0.012595833,0.0,0.012595833279192448
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,MBTI,-0.0061237365,0.0,0.006123736500740051
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,MSE Loss 설명,0.011938111,0.0,0.011938110925257206
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,MSE Loss 용도,0.0014773369,0.0,0.0014773368602618575
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,Multi-Label 에서 CE + Softmax 적용 문제점,-0.0063914303,0.0,0.006391430273652077
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,PEFT 방법 5가지,-0.0010045164,0.0,0.0010045163799077272
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,거대 언어 모델 정의,-0.018788114,0.0,0.018788114190101624
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,기본 경험,0.0077177505,0.0,0.007717750500887632
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,답변 실패,0.0039928383,0.0,0.003992838319391012
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,딥러닝,-0.014540662,0.0,0.014540662057697773
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,마지막 할 말,0.011871583,0.0,0.011871582828462124
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,머신러닝,0.021989232,0.0,0.021989231929183006
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,면접 시작 인사,-0.008944892,0.0,0.008944892324507236
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,상세 경험,-0.002771985,0.0,0.0027719850186258554
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,수식,-0.0068849907,0.0,0.0068849907256662846
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,인공지능,-0.01425655,0.0,0.014256549999117851
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,잠시 휴식,-0.0073390864,0.0,0.0073390863835811615
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,좋아하는 아이돌,-0.0027753697,0.0,0.0027753696776926517
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,핵심 아이디어,-0.013397407,0.0,0.013397406786680222
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,확률 예측에서 MSE Loss 미 사용 이유,0.9749269,1.0,0.025073111057281494
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,BCE 가 좋은 task,0.0010031248,0.0,0.001003124751150608
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,"BCE 가 좋은 task, 이유",-0.0018522268,0.0,0.0018522267928346992
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,LLM Fine-Tuning 의 PEFT,-0.0073606856,0.0,0.007360685616731644
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,LoRA,-0.030914035,0.0,0.030914034694433212
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,LoRA 와 QLoRA 의 차이,-0.01612185,0.0,0.016121849417686462
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,Loss Function 예시,0.010439787,0.0,0.010439787060022354
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,Loss Function 정의,-0.0064977077,0.0,0.006497707683593035
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,MBTI,-0.012587893,0.0,0.012587892822921276
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,MSE Loss 설명,-0.030685443,0.0,0.030685443431138992
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,MSE Loss 용도,-0.011661848,0.0,0.011661848053336143
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,Multi-Label 에서 CE + Softmax 적용 문제점,-0.009104019,0.0,0.009104019030928612
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,PEFT 방법 5가지,-0.038521267,0.0,0.03852126747369766
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,거대 언어 모델 정의,-0.0059763957,0.0,0.005976395681500435
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,기본 경험,0.019570606,0.0,0.0195706058293581
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,답변 실패,0.3322081,0.0,0.3322080969810486
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,딥러닝,0.011317963,0.0,0.011317962780594826
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,마지막 할 말,-0.011822285,0.0,0.011822285130620003
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,머신러닝,-0.025350282,0.0,0.025350281968712807
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,면접 시작 인사,-0.034206398,0.0,0.03420639783143997
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,상세 경험,0.0045349323,0.0,0.004534932319074869
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,수식,0.8549395,1.0,0.1450604796409607
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,인공지능,0.0062249806,0.0,0.0062249805778265
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,잠시 휴식,-0.0294858,0.0,0.029485799372196198
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,좋아하는 아이돌,0.002885987,0.0,0.002885987050831318
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,핵심 아이디어,-0.015640669,0.0,0.01564066857099533
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,확률 예측에서 MSE Loss 미 사용 이유,0.013566139,0.0,0.013566139154136181
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",BCE 가 좋은 task,0.001886724,0.0,0.001886724028736353
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거","BCE 가 좋은 task, 이유",-0.008290923,0.0,0.008290923200547695
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",LLM Fine-Tuning 의 PEFT,0.019258961,0.0,0.019258961081504822
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",LoRA,0.0078034657,0.0,0.0078034657053649426
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",LoRA 와 QLoRA 의 차이,0.012970445,0.0,0.01297044474631548
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",Loss Function 예시,-0.018484985,0.0,0.01848498545587063
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",Loss Function 정의,-0.016199728,0.0,0.016199728474020958
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",MBTI,-0.0018036428,0.0,0.001803642837330699
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",MSE Loss 설명,-0.028146375,0.0,0.028146374970674515
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",MSE Loss 용도,-0.0015734445,0.0,0.0015734444605186582
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",Multi-Label 에서 CE + Softmax 적용 문제점,-0.0022791126,0.0,0.002279112581163645
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",PEFT 방법 5가지,-0.0029302766,0.0,0.0029302765615284443
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",거대 언어 모델 정의,0.007166845,0.0,0.007166844792664051
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",기본 경험,0.0058375755,0.0,0.0058375755324959755
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",답변 실패,-0.0072092037,0.0,0.0072092036716639996
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",딥러닝,-0.004108251,0.0,0.0041082510724663734
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",마지막 할 말,0.0008289027,0.0,0.0008289027027785778
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",머신러닝,0.020651165,0.0,0.020651165395975113
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",면접 시작 인사,0.015305414,0.0,0.015305413864552975
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",상세 경험,0.0041976906,0.0,0.004197690635919571
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",수식,-0.021563258,0.0,0.02156325802206993
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",인공지능,0.00875705,0.0,0.008757050149142742
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",잠시 휴식,0.002554174,0.0,0.0025541740469634533
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",좋아하는 아이돌,0.015662858,0.0,0.01566285826265812
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",핵심 아이디어,0.94486254,1.0,0.055137455463409424
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",확률 예측에서 MSE Loss 미 사용 이유,-0.017488316,0.0,0.01748831570148468
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,BCE 가 좋은 task,-0.02440259,0.0,0.024402590468525887
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,"BCE 가 좋은 task, 이유",0.016784836,0.0,0.01678483560681343
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,LLM Fine-Tuning 의 PEFT,-0.024705844,0.0,0.02470584399998188
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,LoRA,-0.01809208,0.0,0.01809207908809185
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,LoRA 와 QLoRA 의 차이,-0.044304173,0.0,0.044304173439741135
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,Loss Function 예시,-0.03475767,0.0,0.034757670015096664
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,Loss Function 정의,0.0027250128,0.0,0.002725012833252549
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,MBTI,-0.020335494,0.0,0.020335493609309196
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,MSE Loss 설명,0.0049692797,0.0,0.004969279747456312
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,MSE Loss 용도,0.010440433,0.0,0.010440433397889137
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,Multi-Label 에서 CE + Softmax 적용 문제점,0.63608706,0.0,0.6360870599746704
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,PEFT 방법 5가지,0.0041381847,0.0,0.0041381847113370895
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,거대 언어 모델 정의,0.0163102,0.0,0.016310200095176697
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,기본 경험,-0.0046046576,0.0,0.004604657646268606
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,답변 실패,0.15058564,1.0,0.8494143635034561
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,딥러닝,0.0034086248,0.0,0.003408624790608883
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,마지막 할 말,-0.03234627,0.0,0.032346270978450775
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,머신러닝,0.0098190075,0.0,0.009819007478654385
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,면접 시작 인사,-0.016661856,0.0,0.016661856323480606
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,상세 경험,0.0014776828,0.0,0.0014776828465983272
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,수식,0.050670877,0.0,0.05067087709903717
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,인공지능,-0.014581386,0.0,0.014581385999917984
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,잠시 휴식,-0.029790387,0.0,0.02979038655757904
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,좋아하는 아이돌,0.017251467,0.0,0.01725146733224392
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,핵심 아이디어,0.34452567,0.0,0.34452566504478455
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,확률 예측에서 MSE Loss 미 사용 이유,-0.14034362,0.0,0.14034362137317657
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",BCE 가 좋은 task,-0.03061878,0.0,0.030618779361248016
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고","BCE 가 좋은 task, 이유",0.022925192,0.0,0.02292519249022007
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",LLM Fine-Tuning 의 PEFT,-0.02350802,0.0,0.023508019745349884
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",LoRA,0.016337618,0.0,0.016337618231773376
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",LoRA 와 QLoRA 의 차이,0.019358171,0.0,0.01935817115008831
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",Loss Function 예시,-0.0020354248,0.0,0.0020354248117655516
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",Loss Function 정의,-0.012486239,0.0,0.012486238963901997
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",MBTI,-0.008130576,0.0,0.008130576461553574
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",MSE Loss 설명,-0.016764408,0.0,0.016764407977461815
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",MSE Loss 용도,0.01565275,0.0,0.015652749687433243
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",Multi-Label 에서 CE + Softmax 적용 문제점,-0.016355624,0.0,0.016355624422430992
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",PEFT 방법 5가지,-0.0021388584,0.0,0.0021388584282249212
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",거대 언어 모델 정의,7.397446e-05,0.0,7.39744573365897e-05
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",기본 경험,0.015543122,0.0,0.015543121844530106
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",답변 실패,-0.0052676187,0.0,0.00526761868968606
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",딥러닝,0.014914818,0.0,0.014914818108081818
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",마지막 할 말,-0.009005299,0.0,0.009005298838019371
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",머신러닝,0.024068179,0.0,0.024068178609013557
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",면접 시작 인사,0.0110140545,0.0,0.011014054529368877
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",상세 경험,0.0015233655,0.0,0.0015233654994517565
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",수식,0.0070012677,0.0,0.007001267746090889
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",인공지능,0.005192585,0.0,0.005192584823817015
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",잠시 휴식,0.0043047387,0.0,0.004304738715291023
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",좋아하는 아이돌,-0.010652199,0.0,0.010652199387550354
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",핵심 아이디어,0.9308022,1.0,0.06919777393341064
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",확률 예측에서 MSE Loss 미 사용 이유,-0.017001059,0.0,0.017001058906316757
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",BCE 가 좋은 task,0.243743,0.0,0.24374300241470337
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!","BCE 가 좋은 task, 이유",-0.022171002,0.0,0.022171001881361008
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",LLM Fine-Tuning 의 PEFT,-0.010512322,0.0,0.010512322187423706
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",LoRA,0.00026705663,0.0,0.00026705663185566664
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",LoRA 와 QLoRA 의 차이,-0.004650555,0.0,0.004650555085390806
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",Loss Function 예시,-0.023693625,0.0,0.023693624883890152
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",Loss Function 정의,0.055509787,0.0,0.0555097870528698
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",MBTI,-0.030138353,0.0,0.030138352885842323
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",MSE Loss 설명,-0.050093636,0.0,0.0500936359167099
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",MSE Loss 용도,0.00048843515,0.0,0.0004884351510554552
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",Multi-Label 에서 CE + Softmax 적용 문제점,-0.052720148,0.0,0.052720148116350174
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",PEFT 방법 5가지,-0.008359798,0.0,0.008359798230230808
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",거대 언어 모델 정의,-0.018914154,0.0,0.018914153799414635
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",기본 경험,-0.01876881,0.0,0.018768809735774994
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",답변 실패,0.635082,1.0,0.3649179935455322
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",딥러닝,-0.019585213,0.0,0.01958521269261837
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",마지막 할 말,-0.03203018,0.0,0.03203018009662628
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",머신러닝,-0.022217514,0.0,0.022217513993382454
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",면접 시작 인사,-0.010966199,0.0,0.010966199450194836
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",상세 경험,-0.042548914,0.0,0.04254891350865364
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",수식,0.009084359,0.0,0.009084358811378479
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",인공지능,0.026980735,0.0,0.02698073536157608
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",잠시 휴식,-0.014080485,0.0,0.014080485329031944
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",좋아하는 아이돌,-0.011528558,0.0,0.01152855809777975
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",핵심 아이디어,-0.022106966,0.0,0.022106966003775597
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",확률 예측에서 MSE Loss 미 사용 이유,0.0047741914,0.0,0.004774191416800022
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",BCE 가 좋은 task,0.0023524133,1.0,0.9976475867442787
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어","BCE 가 좋은 task, 이유",0.004704314,0.0,0.0047043138183653355
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",LLM Fine-Tuning 의 PEFT,-0.005421357,0.0,0.005421357229351997
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",LoRA,0.002539351,0.0,0.0025393508840352297
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",LoRA 와 QLoRA 의 차이,0.0011642798,0.0,0.001164279761724174
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",Loss Function 예시,-0.008959552,0.0,0.008959552273154259
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",Loss Function 정의,-0.0019991633,0.0,0.0019991633016616106
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",MBTI,0.001828538,0.0,0.0018285380210727453
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",MSE Loss 설명,-0.004332408,0.0,0.004332407843321562
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",MSE Loss 용도,-0.00537473,0.0,0.005374730098992586
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",Multi-Label 에서 CE + Softmax 적용 문제점,0.00097413064,0.0,0.0009741306421346962
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",PEFT 방법 5가지,-0.006606612,0.0,0.006606611888855696
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",거대 언어 모델 정의,-0.0026255234,0.0,0.002625523367896676
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",기본 경험,-0.013590554,0.0,0.013590553775429726
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",답변 실패,0.9840961,0.0,0.984096109867096
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",딥러닝,-0.0044672205,0.0,0.004467220511287451
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",마지막 할 말,5.7275345e-05,0.0,5.727534517063759e-05
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",머신러닝,-0.0017185248,0.0,0.001718524843454361
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",면접 시작 인사,-0.006439784,0.0,0.006439784076064825
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",상세 경험,-0.005160282,0.0,0.005160281900316477
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",수식,-0.008832542,0.0,0.008832542225718498
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",인공지능,-0.018025473,0.0,0.0180254727602005
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",잠시 휴식,-0.010094186,0.0,0.010094186291098595
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",좋아하는 아이돌,-0.007823728,0.0,0.007823728024959564
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",핵심 아이디어,-0.007858399,0.0,0.00785839930176735
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",확률 예측에서 MSE Loss 미 사용 이유,-0.010188206,0.0,0.010188206098973751
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",BCE 가 좋은 task,0.033121653,0.0,0.03312165290117264
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아","BCE 가 좋은 task, 이유",0.9234435,1.0,0.0765565037727356
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",LLM Fine-Tuning 의 PEFT,-0.018862749,0.0,0.01886274851858616
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",LoRA,-0.0119691035,0.0,0.01196910347789526
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",LoRA 와 QLoRA 의 차이,0.009514998,0.0,0.009514997713267803
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",Loss Function 예시,-0.0053723925,0.0,0.005372392479330301
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",Loss Function 정의,-0.010146207,0.0,0.010146207176148891
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",MBTI,-0.017640926,0.0,0.01764092594385147
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",MSE Loss 설명,-0.009181408,0.0,0.00918140821158886
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",MSE Loss 용도,-0.027779317,0.0,0.027779316529631615
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",Multi-Label 에서 CE + Softmax 적용 문제점,-0.009040641,0.0,0.00904064066708088
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",PEFT 방법 5가지,-0.016120674,0.0,0.016120674088597298
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",거대 언어 모델 정의,-0.021399925,0.0,0.021399924531579018
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",기본 경험,0.006115972,0.0,0.006115972064435482
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",답변 실패,0.013090027,0.0,0.013090026564896107
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",딥러닝,-0.010086727,0.0,0.0100867273285985
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",마지막 할 말,0.0012686396,0.0,0.0012686395784839988
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",머신러닝,-0.039350536,0.0,0.03935053572058678
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",면접 시작 인사,-0.013544996,0.0,0.01354499626904726
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",상세 경험,0.013851947,0.0,0.013851947151124477
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",수식,0.008460695,0.0,0.008460694923996925
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",인공지능,-0.030326754,0.0,0.030326753854751587
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",잠시 휴식,-0.0073209414,0.0,0.007320941425859928
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",좋아하는 아이돌,-0.013553638,0.0,0.013553638011217117
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",핵심 아이디어,-0.039745867,0.0,0.039745867252349854
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",확률 예측에서 MSE Loss 미 사용 이유,-0.030790605,0.0,0.030790604650974274
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,BCE 가 좋은 task,-0.016367303,0.0,0.01636730320751667
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,"BCE 가 좋은 task, 이유",0.012855801,0.0,0.012855800800025463
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,LLM Fine-Tuning 의 PEFT,-0.010649498,0.0,0.010649497620761395
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,LoRA,-0.00060629216,0.0,0.0006062921602278948
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,LoRA 와 QLoRA 의 차이,1.7848271e-05,0.0,1.784827145456802e-05
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,Loss Function 예시,-0.01381878,0.0,0.013818779960274696
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,Loss Function 정의,-0.017186338,0.0,0.01718633808195591
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,MBTI,-0.0035862627,0.0,0.003586262697353959
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,MSE Loss 설명,-0.0018229106,0.0,0.0018229106208309531
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,MSE Loss 용도,-0.009721839,0.0,0.009721838869154453
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,Multi-Label 에서 CE + Softmax 적용 문제점,0.96499604,1.0,0.03500396013259888
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,PEFT 방법 5가지,0.0041746995,0.0,0.004174699541181326
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,거대 언어 모델 정의,-0.011321203,0.0,0.011321202851831913
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,기본 경험,-0.00029148892,0.0,0.00029148891917429864
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,답변 실패,-0.013790309,0.0,0.013790309429168701
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,딥러닝,-0.0061213127,0.0,0.0061213127337396145
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,마지막 할 말,-0.007734096,0.0,0.007734096143394709
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,머신러닝,-0.0021415388,0.0,0.0021415387745946646
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,면접 시작 인사,0.011188304,0.0,0.011188304051756859
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,상세 경험,-0.00036260267,0.0,0.0003626026737038046
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,수식,0.017892564,0.0,0.017892563715577126
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,인공지능,-0.012981044,0.0,0.012981044128537178
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,잠시 휴식,-0.014339612,0.0,0.014339611865580082
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,좋아하는 아이돌,0.00075298443,0.0,0.0007529844297096133
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,핵심 아이디어,-0.012253865,0.0,0.012253864668309689
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,확률 예측에서 MSE Loss 미 사용 이유,-0.0045758197,0.0,0.004575819708406925
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,BCE 가 좋은 task,0.0018528216,0.0,0.001852821558713913
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,"BCE 가 좋은 task, 이유",-0.005238065,0.0,0.005238065030425787
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,LLM Fine-Tuning 의 PEFT,-0.0014735997,0.0,0.0014735996956005692
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,LoRA,0.0077859876,0.0,0.007785987574607134
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,LoRA 와 QLoRA 의 차이,-0.0057514915,0.0,0.005751491524279118
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,Loss Function 예시,-0.0068053445,0.0,0.006805344484746456
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,Loss Function 정의,-0.007086959,0.0,0.0070869592018425465
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,MBTI,0.0018887183,0.0,0.0018887183396145701
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,MSE Loss 설명,0.003570885,0.0,0.003570884931832552
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,MSE Loss 용도,-0.010204873,0.0,0.01020487304776907
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,Multi-Label 에서 CE + Softmax 적용 문제점,0.0019998113,0.0,0.0019998112693428993
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,PEFT 방법 5가지,-0.0013688615,0.0,0.001368861529044807
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,거대 언어 모델 정의,-0.009646366,0.0,0.009646366350352764
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,기본 경험,-0.015958456,0.0,0.015958456322550774
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,답변 실패,0.98494476,1.0,0.015055239200592041
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,딥러닝,-0.004694674,0.0,0.004694674164056778
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,마지막 할 말,-0.006309112,0.0,0.0063091120682656765
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,머신러닝,-0.006254349,0.0,0.0062543489038944244
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,면접 시작 인사,0.00782484,0.0,0.007824840024113655
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,상세 경험,0.001233869,0.0,0.0012338689994066954
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,수식,-0.019597486,0.0,0.019597485661506653
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,인공지능,-0.004996345,0.0,0.004996344912797213
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,잠시 휴식,-0.0025437048,0.0,0.002543704817071557
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,좋아하는 아이돌,-0.0026560184,0.0,0.0026560183614492416
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,핵심 아이디어,0.0025482674,0.0,0.0025482673663645983
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,확률 예측에서 MSE Loss 미 사용 이유,-0.016190585,0.0,0.016190584748983383
Loss Function 관련 실무 경험 -> 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,BCE 가 좋은 task,-0.012470249,0.0,0.012470249086618423
Loss Function 관련 실무 경험 -> 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,"BCE 가 좋은 task, 이유",-0.0343569,0.0,0.03435689955949783
Loss Function 관련 실무 경험 -> 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,LLM Fine-Tuning 의 PEFT,0.02319225,0.0,0.02319224923849106
Loss Function 관련 실무 경험 -> 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,LoRA,0.007447205,0.0,0.007447205018252134
Loss Function 관련 실무 경험 -> 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,LoRA 와 QLoRA 의 차이,0.01082638,0.0,0.010826379992067814
Loss Function 관련 실무 경험 -> 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,Loss Function 예시,0.029624423,0.0,0.029624423012137413
Loss Function 관련 실무 경험 -> 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,Loss Function 정의,0.0010229696,0.0,0.0010229696054011583
Loss Function 관련 실무 경험 -> 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,MBTI,-0.0049879523,0.0,0.004987952299416065
Loss Function 관련 실무 경험 -> 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,MSE Loss 설명,0.02941774,0.0,0.029417740181088448
Loss Function 관련 실무 경험 -> 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,MSE Loss 용도,0.0033648808,0.0,0.0033648808021098375
Loss Function 관련 실무 경험 -> 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,Multi-Label 에서 CE + Softmax 적용 문제점,-0.007868942,0.0,0.007868941873311996
Loss Function 관련 실무 경험 -> 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,PEFT 방법 5가지,-0.028700124,0.0,0.028700124472379684
Loss Function 관련 실무 경험 -> 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,거대 언어 모델 정의,-0.014543472,0.0,0.014543471857905388
Loss Function 관련 실무 경험 -> 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,기본 경험,0.082493655,1.0,0.9175063446164131
Loss Function 관련 실무 경험 -> 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,답변 실패,-0.02620953,0.0,0.026209529489278793
Loss Function 관련 실무 경험 -> 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,딥러닝,0.028701352,0.0,0.028701351955533028
Loss Function 관련 실무 경험 -> 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,마지막 할 말,-0.054960586,0.0,0.05496058613061905
Loss Function 관련 실무 경험 -> 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,머신러닝,-0.010271421,0.0,0.010271420702338219
Loss Function 관련 실무 경험 -> 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,면접 시작 인사,0.009009878,0.0,0.009009878151118755
Loss Function 관련 실무 경험 -> 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,상세 경험,0.8916859,0.0,0.8916859030723572
Loss Function 관련 실무 경험 -> 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,수식,-0.032251514,0.0,0.0322515144944191
Loss Function 관련 실무 경험 -> 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,인공지능,-0.0029980277,0.0,0.00299802771769464
Loss Function 관련 실무 경험 -> 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,잠시 휴식,-0.015830558,0.0,0.01583055779337883
Loss Function 관련 실무 경험 -> 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,좋아하는 아이돌,-0.014846551,0.0,0.014846551232039928
Loss Function 관련 실무 경험 -> 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,핵심 아이디어,-0.01655518,0.0,0.016555180773139
Loss Function 관련 실무 경험 -> 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,확률 예측에서 MSE Loss 미 사용 이유,0.012537193,0.0,0.012537192553281784
Loss Function 관련 실무 경험 -> 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,BCE 가 좋은 task,0.0114018265,0.0,0.0114018265157938
Loss Function 관련 실무 경험 -> 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,"BCE 가 좋은 task, 이유",0.0028027038,0.0,0.0028027037624269724
Loss Function 관련 실무 경험 -> 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,LLM Fine-Tuning 의 PEFT,0.016135737,0.0,0.01613573729991913
Loss Function 관련 실무 경험 -> 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,LoRA,-0.0029775435,0.0,0.0029775435104966164
Loss Function 관련 실무 경험 -> 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,LoRA 와 QLoRA 의 차이,0.004071954,0.0,0.004071954172104597
Loss Function 관련 실무 경험 -> 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,Loss Function 예시,-0.016881058,0.0,0.016881057992577553
Loss Function 관련 실무 경험 -> 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,Loss Function 정의,-0.026656765,0.0,0.02665676549077034
Loss Function 관련 실무 경험 -> 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,MBTI,-0.019873636,0.0,0.019873635843396187
Loss Function 관련 실무 경험 -> 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,MSE Loss 설명,-0.0415331,0.0,0.041533101350069046
Loss Function 관련 실무 경험 -> 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,MSE Loss 용도,-0.017119763,0.0,0.017119763419032097
Loss Function 관련 실무 경험 -> 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,Multi-Label 에서 CE + Softmax 적용 문제점,0.008499053,0.0,0.008499053306877613
Loss Function 관련 실무 경험 -> 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,PEFT 방법 5가지,-0.0069859987,0.0,0.006985998712480068
Loss Function 관련 실무 경험 -> 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,거대 언어 모델 정의,-0.031364378,0.0,0.031364377588033676
Loss Function 관련 실무 경험 -> 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,기본 경험,0.84835196,0.0,0.8483519554138184
Loss Function 관련 실무 경험 -> 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,답변 실패,0.15646283,0.0,0.15646283328533173
Loss Function 관련 실무 경험 -> 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,딥러닝,-0.015979704,0.0,0.01597970351576805
Loss Function 관련 실무 경험 -> 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,마지막 할 말,0.014943238,0.0,0.014943238347768784
Loss Function 관련 실무 경험 -> 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,머신러닝,-0.0047663003,0.0,0.004766300320625305
Loss Function 관련 실무 경험 -> 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,면접 시작 인사,-0.014503115,0.0,0.014503114856779575
Loss Function 관련 실무 경험 -> 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,상세 경험,0.047983974,1.0,0.9520160257816315
Loss Function 관련 실무 경험 -> 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,수식,-0.025861362,0.0,0.025861361995339394
Loss Function 관련 실무 경험 -> 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,인공지능,-0.0023478486,0.0,0.0023478486109524965
Loss Function 관련 실무 경험 -> 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,잠시 휴식,-0.0040186937,0.0,0.004018693696707487
Loss Function 관련 실무 경험 -> 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,좋아하는 아이돌,-0.017002095,0.0,0.01700209453701973
Loss Function 관련 실무 경험 -> 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,핵심 아이디어,0.00045482523,0.0,0.00045482523273676634
Loss Function 관련 실무 경험 -> 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,확률 예측에서 MSE Loss 미 사용 이유,-0.032909174,0.0,0.032909173518419266
Loss Function 관련 실무 경험 -> 근데 기억 진짜 안나는데 어떡하지?,BCE 가 좋은 task,-0.0049113277,0.0,0.004911327734589577
Loss Function 관련 실무 경험 -> 근데 기억 진짜 안나는데 어떡하지?,"BCE 가 좋은 task, 이유",0.0055005294,0.0,0.0055005294270813465
Loss Function 관련 실무 경험 -> 근데 기억 진짜 안나는데 어떡하지?,LLM Fine-Tuning 의 PEFT,0.00012464501,0.0,0.0001246450119651854
Loss Function 관련 실무 경험 -> 근데 기억 진짜 안나는데 어떡하지?,LoRA,-0.0063917087,0.0,0.006391708739101887
Loss Function 관련 실무 경험 -> 근데 기억 진짜 안나는데 어떡하지?,LoRA 와 QLoRA 의 차이,-0.004544833,0.0,0.004544833209365606
Loss Function 관련 실무 경험 -> 근데 기억 진짜 안나는데 어떡하지?,Loss Function 예시,0.0006317269,0.0,0.000631726928986609
Loss Function 관련 실무 경험 -> 근데 기억 진짜 안나는데 어떡하지?,Loss Function 정의,-0.010984422,0.0,0.010984421707689762
Loss Function 관련 실무 경험 -> 근데 기억 진짜 안나는데 어떡하지?,MBTI,-0.0008113128,0.0,0.0008113128133118153
Loss Function 관련 실무 경험 -> 근데 기억 진짜 안나는데 어떡하지?,MSE Loss 설명,-0.003900818,0.0,0.0039008180610835552
Loss Function 관련 실무 경험 -> 근데 기억 진짜 안나는데 어떡하지?,MSE Loss 용도,-0.005605457,0.0,0.005605456884950399
Loss Function 관련 실무 경험 -> 근데 기억 진짜 안나는데 어떡하지?,Multi-Label 에서 CE + Softmax 적용 문제점,-0.0038720414,0.0,0.003872041357681155
Loss Function 관련 실무 경험 -> 근데 기억 진짜 안나는데 어떡하지?,PEFT 방법 5가지,-0.0010405276,0.0,0.0010405275970697403
Loss Function 관련 실무 경험 -> 근데 기억 진짜 안나는데 어떡하지?,거대 언어 모델 정의,-0.0033950428,0.0,0.003395042847841978
Loss Function 관련 실무 경험 -> 근데 기억 진짜 안나는데 어떡하지?,기본 경험,0.021551628,0.0,0.021551627665758133
Loss Function 관련 실무 경험 -> 근데 기억 진짜 안나는데 어떡하지?,답변 실패,0.9872987,1.0,0.012701272964477539
Loss Function 관련 실무 경험 -> 근데 기억 진짜 안나는데 어떡하지?,딥러닝,-0.011694347,0.0,0.01169434655457735
Loss Function 관련 실무 경험 -> 근데 기억 진짜 안나는데 어떡하지?,마지막 할 말,-0.0065697995,0.0,0.00656979950144887
Loss Function 관련 실무 경험 -> 근데 기억 진짜 안나는데 어떡하지?,머신러닝,-0.009755652,0.0,0.009755652397871017
Loss Function 관련 실무 경험 -> 근데 기억 진짜 안나는데 어떡하지?,면접 시작 인사,-0.0077628214,0.0,0.007762821391224861
Loss Function 관련 실무 경험 -> 근데 기억 진짜 안나는데 어떡하지?,상세 경험,0.0040473538,0.0,0.0040473537519574165
Loss Function 관련 실무 경험 -> 근데 기억 진짜 안나는데 어떡하지?,수식,-0.014164876,0.0,0.014164876192808151
Loss Function 관련 실무 경험 -> 근데 기억 진짜 안나는데 어떡하지?,인공지능,-0.016228128,0.0,0.016228128224611282
Loss Function 관련 실무 경험 -> 근데 기억 진짜 안나는데 어떡하지?,잠시 휴식,-0.0043095252,0.0,0.0043095252476632595
Loss Function 관련 실무 경험 -> 근데 기억 진짜 안나는데 어떡하지?,좋아하는 아이돌,0.0018813793,0.0,0.0018813792848959565
Loss Function 관련 실무 경험 -> 근데 기억 진짜 안나는데 어떡하지?,핵심 아이디어,-0.0038494028,0.0,0.003849402768537402
Loss Function 관련 실무 경험 -> 근데 기억 진짜 안나는데 어떡하지?,확률 예측에서 MSE Loss 미 사용 이유,-0.008512988,0.0,0.00851298775523901
Loss Function 관련 실무 경험 -> 나 아직 실무 경험 없는데 ㅠㅠ,BCE 가 좋은 task,-0.0064848554,0.0,0.006484855432063341
Loss Function 관련 실무 경험 -> 나 아직 실무 경험 없는데 ㅠㅠ,"BCE 가 좋은 task, 이유",0.0059592673,0.0,0.005959267262369394
Loss Function 관련 실무 경험 -> 나 아직 실무 경험 없는데 ㅠㅠ,LLM Fine-Tuning 의 PEFT,0.0025563247,0.0,0.002556324703618884
Loss Function 관련 실무 경험 -> 나 아직 실무 경험 없는데 ㅠㅠ,LoRA,0.0006282096,0.0,0.0006282096146605909
Loss Function 관련 실무 경험 -> 나 아직 실무 경험 없는데 ㅠㅠ,LoRA 와 QLoRA 의 차이,0.00047569987,0.0,0.0004756998678203672
Loss Function 관련 실무 경험 -> 나 아직 실무 경험 없는데 ㅠㅠ,Loss Function 예시,0.01224931,0.0,0.012249309569597244
Loss Function 관련 실무 경험 -> 나 아직 실무 경험 없는데 ㅠㅠ,Loss Function 정의,-0.015962265,0.0,0.01596226543188095
Loss Function 관련 실무 경험 -> 나 아직 실무 경험 없는데 ㅠㅠ,MBTI,-0.0060717673,0.0,0.006071767304092646
Loss Function 관련 실무 경험 -> 나 아직 실무 경험 없는데 ㅠㅠ,MSE Loss 설명,-0.0057093194,0.0,0.0057093193754553795
Loss Function 관련 실무 경험 -> 나 아직 실무 경험 없는데 ㅠㅠ,MSE Loss 용도,0.0070941104,0.0,0.007094110362231731
Loss Function 관련 실무 경험 -> 나 아직 실무 경험 없는데 ㅠㅠ,Multi-Label 에서 CE + Softmax 적용 문제점,-0.002214913,0.0,0.002214913023635745
Loss Function 관련 실무 경험 -> 나 아직 실무 경험 없는데 ㅠㅠ,PEFT 방법 5가지,-0.0063549303,0.0,0.006354930344969034
Loss Function 관련 실무 경험 -> 나 아직 실무 경험 없는데 ㅠㅠ,거대 언어 모델 정의,-0.0061249807,0.0,0.0061249807476997375
Loss Function 관련 실무 경험 -> 나 아직 실무 경험 없는데 ㅠㅠ,기본 경험,0.95849854,1.0,0.04150146245956421
Loss Function 관련 실무 경험 -> 나 아직 실무 경험 없는데 ㅠㅠ,답변 실패,0.013949241,0.0,0.013949240557849407
Loss Function 관련 실무 경험 -> 나 아직 실무 경험 없는데 ㅠㅠ,딥러닝,-0.0015555503,0.0,0.00155555026140064
Loss Function 관련 실무 경험 -> 나 아직 실무 경험 없는데 ㅠㅠ,마지막 할 말,0.0032095546,0.0,0.0032095545902848244
Loss Function 관련 실무 경험 -> 나 아직 실무 경험 없는데 ㅠㅠ,머신러닝,-0.003353802,0.0,0.0033538020215928555
Loss Function 관련 실무 경험 -> 나 아직 실무 경험 없는데 ㅠㅠ,면접 시작 인사,-0.0017157742,0.0,0.0017157741822302341
Loss Function 관련 실무 경험 -> 나 아직 실무 경험 없는데 ㅠㅠ,상세 경험,-0.04407561,0.0,0.044075608253479004
Loss Function 관련 실무 경험 -> 나 아직 실무 경험 없는데 ㅠㅠ,수식,-0.01011086,0.0,0.010110859759151936
Loss Function 관련 실무 경험 -> 나 아직 실무 경험 없는데 ㅠㅠ,인공지능,-0.009069314,0.0,0.00906931422650814
Loss Function 관련 실무 경험 -> 나 아직 실무 경험 없는데 ㅠㅠ,잠시 휴식,-0.010032598,0.0,0.010032597929239273
Loss Function 관련 실무 경험 -> 나 아직 실무 경험 없는데 ㅠㅠ,좋아하는 아이돌,-0.0023475764,0.0,0.002347576431930065
Loss Function 관련 실무 경험 -> 나 아직 실무 경험 없는데 ㅠㅠ,핵심 아이디어,0.00033858218,0.0,0.00033858217648230493
Loss Function 관련 실무 경험 -> 나 아직 실무 경험 없는데 ㅠㅠ,확률 예측에서 MSE Loss 미 사용 이유,0.004798106,0.0,0.004798105917870998
Loss Function 관련 실무 경험 -> 대답하기 싫은데 힝,BCE 가 좋은 task,-0.0041441773,0.0,0.004144177306443453
Loss Function 관련 실무 경험 -> 대답하기 싫은데 힝,"BCE 가 좋은 task, 이유",-0.0007379627,0.0,0.0007379627204500139
Loss Function 관련 실무 경험 -> 대답하기 싫은데 힝,LLM Fine-Tuning 의 PEFT,-0.0012777062,0.0,0.0012777062365785241
Loss Function 관련 실무 경험 -> 대답하기 싫은데 힝,LoRA,-0.0013554385,0.0,0.001355438493192196
Loss Function 관련 실무 경험 -> 대답하기 싫은데 힝,LoRA 와 QLoRA 의 차이,-0.0059464206,0.0,0.005946420598775148
Loss Function 관련 실무 경험 -> 대답하기 싫은데 힝,Loss Function 예시,0.0016091451,0.0,0.001609145081602037
Loss Function 관련 실무 경험 -> 대답하기 싫은데 힝,Loss Function 정의,-0.009912064,0.0,0.009912064298987389
Loss Function 관련 실무 경험 -> 대답하기 싫은데 힝,MBTI,-0.0028640914,0.0,0.0028640914242714643
Loss Function 관련 실무 경험 -> 대답하기 싫은데 힝,MSE Loss 설명,-0.0025740531,0.0,0.0025740531273186207
Loss Function 관련 실무 경험 -> 대답하기 싫은데 힝,MSE Loss 용도,-0.007330003,0.0,0.007330003194510937
Loss Function 관련 실무 경험 -> 대답하기 싫은데 힝,Multi-Label 에서 CE + Softmax 적용 문제점,-0.001836169,0.0,0.001836169045418501
Loss Function 관련 실무 경험 -> 대답하기 싫은데 힝,PEFT 방법 5가지,-0.0040946766,0.0,0.0040946765802800655
Loss Function 관련 실무 경험 -> 대답하기 싫은데 힝,거대 언어 모델 정의,-0.008055861,0.0,0.008055861108005047
Loss Function 관련 실무 경험 -> 대답하기 싫은데 힝,기본 경험,0.007434873,0.0,0.0074348729103803635
Loss Function 관련 실무 경험 -> 대답하기 싫은데 힝,답변 실패,0.9901998,1.0,0.009800195693969727
Loss Function 관련 실무 경험 -> 대답하기 싫은데 힝,딥러닝,-0.0039332877,0.0,0.003933287691324949
Loss Function 관련 실무 경험 -> 대답하기 싫은데 힝,마지막 할 말,-0.007717625,0.0,0.007717624772340059
Loss Function 관련 실무 경험 -> 대답하기 싫은데 힝,머신러닝,-0.0077101216,0.0,0.00771012157201767
Loss Function 관련 실무 경험 -> 대답하기 싫은데 힝,면접 시작 인사,-0.0033182953,0.0,0.0033182953484356403
Loss Function 관련 실무 경험 -> 대답하기 싫은데 힝,상세 경험,0.0031637072,0.0,0.00316370720975101
Loss Function 관련 실무 경험 -> 대답하기 싫은데 힝,수식,-0.008683713,0.0,0.008683713153004646
Loss Function 관련 실무 경험 -> 대답하기 싫은데 힝,인공지능,-0.0066954945,0.0,0.006695494521409273
Loss Function 관련 실무 경험 -> 대답하기 싫은데 힝,잠시 휴식,-0.007834326,0.0,0.007834325544536114
Loss Function 관련 실무 경험 -> 대답하기 싫은데 힝,좋아하는 아이돌,0.0008532087,0.0,0.000853208708576858
Loss Function 관련 실무 경험 -> 대답하기 싫은데 힝,핵심 아이디어,-0.0056851613,0.0,0.005685161333531141
Loss Function 관련 실무 경험 -> 대답하기 싫은데 힝,확률 예측에서 MSE Loss 미 사용 이유,-0.012821533,0.0,0.012821532785892487
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,BCE 가 좋은 task,0.013916862,0.0,0.013916862197220325
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,"BCE 가 좋은 task, 이유",0.0034073098,0.0,0.003407309763133526
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,LLM Fine-Tuning 의 PEFT,0.004966101,0.0,0.0049661011435091496
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,LoRA,-0.041936114,0.0,0.04193611443042755
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,LoRA 와 QLoRA 의 차이,0.00311754,0.0,0.003117539919912815
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,Loss Function 예시,-0.0054313457,0.0,0.005431345663964748
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,Loss Function 정의,-0.0076435874,0.0,0.007643587421625853
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,MBTI,0.9655486,1.0,0.034451425075531006
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,MSE Loss 설명,-0.023253186,0.0,0.02325318567454815
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,MSE Loss 용도,-0.02438084,0.0,0.024380840361118317
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,Multi-Label 에서 CE + Softmax 적용 문제점,-0.03530322,0.0,0.03530322015285492
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,PEFT 방법 5가지,-0.003791011,0.0,0.0037910109385848045
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,거대 언어 모델 정의,-0.0099075325,0.0,0.00990753248333931
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,기본 경험,-0.0011259157,0.0,0.0011259156744927168
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,답변 실패,-0.01003406,0.0,0.01003406010568142
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,딥러닝,0.009061305,0.0,0.009061304852366447
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,마지막 할 말,0.0073678456,0.0,0.0073678456246852875
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,머신러닝,0.02456795,0.0,0.024567950516939163
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,면접 시작 인사,0.0070678727,0.0,0.007067872676998377
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,상세 경험,0.0017820267,0.0,0.0017820267239585519
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,수식,-0.025856595,0.0,0.02585659548640251
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,인공지능,-0.027913181,0.0,0.027913181111216545
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,잠시 휴식,-0.019904902,0.0,0.019904902204871178
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,좋아하는 아이돌,-0.009803671,0.0,0.009803671389818192
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,핵심 아이디어,-0.018921196,0.0,0.018921196460723877
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,확률 예측에서 MSE Loss 미 사용 이유,-0.019929172,0.0,0.019929172471165657
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,BCE 가 좋은 task,0.0060168155,0.0,0.006016815546900034
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,"BCE 가 좋은 task, 이유",0.008346903,0.0,0.008346903137862682
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,LLM Fine-Tuning 의 PEFT,0.020784426,0.0,0.020784426480531693
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,LoRA,-0.00079722225,0.0,0.0007972222520038486
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,LoRA 와 QLoRA 의 차이,0.008802979,0.0,0.008802979253232479
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,Loss Function 예시,0.0029202919,0.0,0.0029202918522059917
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,Loss Function 정의,0.008832309,0.0,0.008832309395074844
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,MBTI,-0.00095471874,0.0,0.0009547187364660203
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,MSE Loss 설명,0.0032956977,0.0,0.0032956977374851704
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,MSE Loss 용도,-5.193561e-05,0.0,5.1935610827058554e-05
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,Multi-Label 에서 CE + Softmax 적용 문제점,0.013079519,0.0,0.013079519383609295
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,PEFT 방법 5가지,-0.011669929,0.0,0.011669929139316082
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,거대 언어 모델 정의,-0.0030504935,0.0,0.0030504935421049595
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,기본 경험,-0.010516927,0.0,0.010516926646232605
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,답변 실패,-0.004598574,0.0,0.004598573781549931
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,딥러닝,-0.009647552,0.0,0.00964755192399025
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,마지막 할 말,-0.010276163,0.0,0.01027616299688816
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,머신러닝,0.0075785313,0.0,0.00757853128015995
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,면접 시작 인사,-0.004489504,0.0,0.004489503800868988
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,상세 경험,0.01189648,0.0,0.01189647987484932
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,수식,-0.0057440423,0.0,0.005744042340666056
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,인공지능,-0.013012503,0.0,0.013012503273785114
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,잠시 휴식,-0.0050547635,0.0,0.00505476351827383
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,좋아하는 아이돌,0.97420967,1.0,0.02579033374786377
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,핵심 아이디어,-0.007314657,0.0,0.007314656861126423
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,확률 예측에서 MSE Loss 미 사용 이유,-0.015879387,0.0,0.01587938703596592
잠시 휴식 -> 재미있는 이야기 해줄래?,BCE 가 좋은 task,-0.000429051,0.0,0.0004290509968996048
잠시 휴식 -> 재미있는 이야기 해줄래?,"BCE 가 좋은 task, 이유",0.00012306606,0.0,0.00012306605640333146
잠시 휴식 -> 재미있는 이야기 해줄래?,LLM Fine-Tuning 의 PEFT,-0.0077690873,0.0,0.007769087329506874
잠시 휴식 -> 재미있는 이야기 해줄래?,LoRA,0.0051951935,0.0,0.005195193458348513
잠시 휴식 -> 재미있는 이야기 해줄래?,LoRA 와 QLoRA 의 차이,-0.008109501,0.0,0.008109500631690025
잠시 휴식 -> 재미있는 이야기 해줄래?,Loss Function 예시,-0.0021400796,0.0,0.0021400796249508858
잠시 휴식 -> 재미있는 이야기 해줄래?,Loss Function 정의,-0.0112325745,0.0,0.011232574470341206
잠시 휴식 -> 재미있는 이야기 해줄래?,MBTI,-0.006699697,0.0,0.0066996971145272255
잠시 휴식 -> 재미있는 이야기 해줄래?,MSE Loss 설명,0.011656365,0.0,0.011656365357339382
잠시 휴식 -> 재미있는 이야기 해줄래?,MSE Loss 용도,-0.0023378835,0.0,0.002337883459404111
잠시 휴식 -> 재미있는 이야기 해줄래?,Multi-Label 에서 CE + Softmax 적용 문제점,-0.0076343273,0.0,0.007634327281266451
잠시 휴식 -> 재미있는 이야기 해줄래?,PEFT 방법 5가지,-0.010543949,0.0,0.010543948970735073
잠시 휴식 -> 재미있는 이야기 해줄래?,거대 언어 모델 정의,-0.0050902446,0.0,0.005090244580060244
잠시 휴식 -> 재미있는 이야기 해줄래?,기본 경험,-0.016818097,0.0,0.016818096861243248
잠시 휴식 -> 재미있는 이야기 해줄래?,답변 실패,-0.020935347,0.0,0.02093534730374813
잠시 휴식 -> 재미있는 이야기 해줄래?,딥러닝,0.0022370305,0.0,0.0022370305377990007
잠시 휴식 -> 재미있는 이야기 해줄래?,마지막 할 말,0.009185822,0.0,0.009185821749269962
잠시 휴식 -> 재미있는 이야기 해줄래?,머신러닝,-0.021619668,0.0,0.02161966823041439
잠시 휴식 -> 재미있는 이야기 해줄래?,면접 시작 인사,-0.0056038983,0.0,0.00560389831662178
잠시 휴식 -> 재미있는 이야기 해줄래?,상세 경험,-0.001438434,0.0,0.0014384340029209852
잠시 휴식 -> 재미있는 이야기 해줄래?,수식,-0.00468211,0.0,0.004682110156863928
잠시 휴식 -> 재미있는 이야기 해줄래?,인공지능,-0.0155462455,0.0,0.015546245500445366
잠시 휴식 -> 재미있는 이야기 해줄래?,잠시 휴식,0.9872866,1.0,0.012713372707366943
잠시 휴식 -> 재미있는 이야기 해줄래?,좋아하는 아이돌,-0.0087485295,0.0,0.008748529478907585
잠시 휴식 -> 재미있는 이야기 해줄래?,핵심 아이디어,-0.0037920643,0.0,0.0037920642644166946
잠시 휴식 -> 재미있는 이야기 해줄래?,확률 예측에서 MSE Loss 미 사용 이유,-0.013361872,0.0,0.013361872173845768
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",BCE 가 좋은 task,-0.027123822,0.0,0.027123821899294853
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야","BCE 가 좋은 task, 이유",0.028681213,0.0,0.028681213036179543
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",LLM Fine-Tuning 의 PEFT,0.9658171,1.0,0.03418290615081787
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",LoRA,-0.007603348,0.0,0.007603348232805729
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",LoRA 와 QLoRA 의 차이,0.017458282,0.0,0.01745828241109848
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",Loss Function 예시,-0.016775155,0.0,0.016775155439972878
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",Loss Function 정의,-0.022967981,0.0,0.022967981174588203
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",MBTI,0.006514989,0.0,0.006514988839626312
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",MSE Loss 설명,0.023634437,0.0,0.02363443747162819
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",MSE Loss 용도,0.00072574086,0.0,0.0007257408578880131
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",Multi-Label 에서 CE + Softmax 적용 문제점,-0.0016864556,0.0,0.0016864555655047297
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",PEFT 방법 5가지,-0.016094565,0.0,0.016094565391540527
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",거대 언어 모델 정의,-0.023838663,0.0,0.02383866347372532
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",기본 경험,-0.02238341,0.0,0.02238341048359871
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",답변 실패,0.003991275,0.0,0.00399127509444952
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",딥러닝,-0.0032447204,0.0,0.0032447203993797302
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",마지막 할 말,-0.0042648017,0.0,0.004264801740646362
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",머신러닝,-0.016145598,0.0,0.016145598143339157
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",면접 시작 인사,-0.009554197,0.0,0.009554197080433369
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",상세 경험,0.00046624223,0.0,0.0004662422288674861
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",수식,0.017798532,0.0,0.0177985318005085
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",인공지능,-0.012581069,0.0,0.012581069022417068
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",잠시 휴식,-0.0075616357,0.0,0.0075616356916725636
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",좋아하는 아이돌,0.0057513164,0.0,0.00575131643563509
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",핵심 아이디어,-0.012674997,0.0,0.012674996629357338
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",확률 예측에서 MSE Loss 미 사용 이유,0.0120388735,0.0,0.012038873508572578
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,BCE 가 좋은 task,-0.0028904493,0.0,0.0028904492501169443
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,"BCE 가 좋은 task, 이유",0.0027119338,0.0,0.0027119338046759367
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,LLM Fine-Tuning 의 PEFT,0.0045320317,0.0,0.004532031714916229
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,LoRA,0.00019817786,0.0,0.00019817786233033985
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,LoRA 와 QLoRA 의 차이,-0.002926217,0.0,0.0029262169264256954
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,Loss Function 예시,-0.0073872437,0.0,0.007387243676930666
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,Loss Function 정의,-0.0068666944,0.0,0.006866694428026676
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,MBTI,-0.0038428444,0.0,0.00384284439496696
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,MSE Loss 설명,-0.0073262965,0.0,0.007326296530663967
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,MSE Loss 용도,-0.008025372,0.0,0.00802537240087986
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,Multi-Label 에서 CE + Softmax 적용 문제점,-0.0011951798,0.0,0.0011951797641813755
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,PEFT 방법 5가지,0.00278834,0.0,0.002788339974358678
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,거대 언어 모델 정의,-0.0078342855,0.0,0.007834285497665405
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,기본 경험,0.0023981782,0.0,0.0023981782142072916
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,답변 실패,0.9847719,1.0,0.015228092670440674
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,딥러닝,-0.0103039015,0.0,0.010303901508450508
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,마지막 할 말,0.0008780404,0.0,0.0008780403877608478
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,머신러닝,-0.0035229684,0.0,0.0035229683853685856
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,면접 시작 인사,0.0070912074,0.0,0.007091207429766655
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,상세 경험,0.0012337681,0.0,0.0012337680673226714
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,수식,-0.011488294,0.0,0.0114882942289114
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,인공지능,-0.008877603,0.0,0.008877603337168694
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,잠시 휴식,-0.009153824,0.0,0.009153824299573898
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,좋아하는 아이돌,-0.004562952,0.0,0.0045629520900547504
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,핵심 아이디어,0.0021982694,0.0,0.002198269357904792
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,확률 예측에서 MSE Loss 미 사용 이유,-0.016188584,0.0,0.01618858426809311
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",BCE 가 좋은 task,0.004189979,0.0,0.0041899788193404675
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?","BCE 가 좋은 task, 이유",-0.024759822,0.0,0.024759821593761444
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",LLM Fine-Tuning 의 PEFT,-0.03766405,0.0,0.03766404837369919
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",LoRA,0.0033552747,0.0,0.003355274675413966
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",LoRA 와 QLoRA 의 차이,0.032584097,0.0,0.03258409723639488
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",Loss Function 예시,0.0064623845,0.0,0.006462384480983019
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",Loss Function 정의,-0.0010280467,0.0,0.0010280467104166746
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",MBTI,-0.0028254301,0.0,0.002825430128723383
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",MSE Loss 설명,-0.0030962417,0.0,0.0030962417367845774
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",MSE Loss 용도,0.017244948,0.0,0.01724494807422161
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",Multi-Label 에서 CE + Softmax 적용 문제점,0.021237947,0.0,0.021237947046756744
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",PEFT 방법 5가지,0.96278703,1.0,0.03721296787261963
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",거대 언어 모델 정의,-0.015899403,0.0,0.015899403020739555
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",기본 경험,-0.0020222904,0.0,0.0020222903694957495
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",답변 실패,0.0022535692,0.0,0.002253569196909666
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",딥러닝,-0.012616628,0.0,0.012616627849638462
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",마지막 할 말,-0.026384898,0.0,0.026384897530078888
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",머신러닝,0.021566326,0.0,0.021566325798630714
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",면접 시작 인사,0.0046531395,0.0,0.004653139505535364
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",상세 경험,-0.020516237,0.0,0.02051623724400997
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",수식,-0.022314694,0.0,0.02231469377875328
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",인공지능,-0.003845283,0.0,0.0038452830631285906
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",잠시 휴식,-0.0025713542,0.0,0.002571354154497385
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",좋아하는 아이돌,-0.0041703293,0.0,0.004170329309999943
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",핵심 아이디어,-0.0069887657,0.0,0.006988765671849251
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",확률 예측에서 MSE Loss 미 사용 이유,-0.0074639963,0.0,0.007463996298611164
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,BCE 가 좋은 task,-0.026960354,0.0,0.026960354298353195
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,"BCE 가 좋은 task, 이유",0.015312057,0.0,0.015312056988477707
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,LLM Fine-Tuning 의 PEFT,-0.026219679,0.0,0.026219679042696953
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,LoRA,-0.00016307387,0.0,0.0001630738697713241
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,LoRA 와 QLoRA 의 차이,-0.036280178,0.0,0.036280177533626556
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,Loss Function 예시,0.01883521,0.0,0.01883520931005478
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,Loss Function 정의,-0.031714402,0.0,0.03171440213918686
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,MBTI,-0.0061297403,0.0,0.00612974027171731
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,MSE Loss 설명,-0.022357633,0.0,0.022357633337378502
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,MSE Loss 용도,-0.017251559,0.0,0.01725155860185623
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,Multi-Label 에서 CE + Softmax 적용 문제점,-0.013849908,0.0,0.01384990755468607
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,PEFT 방법 5가지,0.33279258,0.0,0.33279258012771606
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,거대 언어 모델 정의,-0.014114804,0.0,0.01411480363458395
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,기본 경험,-0.012613019,0.0,0.012613018974661827
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,답변 실패,0.84812695,1.0,0.15187305212020874
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,딥러닝,0.007311821,0.0,0.007311820983886719
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,마지막 할 말,-0.0111759,0.0,0.011175899766385555
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,머신러닝,-0.006014981,0.0,0.0060149808414280415
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,면접 시작 인사,-0.006880459,0.0,0.006880458910018206
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,상세 경험,-0.016376978,0.0,0.016376977786421776
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,수식,-0.051313825,0.0,0.05131382495164871
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,인공지능,0.00929044,0.0,0.009290440008044243
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,잠시 휴식,-0.017353749,0.0,0.01735374890267849
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,좋아하는 아이돌,0.041880764,0.0,0.041880764067173004
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,핵심 아이디어,-0.03687609,0.0,0.03687608987092972
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,확률 예측에서 MSE Loss 미 사용 이유,0.008048985,0.0,0.00804898515343666
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,BCE 가 좋은 task,-0.010686148,0.0,0.010686147958040237
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,"BCE 가 좋은 task, 이유",0.0031310613,0.0,0.00313106132671237
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,LLM Fine-Tuning 의 PEFT,-0.008993767,0.0,0.008993767201900482
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,LoRA,0.96586627,1.0,0.034133732318878174
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,LoRA 와 QLoRA 의 차이,0.04333932,0.0,0.0433393195271492
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,Loss Function 예시,-0.006967041,0.0,0.006967041175812483
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,Loss Function 정의,-0.039632943,0.0,0.03963294252753258
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,MBTI,-0.017987281,0.0,0.01798728108406067
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,MSE Loss 설명,-0.042181972,0.0,0.04218197241425514
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,MSE Loss 용도,-0.0080317585,0.0,0.008031758479773998
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,Multi-Label 에서 CE + Softmax 적용 문제점,0.005211002,0.0,0.005211002193391323
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,PEFT 방법 5가지,-0.0033565804,0.0,0.003356580389663577
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,거대 언어 모델 정의,-0.027948,0.0,0.02794799953699112
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,기본 경험,0.0033222542,0.0,0.003322254167869687
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,답변 실패,-0.0070498623,0.0,0.007049862295389175
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,딥러닝,-0.017737107,0.0,0.01773710735142231
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,마지막 할 말,-0.010293009,0.0,0.010293008759617805
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,머신러닝,-0.01736309,0.0,0.017363090068101883
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,면접 시작 인사,-0.0072575836,0.0,0.007257583551108837
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,상세 경험,-0.013286029,0.0,0.013286028988659382
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,수식,0.008415294,0.0,0.008415293879806995
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,인공지능,-0.012743654,0.0,0.012743653729557991
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,잠시 휴식,-0.011443347,0.0,0.011443346738815308
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,좋아하는 아이돌,0.00032640298,0.0,0.00032640298013575375
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,핵심 아이디어,0.0035258662,0.0,0.0035258661955595016
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,확률 예측에서 MSE Loss 미 사용 이유,0.00358825,0.0,0.003588249906897545
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,BCE 가 좋은 task,-0.0024143637,0.0,0.002414363669231534
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,"BCE 가 좋은 task, 이유",-0.002187386,0.0,0.0021873859222978354
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,LLM Fine-Tuning 의 PEFT,-0.011357586,0.0,0.011357585899531841
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,LoRA,0.028220477,0.0,0.02822047658264637
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,LoRA 와 QLoRA 의 차이,-0.0047659934,0.0,0.004765993449836969
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,Loss Function 예시,-0.00013278378,0.0,0.00013278378173708916
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,Loss Function 정의,-0.01460802,0.0,0.014608019962906837
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,MBTI,0.004534575,0.0,0.004534575156867504
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,MSE Loss 설명,-0.0060640667,0.0,0.0060640666633844376
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,MSE Loss 용도,-0.018396338,0.0,0.01839633844792843
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,Multi-Label 에서 CE + Softmax 적용 문제점,-0.009366657,0.0,0.00936665665358305
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,PEFT 방법 5가지,-0.013550369,0.0,0.013550369068980217
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,거대 언어 모델 정의,-0.008628853,0.0,0.008628852665424347
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,기본 경험,0.004348933,0.0,0.004348933231085539
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,답변 실패,0.97822136,1.0,0.021778643131256104
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,딥러닝,-0.000609286,0.0,0.0006092860130593181
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,마지막 할 말,-0.010511342,0.0,0.01051134243607521
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,머신러닝,0.016327977,0.0,0.016327977180480957
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,면접 시작 인사,0.0039019908,0.0,0.0039019908290356398
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,상세 경험,0.00076765404,0.0,0.0007676540408283472
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,수식,-0.014640637,0.0,0.01464063674211502
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,인공지능,0.003651366,0.0,0.003651366103440523
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,잠시 휴식,0.017629707,0.0,0.017629707232117653
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,좋아하는 아이돌,-0.0004551186,0.0,0.0004551185993477702
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,핵심 아이디어,0.015520051,0.0,0.015520051121711731
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,확률 예측에서 MSE Loss 미 사용 이유,-0.011113717,0.0,0.011113717220723629
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,BCE 가 좋은 task,0.0030091994,0.0,0.00300919939763844
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,"BCE 가 좋은 task, 이유",0.007396672,0.0,0.007396671921014786
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,LLM Fine-Tuning 의 PEFT,0.004040412,0.0,0.00404041213914752
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,LoRA,-0.03815322,0.0,0.03815321996808052
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,LoRA 와 QLoRA 의 차이,0.97124577,1.0,0.028754234313964844
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,Loss Function 예시,0.007346755,0.0,0.007346754893660545
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,Loss Function 정의,-0.007056842,0.0,0.007056842092424631
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,MBTI,0.0020786193,0.0,0.002078619319945574
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,MSE Loss 설명,-0.016204514,0.0,0.016204513609409332
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,MSE Loss 용도,0.0019407112,0.0,0.0019407111685723066
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,Multi-Label 에서 CE + Softmax 적용 문제점,-0.0029809806,0.0,0.002980980556458235
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,PEFT 방법 5가지,0.006517449,0.0,0.006517448928207159
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,거대 언어 모델 정의,0.0034417233,0.0,0.003441723296418786
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,기본 경험,-0.00087269,0.0,0.0008726899977773428
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,답변 실패,-0.01622072,0.0,0.01622072048485279
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,딥러닝,-0.009240905,0.0,0.009240904822945595
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,마지막 할 말,0.0027389522,0.0,0.0027389521710574627
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,머신러닝,0.007609305,0.0,0.0076093049719929695
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,면접 시작 인사,-0.006906889,0.0,0.006906888913363218
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,상세 경험,0.00022604571,0.0,0.00022604571131523699
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,수식,4.799992e-05,0.0,4.79999216622673e-05
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,인공지능,-0.007658428,0.0,0.00765842804685235
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,잠시 휴식,-0.009918339,0.0,0.009918338619172573
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,좋아하는 아이돌,-0.011820154,0.0,0.011820154264569283
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,핵심 아이디어,0.0042082276,0.0,0.0042082276195287704
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,확률 예측에서 MSE Loss 미 사용 이유,0.0029426236,0.0,0.002942623570561409
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,BCE 가 좋은 task,0.0043716445,0.0,0.004371644463390112
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,"BCE 가 좋은 task, 이유",0.0065480615,0.0,0.00654806150123477
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,LLM Fine-Tuning 의 PEFT,-0.011464217,0.0,0.011464216746389866
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,LoRA,0.012469047,0.0,0.012469046749174595
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,LoRA 와 QLoRA 의 차이,0.029405873,0.0,0.029405873268842697
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,Loss Function 예시,0.0040688757,0.0,0.004068875685334206
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,Loss Function 정의,-0.01014853,0.0,0.010148529894649982
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,MBTI,0.003274101,0.0,0.0032741010654717684
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,MSE Loss 설명,-0.0010372577,0.0,0.0010372577235102654
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,MSE Loss 용도,0.00134847,0.0,0.0013484699884429574
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,Multi-Label 에서 CE + Softmax 적용 문제점,-0.0055482234,0.0,0.00554822338744998
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,PEFT 방법 5가지,-0.00095993717,0.0,0.0009599371696822345
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,거대 언어 모델 정의,-0.0074120667,0.0,0.00741206668317318
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,기본 경험,-0.0068805246,0.0,0.006880524568259716
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,답변 실패,0.9724253,1.0,0.02757471799850464
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,딥러닝,-0.0049430667,0.0,0.004943066742271185
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,마지막 할 말,-0.011864467,0.0,0.011864466592669487
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,머신러닝,-0.004927719,0.0,0.004927719011902809
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,면접 시작 인사,0.0031489711,0.0,0.003148971125483513
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,상세 경험,0.0052424786,0.0,0.00524247856810689
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,수식,-0.032063887,0.0,0.032063886523246765
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,인공지능,-0.011041302,0.0,0.011041302233934402
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,잠시 휴식,-0.012651929,0.0,0.012651928700506687
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,좋아하는 아이돌,-0.0024797763,0.0,0.0024797762744128704
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,핵심 아이디어,-0.00032502823,0.0,0.0003250282316002995
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,확률 예측에서 MSE Loss 미 사용 이유,0.00037708116,0.0,0.0003770811599679291
마지막 할 말 -> 로라야 정말 고마워!,BCE 가 좋은 task,-0.005176221,0.0,0.005176221020519733
마지막 할 말 -> 로라야 정말 고마워!,"BCE 가 좋은 task, 이유",0.006546903,0.0,0.006546902935951948
마지막 할 말 -> 로라야 정말 고마워!,LLM Fine-Tuning 의 PEFT,0.009223032,0.0,0.00922303181141615
마지막 할 말 -> 로라야 정말 고마워!,LoRA,0.0054647024,0.0,0.005464702378958464
마지막 할 말 -> 로라야 정말 고마워!,LoRA 와 QLoRA 의 차이,-0.00632763,0.0,0.006327630020678043
마지막 할 말 -> 로라야 정말 고마워!,Loss Function 예시,-0.0005302008,0.0,0.0005302007775753736
마지막 할 말 -> 로라야 정말 고마워!,Loss Function 정의,-0.0030521238,0.0,0.003052123822271824
마지막 할 말 -> 로라야 정말 고마워!,MBTI,-0.01738761,0.0,0.01738760992884636
마지막 할 말 -> 로라야 정말 고마워!,MSE Loss 설명,-0.007145015,0.0,0.0071450150571763515
마지막 할 말 -> 로라야 정말 고마워!,MSE Loss 용도,-0.01589478,0.0,0.015894779935479164
마지막 할 말 -> 로라야 정말 고마워!,Multi-Label 에서 CE + Softmax 적용 문제점,-0.01727548,0.0,0.017275480553507805
마지막 할 말 -> 로라야 정말 고마워!,PEFT 방법 5가지,0.00828747,0.0,0.008287469856441021
마지막 할 말 -> 로라야 정말 고마워!,거대 언어 모델 정의,-0.0023404222,0.0,0.0023404222447425127
마지막 할 말 -> 로라야 정말 고마워!,기본 경험,-0.0063289814,0.0,0.00632898136973381
마지막 할 말 -> 로라야 정말 고마워!,답변 실패,0.00540741,0.0,0.005407410208135843
마지막 할 말 -> 로라야 정말 고마워!,딥러닝,-0.0046045436,0.0,0.004604543559253216
마지막 할 말 -> 로라야 정말 고마워!,마지막 할 말,0.9869717,1.0,0.013028323650360107
마지막 할 말 -> 로라야 정말 고마워!,머신러닝,0.000577434,0.0,0.0005774340243078768
마지막 할 말 -> 로라야 정말 고마워!,면접 시작 인사,-0.009048466,0.0,0.009048465639352798
마지막 할 말 -> 로라야 정말 고마워!,상세 경험,-0.025128152,0.0,0.02512815222144127
마지막 할 말 -> 로라야 정말 고마워!,수식,-0.0006212838,0.0,0.000621283776126802
마지막 할 말 -> 로라야 정말 고마워!,인공지능,0.00850517,0.0,0.008505170233547688
마지막 할 말 -> 로라야 정말 고마워!,잠시 휴식,-0.008855538,0.0,0.008855538442730904
마지막 할 말 -> 로라야 정말 고마워!,좋아하는 아이돌,0.008555498,0.0,0.008555497974157333
마지막 할 말 -> 로라야 정말 고마워!,핵심 아이디어,-0.0063940496,0.0,0.006394049618393183
마지막 할 말 -> 로라야 정말 고마워!,확률 예측에서 MSE Loss 미 사용 이유,0.014592209,0.0,0.01459220889955759
마지막 할 말 -> 로라야 사랑해,BCE 가 좋은 task,-0.0088137705,0.0,0.008813770487904549
마지막 할 말 -> 로라야 사랑해,"BCE 가 좋은 task, 이유",0.0017398722,0.0,0.0017398721538484097
마지막 할 말 -> 로라야 사랑해,LLM Fine-Tuning 의 PEFT,0.00941237,0.0,0.009412369690835476
마지막 할 말 -> 로라야 사랑해,LoRA,0.0036068787,0.0,0.003606878686696291
마지막 할 말 -> 로라야 사랑해,LoRA 와 QLoRA 의 차이,-0.0030139617,0.0,0.0030139617156237364
마지막 할 말 -> 로라야 사랑해,Loss Function 예시,-0.004485815,0.0,0.004485814832150936
마지막 할 말 -> 로라야 사랑해,Loss Function 정의,-0.006524218,0.0,0.006524217780679464
마지막 할 말 -> 로라야 사랑해,MBTI,-0.010111011,0.0,0.010111010633409023
마지막 할 말 -> 로라야 사랑해,MSE Loss 설명,-0.011649393,0.0,0.011649392545223236
마지막 할 말 -> 로라야 사랑해,MSE Loss 용도,-0.013977449,0.0,0.013977449387311935
마지막 할 말 -> 로라야 사랑해,Multi-Label 에서 CE + Softmax 적용 문제점,-0.019755071,0.0,0.01975507102906704
마지막 할 말 -> 로라야 사랑해,PEFT 방법 5가지,0.002824491,0.0,0.0028244908899068832
마지막 할 말 -> 로라야 사랑해,거대 언어 모델 정의,-0.0011876835,0.0,0.0011876835487782955
마지막 할 말 -> 로라야 사랑해,기본 경험,-0.007895678,0.0,0.007895678281784058
마지막 할 말 -> 로라야 사랑해,답변 실패,0.0082448535,0.0,0.008244853466749191
마지막 할 말 -> 로라야 사랑해,딥러닝,-0.002249547,0.0,0.0022495470475405455
마지막 할 말 -> 로라야 사랑해,마지막 할 말,0.9887938,1.0,0.011206209659576416
마지막 할 말 -> 로라야 사랑해,머신러닝,-0.0020136703,0.0,0.0020136702805757523
마지막 할 말 -> 로라야 사랑해,면접 시작 인사,-0.011704347,0.0,0.011704347096383572
마지막 할 말 -> 로라야 사랑해,상세 경험,-0.032798648,0.0,0.0327986478805542
마지막 할 말 -> 로라야 사랑해,수식,0.0022881273,0.0,0.0022881273180246353
마지막 할 말 -> 로라야 사랑해,인공지능,0.001447793,0.0,0.001447792979888618
마지막 할 말 -> 로라야 사랑해,잠시 휴식,-0.0036835026,0.0,0.0036835025530308485
마지막 할 말 -> 로라야 사랑해,좋아하는 아이돌,0.019232966,0.0,0.019232966005802155
마지막 할 말 -> 로라야 사랑해,핵심 아이디어,-0.00600534,0.0,0.006005339790135622
마지막 할 말 -> 로라야 사랑해,확률 예측에서 MSE Loss 미 사용 이유,0.015612843,0.0,0.015612843446433544
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,BCE 가 좋은 task,-0.006585776,0.0,0.006585775874555111
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,"BCE 가 좋은 task, 이유",0.0046864864,0.0,0.004686486441642046
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,LLM Fine-Tuning 의 PEFT,0.011820577,0.0,0.011820577085018158
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,LoRA,0.0013447624,0.0,0.0013447623932734132
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,LoRA 와 QLoRA 의 차이,-0.005733854,0.0,0.00573385413736105
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,Loss Function 예시,0.003933513,0.0,0.003933513071388006
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,Loss Function 정의,0.0011974702,0.0,0.0011974702356383204
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,MBTI,-0.009176692,0.0,0.009176691994071007
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,MSE Loss 설명,-0.016385186,0.0,0.016385186463594437
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,MSE Loss 용도,-0.011860995,0.0,0.01186099462211132
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,Multi-Label 에서 CE + Softmax 적용 문제점,-0.011868097,0.0,0.011868096888065338
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,PEFT 방법 5가지,0.0017191488,0.0,0.0017191488295793533
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,거대 언어 모델 정의,-0.0039663203,0.0,0.003966320306062698
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,기본 경험,-0.008159081,0.0,0.00815908145159483
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,답변 실패,-0.0032089364,0.0,0.0032089364249259233
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,딥러닝,-0.004966822,0.0,0.004966821987181902
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,마지막 할 말,0.9854754,1.0,0.014524579048156738
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,머신러닝,-0.004338842,0.0,0.004338841885328293
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,면접 시작 인사,-0.014916598,0.0,0.014916597865521908
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,상세 경험,-0.028415442,0.0,0.028415441513061523
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,수식,0.0012703309,0.0,0.0012703308602795005
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,인공지능,0.0025810713,0.0,0.0025810713414102793
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,잠시 휴식,-0.002260387,0.0,0.0022603869438171387
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,좋아하는 아이돌,0.010378331,0.0,0.010378330945968628
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,핵심 아이디어,-0.007978885,0.0,0.007978885434567928
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,확률 예측에서 MSE Loss 미 사용 이유,0.018410457,0.0,0.0184104572981596
