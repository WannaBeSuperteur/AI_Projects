input_part,output_answer,predicted_score,ground_truth_score,absolute_error
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,BCE 가 좋은 task,-0.0084777875,0.0,0.008477787487208843
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,BCE 가 좋은 이유,-0.016713737,0.0,0.016713736578822136
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,LLM Fine-Tuning 의 PEFT,0.003397049,0.0,0.0033970489166677
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,LoRA,-0.0041447445,0.0,0.004144744481891394
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,LoRA 와 QLoRA 의 차이,0.006849033,0.0,0.006849032826721668
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,Loss Function 예시,-0.003041335,0.0,0.003041334915906191
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,Loss Function 정의,0.0005159378,0.0,0.0005159378051757812
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,MBTI,-0.02709167,0.0,0.027091670781373978
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,MSE Loss 설명,0.0048393626,0.0,0.0048393625766038895
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,MSE Loss 용도,-0.008356221,0.0,0.00835622102022171
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,Multi-Label 에서 CE + Softmax 적용 문제점,-0.0010077835,0.0,0.0010077834594994783
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,PEFT 방법 5가지,0.0055692475,0.0,0.005569247528910637
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,거대 언어 모델 정의,0.000105908206,0.0,0.00010590820602374151
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,기본 경험,-0.0066002053,0.0,0.006600205320864916
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,답변 실패,-0.00031507688,0.0,0.00031507687526755035
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,딥러닝,0.004353604,0.0,0.0043536038137972355
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,마지막 할 말,-0.006415006,0.0,0.006415005773305893
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,머신러닝,0.00027050453,0.0,0.0002705045335460454
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,면접 시작 인사,0.9883058,1.0,0.011694192886352539
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,상세 경험,-0.014567196,0.0,0.014567196369171143
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,수식,-0.008715749,0.0,0.00871574878692627
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,용어 질문,0.002623051,0.0,0.0026230509392917156
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,인공지능,-0.015621162,0.0,0.01562116201967001
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,잠시 휴식,-0.0134136425,0.0,0.013413642533123493
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,좋아하는 아이돌,-0.013474934,0.0,0.013474933803081512
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,핵심 아이디어,-0.004244687,0.0,0.004244687035679817
면접 시작 인사 -> 오늘 뭐 물어볼 거야 그래서?,확률 예측에서 MSE Loss 미 사용 이유,0.0030500018,0.0,0.0030500018037855625
면접 시작 인사 -> 로라야 안녕 정말 반가워,BCE 가 좋은 task,-0.0071039256,0.0,0.007103925570845604
면접 시작 인사 -> 로라야 안녕 정말 반가워,BCE 가 좋은 이유,-0.008175996,0.0,0.008175996132194996
면접 시작 인사 -> 로라야 안녕 정말 반가워,LLM Fine-Tuning 의 PEFT,-0.0028950223,0.0,0.00289502227678895
면접 시작 인사 -> 로라야 안녕 정말 반가워,LoRA,0.0026672713,0.0,0.002667271299287677
면접 시작 인사 -> 로라야 안녕 정말 반가워,LoRA 와 QLoRA 의 차이,0.0029972703,0.0,0.002997270319610834
면접 시작 인사 -> 로라야 안녕 정말 반가워,Loss Function 예시,-0.0059493356,0.0,0.005949335638433695
면접 시작 인사 -> 로라야 안녕 정말 반가워,Loss Function 정의,0.00018930418,0.0,0.00018930417718365788
면접 시작 인사 -> 로라야 안녕 정말 반가워,MBTI,-0.016993104,0.0,0.016993103548884392
면접 시작 인사 -> 로라야 안녕 정말 반가워,MSE Loss 설명,0.006747146,0.0,0.006747146137058735
면접 시작 인사 -> 로라야 안녕 정말 반가워,MSE Loss 용도,-0.013504077,0.0,0.01350407674908638
면접 시작 인사 -> 로라야 안녕 정말 반가워,Multi-Label 에서 CE + Softmax 적용 문제점,-0.0053986455,0.0,0.005398645531386137
면접 시작 인사 -> 로라야 안녕 정말 반가워,PEFT 방법 5가지,-0.013014121,0.0,0.013014120981097221
면접 시작 인사 -> 로라야 안녕 정말 반가워,거대 언어 모델 정의,0.006474301,0.0,0.006474301218986511
면접 시작 인사 -> 로라야 안녕 정말 반가워,기본 경험,-0.0006584104,0.0,0.0006584104266948998
면접 시작 인사 -> 로라야 안녕 정말 반가워,답변 실패,-0.0004806166,0.0,0.0004806166107300669
면접 시작 인사 -> 로라야 안녕 정말 반가워,딥러닝,0.0028464238,0.0,0.0028464237693697214
면접 시작 인사 -> 로라야 안녕 정말 반가워,마지막 할 말,0.00070502417,0.0,0.0007050241692923009
면접 시작 인사 -> 로라야 안녕 정말 반가워,머신러닝,0.0055729016,0.0,0.005572901573032141
면접 시작 인사 -> 로라야 안녕 정말 반가워,면접 시작 인사,0.98853993,1.0,0.011460065841674805
면접 시작 인사 -> 로라야 안녕 정말 반가워,상세 경험,-0.01307641,0.0,0.013076409697532654
면접 시작 인사 -> 로라야 안녕 정말 반가워,수식,-0.017539592,0.0,0.01753959245979786
면접 시작 인사 -> 로라야 안녕 정말 반가워,용어 질문,-0.01270056,0.0,0.012700559571385384
면접 시작 인사 -> 로라야 안녕 정말 반가워,인공지능,-0.012191283,0.0,0.012191282585263252
면접 시작 인사 -> 로라야 안녕 정말 반가워,잠시 휴식,-0.010511257,0.0,0.010511256754398346
면접 시작 인사 -> 로라야 안녕 정말 반가워,좋아하는 아이돌,-0.012485748,0.0,0.012485748156905174
면접 시작 인사 -> 로라야 안녕 정말 반가워,핵심 아이디어,-0.01276526,0.0,0.01276526041328907
면접 시작 인사 -> 로라야 안녕 정말 반가워,확률 예측에서 MSE Loss 미 사용 이유,-0.002963125,0.0,0.002963125007227063
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,BCE 가 좋은 task,-0.008271943,0.0,0.008271942846477032
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,BCE 가 좋은 이유,-0.016461525,0.0,0.016461525112390518
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,LLM Fine-Tuning 의 PEFT,-0.004402582,0.0,0.0044025820679962635
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,LoRA,0.0068470817,0.0,0.006847081705927849
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,LoRA 와 QLoRA 의 차이,0.0021472143,0.0,0.0021472142543643713
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,Loss Function 예시,-0.0036492804,0.0,0.003649280406534672
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,Loss Function 정의,-0.0066489717,0.0,0.006648971699178219
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,MBTI,-0.029612187,0.0,0.029612187296152115
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,MSE Loss 설명,0.007821586,0.0,0.007821585983037949
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,MSE Loss 용도,-0.0060185166,0.0,0.006018516607582569
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,Multi-Label 에서 CE + Softmax 적용 문제점,-0.0027119666,0.0,0.002711966633796692
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,PEFT 방법 5가지,-0.012186797,0.0,0.012186797335743904
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,거대 언어 모델 정의,-0.0026644492,0.0,0.0026644491590559483
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,기본 경험,0.008907635,0.0,0.008907634764909744
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,답변 실패,0.0023540533,0.0,0.0023540533147752285
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,딥러닝,0.006826665,0.0,0.006826664786785841
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,마지막 할 말,-0.0031963023,0.0,0.0031963023357093334
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,머신러닝,-0.0023928902,0.0,0.002392890164628625
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,면접 시작 인사,0.9847453,1.0,0.015254676342010498
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,상세 경험,-0.012279134,0.0,0.01227913424372673
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,수식,-0.009257591,0.0,0.00925759132951498
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,용어 질문,-0.00894143,0.0,0.008941429667174816
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,인공지능,-0.018797934,0.0,0.01879793405532837
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,잠시 휴식,-0.018547308,0.0,0.018547307699918747
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,좋아하는 아이돌,-0.016767317,0.0,0.016767317429184914
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,핵심 아이디어,-0.012538074,0.0,0.01253807358443737
면접 시작 인사 -> 로라야 그럼 네가 면접관이야?,확률 예측에서 MSE Loss 미 사용 이유,-0.0058232066,0.0,0.005823206622153521
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,BCE 가 좋은 task,-0.00730659,0.0,0.007306590210646391
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,BCE 가 좋은 이유,-0.012302937,0.0,0.012302936986088753
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,LLM Fine-Tuning 의 PEFT,0.0063937376,0.0,0.006393737625330687
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,LoRA,-0.00035904092,0.0,0.0003590409178286791
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,LoRA 와 QLoRA 의 차이,0.002773329,0.0,0.0027733289171010256
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,Loss Function 예시,-0.005848826,0.0,0.0058488259091973305
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,Loss Function 정의,6.189825e-05,0.0,6.189825217006728e-05
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,MBTI,-0.03385276,0.0,0.03385275974869728
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,MSE Loss 설명,0.0010818328,0.0,0.0010818328009918332
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,MSE Loss 용도,-0.013239531,0.0,0.013239530846476555
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,Multi-Label 에서 CE + Softmax 적용 문제점,-0.0051257485,0.0,0.005125748459249735
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,PEFT 방법 5가지,-0.0001472559,0.0,0.00014725589426234365
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,거대 언어 모델 정의,0.0013268767,0.0,0.0013268766924738884
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,기본 경험,0.005660237,0.0,0.0056602368131279945
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,답변 실패,-0.00173563,0.0,0.0017356299795210361
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,딥러닝,0.011384627,0.0,0.011384626850485802
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,마지막 할 말,0.004498629,0.0,0.0044986288994550705
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,머신러닝,-9.141512e-05,0.0,9.141511691268533e-05
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,면접 시작 인사,0.9845291,1.0,0.015470921993255615
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,상세 경험,-0.018059505,0.0,0.0180595051497221
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,수식,-0.01749873,0.0,0.01749872975051403
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,용어 질문,-0.0257261,0.0,0.02572610042989254
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,인공지능,-0.022260856,0.0,0.02226085588335991
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,잠시 휴식,-0.011543976,0.0,0.01154397614300251
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,좋아하는 아이돌,-0.013337249,0.0,0.01333724893629551
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,핵심 아이디어,-0.004846453,0.0,0.004846453201025724
면접 시작 인사 -> 내 인생도 이제 오로라처럼 빛날 거니까,확률 예측에서 MSE Loss 미 사용 이유,0.0056699123,0.0,0.005669912323355675
면접 시작 인사 -> 파이팅! 시작하자,BCE 가 좋은 task,-0.008798402,0.0,0.008798401802778244
면접 시작 인사 -> 파이팅! 시작하자,BCE 가 좋은 이유,-0.0043584113,0.0,0.004358411300927401
면접 시작 인사 -> 파이팅! 시작하자,LLM Fine-Tuning 의 PEFT,0.009651247,0.0,0.009651247411966324
면접 시작 인사 -> 파이팅! 시작하자,LoRA,-0.00044246213,0.0,0.00044246212928555906
면접 시작 인사 -> 파이팅! 시작하자,LoRA 와 QLoRA 의 차이,-0.00040740028,0.0,0.00040740027907304466
면접 시작 인사 -> 파이팅! 시작하자,Loss Function 예시,-0.002858496,0.0,0.0028584960382431746
면접 시작 인사 -> 파이팅! 시작하자,Loss Function 정의,-0.0012372975,0.0,0.0012372975470498204
면접 시작 인사 -> 파이팅! 시작하자,MBTI,-0.017971478,0.0,0.017971478402614594
면접 시작 인사 -> 파이팅! 시작하자,MSE Loss 설명,0.006033105,0.0,0.006033104844391346
면접 시작 인사 -> 파이팅! 시작하자,MSE Loss 용도,-0.008921799,0.0,0.008921799249947071
면접 시작 인사 -> 파이팅! 시작하자,Multi-Label 에서 CE + Softmax 적용 문제점,0.0045166123,0.0,0.004516612272709608
면접 시작 인사 -> 파이팅! 시작하자,PEFT 방법 5가지,0.0057780715,0.0,0.005778071470558643
면접 시작 인사 -> 파이팅! 시작하자,거대 언어 모델 정의,0.0017460469,0.0,0.001746046938933432
면접 시작 인사 -> 파이팅! 시작하자,기본 경험,-0.009965239,0.0,0.009965239092707634
면접 시작 인사 -> 파이팅! 시작하자,답변 실패,-0.0056104762,0.0,0.0056104762479662895
면접 시작 인사 -> 파이팅! 시작하자,딥러닝,0.006437117,0.0,0.006437116768211126
면접 시작 인사 -> 파이팅! 시작하자,마지막 할 말,-0.007955221,0.0,0.007955221459269524
면접 시작 인사 -> 파이팅! 시작하자,머신러닝,0.0037131696,0.0,0.003713169600814581
면접 시작 인사 -> 파이팅! 시작하자,면접 시작 인사,0.99115735,1.0,0.008842647075653076
면접 시작 인사 -> 파이팅! 시작하자,상세 경험,-0.023028864,0.0,0.023028863593935966
면접 시작 인사 -> 파이팅! 시작하자,수식,-0.01811354,0.0,0.01811354048550129
면접 시작 인사 -> 파이팅! 시작하자,용어 질문,-0.020376325,0.0,0.02037632465362549
면접 시작 인사 -> 파이팅! 시작하자,인공지능,-0.021955574,0.0,0.021955573931336403
면접 시작 인사 -> 파이팅! 시작하자,잠시 휴식,-0.008523594,0.0,0.008523593656718731
면접 시작 인사 -> 파이팅! 시작하자,좋아하는 아이돌,-0.0054741013,0.0,0.005474101286381483
면접 시작 인사 -> 파이팅! 시작하자,핵심 아이디어,-7.6578326e-05,0.0,7.657832611585036e-05
면접 시작 인사 -> 파이팅! 시작하자,확률 예측에서 MSE Loss 미 사용 이유,0.00014868299,0.0,0.00014868298603687435
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",BCE 가 좋은 task,-0.0016955598,0.0,0.0016955598257482052
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",BCE 가 좋은 이유,-0.0004875659,0.0,0.00048756590695120394
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",LLM Fine-Tuning 의 PEFT,-0.0077493,0.0,0.007749299984425306
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",LoRA,0.0021882956,0.0,0.002188295591622591
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",LoRA 와 QLoRA 의 차이,-0.0019188102,0.0,0.001918810186907649
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",Loss Function 예시,-0.016326284,0.0,0.016326284036040306
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",Loss Function 정의,-0.02225756,0.0,0.02225756086409092
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",MBTI,-0.00023898667,0.0,0.0002389866713201627
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",MSE Loss 설명,-0.012865392,0.0,0.012865391559898853
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",MSE Loss 용도,0.0039898176,0.0,0.003989817574620247
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",Multi-Label 에서 CE + Softmax 적용 문제점,-0.005240157,0.0,0.005240156780928373
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",PEFT 방법 5가지,-0.00363407,0.0,0.003634070046246052
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",거대 언어 모델 정의,-0.011258019,0.0,0.011258019134402275
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",기본 경험,-0.0015914378,0.0,0.0015914378454908729
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",답변 실패,0.9845563,1.0,0.015443682670593262
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",딥러닝,0.004051065,0.0,0.004051065072417259
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",마지막 할 말,0.006565431,0.0,0.006565431132912636
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",머신러닝,0.004709264,0.0,0.004709263797849417
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",면접 시작 인사,0.001427137,0.0,0.0014271369436755776
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",상세 경험,-0.006110844,0.0,0.006110844202339649
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",수식,0.001628281,0.0,0.0016282809665426612
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",용어 질문,-0.007935326,0.0,0.007935325615108013
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",인공지능,-0.004357306,0.0,0.004357305821031332
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",잠시 휴식,0.0054369485,0.0,0.005436948500573635
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",좋아하는 아이돌,0.014900494,0.0,0.014900494366884232
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",핵심 아이디어,0.0124024525,0.0,0.012402452528476715
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 나도 개발해보고 싶은데",확률 예측에서 MSE Loss 미 사용 이유,-0.013251244,0.0,0.013251244090497494
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",BCE 가 좋은 task,0.0027594334,0.0,0.002759433351457119
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",BCE 가 좋은 이유,-0.02318162,0.0,0.023181619122624397
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",LLM Fine-Tuning 의 PEFT,0.0057787024,0.0,0.005778702441602945
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",LoRA,-0.004809264,0.0,0.004809264093637466
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",LoRA 와 QLoRA 의 차이,0.009639888,0.0,0.009639888070523739
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",Loss Function 예시,-0.005391041,0.0,0.005391040816903114
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",Loss Function 정의,-0.005220485,0.0,0.005220484919846058
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",MBTI,0.024677927,0.0,0.024677926674485207
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",MSE Loss 설명,0.006596249,0.0,0.006596249062567949
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",MSE Loss 용도,-0.0096775,0.0,0.00967750046402216
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",Multi-Label 에서 CE + Softmax 적용 문제점,0.023288796,0.0,0.023288795724511147
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",PEFT 방법 5가지,0.0033424513,0.0,0.0033424512948840857
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",거대 언어 모델 정의,-0.023671297,0.0,0.02367129735648632
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",기본 경험,0.018337268,0.0,0.018337268382310867
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",답변 실패,-0.006933582,0.0,0.00693358201533556
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",딥러닝,-0.06866577,0.0,0.0686657726764679
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",마지막 할 말,-0.024538381,0.0,0.024538381025195122
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",머신러닝,0.010540968,0.0,0.010540967807173729
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",면접 시작 인사,-0.02096974,0.0,0.02096973918378353
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",상세 경험,-0.051473558,0.0,0.05147355794906616
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",수식,0.028836204,0.0,0.02883620373904705
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",용어 질문,0.017445708,0.0,0.017445707693696022
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",인공지능,0.8546355,1.0,0.14536452293395996
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",잠시 휴식,-0.021800537,0.0,0.021800536662340164
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",좋아하는 아이돌,-0.0047350293,0.0,0.004735029302537441
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",핵심 아이디어,0.00701611,0.0,0.0070161097683012486
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람의 지능을 로봇이나 컴퓨터가 흉내내는 거!",확률 예측에서 MSE Loss 미 사용 이유,-0.0052829124,0.0,0.005282912403345108
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",BCE 가 좋은 task,-0.00210903,0.0,0.0021090300288051367
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",BCE 가 좋은 이유,-0.020765258,0.0,0.020765257999300957
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",LLM Fine-Tuning 의 PEFT,-0.008214098,0.0,0.008214098401367664
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",LoRA,-0.007422059,0.0,0.007422058843076229
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",LoRA 와 QLoRA 의 차이,-0.00951882,0.0,0.009518819861114025
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",Loss Function 예시,-0.0042108074,0.0,0.004210807383060455
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",Loss Function 정의,-0.005094011,0.0,0.005094010848551989
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",MBTI,-0.025462138,0.0,0.025462137535214424
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",MSE Loss 설명,0.0030082464,0.0,0.003008246421813965
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",MSE Loss 용도,0.019335698,0.0,0.01933569833636284
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",Multi-Label 에서 CE + Softmax 적용 문제점,0.0065005776,0.0,0.006500577554106712
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",PEFT 방법 5가지,-0.013144086,0.0,0.013144086115062237
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",거대 언어 모델 정의,0.01955599,0.0,0.019555989652872086
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",기본 경험,0.001310766,0.0,0.0013107659760862589
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",답변 실패,-0.0038507998,0.0,0.0038507997523993254
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",딥러닝,-0.044113003,0.0,0.044113002717494965
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",마지막 할 말,0.000423287,0.0,0.00042328701238147914
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",머신러닝,0.97055304,1.0,0.029446959495544434
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",면접 시작 인사,0.010651196,0.0,0.010651196353137493
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",상세 경험,-0.024203837,0.0,0.024203836917877197
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",수식,0.00046818273,0.0,0.00046818272676318884
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",용어 질문,-0.0074832616,0.0,0.00748326163738966
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",인공지능,-0.04324359,0.0,0.043243590742349625
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",잠시 휴식,-0.017475048,0.0,0.017475048080086708
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",좋아하는 아이돌,-0.002789916,0.0,0.002789916004985571
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",핵심 아이디어,-0.0015616602,0.0,0.0015616602031514049
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 데이터 패턴을 학습하고 새로운 데이터를 예측하는 거지",확률 예측에서 MSE Loss 미 사용 이유,0.012321038,0.0,0.01232103817164898
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",BCE 가 좋은 task,-0.007560737,0.0,0.00756073696538806
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",BCE 가 좋은 이유,0.0024875137,0.0,0.002487513702362776
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",LLM Fine-Tuning 의 PEFT,-0.02672686,0.0,0.0267268605530262
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",LoRA,-0.009593091,0.0,0.00959309097379446
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",LoRA 와 QLoRA 의 차이,0.010158225,0.0,0.01015822496265173
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",Loss Function 예시,-0.02146691,0.0,0.02146691083908081
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",Loss Function 정의,-0.027116636,0.0,0.02711663581430912
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",MBTI,-0.010647215,0.0,0.010647214949131012
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",MSE Loss 설명,0.004616591,0.0,0.004616591148078442
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",MSE Loss 용도,-0.017937593,0.0,0.017937593162059784
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",Multi-Label 에서 CE + Softmax 적용 문제점,-0.0004515392,0.0,0.0004515392065513879
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",PEFT 방법 5가지,-0.028434345,0.0,0.02843434549868107
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",거대 언어 모델 정의,-0.006763236,0.0,0.006763236131519079
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",기본 경험,0.0038706297,0.0,0.003870629705488682
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",답변 실패,0.00046534187,0.0,0.0004653418727684766
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",딥러닝,0.96449906,1.0,0.035500943660736084
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",마지막 할 말,-0.00823772,0.0,0.008237719535827637
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",머신러닝,-0.010317075,0.0,0.010317075066268444
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",면접 시작 인사,-0.0007988702,0.0,0.0007988702272996306
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",상세 경험,-0.0023708711,0.0,0.002370871137827635
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",수식,-0.0011574131,0.0,0.001157413120381534
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",용어 질문,-0.013082565,0.0,0.013082564808428288
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",인공지능,-0.039902836,0.0,0.039902836084365845
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",잠시 휴식,0.0006011824,0.0,0.000601182400714606
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",좋아하는 아이돌,-0.005212144,0.0,0.005212143994867802
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",핵심 아이디어,0.00032899366,0.0,0.0003289936576038599
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 심층신경망을 사용한 머신러닝 알고리즘이야 맞지?",확률 예측에서 MSE Loss 미 사용 이유,-0.0012338189,0.0,0.0012338189408183098
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",BCE 가 좋은 task,-0.044428233,0.0,0.04442823305726051
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",BCE 가 좋은 이유,-0.020951206,0.0,0.020951205864548683
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",LLM Fine-Tuning 의 PEFT,-0.044491418,0.0,0.044491417706012726
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",LoRA,-0.0074933916,0.0,0.0074933916330337524
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",LoRA 와 QLoRA 의 차이,-0.0030060327,0.0,0.003006032668054104
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",Loss Function 예시,-0.021065423,0.0,0.021065423265099525
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",Loss Function 정의,-0.018669585,0.0,0.01866958476603031
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",MBTI,-0.012430609,0.0,0.012430609203875065
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",MSE Loss 설명,-0.0057303593,0.0,0.005730359349399805
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",MSE Loss 용도,0.0033999719,0.0,0.0033999718725681305
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",Multi-Label 에서 CE + Softmax 적용 문제점,-0.046404116,0.0,0.04640411585569382
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",PEFT 방법 5가지,-0.033135194,0.0,0.03313519433140755
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",거대 언어 모델 정의,-0.038783003,0.0,0.0387830026447773
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",기본 경험,-0.016226595,0.0,0.016226595267653465
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",답변 실패,0.585973,0.0,0.5859730243682861
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",딥러닝,0.62170005,1.0,0.3782999515533447
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",마지막 할 말,-0.0069328155,0.0,0.006932815536856651
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",머신러닝,0.0865543,0.0,0.08655429631471634
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",면접 시작 인사,-0.025016781,0.0,0.02501678094267845
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",상세 경험,-0.02769662,0.0,0.027696620672941208
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",수식,-4.7028276e-05,0.0,4.7028275730554014e-05
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",용어 질문,-0.038838197,0.0,0.03883819654583931
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",인공지능,0.020854348,0.0,0.020854348316788673
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",잠시 휴식,-0.0065622036,0.0,0.006562203634530306
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",좋아하는 아이돌,-0.015747426,0.0,0.015747426077723503
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",핵심 아이디어,-0.024974328,0.0,0.02497432753443718
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기반 머신러닝이야",확률 예측에서 MSE Loss 미 사용 이유,-0.016738778,0.0,0.016738777980208397
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",BCE 가 좋은 task,-0.06567576,0.0,0.0656757578253746
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",BCE 가 좋은 이유,-0.0118918605,0.0,0.011891860514879227
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",LLM Fine-Tuning 의 PEFT,-0.02416089,0.0,0.02416088990867138
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",LoRA,-0.022369,0.0,0.02236899919807911
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",LoRA 와 QLoRA 의 차이,-0.017297866,0.0,0.017297865822911263
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",Loss Function 예시,-0.03276536,0.0,0.032765358686447144
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",Loss Function 정의,-0.021520883,0.0,0.021520882844924927
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",MBTI,-0.043375168,0.0,0.0433751679956913
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",MSE Loss 설명,0.001120026,0.0,0.0011200259905308485
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",MSE Loss 용도,0.027762955,0.0,0.02776295505464077
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",Multi-Label 에서 CE + Softmax 적용 문제점,0.011666607,0.0,0.011666607111692429
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",PEFT 방법 5가지,0.012714441,0.0,0.012714440934360027
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",거대 언어 모델 정의,-0.01598204,0.0,0.015982039272785187
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",기본 경험,-0.0010826312,0.0,0.0010826311772689223
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",답변 실패,0.4781087,1.0,0.5218912959098816
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",딥러닝,-0.008983492,0.0,0.00898349191993475
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",마지막 할 말,-0.009843093,0.0,0.00984309334307909
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",머신러닝,0.6873768,0.0,0.6873767971992493
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",면접 시작 인사,-0.011558459,0.0,0.011558459140360355
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",상세 경험,-0.040728536,0.0,0.04072853550314903
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",수식,0.03881297,0.0,0.038812968879938126
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",용어 질문,-0.04860371,0.0,0.048603709787130356
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",인공지능,0.02137395,0.0,0.021373949944972992
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",잠시 휴식,0.010577912,0.0,0.01057791244238615
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",좋아하는 아이돌,-0.043014158,0.0,0.04301415756344795
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",핵심 아이디어,0.017581072,0.0,0.017581071704626083
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 말 그대로 기계가 학습하는 거 아니야?",확률 예측에서 MSE Loss 미 사용 이유,-0.0042396253,0.0,0.004239625297486782
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아",BCE 가 좋은 task,-0.0008968826,0.0,0.0008968826150521636
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아",BCE 가 좋은 이유,-0.006695245,0.0,0.006695244926959276
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아",LLM Fine-Tuning 의 PEFT,-0.003130081,0.0,0.003130081109702587
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아",LoRA,0.00010639542,0.0,0.00010639541869750246
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아",LoRA 와 QLoRA 의 차이,-0.00050802174,0.0,0.0005080217379145324
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아",Loss Function 예시,0.002364083,0.0,0.0023640829604119062
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아",Loss Function 정의,-0.01816342,0.0,0.018163420259952545
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아",MBTI,-0.004156521,0.0,0.004156521055847406
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아",MSE Loss 설명,-0.0027050087,0.0,0.0027050087228417397
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아",MSE Loss 용도,0.0009297337,0.0,0.0009297336800955236
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아",Multi-Label 에서 CE + Softmax 적용 문제점,-0.005116108,0.0,0.005116107873618603
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아",PEFT 방법 5가지,-0.0036282686,0.0,0.0036282686050981283
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아",거대 언어 모델 정의,-0.006694754,0.0,0.006694754119962454
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아",기본 경험,-0.007708588,0.0,0.007708588149398565
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아",답변 실패,0.9916887,1.0,0.008311271667480469
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아",딥러닝,0.004162776,0.0,0.004162775818258524
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아",마지막 할 말,-0.0050325366,0.0,0.005032536573708057
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아",머신러닝,0.011829765,0.0,0.01182976458221674
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아",면접 시작 인사,-0.0040746904,0.0,0.004074690397828817
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아",상세 경험,-0.005569281,0.0,0.005569281056523323
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아",수식,-0.0059114764,0.0,0.005911476444453001
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아",용어 질문,-0.007346586,0.0,0.007346585858613253
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아",인공지능,-0.0034564182,0.0,0.0034564181696623564
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아",잠시 휴식,-0.0004337278,0.0,0.00043372780783101916
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아",좋아하는 아이돌,0.0011401343,0.0,0.0011401342926546931
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아",핵심 아이디어,0.007799157,0.0,0.007799156941473484
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능 요새 엄청 뜨는 분야잖아",확률 예측에서 MSE Loss 미 사용 이유,0.00041350818,0.0,0.00041350818355567753
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지?",BCE 가 좋은 task,0.000826771,0.0,0.0008267710218206048
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지?",BCE 가 좋은 이유,-0.015479925,0.0,0.015479925088584423
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지?",LLM Fine-Tuning 의 PEFT,0.018447228,0.0,0.018447227776050568
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지?",LoRA,-0.009169262,0.0,0.009169261902570724
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지?",LoRA 와 QLoRA 의 차이,0.016135622,0.0,0.016135621815919876
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지?",Loss Function 예시,-0.030901195,0.0,0.030901195481419563
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지?",Loss Function 정의,-0.02782364,0.0,0.027823640033602715
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지?",MBTI,0.020012243,0.0,0.02001224271953106
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지?",MSE Loss 설명,0.02760399,0.0,0.027603989467024803
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지?",MSE Loss 용도,-0.016029356,0.0,0.0160293560475111
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지?",Multi-Label 에서 CE + Softmax 적용 문제점,0.016129378,0.0,0.016129378229379654
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지?",PEFT 방법 5가지,0.010171038,0.0,0.01017103809863329
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지?",거대 언어 모델 정의,-0.031575885,0.0,0.03157588467001915
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지?",기본 경험,0.016059292,0.0,0.01605929248034954
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지?",답변 실패,-0.0053816237,0.0,0.00538162374868989
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지?",딥러닝,-0.050854456,0.0,0.050854455679655075
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지?",마지막 할 말,-0.024266565,0.0,0.024266565218567848
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지?",머신러닝,0.0061496156,0.0,0.006149615626782179
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지?",면접 시작 인사,-0.028129525,0.0,0.02812952548265457
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지?",상세 경험,-0.026657587,0.0,0.02665758691728115
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지?",수식,0.021145336,0.0,0.02114533632993698
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지?",용어 질문,0.026971966,0.0,0.0269719660282135
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지?",인공지능,0.84602326,1.0,0.15397673845291138
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지?",잠시 휴식,-0.01644426,0.0,0.016444260254502296
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지?",좋아하는 아이돌,-0.003633651,0.0,0.003633650951087475
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지?",핵심 아이디어,-0.015148837,0.0,0.015148837119340897
"인공지능, 머신러닝, 딥러닝 차이 -> 인공지능은 사람이 가지고 있는 두뇌의 능력을 알고리즘으로 구현한 거지. 맞지?",확률 예측에서 MSE Loss 미 사용 이유,-0.010573438,0.0,0.010573438368737698
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야!",BCE 가 좋은 task,-0.0045679337,0.0,0.004567933734506369
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야!",BCE 가 좋은 이유,-0.027619988,0.0,0.027619987726211548
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야!",LLM Fine-Tuning 의 PEFT,0.00089731434,0.0,0.0008973143412731588
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야!",LoRA,0.008936072,0.0,0.008936071768403053
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야!",LoRA 와 QLoRA 의 차이,-0.009750976,0.0,0.009750976227223873
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야!",Loss Function 예시,-0.009256324,0.0,0.009256323799490929
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야!",Loss Function 정의,-0.003094579,0.0,0.003094579093158245
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야!",MBTI,-0.020504624,0.0,0.020504623651504517
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야!",MSE Loss 설명,-0.0030642042,0.0,0.003064204240217805
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야!",MSE Loss 용도,0.009306462,0.0,0.009306461550295353
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야!",Multi-Label 에서 CE + Softmax 적용 문제점,0.01795949,0.0,0.017959490418434143
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야!",PEFT 방법 5가지,-0.01063907,0.0,0.010639069601893425
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야!",거대 언어 모델 정의,0.017071774,0.0,0.01707177422940731
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야!",기본 경험,0.0011692604,0.0,0.0011692603584378958
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야!",답변 실패,0.00022698382,0.0,0.00022698381508234888
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야!",딥러닝,-0.040665537,0.0,0.04066553711891174
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야!",마지막 할 말,-0.005454572,0.0,0.005454571917653084
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야!",머신러닝,0.9683483,1.0,0.03165167570114136
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야!",면접 시작 인사,0.007858763,0.0,0.007858763448894024
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야!",상세 경험,-0.016069625,0.0,0.016069624572992325
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야!",수식,-0.008898145,0.0,0.008898144587874413
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야!",용어 질문,-0.0039006872,0.0,0.0039006872102618217
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야!",인공지능,-0.04574301,0.0,0.04574301093816757
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야!",잠시 휴식,-0.020060644,0.0,0.020060643553733826
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야!",좋아하는 아이돌,-0.008705415,0.0,0.008705414831638336
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야!",핵심 아이디어,-0.007691238,0.0,0.007691238075494766
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 알고리즘을 통해 데이터 패턴을 학습하고, 이걸 새로운 데이터에 적용하는 분야야!",확률 예측에서 MSE Loss 미 사용 이유,0.00016526999,0.0,0.0001652699866099283
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지",BCE 가 좋은 task,-0.006394333,0.0,0.006394333206117153
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지",BCE 가 좋은 이유,-0.003925054,0.0,0.003925053868442774
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지",LLM Fine-Tuning 의 PEFT,-0.019117465,0.0,0.019117465242743492
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지",LoRA,-0.012996019,0.0,0.01299601886421442
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지",LoRA 와 QLoRA 의 차이,0.012846833,0.0,0.01284683309495449
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지",Loss Function 예시,-0.015790042,0.0,0.015790041536092758
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지",Loss Function 정의,-0.026646318,0.0,0.026646317914128304
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지",MBTI,-0.0017916545,0.0,0.001791654503904283
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지",MSE Loss 설명,-0.0073422203,0.0,0.007342220284044743
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지",MSE Loss 용도,-0.015258312,0.0,0.015258312225341797
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지",Multi-Label 에서 CE + Softmax 적용 문제점,0.0064677396,0.0,0.006467739585787058
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지",PEFT 방법 5가지,-0.024889356,0.0,0.024889355525374413
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지",거대 언어 모델 정의,-0.012511163,0.0,0.012511163018643856
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지",기본 경험,-0.0031379221,0.0,0.0031379221472889185
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지",답변 실패,-0.0013875633,0.0,0.0013875633012503386
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지",딥러닝,0.96493363,1.0,0.03506636619567871
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지",마지막 할 말,-0.010818824,0.0,0.010818824172019958
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지",머신러닝,-0.020096917,0.0,0.02009691670536995
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지",면접 시작 인사,-0.004310926,0.0,0.004310925956815481
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지",상세 경험,0.0087395795,0.0,0.00873957946896553
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지",수식,-0.006559374,0.0,0.006559373810887337
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지",용어 질문,-0.015142614,0.0,0.015142614021897316
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지",인공지능,-0.026651759,0.0,0.026651758700609207
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지",잠시 휴식,0.004469873,0.0,0.004469872917979956
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지",좋아하는 아이돌,-0.008103678,0.0,0.00810367800295353
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지",핵심 아이디어,-0.001923956,0.0,0.0019239559769630432
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망이라는 기술을 이용해서 머신러닝 하는 거지",확률 예측에서 MSE Loss 미 사용 이유,-0.0024820205,0.0,0.00248202052898705
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야.",BCE 가 좋은 task,-0.009681988,0.0,0.009681987576186657
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야.",BCE 가 좋은 이유,0.0039759246,0.0,0.0039759245701134205
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야.",LLM Fine-Tuning 의 PEFT,-0.017176963,0.0,0.01717696338891983
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야.",LoRA,-0.0038458398,0.0,0.003845839761197567
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야.",LoRA 와 QLoRA 의 차이,0.006820189,0.0,0.006820188835263252
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야.",Loss Function 예시,-0.019554768,0.0,0.01955476775765419
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야.",Loss Function 정의,-0.018279169,0.0,0.018279168754816055
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야.",MBTI,-0.010653137,0.0,0.010653137229382992
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야.",MSE Loss 설명,-0.0006267369,0.0,0.0006267369026318192
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야.",MSE Loss 용도,-0.023491673,0.0,0.023491673171520233
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야.",Multi-Label 에서 CE + Softmax 적용 문제점,0.009699327,0.0,0.009699326939880848
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야.",PEFT 방법 5가지,-0.03214947,0.0,0.03214947134256363
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야.",거대 언어 모델 정의,-0.012382892,0.0,0.012382891960442066
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야.",기본 경험,0.00075246417,0.0,0.0007524641696363688
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야.",답변 실패,0.0011663106,0.0,0.001166310627013445
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야.",딥러닝,0.9666861,1.0,0.033313870429992676
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야.",마지막 할 말,-0.010812068,0.0,0.010812068358063698
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야.",머신러닝,-0.01876606,0.0,0.01876606047153473
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야.",면접 시작 인사,-0.0020765888,0.0,0.0020765888039022684
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야.",상세 경험,0.011134889,0.0,0.011134888976812363
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야.",수식,-0.008572865,0.0,0.008572865277528763
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야.",용어 질문,-0.014487905,0.0,0.01448790542781353
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야.",인공지능,-0.041116312,0.0,0.04111631214618683
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야.",잠시 휴식,0.004527181,0.0,0.0045271809212863445
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야.",좋아하는 아이돌,-0.00220723,0.0,0.0022072300780564547
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야.",핵심 아이디어,-0.011511975,0.0,0.011511974968016148
"인공지능, 머신러닝, 딥러닝 차이 -> 딥러닝은 인공신경망 기술로 머신러닝을 학습시키는 거야.",확률 예측에서 MSE Loss 미 사용 이유,-0.0009313934,0.0,0.0009313934133388102
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로",BCE 가 좋은 task,-0.005958171,0.0,0.005958171095699072
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로",BCE 가 좋은 이유,-0.0041855676,0.0,0.004185567609965801
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로",LLM Fine-Tuning 의 PEFT,-0.00044878563,0.0,0.0004487856349442154
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로",LoRA,-0.004300697,0.0,0.004300696775317192
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로",LoRA 와 QLoRA 의 차이,0.00070894265,0.0,0.0007089426508173347
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로",Loss Function 예시,-0.008687238,0.0,0.008687238208949566
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로",Loss Function 정의,-0.012998051,0.0,0.012998051010072231
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로",MBTI,-0.006213888,0.0,0.006213888060301542
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로",MSE Loss 설명,-0.0039483947,0.0,0.003948394674807787
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로",MSE Loss 용도,0.0038643146,0.0,0.003864314639940858
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로",Multi-Label 에서 CE + Softmax 적용 문제점,-0.0053869653,0.0,0.005386965349316597
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로",PEFT 방법 5가지,-0.001580126,0.0,0.0015801260014995933
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로",거대 언어 모델 정의,-0.0081611285,0.0,0.008161128498613834
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로",기본 경험,-0.0032907543,0.0,0.0032907542772591114
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로",답변 실패,0.99151653,1.0,0.008483469486236572
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로",딥러닝,0.0018321583,0.0,0.0018321583047509193
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로",마지막 할 말,-0.0012210779,0.0,0.001221077865920961
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로",머신러닝,0.0208108,0.0,0.020810799673199654
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로",면접 시작 인사,-0.003530736,0.0,0.003530736081302166
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로",상세 경험,-0.007365469,0.0,0.007365468889474869
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로",수식,0.0034303262,0.0,0.003430326236411929
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로",용어 질문,-0.011702144,0.0,0.011702143587172031
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로",인공지능,-0.006537227,0.0,0.006537226960062981
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로",잠시 휴식,-0.0025359055,0.0,0.0025359054561704397
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로",좋아하는 아이돌,-0.007394313,0.0,0.007394312880933285
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로",핵심 아이디어,0.007632267,0.0,0.0076322671957314014
"인공지능, 머신러닝, 딥러닝 차이 -> 머신러닝은 기계가 공부하는 거지 말 그대로",확률 예측에서 MSE Loss 미 사용 이유,-0.00090305554,0.0,0.0009030555374920368
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,BCE 가 좋은 task,4.3363896e-05,0.0,4.336389611125924e-05
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,BCE 가 좋은 이유,0.0066801016,0.0,0.0066801016218960285
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,LLM Fine-Tuning 의 PEFT,-0.003744889,0.0,0.003744889050722122
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,LoRA,0.0070116906,0.0,0.007011690642684698
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,LoRA 와 QLoRA 의 차이,0.005213871,0.0,0.005213871132582426
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,Loss Function 예시,0.014805661,0.0,0.014805660583078861
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,Loss Function 정의,-0.045104276,0.0,0.04510427638888359
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,MBTI,0.007194374,0.0,0.007194374222308397
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,MSE Loss 설명,-0.010004175,0.0,0.010004174895584583
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,MSE Loss 용도,-0.00026830373,0.0,0.0002683037309907377
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,Multi-Label 에서 CE + Softmax 적용 문제점,0.0021497135,0.0,0.002149713458493352
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,PEFT 방법 5가지,0.0090373065,0.0,0.009037306532263756
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,거대 언어 모델 정의,0.98863554,1.0,0.011364459991455078
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,기본 경험,0.000340121,0.0,0.00034012101241387427
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,답변 실패,-0.0025220118,0.0,0.0025220117531716824
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,딥러닝,-0.0036218083,0.0,0.0036218082532286644
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,마지막 할 말,-0.01638877,0.0,0.016388770192861557
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,머신러닝,0.016088596,0.0,0.016088595613837242
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,면접 시작 인사,-0.0013962759,0.0,0.0013962759403511882
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,상세 경험,-0.003791484,0.0,0.003791484050452709
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,수식,-0.011993637,0.0,0.011993637308478355
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,용어 질문,-0.00048389423,0.0,0.00048389422590844333
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,인공지능,-0.009647873,0.0,0.009647873230278492
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,잠시 휴식,0.0015637181,0.0,0.0015637180767953396
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,좋아하는 아이돌,-0.00015325396,0.0,0.0001532539608888328
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,핵심 아이디어,0.018664477,0.0,0.01866447739303112
거대 언어 모델 정의 -> 거대 언어 모델은 수백억 개 이상의 파라미터를 가진 아주 큰 언어 모델이야,확률 예측에서 MSE Loss 미 사용 이유,0.02283173,0.0,0.022831730544567108
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,BCE 가 좋은 task,-0.028419927,0.0,0.02841992676258087
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,BCE 가 좋은 이유,0.0024374384,0.0,0.0024374383501708508
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,LLM Fine-Tuning 의 PEFT,0.011148916,0.0,0.011148915626108646
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,LoRA,-0.044720206,0.0,0.044720206409692764
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,LoRA 와 QLoRA 의 차이,0.0005698279,0.0,0.0005698279128409922
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,Loss Function 예시,-0.009067934,0.0,0.00906793400645256
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,Loss Function 정의,-0.0328487,0.0,0.032848700881004333
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,MBTI,-0.035089772,0.0,0.03508977219462395
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,MSE Loss 설명,-0.0026895893,0.0,0.0026895892806351185
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,MSE Loss 용도,0.001291789,0.0,0.0012917889980599284
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,Multi-Label 에서 CE + Softmax 적용 문제점,-0.0166245,0.0,0.01662449911236763
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,PEFT 방법 5가지,-0.03166427,0.0,0.03166427090764046
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,거대 언어 모델 정의,0.88004,0.0,0.8800399899482727
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,기본 경험,-0.0050597265,0.0,0.005059726536273956
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,답변 실패,0.20054772,1.0,0.7994522750377655
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,딥러닝,0.02422534,0.0,0.02422533929347992
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,마지막 할 말,-0.003510984,0.0,0.00351098389364779
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,머신러닝,0.017695723,0.0,0.017695723101496696
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,면접 시작 인사,-0.011202309,0.0,0.011202309280633926
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,상세 경험,0.03729403,0.0,0.03729403018951416
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,수식,-0.005306251,0.0,0.005306250881403685
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,용어 질문,0.019816494,0.0,0.01981649361550808
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,인공지능,-0.050842475,0.0,0.05084247514605522
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,잠시 휴식,-0.021062896,0.0,0.02106289565563202
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,좋아하는 아이돌,-4.1333567e-07,0.0,4.1333566969115054e-07
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,핵심 아이디어,0.010824347,0.0,0.010824346914887428
거대 언어 모델 정의 -> 파라미터 엄청나게 많다던데,확률 예측에서 MSE Loss 미 사용 이유,-0.007366469,0.0,0.007366469129920006
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델,BCE 가 좋은 task,0.004529536,0.0,0.00452953577041626
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델,BCE 가 좋은 이유,0.012351646,0.0,0.012351646088063717
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델,LLM Fine-Tuning 의 PEFT,-0.0012297491,0.0,0.0012297490611672401
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델,LoRA,0.011877764,0.0,0.011877764016389847
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델,LoRA 와 QLoRA 의 차이,0.0060103564,0.0,0.006010356359183788
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델,Loss Function 예시,0.011453709,0.0,0.01145370863378048
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델,Loss Function 정의,-0.01803686,0.0,0.01803685910999775
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델,MBTI,0.008512732,0.0,0.00851273164153099
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델,MSE Loss 설명,-0.018959688,0.0,0.018959688022732735
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델,MSE Loss 용도,-0.010137105,0.0,0.010137105360627174
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델,Multi-Label 에서 CE + Softmax 적용 문제점,0.005848785,0.0,0.005848784931004047
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델,PEFT 방법 5가지,0.011072261,0.0,0.01107226125895977
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델,거대 언어 모델 정의,0.9780023,1.0,0.021997690200805664
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델,기본 경험,-0.011544926,0.0,0.011544926092028618
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델,답변 실패,-0.0077949236,0.0,0.007794923614710569
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델,딥러닝,-0.0140968505,0.0,0.014096850529313087
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델,마지막 할 말,-0.0148361875,0.0,0.014836187474429607
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델,머신러닝,0.0076228483,0.0,0.007622848264873028
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델,면접 시작 인사,0.001644877,0.0,0.0016448770184069872
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델,상세 경험,0.013722717,0.0,0.013722716830670834
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델,수식,-0.015116772,0.0,0.015116771683096886
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델,용어 질문,-0.0041268975,0.0,0.004126897547394037
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델,인공지능,-0.011107015,0.0,0.011107015423476696
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델,잠시 휴식,-0.0094301915,0.0,0.009430191479623318
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델,좋아하는 아이돌,-0.005195431,0.0,0.00519543094560504
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델,핵심 아이디어,0.021881385,0.0,0.021881384775042534
거대 언어 모델 정의 -> 엄청나게 많은 파라미터를 가져서 사람이 이해할 수 있는 언어를 생성할 수 있을 정도의 큰 언어 모델,확률 예측에서 MSE Loss 미 사용 이유,0.026202234,0.0,0.026202233508229256
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지.,BCE 가 좋은 task,-0.028996333,0.0,0.028996333479881287
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지.,BCE 가 좋은 이유,7.25849e-07,0.0,7.258490200001688e-07
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지.,LLM Fine-Tuning 의 PEFT,0.007127639,0.0,0.007127638906240463
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지.,LoRA,-0.026127255,0.0,0.026127254590392113
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지.,LoRA 와 QLoRA 의 차이,-0.0026598405,0.0,0.0026598405092954636
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지.,Loss Function 예시,-0.025580784,0.0,0.025580784305930138
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지.,Loss Function 정의,-0.040412877,0.0,0.04041287675499916
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지.,MBTI,-0.016658267,0.0,0.016658267006278038
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지.,MSE Loss 설명,-0.0015156224,0.0,0.0015156223671510816
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지.,MSE Loss 용도,-0.010186661,0.0,0.010186661034822464
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지.,Multi-Label 에서 CE + Softmax 적용 문제점,-0.03767664,0.0,0.03767663985490799
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지.,PEFT 방법 5가지,-0.023081083,0.0,0.023081082850694656
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지.,거대 언어 모델 정의,0.46384028,0.0,0.4638402760028839
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지.,기본 경험,-0.011437151,0.0,0.01143715064972639
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지.,답변 실패,0.7770473,1.0,0.2229527235031128
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지.,딥러닝,-0.008019071,0.0,0.008019071072340012
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지.,마지막 할 말,-0.0149029475,0.0,0.014902947470545769
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지.,머신러닝,0.02062835,0.0,0.020628349855542183
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지.,면접 시작 인사,0.013142242,0.0,0.013142242096364498
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지.,상세 경험,0.0022725293,0.0,0.0022725292947143316
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지.,수식,-0.024698092,0.0,0.02469809167087078
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지.,용어 질문,-0.014631988,0.0,0.014631987549364567
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지.,인공지능,-0.046126727,0.0,0.046126727014780045
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지.,잠시 휴식,-0.013303194,0.0,0.01330319419503212
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지.,좋아하는 아이돌,-0.007096063,0.0,0.007096062880009413
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지.,핵심 아이디어,0.016689485,0.0,0.01668948493897915
거대 언어 모델 정의 -> 말 그대로 거대한 언어 모델이지.,확률 예측에서 MSE Loss 미 사용 이유,-0.031485174,0.0,0.0314851738512516
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,BCE 가 좋은 task,-0.0019739126,0.0,0.001973912585526705
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,BCE 가 좋은 이유,-0.0019218617,0.0,0.0019218616653233767
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,LLM Fine-Tuning 의 PEFT,0.0009050223,0.0,0.000905022316146642
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,LoRA,-0.025113985,0.0,0.025113984942436218
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,LoRA 와 QLoRA 의 차이,0.00012520827,0.0,0.0001252082729479298
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,Loss Function 예시,0.00041965017,0.0,0.00041965016862377524
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,Loss Function 정의,0.979706,0.0,0.9797059893608093
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,MBTI,0.020016547,0.0,0.020016547292470932
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,MSE Loss 설명,-0.0035229516,0.0,0.0035229516215622425
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,MSE Loss 용도,0.024875773,0.0,0.02487577311694622
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,Multi-Label 에서 CE + Softmax 적용 문제점,-0.011998923,0.0,0.011998922564089298
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,PEFT 방법 5가지,-0.006931624,0.0,0.006931623909622431
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,거대 언어 모델 정의,-0.024233708,0.0,0.024233708158135414
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,기본 경험,0.006438653,0.0,0.0064386529847979546
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,답변 실패,-0.0035989683,1.0,1.0035989682655782
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,딥러닝,-0.022031331,0.0,0.022031331434845924
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,마지막 할 말,0.007334675,0.0,0.007334675174206495
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,머신러닝,0.00029751044,0.0,0.00029751044348813593
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,면접 시작 인사,-0.009932593,0.0,0.009932593442499638
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,상세 경험,0.008769419,0.0,0.00876941904425621
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,수식,0.009048265,0.0,0.009048265404999256
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,용어 질문,0.0054258653,0.0,0.005425865296274424
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,인공지능,-0.01986308,0.0,0.019863080233335495
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,잠시 휴식,0.0028051254,0.0,0.0028051254339516163
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,좋아하는 아이돌,-0.008710198,0.0,0.008710198104381561
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,핵심 아이디어,-0.016897874,0.0,0.01689787395298481
Loss Function 정의 -> 손실 함수는 딥러닝 모델이 얼마나 돈을 잃었는지 나타내는 거야,확률 예측에서 MSE Loss 미 사용 이유,0.009741777,0.0,0.009741776622831821
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",BCE 가 좋은 task,-0.00761986,0.0,0.007619860116392374
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",BCE 가 좋은 이유,-0.0019012456,0.0,0.001901245559565723
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",LLM Fine-Tuning 의 PEFT,0.010856175,0.0,0.010856174863874912
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",LoRA,-0.0043050777,0.0,0.004305077716708183
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",LoRA 와 QLoRA 의 차이,0.0034857416,0.0,0.0034857415594160557
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",Loss Function 예시,0.0008963233,0.0,0.0008963232976384461
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",Loss Function 정의,0.9798418,1.0,0.02015817165374756
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",MBTI,0.024510497,0.0,0.024510497227311134
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",MSE Loss 설명,-0.011169192,0.0,0.011169192381203175
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",MSE Loss 용도,0.023028424,0.0,0.023028424009680748
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",Multi-Label 에서 CE + Softmax 적용 문제점,-0.0014777569,0.0,0.001477756886743009
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",PEFT 방법 5가지,0.0009231177,0.0,0.0009231176809407771
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",거대 언어 모델 정의,-0.04068142,0.0,0.040681421756744385
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",기본 경험,-0.0006905312,0.0,0.0006905312184244394
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",답변 실패,-0.007456492,0.0,0.0074564921669662
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",딥러닝,-0.036704853,0.0,0.03670485317707062
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",마지막 할 말,-0.012264618,0.0,0.012264617718756199
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",머신러닝,0.0060989694,0.0,0.0060989693738520145
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",면접 시작 인사,0.0011068978,0.0,0.001106897834688425
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",상세 경험,-0.015654651,0.0,0.015654651448130608
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",수식,0.0092851,0.0,0.009285099804401398
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",용어 질문,0.008309643,0.0,0.008309642784297466
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",인공지능,-0.013426675,0.0,0.013426675461232662
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",잠시 휴식,-0.0019686643,0.0,0.001968664349988103
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",좋아하는 아이돌,-0.011531432,0.0,0.011531432159245014
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",핵심 아이디어,-0.04042567,0.0,0.04042566940188408
"Loss Function 정의 -> Loss Function 은 모델의 오차를 일정한 수식, 즉 함수로 나타낸 거야! 이걸 최대한 줄이는 식으로 학습되지!",확률 예측에서 MSE Loss 미 사용 이유,-0.006765684,0.0,0.006765684112906456
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야,BCE 가 좋은 task,-0.0062827836,0.0,0.006282783579081297
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야,BCE 가 좋은 이유,-0.023504505,0.0,0.023504504933953285
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야,LLM Fine-Tuning 의 PEFT,0.008269708,0.0,0.008269707672297955
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야,LoRA,-0.010815991,0.0,0.010815991088747978
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야,LoRA 와 QLoRA 의 차이,0.0055994573,0.0,0.0055994573049247265
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야,Loss Function 예시,0.021887375,0.0,0.02188737504184246
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야,Loss Function 정의,0.94468886,0.0,0.9446888566017151
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야,MBTI,0.02569083,0.0,0.025690829381346703
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야,MSE Loss 설명,-0.009276346,0.0,0.009276346303522587
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야,MSE Loss 용도,0.0087938355,0.0,0.008793835528194904
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야,Multi-Label 에서 CE + Softmax 적용 문제점,-0.02153656,0.0,0.021536560729146004
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야,PEFT 방법 5가지,-0.01417879,0.0,0.014178790152072906
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야,거대 언어 모델 정의,-0.055053405,0.0,0.055053405463695526
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야,기본 경험,-0.0015555606,0.0,0.0015555606223642826
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야,답변 실패,0.10049862,1.0,0.899501383304596
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야,딥러닝,-0.030942138,0.0,0.03094213828444481
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야,마지막 할 말,0.012277816,0.0,0.01227781642228365
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야,머신러닝,-0.0026515487,0.0,0.0026515487115830183
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야,면접 시작 인사,0.006814063,0.0,0.006814063061028719
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야,상세 경험,-0.014479078,0.0,0.014479078352451324
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야,수식,0.016718036,0.0,0.01671803556382656
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야,용어 질문,0.027713697,0.0,0.027713697403669357
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야,인공지능,-0.012122449,0.0,0.012122449465095997
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야,잠시 휴식,-0.015482631,0.0,0.015482630580663681
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야,좋아하는 아이돌,-0.016592504,0.0,0.01659250445663929
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야,핵심 아이디어,-0.0056472677,0.0,0.005647267680615187
Loss Function 정의 -> Loss Function 은 딥러닝 모델의 손해를 나타내는 함수야,확률 예측에서 MSE Loss 미 사용 이유,-0.0042097755,0.0,0.004209775477647781
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지,BCE 가 좋은 task,-0.0019977354,0.0,0.0019977353513240814
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지,BCE 가 좋은 이유,0.0015578637,0.0,0.0015578636666759849
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지,LLM Fine-Tuning 의 PEFT,-0.00421561,0.0,0.004215610213577747
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지,LoRA,-0.005444829,0.0,0.005444828886538744
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지,LoRA 와 QLoRA 의 차이,0.0005562726,0.0,0.0005562725709751248
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지,Loss Function 예시,-0.0054927976,0.0,0.005492797587066889
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지,Loss Function 정의,0.97933745,1.0,0.020662546157836914
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지,MBTI,0.029010037,0.0,0.02901003696024418
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지,MSE Loss 설명,-0.012588944,0.0,0.012588944286108017
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지,MSE Loss 용도,0.023658147,0.0,0.02365814708173275
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지,Multi-Label 에서 CE + Softmax 적용 문제점,-0.012280127,0.0,0.01228012703359127
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지,PEFT 방법 5가지,0.001779418,0.0,0.001779417973011732
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지,거대 언어 모델 정의,-0.012455401,0.0,0.012455401010811329
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지,기본 경험,0.0073962826,0.0,0.0073962826281785965
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지,답변 실패,-0.0149522275,0.0,0.014952227473258972
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지,딥러닝,-0.019234663,0.0,0.019234662875533104
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지,마지막 할 말,-0.007873279,0.0,0.00787327904254198
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지,머신러닝,0.016762508,0.0,0.0167625080794096
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지,면접 시작 인사,0.00133837,0.0,0.0013383700279518962
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지,상세 경험,-0.009502076,0.0,0.009502075612545013
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지,수식,0.0071818214,0.0,0.0071818213909864426
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지,용어 질문,-0.00041408045,0.0,0.0004140804521739483
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지,인공지능,-0.027229467,0.0,0.027229467406868935
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지,잠시 휴식,0.0013036683,0.0,0.0013036682503297925
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지,좋아하는 아이돌,-0.016934197,0.0,0.016934197396039963
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지,핵심 아이디어,-0.028778838,0.0,0.028778837993741035
Loss Function 정의 -> 모델의 예측과 실제 값의 오차를 일정한 수식으로 정의하는 그 수식이지,확률 예측에서 MSE Loss 미 사용 이유,-0.0005342788,0.0,0.0005342788062989712
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,BCE 가 좋은 task,0.00053039746,0.0,0.0005303974612616003
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,BCE 가 좋은 이유,-0.0030951258,0.0,0.0030951257795095444
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,LLM Fine-Tuning 의 PEFT,-0.0011191039,0.0,0.0011191038647666574
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,LoRA,0.0002352348,0.0,0.00023523479467257857
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,LoRA 와 QLoRA 의 차이,0.002692092,0.0,0.002692091977223754
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,Loss Function 예시,0.038280368,0.0,0.038280367851257324
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,Loss Function 정의,-0.012106523,0.0,0.012106522917747498
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,MBTI,0.00057658367,0.0,0.000576583668589592
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,MSE Loss 설명,-0.0042247,0.0,0.004224699921905994
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,MSE Loss 용도,0.0025299834,0.0,0.0025299834087491035
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,Multi-Label 에서 CE + Softmax 적용 문제점,-0.004799634,0.0,0.004799634218215942
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,PEFT 방법 5가지,-0.004161977,0.0,0.004161977209150791
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,거대 언어 모델 정의,-0.007570711,0.0,0.007570710964500904
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,기본 경험,-0.009204594,0.0,0.009204594418406487
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,답변 실패,0.99222004,1.0,0.007779955863952637
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,딥러닝,-0.0044717574,0.0,0.00447175744920969
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,마지막 할 말,-0.0016143465,0.0,0.0016143465181812644
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,머신러닝,0.0032435788,0.0,0.0032435788307338953
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,면접 시작 인사,-0.0039030777,0.0,0.003903077682480216
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,상세 경험,-0.009816479,0.0,0.009816478937864304
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,수식,-0.00911102,0.0,0.009111019782721996
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,용어 질문,-0.01583912,0.0,0.015839120373129845
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,인공지능,-0.014959246,0.0,0.014959245920181274
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,잠시 휴식,0.0015116397,0.0,0.0015116396825760603
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,좋아하는 아이돌,0.0012838232,0.0,0.0012838231632485986
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,핵심 아이디어,0.0038469697,0.0,0.003846969688311219
Loss Function 예시 -> ReLU 랑 Tanh 생각나는데,확률 예측에서 MSE Loss 미 사용 이유,0.0005989413,0.0,0.0005989412893541157
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",BCE 가 좋은 task,0.0007151741,0.0,0.0007151740719564259
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",BCE 가 좋은 이유,0.001133632,0.0,0.0011336320312693715
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",LLM Fine-Tuning 의 PEFT,-0.019398887,0.0,0.01939888671040535
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",LoRA,-0.0022505107,0.0,0.002250510733574629
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",LoRA 와 QLoRA 의 차이,0.0032420342,0.0,0.0032420342322438955
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",Loss Function 예시,0.98582745,1.0,0.014172554016113281
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",Loss Function 정의,-0.0023240156,0.0,0.002324015600606799
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",MBTI,0.006956225,0.0,0.006956224795430899
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",MSE Loss 설명,-0.008672995,0.0,0.008672994561493397
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",MSE Loss 용도,-0.011629021,0.0,0.011629020795226097
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",Multi-Label 에서 CE + Softmax 적용 문제점,0.0097527,0.0,0.009752700105309486
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",PEFT 방법 5가지,-0.0056560007,0.0,0.005656000692397356
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",거대 언어 모델 정의,0.012815373,0.0,0.01281537301838398
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",기본 경험,0.00019181508,0.0,0.00019181508105248213
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",답변 실패,0.020980816,0.0,0.020980816334486008
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",딥러닝,-0.011240085,0.0,0.011240084655582905
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",마지막 할 말,-0.0066973143,0.0,0.006697314325720072
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",머신러닝,-0.008742688,0.0,0.008742688223719597
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",면접 시작 인사,-0.006268408,0.0,0.00626840814948082
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",상세 경험,-0.010778004,0.0,0.010778004303574562
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",수식,-0.0057448125,0.0,0.005744812544435263
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",용어 질문,-0.009284318,0.0,0.009284318424761295
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",인공지능,-0.024119489,0.0,0.024119488894939423
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",잠시 휴식,0.00091187336,0.0,0.0009118733578361571
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",좋아하는 아이돌,-0.008099606,0.0,0.00809960626065731
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",핵심 아이디어,-0.024187611,0.0,0.024187611415982246
"Loss Function 예시 -> MSE, MAE, Cross Entropy Loss 같은 것들이 있지",확률 예측에서 MSE Loss 미 사용 이유,-0.00469505,0.0,0.004695049952715635
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어",BCE 가 좋은 task,0.0062070047,0.0,0.006207004655152559
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어",BCE 가 좋은 이유,0.0029221012,0.0,0.002922101179137826
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어",LLM Fine-Tuning 의 PEFT,-0.012473711,0.0,0.01247371081262827
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어",LoRA,0.0065338844,0.0,0.006533884443342686
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어",LoRA 와 QLoRA 의 차이,0.0015934618,0.0,0.001593461842276156
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어",Loss Function 예시,0.98680246,1.0,0.013197541236877441
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어",Loss Function 정의,-0.011067828,0.0,0.0110678281635046
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어",MBTI,0.01214693,0.0,0.01214693021029234
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어",MSE Loss 설명,-0.012224308,0.0,0.012224308215081692
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어",MSE Loss 용도,-0.024708368,0.0,0.024708367884159088
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어",Multi-Label 에서 CE + Softmax 적용 문제점,0.010316641,0.0,0.010316641069948673
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어",PEFT 방법 5가지,-0.008504939,0.0,0.008504939265549183
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어",거대 언어 모델 정의,0.026342982,0.0,0.026342982426285744
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어",기본 경험,-0.0025897021,0.0,0.0025897021405398846
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어",답변 실패,-0.0011661032,0.0,0.0011661031749099493
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어",딥러닝,-0.00535814,0.0,0.005358139984309673
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어",마지막 할 말,-0.01007631,0.0,0.01007630955427885
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어",머신러닝,-0.008287809,0.0,0.008287808857858181
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어",면접 시작 인사,-0.0012593591,0.0,0.0012593590654432774
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어",상세 경험,0.003496397,0.0,0.003496397053822875
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어",수식,-0.018307965,0.0,0.018307965248823166
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어",용어 질문,-0.010203139,0.0,0.010203138925135136
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어",인공지능,-0.032666933,0.0,0.03266693279147148
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어",잠시 휴식,0.0011280918,0.0,0.0011280918261036277
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어",좋아하는 아이돌,-0.0055111726,0.0,0.005511172581464052
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어",핵심 아이디어,-0.02336858,0.0,0.023368580266833305
"Loss Function 예시 -> Binary Cross Entropy, 그냥 Cross Entropy, 그 외에도 DICE 같은 엄청 다양한 것들이 있어",확률 예측에서 MSE Loss 미 사용 이유,-0.006288432,0.0,0.00628843205049634
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야?",BCE 가 좋은 task,-0.0006981817,0.0,0.0006981816841289401
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야?",BCE 가 좋은 이유,-0.001855335,0.0,0.0018553349655121565
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야?",LLM Fine-Tuning 의 PEFT,-0.002929462,0.0,0.0029294618871062994
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야?",LoRA,0.0022044824,0.0,0.0022044824436306953
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야?",LoRA 와 QLoRA 의 차이,0.0018877039,0.0,0.0018877038965001702
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야?",Loss Function 예시,0.008667593,0.0,0.008667592890560627
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야?",Loss Function 정의,-0.008584713,0.0,0.008584712632000446
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야?",MBTI,0.0031033864,0.0,0.00310338637791574
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야?",MSE Loss 설명,-0.00067299575,0.0,0.000672995753120631
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야?",MSE Loss 용도,0.0017199514,0.0,0.0017199513968080282
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야?",Multi-Label 에서 CE + Softmax 적용 문제점,-0.0018579592,0.0,0.0018579591996967793
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야?",PEFT 방법 5가지,-0.0024911836,0.0,0.002491183578968048
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야?",거대 언어 모델 정의,-0.0073169093,0.0,0.00731690926477313
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야?",기본 경험,-0.00815722,0.0,0.008157219737768173
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야?",답변 실패,0.99324656,1.0,0.006753444671630859
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야?",딥러닝,-0.0012318272,0.0,0.0012318271910771728
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야?",마지막 할 말,-0.0039163255,0.0,0.0039163255132734776
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야?",머신러닝,-0.00056171516,0.0,0.0005617151618935168
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야?",면접 시작 인사,-0.00454719,0.0,0.004547189921140671
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야?",상세 경험,-0.0041313954,0.0,0.004131395369768143
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야?",수식,-0.008731243,0.0,0.008731243200600147
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야?",용어 질문,-0.009599424,0.0,0.009599423967301846
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야?",인공지능,-0.0075532948,0.0,0.007553294766694307
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야?",잠시 휴식,0.002099812,0.0,0.0020998120307922363
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야?",좋아하는 아이돌,-0.0013907666,0.0,0.0013907665852457285
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야?",핵심 아이디어,0.004635189,0.0,0.004635189194232225
"Loss Function 예시 -> Sigmoid, Tanh, Adam, AdamW 이런 거 아니야?",확률 예측에서 MSE Loss 미 사용 이유,0.00025340542,0.0,0.0002534054219722748
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어",BCE 가 좋은 task,0.004858904,0.0,0.004858904052525759
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어",BCE 가 좋은 이유,0.00402216,0.0,0.004022160079330206
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어",LLM Fine-Tuning 의 PEFT,-0.0066221897,0.0,0.0066221896559000015
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어",LoRA,0.0021312933,0.0,0.0021312932949513197
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어",LoRA 와 QLoRA 의 차이,0.0041113324,0.0,0.004111332353204489
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어",Loss Function 예시,0.9890394,1.0,0.010960578918457031
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어",Loss Function 정의,0.0022440618,0.0,0.002244061790406704
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어",MBTI,0.01605418,0.0,0.0160541795194149
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어",MSE Loss 설명,-0.008326823,0.0,0.008326822891831398
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어",MSE Loss 용도,-0.014353537,0.0,0.014353537000715733
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어",Multi-Label 에서 CE + Softmax 적용 문제점,0.0052247504,0.0,0.005224750377237797
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어",PEFT 방법 5가지,-0.006572479,0.0,0.0065724789164960384
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어",거대 언어 모델 정의,0.017143158,0.0,0.017143158242106438
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어",기본 경험,-0.010369778,0.0,0.01036977767944336
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어",답변 실패,-0.005245736,0.0,0.005245735868811607
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어",딥러닝,-0.016701734,0.0,0.01670173369348049
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어",마지막 할 말,-0.008356093,0.0,0.008356093429028988
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어",머신러닝,-0.0026372303,0.0,0.0026372303254902363
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어",면접 시작 인사,-0.0054762834,0.0,0.005476283375173807
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어",상세 경험,-0.007673968,0.0,0.007673968095332384
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어",수식,-0.0074063176,0.0,0.007406317628920078
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어",용어 질문,-0.0072485954,0.0,0.007248595356941223
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어",인공지능,-0.02434917,0.0,0.024349169805645943
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어",잠시 휴식,-0.0055236258,0.0,0.005523625761270523
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어",좋아하는 아이돌,-0.010378868,0.0,0.010378868319094181
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어",핵심 아이디어,-0.02074311,0.0,0.02074310928583145
"Loss Function 예시 -> Mean Squared/Absolute Error (MSE, MAE) 가 대표적으로 쓰이고 그 외에도 Cosine Similarity Loss 같은 게 있어",확률 예측에서 MSE Loss 미 사용 이유,-0.008840141,0.0,0.008840140886604786
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아",BCE 가 좋은 task,-0.0033795743,0.0,0.003379574278369546
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아",BCE 가 좋은 이유,0.0012729253,0.0,0.0012729252921417356
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아",LLM Fine-Tuning 의 PEFT,-0.013275968,0.0,0.01327596791088581
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아",LoRA,-0.006181676,0.0,0.006181675940752029
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아",LoRA 와 QLoRA 의 차이,-0.0006642054,0.0,0.0006642054067924619
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아",Loss Function 예시,0.97574335,1.0,0.024256646633148193
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아",Loss Function 정의,-0.0071004615,0.0,0.007100461516529322
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아",MBTI,0.000136229,0.0,0.00013622900587506592
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아",MSE Loss 설명,-0.019006284,0.0,0.019006283953785896
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아",MSE Loss 용도,-0.022220772,0.0,0.02222077175974846
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아",Multi-Label 에서 CE + Softmax 적용 문제점,0.0070433584,0.0,0.007043358404189348
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아",PEFT 방법 5가지,-0.0025987898,0.0,0.0025987897533923388
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아",거대 언어 모델 정의,0.014950686,0.0,0.014950686134397984
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아",기본 경험,-0.0048691886,0.0,0.0048691886477172375
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아",답변 실패,0.04855791,0.0,0.048557911068201065
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아",딥러닝,-0.009965173,0.0,0.009965172968804836
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아",마지막 할 말,-0.0052297814,0.0,0.00522978138178587
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아",머신러닝,-0.015686994,0.0,0.015686994418501854
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아",면접 시작 인사,-0.008134831,0.0,0.008134830743074417
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아",상세 경험,-0.0003425185,0.0,0.0003425184986554086
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아",수식,-0.009923707,0.0,0.009923706762492657
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아",용어 질문,-0.0076210606,0.0,0.007621060591191053
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아",인공지능,-0.022437386,0.0,0.022437386214733124
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아",잠시 휴식,-0.00034774726,0.0,0.00034774726373143494
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아",좋아하는 아이돌,-0.010945839,0.0,0.010945838876068592
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아",핵심 아이디어,-0.02427678,0.0,0.02427677996456623
"Loss Function 예시 -> BCE (Binary Cross Entropy), CE (Cross Entropy), DICE Loss 같은 것들이 있잖아",확률 예측에서 MSE Loss 미 사용 이유,-0.012918099,0.0,0.012918098829686642
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,BCE 가 좋은 task,0.00047290156,0.0,0.00047290156362578273
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,BCE 가 좋은 이유,-0.025068406,0.0,0.025068406015634537
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,LLM Fine-Tuning 의 PEFT,0.006982128,0.0,0.006982128135859966
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,LoRA,-0.016113793,0.0,0.01611379347741604
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,LoRA 와 QLoRA 의 차이,0.0070491666,0.0,0.007049166597425938
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,Loss Function 예시,0.0015094471,0.0,0.0015094471164047718
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,Loss Function 정의,-0.012985711,0.0,0.012985710985958576
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,MBTI,0.028153459,0.0,0.02815345861017704
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,MSE Loss 설명,0.9807916,1.0,0.01920837163925171
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,MSE Loss 용도,-0.0075128716,0.0,0.007512871641665697
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,Multi-Label 에서 CE + Softmax 적용 문제점,0.0058068973,0.0,0.005806897301226854
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,PEFT 방법 5가지,-0.003682218,0.0,0.00368221802636981
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,거대 언어 모델 정의,-0.0005605258,0.0,0.0005605258047580719
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,기본 경험,-0.0031898185,0.0,0.0031898184679448605
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,답변 실패,0.0051069716,0.0,0.005106971599161625
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,딥러닝,-0.005155728,0.0,0.0051557281985878944
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,마지막 할 말,0.0015676718,0.0,0.001567671773955226
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,머신러닝,-0.0030367356,0.0,0.0030367355793714523
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,면접 시작 인사,0.010518341,0.0,0.010518341325223446
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,상세 경험,-0.012141623,0.0,0.012141622602939606
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,수식,0.011647264,0.0,0.011647263541817665
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,용어 질문,-0.007608179,0.0,0.007608179003000259
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,인공지능,-0.007945677,0.0,0.007945677265524864
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,잠시 휴식,0.0006564481,0.0,0.0006564480718225241
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,좋아하는 아이돌,-0.0036347155,0.0,0.0036347154527902603
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,핵심 아이디어,-0.018892795,0.0,0.018892794847488403
MSE Loss 설명 -> MSE Loss 는 오차의 제곱을 평균한 값이야,확률 예측에서 MSE Loss 미 사용 이유,-0.021334514,0.0,0.021334514021873474
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,BCE 가 좋은 task,0.0028406293,0.0,0.0028406293131411076
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,BCE 가 좋은 이유,-0.026548712,0.0,0.026548711583018303
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,LLM Fine-Tuning 의 PEFT,0.0054640956,0.0,0.005464095622301102
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,LoRA,-0.03485103,0.0,0.03485102951526642
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,LoRA 와 QLoRA 의 차이,0.006262231,0.0,0.0062622311525046825
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,Loss Function 예시,-0.00840115,0.0,0.00840114988386631
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,Loss Function 정의,-0.012278662,0.0,0.0122786620631814
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,MBTI,0.014115322,0.0,0.01411532238125801
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,MSE Loss 설명,0.956561,0.0,0.9565610289573669
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,MSE Loss 용도,-0.03170525,0.0,0.03170524910092354
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,Multi-Label 에서 CE + Softmax 적용 문제점,0.0025398368,0.0,0.0025398368015885353
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,PEFT 방법 5가지,0.0028398645,0.0,0.0028398644644767046
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,거대 언어 모델 정의,0.007962314,0.0,0.007962314411997795
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,기본 경험,-0.004170624,0.0,0.004170624073594809
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,답변 실패,0.03457674,1.0,0.965423259884119
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,딥러닝,0.0007920018,0.0,0.0007920017815195024
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,마지막 할 말,0.008527985,0.0,0.008527984842658043
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,머신러닝,-0.01734552,0.0,0.017345519736409187
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,면접 시작 인사,-0.0021392656,0.0,0.002139265649020672
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,상세 경험,0.02109915,0.0,0.02109915018081665
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,수식,-0.0018374112,0.0,0.0018374111969023943
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,용어 질문,-0.021924833,0.0,0.021924832835793495
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,인공지능,0.007905229,0.0,0.00790522899478674
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,잠시 휴식,-0.01713112,0.0,0.017131119966506958
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,좋아하는 아이돌,-0.0127530545,0.0,0.01275305449962616
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,핵심 아이디어,-0.0050987112,0.0,0.005098711233586073
MSE Loss 설명 -> MSE Loss 는 오차 그 자체를 평균한 값이지. 맞지?,확률 예측에서 MSE Loss 미 사용 이유,-0.026783401,0.0,0.026783401146531105
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",BCE 가 좋은 task,0.0007509049,0.0,0.0007509049028158188
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",BCE 가 좋은 이유,-0.0015016095,0.0,0.0015016094548627734
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",LLM Fine-Tuning 의 PEFT,-0.022782011,0.0,0.022782010957598686
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",LoRA,-0.030847302,0.0,0.030847301706671715
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",LoRA 와 QLoRA 의 차이,-0.0052017053,0.0,0.005201705265790224
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",Loss Function 예시,-0.030192398,0.0,0.03019239753484726
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",Loss Function 정의,0.006385546,0.0,0.006385546177625656
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",MBTI,0.014691995,0.0,0.014691995456814766
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",MSE Loss 설명,-0.010069213,0.0,0.01006921287626028
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",MSE Loss 용도,0.9770519,1.0,0.022948086261749268
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",Multi-Label 에서 CE + Softmax 적용 문제점,0.011371605,0.0,0.011371605098247528
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",PEFT 방법 5가지,0.008176007,0.0,0.008176007308065891
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",거대 언어 모델 정의,0.013148432,0.0,0.013148431666195393
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",기본 경험,0.0019993018,0.0,0.0019993018358945847
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",답변 실패,0.0029847927,0.0,0.0029847926925867796
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",딥러닝,-0.020907065,0.0,0.020907064899802208
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",마지막 할 말,-0.021173486,0.0,0.02117348648607731
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",머신러닝,0.0111635355,0.0,0.01116353552788496
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",면접 시작 인사,-0.006800937,0.0,0.006800937000662088
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",상세 경험,-0.013281343,0.0,0.013281342573463917
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",수식,0.007223808,0.0,0.007223808206617832
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",용어 질문,-0.009922538,0.0,0.009922537952661514
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",인공지능,0.0074818083,0.0,0.007481808308511972
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",잠시 휴식,-0.024672054,0.0,0.02467205375432968
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",좋아하는 아이돌,0.00078343105,0.0,0.0007834310526959598
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",핵심 아이디어,-0.039298125,0.0,0.039298124611377716
"MSE Loss 용도 -> 회귀 문제, 즉 연속적인 값을 딥러닝으로 예측할때 사용되지",확률 예측에서 MSE Loss 미 사용 이유,0.0057947277,0.0,0.005794727709144354
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,BCE 가 좋은 task,0.005092373,0.0,0.005092373117804527
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,BCE 가 좋은 이유,-0.0048675616,0.0,0.004867561627179384
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,LLM Fine-Tuning 의 PEFT,0.0024579251,0.0,0.0024579251185059547
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,LoRA,-0.0012076146,0.0,0.0012076145503669977
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,LoRA 와 QLoRA 의 차이,0.0020354397,0.0,0.0020354397129267454
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,Loss Function 예시,0.00052242546,0.0,0.0005224254564382136
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,Loss Function 정의,-0.012123781,0.0,0.012123781256377697
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,MBTI,-0.006585454,0.0,0.006585454102605581
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,MSE Loss 설명,0.00066781155,0.0,0.0006678115460090339
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,MSE Loss 용도,0.013140489,0.0,0.013140489347279072
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,Multi-Label 에서 CE + Softmax 적용 문제점,-0.005036789,0.0,0.005036788992583752
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,PEFT 방법 5가지,-0.00896002,0.0,0.008960019797086716
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,거대 언어 모델 정의,0.0027169655,0.0,0.0027169655077159405
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,기본 경험,-0.011773141,0.0,0.011773141101002693
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,답변 실패,0.99142617,1.0,0.008573830127716064
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,딥러닝,-0.010270867,0.0,0.010270866565406322
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,마지막 할 말,-0.0072610495,0.0,0.007261049468070269
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,머신러닝,-0.0014420928,0.0,0.001442092820070684
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,면접 시작 인사,-0.00044091148,0.0,0.0004409114771988243
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,상세 경험,0.0024124878,0.0,0.0024124877527356148
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,수식,-0.0072201765,0.0,0.007220176514238119
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,용어 질문,-0.0038959556,0.0,0.003895955625921488
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,인공지능,-0.012421712,0.0,0.012421712279319763
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,잠시 휴식,0.004513619,0.0,0.004513619001954794
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,좋아하는 아이돌,-0.0049910196,0.0,0.004991019610315561
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,핵심 아이디어,-0.0040996694,0.0,0.004099669400602579
MSE Loss 용도 -> MSE Loss 는 분류 문제에서 많이 사용되지,확률 예측에서 MSE Loss 미 사용 이유,0.0009742777,0.0,0.0009742776746861637
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,BCE 가 좋은 task,-0.010313091,0.0,0.010313090868294239
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,BCE 가 좋은 이유,-0.003004577,0.0,0.00300457701086998
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,LLM Fine-Tuning 의 PEFT,0.005361842,0.0,0.00536184199154377
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,LoRA,0.010239123,0.0,0.010239123366773129
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,LoRA 와 QLoRA 의 차이,-0.0047699036,0.0,0.0047699036076664925
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,Loss Function 예시,0.0038665584,0.0,0.0038665584288537502
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,Loss Function 정의,-0.0010169264,0.0,0.0010169263696298003
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,MBTI,-0.0010942607,0.0,0.0010942607186734676
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,MSE Loss 설명,-0.011380428,0.0,0.011380428448319435
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,MSE Loss 용도,0.016087163,0.0,0.016087163239717484
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,Multi-Label 에서 CE + Softmax 적용 문제점,0.004640323,0.0,0.004640323109924793
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,PEFT 방법 5가지,-0.0076969927,0.0,0.007696992717683315
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,거대 언어 모델 정의,-0.005160858,0.0,0.0051608579233288765
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,기본 경험,-0.005876634,0.0,0.005876633804291487
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,답변 실패,0.9694258,1.0,0.03057420253753662
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,딥러닝,0.0038238321,0.0,0.003823832143098116
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,마지막 할 말,0.012128765,0.0,0.012128764763474464
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,머신러닝,0.0031112386,0.0,0.0031112385913729668
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,면접 시작 인사,-0.004533169,0.0,0.004533168859779835
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,상세 경험,-0.000509081,0.0,0.0005090810009278357
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,수식,0.0023238861,0.0,0.0023238861467689276
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,용어 질문,-0.01808443,0.0,0.01808442920446396
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,인공지능,-0.010291796,0.0,0.010291796177625656
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,잠시 휴식,-0.000398583,0.0,0.00039858301170170307
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,좋아하는 아이돌,-0.010985403,0.0,0.010985403321683407
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,핵심 아이디어,-0.0023995764,0.0,0.002399576362222433
확률 예측에서 MSE Loss 미 사용 이유 -> 확률도 연속된 값이니까 MSE로 해도 상관없는 거 아니야?,확률 예측에서 MSE Loss 미 사용 이유,0.046526838,0.0,0.04652683809399605
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,BCE 가 좋은 task,0.013685613,0.0,0.013685612939298153
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,BCE 가 좋은 이유,0.0002182894,0.0,0.00021828939497936517
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,LLM Fine-Tuning 의 PEFT,0.0019591234,0.0,0.0019591234158724546
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,LoRA,-0.013122236,0.0,0.013122236356139183
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,LoRA 와 QLoRA 의 차이,-0.0025809372,0.0,0.0025809372309595346
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,Loss Function 예시,-0.0011079233,0.0,0.0011079233372583985
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,Loss Function 정의,0.008131196,0.0,0.008131195791065693
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,MBTI,0.0024917813,0.0,0.0024917812552303076
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,MSE Loss 설명,-0.0064248433,0.0,0.006424843333661556
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,MSE Loss 용도,-0.008953341,0.0,0.008953341282904148
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,Multi-Label 에서 CE + Softmax 적용 문제점,-0.013773927,0.0,0.013773927465081215
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,PEFT 방법 5가지,0.0027972725,0.0,0.0027972725220024586
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,거대 언어 모델 정의,0.010906133,0.0,0.010906132869422436
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,기본 경험,-0.00980486,0.0,0.009804859757423401
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,답변 실패,0.00045833233,0.0,0.0004583323316182941
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,딥러닝,0.0014428332,0.0,0.0014428332215175033
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,마지막 할 말,-0.0023549937,0.0,0.0023549937177449465
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,머신러닝,0.017130759,0.0,0.017130758613348007
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,면접 시작 인사,0.0015390294,0.0,0.0015390294138342142
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,상세 경험,0.0023673065,0.0,0.002367306500673294
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,수식,0.005689835,0.0,0.005689835175871849
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,용어 질문,-0.009241992,0.0,0.009241991676390171
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,인공지능,0.00048537052,0.0,0.00048537051770836115
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,잠시 휴식,-0.0006302515,0.0,0.0006302514811977744
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,좋아하는 아이돌,0.0052648354,0.0,0.005264835432171822
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,핵심 아이디어,-0.0067417645,0.0,0.006741764489561319
확률 예측에서 MSE Loss 미 사용 이유 -> 실제로는 맞는데 맞을 확률을 0%라고 예측하거나 이럴 때 페넡티 크게 주려고?,확률 예측에서 MSE Loss 미 사용 이유,0.9880764,1.0,0.011923611164093018
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,BCE 가 좋은 task,0.0057534343,0.0,0.0057534342631697655
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,BCE 가 좋은 이유,0.00039565493,0.0,0.000395654933527112
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,LLM Fine-Tuning 의 PEFT,-0.0027466093,0.0,0.0027466092724353075
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,LoRA,-0.022797702,0.0,0.022797701880335808
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,LoRA 와 QLoRA 의 차이,0.00827572,0.0,0.008275720290839672
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,Loss Function 예시,0.0087344805,0.0,0.00873448047786951
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,Loss Function 정의,0.02103212,0.0,0.021032119169831276
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,MBTI,-0.0074393926,0.0,0.007439392618834972
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,MSE Loss 설명,-0.007763203,0.0,0.007763202767819166
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,MSE Loss 용도,-0.0008514975,0.0,0.0008514975197613239
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,Multi-Label 에서 CE + Softmax 적용 문제점,-0.0055545527,0.0,0.005554552655667067
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,PEFT 방법 5가지,-0.027022826,0.0,0.027022825554013252
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,거대 언어 모델 정의,0.011754502,0.0,0.011754501610994339
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,기본 경험,-0.020295035,0.0,0.02029503509402275
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,답변 실패,0.3808927,0.0,0.38089269399642944
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,딥러닝,-0.0055615087,0.0,0.00556150870397687
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,마지막 할 말,-0.016695488,0.0,0.01669548824429512
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,머신러닝,0.0014568983,0.0,0.0014568982878699899
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,면접 시작 인사,-0.020525858,0.0,0.02052585780620575
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,상세 경험,0.0035259617,0.0,0.0035259616561233997
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,수식,0.7690698,1.0,0.23093020915985107
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,용어 질문,-0.021704463,0.0,0.02170446328818798
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,인공지능,-0.013893907,0.0,0.013893906958401203
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,잠시 휴식,0.0026453608,0.0,0.0026453607715666294
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,좋아하는 아이돌,-0.011660293,0.0,0.011660292744636536
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,핵심 아이디어,0.040370703,0.0,0.04037070274353027
BCE Loss 설명 -> 공식은 -[(1-y) * log(1-y') + y * log(y')] 잖아! 이거 내가 모를 줄 알고,확률 예측에서 MSE Loss 미 사용 이유,0.009598628,0.0,0.00959862768650055
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",BCE 가 좋은 task,-0.01026291,0.0,0.010262910276651382
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",BCE 가 좋은 이유,0.013737118,0.0,0.013737117871642113
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",LLM Fine-Tuning 의 PEFT,-0.015912792,0.0,0.015912791714072227
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",LoRA,0.010955748,0.0,0.0109557481482625
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",LoRA 와 QLoRA 의 차이,-0.0025200462,0.0,0.0025200461968779564
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",Loss Function 예시,-0.018465055,0.0,0.018465055152773857
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",Loss Function 정의,0.009720686,0.0,0.00972068589180708
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",MBTI,-0.01733651,0.0,0.017336510121822357
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",MSE Loss 설명,-0.06231253,0.0,0.0623125284910202
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",MSE Loss 용도,-0.033424806,0.0,0.03342480584979057
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",Multi-Label 에서 CE + Softmax 적용 문제점,-0.011163139,0.0,0.011163138784468174
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",PEFT 방법 5가지,0.0009815,0.0,0.0009815000230446458
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",거대 언어 모델 정의,0.020477602,0.0,0.020477602258324623
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",기본 경험,0.0055336077,0.0,0.005533607676625252
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",답변 실패,0.00093784434,0.0,0.0009378443355672061
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",딥러닝,-0.015204809,0.0,0.015204808674752712
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",마지막 할 말,0.011324832,0.0,0.01132483221590519
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",머신러닝,0.009164836,0.0,0.009164836257696152
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",면접 시작 인사,-0.0040742494,0.0,0.004074249416589737
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",상세 경험,-0.0099267345,0.0,0.009926734492182732
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",수식,0.0018300362,0.0,0.0018300361698493361
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",용어 질문,0.014067518,0.0,0.014067517593502998
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",인공지능,0.0067336047,0.0,0.006733604706823826
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",잠시 휴식,-0.0015426838,0.0,0.0015426838072016835
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",좋아하는 아이돌,-0.004564808,0.0,0.004564808215945959
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",핵심 아이디어,0.9654647,1.0,0.03453528881072998
"BCE Loss 설명 -> 확률을 정반대로, 즉 맞는 것을 확률 0%로 예측했을 때 페널티 크게 주는 거",확률 예측에서 MSE Loss 미 사용 이유,-0.029512046,0.0,0.02951204590499401
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,BCE 가 좋은 task,-0.061449517,0.0,0.06144951656460762
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,BCE 가 좋은 이유,0.030637365,0.0,0.030637364834547043
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,LLM Fine-Tuning 의 PEFT,-0.032546356,0.0,0.032546356320381165
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,LoRA,0.0067241923,0.0,0.0067241922952234745
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,LoRA 와 QLoRA 의 차이,0.0048435037,0.0,0.004843503702431917
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,Loss Function 예시,0.022650369,0.0,0.02265036851167679
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,Loss Function 정의,0.020034503,0.0,0.02003450319170952
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,MBTI,0.010675144,0.0,0.010675144381821156
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,MSE Loss 설명,0.005461778,0.0,0.005461778026074171
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,MSE Loss 용도,-0.04500498,0.0,0.04500497877597809
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,Multi-Label 에서 CE + Softmax 적용 문제점,0.05087808,0.0,0.05087808147072792
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,PEFT 방법 5가지,0.0003988977,0.0,0.00039889771142043173
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,거대 언어 모델 정의,-0.027877867,0.0,0.027877867221832275
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,기본 경험,-0.020488651,0.0,0.02048865146934986
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,답변 실패,0.767891,1.0,0.23210901021957397
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,딥러닝,0.02341029,0.0,0.023410290479660034
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,마지막 할 말,-0.024362609,0.0,0.024362608790397644
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,머신러닝,0.010425522,0.0,0.010425521992146969
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,면접 시작 인사,-0.010985599,0.0,0.010985598899424076
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,상세 경험,-0.02407053,0.0,0.024070529267191887
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,수식,-0.016958099,0.0,0.016958098858594894
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,용어 질문,0.023523595,0.0,0.023523595184087753
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,인공지능,-0.025108669,0.0,0.025108668953180313
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,잠시 휴식,-0.04983072,0.0,0.04983071982860565
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,좋아하는 아이돌,0.014180527,0.0,0.014180527068674564
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,핵심 아이디어,0.24685109,0.0,0.2468510866165161
BCE Loss 설명 -> 그냥 여러 개 활성화 함수 만들어서 각 데이터셋마다 최선의 것을 찾는 거 아니야?,확률 예측에서 MSE Loss 미 사용 이유,-0.10932513,0.0,0.10932513326406479
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",BCE 가 좋은 task,-0.013892933,0.0,0.013892932794988155
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",BCE 가 좋은 이유,-0.0060572927,0.0,0.006057292688637972
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",LLM Fine-Tuning 의 PEFT,-0.032250274,0.0,0.03225027397274971
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",LoRA,0.013844311,0.0,0.013844311237335205
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",LoRA 와 QLoRA 의 차이,-0.008010957,0.0,0.008010957390069962
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",Loss Function 예시,0.010221431,0.0,0.010221431031823158
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",Loss Function 정의,-0.003972448,0.0,0.003972447942942381
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",MBTI,-0.019651731,0.0,0.019651731476187706
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",MSE Loss 설명,-0.06583292,0.0,0.06583292037248611
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",MSE Loss 용도,-0.033656377,0.0,0.03365637734532356
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",Multi-Label 에서 CE + Softmax 적용 문제점,0.00050054386,0.0,0.0005005438579246402
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",PEFT 방법 5가지,-0.0071548717,0.0,0.0071548717096447945
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",거대 언어 모델 정의,0.014682173,0.0,0.014682172797620296
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",기본 경험,0.007880656,0.0,0.00788065604865551
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",답변 실패,0.009152858,0.0,0.009152857586741447
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",딥러닝,-0.00033617564,0.0,0.00033617563894949853
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",마지막 할 말,0.010200585,0.0,0.01020058523863554
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",머신러닝,0.014761374,0.0,0.014761374332010746
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",면접 시작 인사,-0.0036606193,0.0,0.0036606192588806152
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",상세 경험,-0.002578875,0.0,0.0025788750499486923
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",수식,0.0076585133,0.0,0.0076585132628679276
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",용어 질문,0.0043185744,0.0,0.004318574443459511
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",인공지능,-4.0850402e-08,0.0,4.08504021720546e-08
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",잠시 휴식,-0.003462328,0.0,0.0034623281098902225
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",좋아하는 아이돌,-0.012195336,0.0,0.012195335701107979
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",핵심 아이디어,0.9628287,1.0,0.03717130422592163
"BCE Loss 설명 -> 실제로 1인 것을 0으로 예측하거나, 이런 식으로 확률을 정반대로 예측하는 것을 막으려고",확률 예측에서 MSE Loss 미 사용 이유,-0.045143463,0.0,0.04514346271753311
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')]",BCE 가 좋은 task,0.035277832,0.0,0.0352778322994709
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')]",BCE 가 좋은 이유,0.016295804,0.0,0.01629580371081829
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')]",LLM Fine-Tuning 의 PEFT,0.0032455502,0.0,0.0032455502077937126
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')]",LoRA,0.0047070486,0.0,0.004707048647105694
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')]",LoRA 와 QLoRA 의 차이,0.008424205,0.0,0.008424204774200916
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')]",Loss Function 예시,0.026951592,0.0,0.026951592415571213
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')]",Loss Function 정의,0.013065743,0.0,0.013065743260085583
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')]",MBTI,0.019959712,0.0,0.019959712401032448
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')]",MSE Loss 설명,-0.011231549,0.0,0.011231549084186554
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')]",MSE Loss 용도,-3.8003836e-05,0.0,3.8003836380084977e-05
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')]",Multi-Label 에서 CE + Softmax 적용 문제점,0.009474818,0.0,0.009474817663431168
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')]",PEFT 방법 5가지,-0.013344194,0.0,0.013344193808734417
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')]",거대 언어 모델 정의,0.032521456,0.0,0.032521456480026245
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')]",기본 경험,-0.0010017549,0.0,0.0010017548920586705
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')]",답변 실패,0.010662627,0.0,0.010662627406418324
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')]",딥러닝,-0.017262183,0.0,0.017262183129787445
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')]",마지막 할 말,0.0092350785,0.0,0.0092350784689188
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')]",머신러닝,0.02545206,0.0,0.025452060624957085
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')]",면접 시작 인사,-0.01582123,0.0,0.01582122966647148
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')]",상세 경험,-0.0005021943,0.0,0.0005021942779421806
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')]",수식,0.8763445,1.0,0.12365549802780151
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')]",용어 질문,-0.024602138,0.0,0.024602137506008148
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')]",인공지능,-0.016379917,0.0,0.016379917040467262
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')]",잠시 휴식,0.012257634,0.0,0.012257633730769157
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')]",좋아하는 아이돌,0.005123794,0.0,0.005123794078826904
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')]",핵심 아이디어,0.027130168,0.0,0.027130167931318283
"BCE Loss 설명 -> 실제 값 y, 예측값 y'에 대해 (-1) * [y log y' + (1-y) log (1-y')]",확률 예측에서 MSE Loss 미 사용 이유,0.014798588,0.0,0.014798588119447231
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다",BCE 가 좋은 task,-0.0043627075,0.0,0.004362707491964102
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다",BCE 가 좋은 이유,-0.007676088,0.0,0.007676087785512209
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다",LLM Fine-Tuning 의 PEFT,-0.015115379,0.0,0.015115379355847836
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다",LoRA,0.0073351543,0.0,0.007335154339671135
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다",LoRA 와 QLoRA 의 차이,-0.006639555,0.0,0.006639555096626282
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다",Loss Function 예시,-0.009223427,0.0,0.009223426692187786
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다",Loss Function 정의,0.0074219164,0.0,0.007421916350722313
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다",MBTI,-0.019889988,0.0,0.019889988005161285
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다",MSE Loss 설명,-0.048753858,0.0,0.04875385761260986
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다",MSE Loss 용도,-0.04336589,0.0,0.04336588829755783
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다",Multi-Label 에서 CE + Softmax 적용 문제점,0.0027305728,0.0,0.0027305728290230036
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다",PEFT 방법 5가지,-0.0024451516,0.0,0.002445151563733816
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다",거대 언어 모델 정의,0.017586,0.0,0.01758600026369095
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다",기본 경험,0.0070595467,0.0,0.0070595466531813145
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다",답변 실패,0.0038404358,0.0,0.0038404357619583607
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다",딥러닝,-0.018639004,0.0,0.018639003857970238
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다",마지막 할 말,0.007011228,0.0,0.007011227775365114
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다",머신러닝,0.010541879,0.0,0.010541878640651703
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다",면접 시작 인사,-0.0064867307,0.0,0.006486730650067329
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다",상세 경험,-6.606796e-05,0.0,6.606795795960352e-05
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다",수식,-0.000337072,0.0,0.0003370720078237355
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다",용어 질문,0.00819656,0.0,0.008196559734642506
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다",인공지능,-0.0018468453,0.0,0.0018468452617526054
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다",잠시 휴식,-0.006156034,0.0,0.0061560338363051414
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다",좋아하는 아이돌,-0.008308776,0.0,0.0083087757229805
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다",핵심 아이디어,0.96933275,1.0,0.030667245388031006
"BCE Loss 설명 -> 실제로 맞는 걸 0%, 틀린 걸 100%처럼 완전 반대로 예측했을 때 큰 손실을 준다",확률 예측에서 MSE Loss 미 사용 이유,-0.02780163,0.0,0.027801629155874252
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거,BCE 가 좋은 task,-0.009765739,0.0,0.009765738621354103
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거,BCE 가 좋은 이유,0.0064712074,0.0,0.006471207365393639
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거,LLM Fine-Tuning 의 PEFT,0.00045560228,0.0,0.00045560227590613067
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거,LoRA,0.0006630143,0.0,0.0006630143034271896
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거,LoRA 와 QLoRA 의 차이,0.0075746076,0.0,0.007574607618153095
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거,Loss Function 예시,0.004327581,0.0,0.00432758079841733
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거,Loss Function 정의,-0.014910885,0.0,0.014910885132849216
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거,MBTI,0.0012373752,0.0,0.001237375196069479
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거,MSE Loss 설명,0.002054097,0.0,0.0020540968980640173
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거,MSE Loss 용도,0.0022552975,0.0,0.0022552974987775087
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거,Multi-Label 에서 CE + Softmax 적용 문제점,-0.0059156944,0.0,0.0059156944043934345
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거,PEFT 방법 5가지,-0.0060833795,0.0,0.006083379499614239
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거,거대 언어 모델 정의,-0.011610738,0.0,0.01161073800176382
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거,기본 경험,-0.010718163,0.0,0.010718163102865219
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거,답변 실패,0.9895614,1.0,0.010438621044158936
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거,딥러닝,-0.00048802112,0.0,0.0004880211199633777
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거,마지막 할 말,-0.0067744073,0.0,0.006774407345801592
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거,머신러닝,0.009015948,0.0,0.009015947580337524
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거,면접 시작 인사,-0.00064116676,0.0,0.0006411667563952506
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거,상세 경험,0.006147463,0.0,0.006147462874650955
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거,수식,-0.0068566906,0.0,0.006856690626591444
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거,용어 질문,-0.016504114,0.0,0.016504114493727684
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거,인공지능,-0.01179192,0.0,0.01179192028939724
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거,잠시 휴식,-0.0026522775,0.0,0.002652277471497655
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거,좋아하는 아이돌,0.0049741287,0.0,0.004974128678441048
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거,핵심 아이디어,0.008753772,0.0,0.008753771893680096
BCE Loss 설명 -> y랑 1-y랑 서로 경쟁하는 거,확률 예측에서 MSE Loss 미 사용 이유,-0.004366801,0.0,0.004366801120340824
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지",BCE 가 좋은 task,0.009248202,0.0,0.009248201735317707
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지",BCE 가 좋은 이유,0.0019284338,0.0,0.0019284337759017944
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지",LLM Fine-Tuning 의 PEFT,-0.014078356,0.0,0.014078356325626373
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지",LoRA,0.0059154807,0.0,0.00591548066586256
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지",LoRA 와 QLoRA 의 차이,-0.004968504,0.0,0.0049685039557516575
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지",Loss Function 예시,0.028032452,0.0,0.02803245186805725
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지",Loss Function 정의,-0.0027681577,0.0,0.002768157748505473
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지",MBTI,-0.018708019,0.0,0.018708018586039543
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지",MSE Loss 설명,-0.04445291,0.0,0.044452909380197525
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지",MSE Loss 용도,-0.04984718,0.0,0.04984717816114426
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지",Multi-Label 에서 CE + Softmax 적용 문제점,0.006560274,0.0,0.006560273934155703
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지",PEFT 방법 5가지,-0.018659329,0.0,0.018659329041838646
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지",거대 언어 모델 정의,0.026406506,0.0,0.026406506076455116
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지",기본 경험,-0.0031785492,0.0,0.0031785492319613695
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지",답변 실패,-0.0028599908,0.0,0.0028599908109754324
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지",딥러닝,0.021290548,0.0,0.021290548145771027
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지",마지막 할 말,0.0009356193,0.0,0.0009356192895211279
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지",머신러닝,-0.02256724,0.0,0.02256724052131176
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지",면접 시작 인사,-0.007406032,0.0,0.007406032178550959
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지",상세 경험,-0.010301342,0.0,0.010301342234015465
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지",수식,0.033313114,0.0,0.03331311419606209
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지",용어 질문,0.011833412,0.0,0.011833411641418934
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지",인공지능,-0.012481697,0.0,0.012481696903705597
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지",잠시 휴식,-0.017669125,0.0,0.01766912452876568
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지",좋아하는 아이돌,-0.009968893,0.0,0.00996889267116785
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지",핵심 아이디어,0.954474,1.0,0.04552602767944336
"BCE Loss 설명 -> 실제 값이 각각 0, 1일 때 확률을 각각 1,0으로 예측하면 큰 Loss 를 부여하는 것이지",확률 예측에서 MSE Loss 미 사용 이유,-0.084474206,0.0,0.08447420597076416
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야?",BCE 가 좋은 task,-0.010673449,0.0,0.010673449374735355
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야?",BCE 가 좋은 이유,-0.011665376,0.0,0.011665375903248787
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야?",LLM Fine-Tuning 의 PEFT,-0.009903199,0.0,0.009903199039399624
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야?",LoRA,0.004029653,0.0,0.004029653035104275
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야?",LoRA 와 QLoRA 의 차이,0.0012461352,0.0,0.0012461352162063122
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야?",Loss Function 예시,-0.023725333,0.0,0.02372533269226551
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야?",Loss Function 정의,0.0098382635,0.0,0.009838263504207134
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야?",MBTI,-0.01531254,0.0,0.015312540344893932
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야?",MSE Loss 설명,-0.0062290123,0.0,0.00622901227325201
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야?",MSE Loss 용도,-0.025634065,0.0,0.02563406527042389
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야?",Multi-Label 에서 CE + Softmax 적용 문제점,-0.025499098,0.0,0.025499098002910614
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야?",PEFT 방법 5가지,-0.0046739425,0.0,0.00467394245788455
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야?",거대 언어 모델 정의,-0.007566707,0.0,0.007566707208752632
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야?",기본 경험,0.0023683659,0.0,0.002368365880101919
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야?",답변 실패,-0.0032382293,0.0,0.003238229313865304
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야?",딥러닝,-0.005433228,0.0,0.005433227866888046
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야?",마지막 할 말,-0.0014458217,0.0,0.0014458217192441225
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야?",머신러닝,0.0018434906,0.0,0.0018434906378388405
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야?",면접 시작 인사,-0.01120128,0.0,0.011201280169188976
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야?",상세 경험,0.003465364,0.0,0.0034653639886528254
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야?",수식,-0.0030535173,0.0,0.0030535173136740923
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야?",용어 질문,0.9490961,1.0,0.050903916358947754
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야?",인공지능,0.0028637347,0.0,0.0028637347277253866
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야?",잠시 휴식,-0.0001277157,0.0,0.00012771569890901446
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야?",좋아하는 아이돌,-0.0104931835,0.0,0.010493183508515358
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야?",핵심 아이디어,0.007313658,0.0,0.007313658017665148
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> 근데 로라야 멀티라벨이랑 멀티클래스가 뭐야?",확률 예측에서 MSE Loss 미 사용 이유,-0.010492345,0.0,0.010492345318198204
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",BCE 가 좋은 task,0.5672119,0.0,0.567211925983429
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",BCE 가 좋은 이유,-0.06829723,0.0,0.06829722970724106
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",LLM Fine-Tuning 의 PEFT,-0.024091853,0.0,0.024091852828860283
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",LoRA,0.0071114646,0.0,0.007111464627087116
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",LoRA 와 QLoRA 의 차이,-0.0072901417,0.0,0.00729014165699482
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",Loss Function 예시,-0.009871377,0.0,0.009871376678347588
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",Loss Function 정의,0.022463067,0.0,0.02246306650340557
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",MBTI,0.011897202,0.0,0.011897201649844646
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",MSE Loss 설명,-0.022764467,0.0,0.02276446670293808
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",MSE Loss 용도,0.007135368,0.0,0.007135367952287197
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",Multi-Label 에서 CE + Softmax 적용 문제점,-0.006660415,0.0,0.006660414859652519
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",PEFT 방법 5가지,-0.018498376,0.0,0.01849837601184845
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",거대 언어 모델 정의,0.02643162,0.0,0.026431620121002197
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",기본 경험,0.05434446,0.0,0.05434446036815643
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",답변 실패,0.12285284,1.0,0.8771471604704857
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",딥러닝,0.025698908,0.0,0.025698907673358917
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",마지막 할 말,-0.0117585035,0.0,0.011758503504097462
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",머신러닝,-0.011366454,0.0,0.01136645395308733
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",면접 시작 인사,-0.008953534,0.0,0.008953534066677094
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",상세 경험,-0.012056489,0.0,0.012056488543748856
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",수식,0.0032262732,0.0,0.003226273227483034
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",용어 질문,-0.034173485,0.0,0.03417348489165306
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",인공지능,-0.030222323,0.0,0.030222322791814804
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",잠시 휴식,0.022087406,0.0,0.022087406367063522
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",좋아하는 아이돌,0.024846265,0.0,0.024846265092492104
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",핵심 아이디어,-0.017626053,0.0,0.017626052722334862
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Class 지!!",확률 예측에서 MSE Loss 미 사용 이유,0.0043158876,0.0,0.004315887577831745
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",BCE 가 좋은 task,0.0043187314,1.0,0.9956812686286867
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",BCE 가 좋은 이유,-0.0026244144,0.0,0.0026244143955409527
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",LLM Fine-Tuning 의 PEFT,0.0013719643,0.0,0.0013719643466174603
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",LoRA,-0.0014372577,0.0,0.0014372577425092459
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",LoRA 와 QLoRA 의 차이,0.0025264616,0.0,0.002526461612433195
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",Loss Function 예시,0.003341886,0.0,0.003341885982081294
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",Loss Function 정의,-0.00867929,0.0,0.008679290302097797
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",MBTI,-0.0038349729,0.0,0.00383497285656631
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",MSE Loss 설명,0.0007209594,0.0,0.0007209593895822763
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",MSE Loss 용도,0.0010042703,0.0,0.001004270277917385
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",Multi-Label 에서 CE + Softmax 적용 문제점,-0.008487577,0.0,0.008487576618790627
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",PEFT 방법 5가지,-0.003290428,0.0,0.0032904280815273523
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",거대 언어 모델 정의,-0.0017062261,0.0,0.0017062261467799544
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",기본 경험,-0.009764922,0.0,0.009764921851456165
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",답변 실패,0.9933346,0.0,0.9933345913887024
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",딥러닝,-0.007481979,0.0,0.007481979206204414
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",마지막 할 말,-0.007705755,0.0,0.007705755066126585
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",머신러닝,0.0018342488,0.0,0.0018342487746849656
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",면접 시작 인사,-0.0025985844,0.0,0.002598584396764636
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",상세 경험,-0.008107794,0.0,0.00810779444873333
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",수식,-0.0061716023,0.0,0.006171602290123701
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",용어 질문,-0.001510377,0.0,0.0015103770419955254
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",인공지능,-0.008663362,0.0,0.008663361892104149
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",잠시 휴식,-0.00045586957,0.0,0.0004558695654850453
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",좋아하는 아이돌,-0.0055084866,0.0,0.005508486647158861
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",핵심 아이디어,0.0026582016,0.0,0.002658201614394784
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이라고 듣긴 했는데 그 이유는 잘 모르겠어",확률 예측에서 MSE Loss 미 사용 이유,0.0008215289,0.0,0.0008215288980863988
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",BCE 가 좋은 task,0.07664675,0.0,0.07664675265550613
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",BCE 가 좋은 이유,0.94928855,1.0,0.05071145296096802
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",LLM Fine-Tuning 의 PEFT,-0.0030949821,0.0,0.00309498212300241
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",LoRA,0.010687093,0.0,0.010687093250453472
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",LoRA 와 QLoRA 의 차이,0.019114733,0.0,0.01911473274230957
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",Loss Function 예시,-0.016603883,0.0,0.016603883355855942
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",Loss Function 정의,-0.006555372,0.0,0.006555371917784214
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",MBTI,0.010131023,0.0,0.01013102289289236
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",MSE Loss 설명,0.005112845,0.0,0.005112844984978437
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",MSE Loss 용도,-0.015021581,0.0,0.015021581202745438
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",Multi-Label 에서 CE + Softmax 적용 문제점,0.00743711,0.0,0.00743710994720459
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",PEFT 방법 5가지,-0.020004835,0.0,0.020004834979772568
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",거대 언어 모델 정의,-0.0061784736,0.0,0.006178473588079214
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",기본 경험,0.00908764,0.0,0.00908763986080885
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",답변 실패,-0.0094855325,0.0,0.009485532529652119
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",딥러닝,-0.016589256,0.0,0.01658925600349903
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",마지막 할 말,-0.0103532225,0.0,0.010353222489356995
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",머신러닝,-0.028180456,0.0,0.028180455788969994
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",면접 시작 인사,-0.021652399,0.0,0.021652398630976677
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",상세 경험,-0.020815458,0.0,0.02081545814871788
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",수식,9.305439e-05,0.0,9.305439016316086e-05
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",용어 질문,0.028653892,0.0,0.028653891757130623
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",인공지능,-0.009603189,0.0,0.009603189304471016
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",잠시 휴식,-0.013885508,0.0,0.01388550829142332
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",좋아하는 아이돌,-0.024272053,0.0,0.024272052571177483
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",핵심 아이디어,-0.0149534475,0.0,0.014953447505831718
"Multi-Class, Multi-Label 중 BCE 가 좋은 task -> Multi-Label 이 BCE 에 적합하고, BCE 를 각 Class 별로 적용하면 각 Class 간 확률이 독립적인 Multi-Label task 에 맞잖아",확률 예측에서 MSE Loss 미 사용 이유,-0.00886112,0.0,0.008861119858920574
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,BCE 가 좋은 task,-0.008857912,0.0,0.008857912383973598
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,BCE 가 좋은 이유,0.002272451,0.0,0.002272451063618064
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,LLM Fine-Tuning 의 PEFT,-0.011345158,0.0,0.011345158331096172
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,LoRA,0.008962314,0.0,0.008962313644587994
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,LoRA 와 QLoRA 의 차이,0.00515467,0.0,0.005154670216143131
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,Loss Function 예시,0.005988814,0.0,0.005988813936710358
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,Loss Function 정의,-0.0053977747,0.0,0.0053977747447788715
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,MBTI,-0.011102233,0.0,0.011102233082056046
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,MSE Loss 설명,-0.005556462,0.0,0.005556461866945028
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,MSE Loss 용도,0.009003112,0.0,0.009003112092614174
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,Multi-Label 에서 CE + Softmax 적용 문제점,0.9826113,1.0,0.01738870143890381
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,PEFT 방법 5가지,-0.00051239016,0.0,0.0005123901646584272
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,거대 언어 모델 정의,-0.020491054,0.0,0.02049105428159237
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,기본 경험,-0.005440403,0.0,0.005440402776002884
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,답변 실패,0.009555434,0.0,0.009555433876812458
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,딥러닝,-0.0073508206,0.0,0.007350820582360029
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,마지막 할 말,-0.0035266848,0.0,0.0035266848281025887
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,머신러닝,0.013476403,0.0,0.013476403430104256
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,면접 시작 인사,-0.0006677241,0.0,0.0006677241181023419
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,상세 경험,-0.01525063,0.0,0.015250629745423794
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,수식,0.00074408104,0.0,0.0007440810441039503
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,용어 질문,-0.017873975,0.0,0.0178739745169878
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,인공지능,0.024289515,0.0,0.024289514869451523
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,잠시 휴식,-0.0056576715,0.0,0.005657671485096216
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,좋아하는 아이돌,-0.0070320787,0.0,0.007032078690826893
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,핵심 아이디어,-0.00028661857,0.0,0.000286618567770347
Multi-Label 에서 CE + Softmax 적용 문제점 -> 확률의 합이 1인데 그 확률을 여러 개의 '정답 Class'에 분배하면 예측 정확도가 떨어지지,확률 예측에서 MSE Loss 미 사용 이유,-0.009980175,0.0,0.009980174712836742
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,BCE 가 좋은 task,0.006086316,0.0,0.006086315959692001
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,BCE 가 좋은 이유,-0.001708591,0.0,0.0017085910076275468
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,LLM Fine-Tuning 의 PEFT,0.0006162159,0.0,0.0006162159261293709
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,LoRA,0.0026815773,0.0,0.002681577345356345
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,LoRA 와 QLoRA 의 차이,0.003971124,0.0,0.003971124067902565
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,Loss Function 예시,-0.0024366123,0.0,0.002436612267047167
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,Loss Function 정의,-0.008389426,0.0,0.00838942639529705
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,MBTI,0.0038798172,0.0,0.0038798172026872635
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,MSE Loss 설명,-0.0049380907,0.0,0.004938090685755014
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,MSE Loss 용도,0.0016360023,0.0,0.0016360023291781545
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,Multi-Label 에서 CE + Softmax 적용 문제점,0.0065901205,0.0,0.006590120494365692
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,PEFT 방법 5가지,-0.000106798485,0.0,0.00010679848492145538
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,거대 언어 모델 정의,-0.010777966,0.0,0.010777966119349003
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,기본 경험,-0.004867132,0.0,0.004867131821811199
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,답변 실패,0.99245274,1.0,0.007547259330749512
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,딥러닝,-0.0043513337,0.0,0.00435133371502161
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,마지막 할 말,-0.0016899698,0.0,0.0016899697948247194
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,머신러닝,0.003375677,0.0,0.003375676926225424
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,면접 시작 인사,0.00011686442,0.0,0.00011686442303471267
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,상세 경험,-0.0048306016,0.0,0.004830601625144482
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,수식,-0.01406772,0.0,0.01406771969050169
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,용어 질문,-0.009933972,0.0,0.009933971799910069
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,인공지능,-0.009440293,0.0,0.009440292604267597
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,잠시 휴식,-0.003072134,0.0,0.0030721339862793684
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,좋아하는 아이돌,-0.0032997453,0.0,0.0032997452653944492
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,핵심 아이디어,0.011258716,0.0,0.011258715763688087
Multi-Label 에서 CE + Softmax 적용 문제점 -> 그냥 아무거나 사용하고 성능 제일 좋은 거 채택하면 안되나,확률 예측에서 MSE Loss 미 사용 이유,-0.002901871,0.0,0.0029018709901720285
Loss Function 관련 실무 경험 -> 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,BCE 가 좋은 task,-0.014149905,0.0,0.014149905182421207
Loss Function 관련 실무 경험 -> 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,BCE 가 좋은 이유,-0.007356039,0.0,0.0073560387827456
Loss Function 관련 실무 경험 -> 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,LLM Fine-Tuning 의 PEFT,-0.011541975,0.0,0.011541974730789661
Loss Function 관련 실무 경험 -> 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,LoRA,0.0063341837,0.0,0.0063341837376356125
Loss Function 관련 실무 경험 -> 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,LoRA 와 QLoRA 의 차이,0.009623867,0.0,0.009623867459595203
Loss Function 관련 실무 경험 -> 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,Loss Function 예시,-0.00848768,0.0,0.008487679995596409
Loss Function 관련 실무 경험 -> 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,Loss Function 정의,0.0070308237,0.0,0.0070308237336575985
Loss Function 관련 실무 경험 -> 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,MBTI,-0.0049102586,0.0,0.004910258576273918
Loss Function 관련 실무 경험 -> 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,MSE Loss 설명,-0.0013551101,0.0,0.0013551100855693221
Loss Function 관련 실무 경험 -> 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,MSE Loss 용도,-0.0072379746,0.0,0.007237974554300308
Loss Function 관련 실무 경험 -> 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,Multi-Label 에서 CE + Softmax 적용 문제점,0.014426159,0.0,0.014426158741116524
Loss Function 관련 실무 경험 -> 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,PEFT 방법 5가지,0.00077369163,0.0,0.0007736916304565966
Loss Function 관련 실무 경험 -> 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,거대 언어 모델 정의,-0.0021070235,0.0,0.0021070234943181276
Loss Function 관련 실무 경험 -> 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,기본 경험,0.0025234353,1.0,0.997476564720273
Loss Function 관련 실무 경험 -> 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,답변 실패,-0.004130762,0.0,0.004130762070417404
Loss Function 관련 실무 경험 -> 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,딥러닝,-0.0057528825,0.0,0.005752882454544306
Loss Function 관련 실무 경험 -> 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,마지막 할 말,-0.014251543,0.0,0.014251543208956718
Loss Function 관련 실무 경험 -> 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,머신러닝,-0.032950185,0.0,0.03295018523931503
Loss Function 관련 실무 경험 -> 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,면접 시작 인사,-0.019760955,0.0,0.01976095512509346
Loss Function 관련 실무 경험 -> 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,상세 경험,0.9597967,0.0,0.9597967267036438
Loss Function 관련 실무 경험 -> 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,수식,-0.0053711934,0.0,0.005371193401515484
Loss Function 관련 실무 경험 -> 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,용어 질문,0.0006792035,0.0,0.0006792034837417305
Loss Function 관련 실무 경험 -> 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,인공지능,-0.007893627,0.0,0.00789362657815218
Loss Function 관련 실무 경험 -> 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,잠시 휴식,-0.016948491,0.0,0.01694849133491516
Loss Function 관련 실무 경험 -> 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,좋아하는 아이돌,-0.003809306,0.0,0.0038093060720711946
Loss Function 관련 실무 경험 -> 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,핵심 아이디어,-0.04922002,0.0,0.049220021814107895
Loss Function 관련 실무 경험 -> 이미지 세그멘테이션에서 인접한 픽셀의 점수 차이를 Loss Term 으로 추가했어,확률 예측에서 MSE Loss 미 사용 이유,-0.009468414,0.0,0.009468413889408112
Loss Function 관련 실무 경험 -> 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,BCE 가 좋은 task,-0.00346349,0.0,0.0034634899348020554
Loss Function 관련 실무 경험 -> 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,BCE 가 좋은 이유,0.015351616,0.0,0.015351615846157074
Loss Function 관련 실무 경험 -> 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,LLM Fine-Tuning 의 PEFT,-0.009190406,0.0,0.00919040571898222
Loss Function 관련 실무 경험 -> 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,LoRA,-0.029849503,0.0,0.029849503189325333
Loss Function 관련 실무 경험 -> 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,LoRA 와 QLoRA 의 차이,0.0038242107,0.0,0.003824210725724697
Loss Function 관련 실무 경험 -> 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,Loss Function 예시,-0.017453184,0.0,0.017453184351325035
Loss Function 관련 실무 경험 -> 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,Loss Function 정의,-0.004883149,0.0,0.0048831491731107235
Loss Function 관련 실무 경험 -> 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,MBTI,-0.012398214,0.0,0.01239821407943964
Loss Function 관련 실무 경험 -> 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,MSE Loss 설명,0.0073148976,0.0,0.007314897608011961
Loss Function 관련 실무 경험 -> 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,MSE Loss 용도,-0.0083651515,0.0,0.008365151472389698
Loss Function 관련 실무 경험 -> 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,Multi-Label 에서 CE + Softmax 적용 문제점,0.0036371646,0.0,0.0036371645983308554
Loss Function 관련 실무 경험 -> 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,PEFT 방법 5가지,-0.011580695,0.0,0.011580695398151875
Loss Function 관련 실무 경험 -> 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,거대 언어 모델 정의,0.0068862806,0.0,0.006886280607432127
Loss Function 관련 실무 경험 -> 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,기본 경험,0.92506695,0.0,0.9250669479370117
Loss Function 관련 실무 경험 -> 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,답변 실패,-0.012321704,0.0,0.01232170406728983
Loss Function 관련 실무 경험 -> 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,딥러닝,-0.0017486152,0.0,0.0017486151773482561
Loss Function 관련 실무 경험 -> 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,마지막 할 말,0.016042735,0.0,0.016042735427618027
Loss Function 관련 실무 경험 -> 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,머신러닝,-0.005594314,0.0,0.0055943140760064125
Loss Function 관련 실무 경험 -> 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,면접 시작 인사,-0.007200423,0.0,0.007200423162430525
Loss Function 관련 실무 경험 -> 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,상세 경험,0.12743157,1.0,0.8725684285163879
Loss Function 관련 실무 경험 -> 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,수식,-0.008572949,0.0,0.008572949096560478
Loss Function 관련 실무 경험 -> 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,용어 질문,-0.018266384,0.0,0.018266383558511734
Loss Function 관련 실무 경험 -> 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,인공지능,-0.012792288,0.0,0.012792288325726986
Loss Function 관련 실무 경험 -> 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,잠시 휴식,-0.014627304,0.0,0.014627303928136826
Loss Function 관련 실무 경험 -> 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,좋아하는 아이돌,0.005427675,0.0,0.0054276748560369015
Loss Function 관련 실무 경험 -> 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,핵심 아이디어,-0.0030221716,0.0,0.0030221715569496155
Loss Function 관련 실무 경험 -> 성능 5% 향상됐는데 이거 논문 쓸 정도라고 팀장님한테 칭찬 들었다! 부럽지?,확률 예측에서 MSE Loss 미 사용 이유,-0.0009108503,0.0,0.000910850299987942
Loss Function 관련 실무 경험 -> 근데 기억 진짜 안나는데 어떡하지?,BCE 가 좋은 task,0.0012319516,0.0,0.0012319516390562057
Loss Function 관련 실무 경험 -> 근데 기억 진짜 안나는데 어떡하지?,BCE 가 좋은 이유,-0.0039648064,0.0,0.003964806441217661
Loss Function 관련 실무 경험 -> 근데 기억 진짜 안나는데 어떡하지?,LLM Fine-Tuning 의 PEFT,0.0013113689,0.0,0.0013113688910380006
Loss Function 관련 실무 경험 -> 근데 기억 진짜 안나는데 어떡하지?,LoRA,0.0019936068,0.0,0.001993606798350811
Loss Function 관련 실무 경험 -> 근데 기억 진짜 안나는데 어떡하지?,LoRA 와 QLoRA 의 차이,-0.0020234308,0.0,0.002023430773988366
Loss Function 관련 실무 경험 -> 근데 기억 진짜 안나는데 어떡하지?,Loss Function 예시,0.00978279,0.0,0.009782790206372738
Loss Function 관련 실무 경험 -> 근데 기억 진짜 안나는데 어떡하지?,Loss Function 정의,-0.009930812,0.0,0.009930811822414398
Loss Function 관련 실무 경험 -> 근데 기억 진짜 안나는데 어떡하지?,MBTI,0.0030088583,0.0,0.003008858300745487
Loss Function 관련 실무 경험 -> 근데 기억 진짜 안나는데 어떡하지?,MSE Loss 설명,-0.0014205455,0.0,0.0014205455081537366
Loss Function 관련 실무 경험 -> 근데 기억 진짜 안나는데 어떡하지?,MSE Loss 용도,0.0038188142,0.0,0.0038188141770660877
Loss Function 관련 실무 경험 -> 근데 기억 진짜 안나는데 어떡하지?,Multi-Label 에서 CE + Softmax 적용 문제점,-0.0076306826,0.0,0.007630682550370693
Loss Function 관련 실무 경험 -> 근데 기억 진짜 안나는데 어떡하지?,PEFT 방법 5가지,-0.00087232067,0.0,0.0008723206701688468
Loss Function 관련 실무 경험 -> 근데 기억 진짜 안나는데 어떡하지?,거대 언어 모델 정의,-0.0015533051,0.0,0.0015533050755038857
Loss Function 관련 실무 경험 -> 근데 기억 진짜 안나는데 어떡하지?,기본 경험,-0.0050291987,0.0,0.0050291987136006355
Loss Function 관련 실무 경험 -> 근데 기억 진짜 안나는데 어떡하지?,답변 실패,0.9941187,1.0,0.005881309509277344
Loss Function 관련 실무 경험 -> 근데 기억 진짜 안나는데 어떡하지?,딥러닝,-0.005100583,0.0,0.00510058319196105
Loss Function 관련 실무 경험 -> 근데 기억 진짜 안나는데 어떡하지?,마지막 할 말,-0.0042743417,0.0,0.004274341743439436
Loss Function 관련 실무 경험 -> 근데 기억 진짜 안나는데 어떡하지?,머신러닝,0.001560908,0.0,0.001560908043757081
Loss Function 관련 실무 경험 -> 근데 기억 진짜 안나는데 어떡하지?,면접 시작 인사,0.0014440725,0.0,0.001444072462618351
Loss Function 관련 실무 경험 -> 근데 기억 진짜 안나는데 어떡하지?,상세 경험,-0.0011628467,0.0,0.0011628466891124845
Loss Function 관련 실무 경험 -> 근데 기억 진짜 안나는데 어떡하지?,수식,-0.003339281,0.0,0.0033392810728400946
Loss Function 관련 실무 경험 -> 근데 기억 진짜 안나는데 어떡하지?,용어 질문,-0.004611364,0.0,0.004611364100128412
Loss Function 관련 실무 경험 -> 근데 기억 진짜 안나는데 어떡하지?,인공지능,-0.009627422,0.0,0.00962742231786251
Loss Function 관련 실무 경험 -> 근데 기억 진짜 안나는데 어떡하지?,잠시 휴식,-0.0019028552,0.0,0.001902855234220624
Loss Function 관련 실무 경험 -> 근데 기억 진짜 안나는데 어떡하지?,좋아하는 아이돌,0.0017139933,0.0,0.0017139932606369257
Loss Function 관련 실무 경험 -> 근데 기억 진짜 안나는데 어떡하지?,핵심 아이디어,0.0069021313,0.0,0.006902131251990795
Loss Function 관련 실무 경험 -> 근데 기억 진짜 안나는데 어떡하지?,확률 예측에서 MSE Loss 미 사용 이유,0.0014998073,0.0,0.0014998073456808925
Loss Function 관련 실무 경험 -> 나 아직 실무 경험 없는데 ㅠㅠ,BCE 가 좋은 task,-0.006914296,0.0,0.0069142961874604225
Loss Function 관련 실무 경험 -> 나 아직 실무 경험 없는데 ㅠㅠ,BCE 가 좋은 이유,0.0028967855,0.0,0.0028967855032533407
Loss Function 관련 실무 경험 -> 나 아직 실무 경험 없는데 ㅠㅠ,LLM Fine-Tuning 의 PEFT,-0.0016108047,0.0,0.0016108046984300017
Loss Function 관련 실무 경험 -> 나 아직 실무 경험 없는데 ㅠㅠ,LoRA,-0.0067602694,0.0,0.006760269403457642
Loss Function 관련 실무 경험 -> 나 아직 실무 경험 없는데 ㅠㅠ,LoRA 와 QLoRA 의 차이,-0.0061700516,0.0,0.006170051638036966
Loss Function 관련 실무 경험 -> 나 아직 실무 경험 없는데 ㅠㅠ,Loss Function 예시,0.0026041733,0.0,0.0026041732635349035
Loss Function 관련 실무 경험 -> 나 아직 실무 경험 없는데 ㅠㅠ,Loss Function 정의,-0.001053949,0.0,0.0010539490031078458
Loss Function 관련 실무 경험 -> 나 아직 실무 경험 없는데 ㅠㅠ,MBTI,0.004014796,0.0,0.004014796111732721
Loss Function 관련 실무 경험 -> 나 아직 실무 경험 없는데 ㅠㅠ,MSE Loss 설명,-0.0013410215,0.0,0.0013410215033218265
Loss Function 관련 실무 경험 -> 나 아직 실무 경험 없는데 ㅠㅠ,MSE Loss 용도,0.00012963083,0.0,0.00012963083281647414
Loss Function 관련 실무 경험 -> 나 아직 실무 경험 없는데 ㅠㅠ,Multi-Label 에서 CE + Softmax 적용 문제점,-0.00064259046,0.0,0.0006425904575735331
Loss Function 관련 실무 경험 -> 나 아직 실무 경험 없는데 ㅠㅠ,PEFT 방법 5가지,-0.015324785,0.0,0.015324785374104977
Loss Function 관련 실무 경험 -> 나 아직 실무 경험 없는데 ㅠㅠ,거대 언어 모델 정의,-0.0060384846,0.0,0.006038484629243612
Loss Function 관련 실무 경험 -> 나 아직 실무 경험 없는데 ㅠㅠ,기본 경험,0.97562087,1.0,0.02437913417816162
Loss Function 관련 실무 경험 -> 나 아직 실무 경험 없는데 ㅠㅠ,답변 실패,-0.0009302949,0.0,0.0009302949183620512
Loss Function 관련 실무 경험 -> 나 아직 실무 경험 없는데 ㅠㅠ,딥러닝,0.0066238637,0.0,0.006623863708227873
Loss Function 관련 실무 경험 -> 나 아직 실무 경험 없는데 ㅠㅠ,마지막 할 말,-0.0031043568,0.0,0.0031043568160384893
Loss Function 관련 실무 경험 -> 나 아직 실무 경험 없는데 ㅠㅠ,머신러닝,-0.0051367427,0.0,0.005136742722243071
Loss Function 관련 실무 경험 -> 나 아직 실무 경험 없는데 ㅠㅠ,면접 시작 인사,0.0009990203,0.0,0.0009990202961489558
Loss Function 관련 실무 경험 -> 나 아직 실무 경험 없는데 ㅠㅠ,상세 경험,0.001999842,0.0,0.0019998420029878616
Loss Function 관련 실무 경험 -> 나 아직 실무 경험 없는데 ㅠㅠ,수식,0.0019701459,0.0,0.0019701458513736725
Loss Function 관련 실무 경험 -> 나 아직 실무 경험 없는데 ㅠㅠ,용어 질문,0.004253368,0.0,0.004253367893397808
Loss Function 관련 실무 경험 -> 나 아직 실무 경험 없는데 ㅠㅠ,인공지능,0.013465547,0.0,0.013465547002851963
Loss Function 관련 실무 경험 -> 나 아직 실무 경험 없는데 ㅠㅠ,잠시 휴식,-0.0005414376,0.0,0.0005414375918917358
Loss Function 관련 실무 경험 -> 나 아직 실무 경험 없는데 ㅠㅠ,좋아하는 아이돌,-0.004272298,0.0,0.004272297956049442
Loss Function 관련 실무 경험 -> 나 아직 실무 경험 없는데 ㅠㅠ,핵심 아이디어,-0.0030785396,0.0,0.0030785396229475737
Loss Function 관련 실무 경험 -> 나 아직 실무 경험 없는데 ㅠㅠ,확률 예측에서 MSE Loss 미 사용 이유,-0.0036145637,0.0,0.0036145637277513742
Loss Function 관련 실무 경험 -> 대답하기 싫은데 힝,BCE 가 좋은 task,0.0041853073,0.0,0.004185307305306196
Loss Function 관련 실무 경험 -> 대답하기 싫은데 힝,BCE 가 좋은 이유,-0.001915675,0.0,0.0019156750058755279
Loss Function 관련 실무 경험 -> 대답하기 싫은데 힝,LLM Fine-Tuning 의 PEFT,-0.0030922627,0.0,0.0030922626610845327
Loss Function 관련 실무 경험 -> 대답하기 싫은데 힝,LoRA,0.0013239668,0.0,0.0013239667750895023
Loss Function 관련 실무 경험 -> 대답하기 싫은데 힝,LoRA 와 QLoRA 의 차이,0.0019694834,0.0,0.0019694834481924772
Loss Function 관련 실무 경험 -> 대답하기 싫은데 힝,Loss Function 예시,-0.002440356,0.0,0.0024403559509664774
Loss Function 관련 실무 경험 -> 대답하기 싫은데 힝,Loss Function 정의,-0.013054205,0.0,0.013054205104708672
Loss Function 관련 실무 경험 -> 대답하기 싫은데 힝,MBTI,0.002422746,0.0,0.0024227460380643606
Loss Function 관련 실무 경험 -> 대답하기 싫은데 힝,MSE Loss 설명,-0.002854601,0.0,0.002854601014405489
Loss Function 관련 실무 경험 -> 대답하기 싫은데 힝,MSE Loss 용도,0.003691036,0.0,0.003691036021336913
Loss Function 관련 실무 경험 -> 대답하기 싫은데 힝,Multi-Label 에서 CE + Softmax 적용 문제점,-0.0058141137,0.0,0.005814113654196262
Loss Function 관련 실무 경험 -> 대답하기 싫은데 힝,PEFT 방법 5가지,-0.007629637,0.0,0.007629637140780687
Loss Function 관련 실무 경험 -> 대답하기 싫은데 힝,거대 언어 모델 정의,-0.0068801325,0.0,0.006880132481455803
Loss Function 관련 실무 경험 -> 대답하기 싫은데 힝,기본 경험,-0.0045402395,0.0,0.0045402394607663155
Loss Function 관련 실무 경험 -> 대답하기 싫은데 힝,답변 실패,0.99424964,1.0,0.0057503581047058105
Loss Function 관련 실무 경험 -> 대답하기 싫은데 힝,딥러닝,-0.001711729,0.0,0.0017117289826273918
Loss Function 관련 실무 경험 -> 대답하기 싫은데 힝,마지막 할 말,-0.0075467043,0.0,0.007546704262495041
Loss Function 관련 실무 경험 -> 대답하기 싫은데 힝,머신러닝,-0.0009609664,0.0,0.0009609663975425065
Loss Function 관련 실무 경험 -> 대답하기 싫은데 힝,면접 시작 인사,0.0019368279,0.0,0.0019368279026821256
Loss Function 관련 실무 경험 -> 대답하기 싫은데 힝,상세 경험,0.004921112,0.0,0.004921112209558487
Loss Function 관련 실무 경험 -> 대답하기 싫은데 힝,수식,-0.003772201,0.0,0.0037722010165452957
Loss Function 관련 실무 경험 -> 대답하기 싫은데 힝,용어 질문,0.00059071067,0.0,0.0005907106678932905
Loss Function 관련 실무 경험 -> 대답하기 싫은데 힝,인공지능,-0.009829847,0.0,0.009829847142100334
Loss Function 관련 실무 경험 -> 대답하기 싫은데 힝,잠시 휴식,0.00015319139,0.0,0.00015319138765335083
Loss Function 관련 실무 경험 -> 대답하기 싫은데 힝,좋아하는 아이돌,0.0030934915,0.0,0.003093491541221738
Loss Function 관련 실무 경험 -> 대답하기 싫은데 힝,핵심 아이디어,0.0031765187,0.0,0.003176518715918064
Loss Function 관련 실무 경험 -> 대답하기 싫은데 힝,확률 예측에서 MSE Loss 미 사용 이유,-0.0022489564,0.0,0.0022489563561975956
Loss Function 관련 실무 경험 -> 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ,BCE 가 좋은 task,-0.008167124,0.0,0.008167124353349209
Loss Function 관련 실무 경험 -> 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ,BCE 가 좋은 이유,0.00855321,0.0,0.008553209714591503
Loss Function 관련 실무 경험 -> 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ,LLM Fine-Tuning 의 PEFT,0.002256609,0.0,0.002256609033793211
Loss Function 관련 실무 경험 -> 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ,LoRA,-0.00956396,0.0,0.009563960134983063
Loss Function 관련 실무 경험 -> 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ,LoRA 와 QLoRA 의 차이,-0.008510663,0.0,0.00851066317409277
Loss Function 관련 실무 경험 -> 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ,Loss Function 예시,-0.0014415302,0.0,0.0014415301848202944
Loss Function 관련 실무 경험 -> 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ,Loss Function 정의,0.0021625317,0.0,0.002162531716749072
Loss Function 관련 실무 경험 -> 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ,MBTI,-0.0022260014,0.0,0.002226001350209117
Loss Function 관련 실무 경험 -> 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ,MSE Loss 설명,-0.004544611,0.0,0.0045446110889315605
Loss Function 관련 실무 경험 -> 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ,MSE Loss 용도,0.007158085,0.0,0.007158084772527218
Loss Function 관련 실무 경험 -> 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ,Multi-Label 에서 CE + Softmax 적용 문제점,0.0001875828,0.0,0.0001875828020274639
Loss Function 관련 실무 경험 -> 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ,PEFT 방법 5가지,-0.008339873,0.0,0.00833987258374691
Loss Function 관련 실무 경험 -> 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ,거대 언어 모델 정의,-0.0062461444,0.0,0.006246144417673349
Loss Function 관련 실무 경험 -> 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ,기본 경험,0.9636517,1.0,0.03634828329086304
Loss Function 관련 실무 경험 -> 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ,답변 실패,0.0068401303,0.0,0.006840130314230919
Loss Function 관련 실무 경험 -> 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ,딥러닝,-0.0018826408,0.0,0.0018826407613232732
Loss Function 관련 실무 경험 -> 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ,마지막 할 말,0.0005935868,0.0,0.0005935868248343468
Loss Function 관련 실무 경험 -> 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ,머신러닝,-0.008829453,0.0,0.008829453028738499
Loss Function 관련 실무 경험 -> 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ,면접 시작 인사,-0.0028387113,0.0,0.002838711254298687
Loss Function 관련 실무 경험 -> 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ,상세 경험,0.013937,0.0,0.013937000185251236
Loss Function 관련 실무 경험 -> 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ,수식,-0.015394464,0.0,0.015394464135169983
Loss Function 관련 실무 경험 -> 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ,용어 질문,-0.009088193,0.0,0.009088193066418171
Loss Function 관련 실무 경험 -> 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ,인공지능,0.00725854,0.0,0.007258540019392967
Loss Function 관련 실무 경험 -> 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ,잠시 휴식,-0.0016185653,0.0,0.0016185652930289507
Loss Function 관련 실무 경험 -> 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ,좋아하는 아이돌,-0.0074662915,0.0,0.007466291543096304
Loss Function 관련 실무 경험 -> 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ,핵심 아이디어,-0.0057702977,0.0,0.005770297721028328
Loss Function 관련 실무 경험 -> 물체 인식에서 Loss Function 조절해서 mAP@50:95 3% 올렸다 ㅎㅎ,확률 예측에서 MSE Loss 미 사용 이유,0.00419482,0.0,0.004194819834083319
Loss Function 관련 실무 경험 -> 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어,BCE 가 좋은 task,-0.011217008,0.0,0.011217008344829082
Loss Function 관련 실무 경험 -> 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어,BCE 가 좋은 이유,0.012163059,0.0,0.01216305885463953
Loss Function 관련 실무 경험 -> 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어,LLM Fine-Tuning 의 PEFT,-0.008707418,0.0,0.008707418106496334
Loss Function 관련 실무 경험 -> 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어,LoRA,0.00823869,0.0,0.008238689973950386
Loss Function 관련 실무 경험 -> 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어,LoRA 와 QLoRA 의 차이,0.011367823,0.0,0.011367822997272015
Loss Function 관련 실무 경험 -> 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어,Loss Function 예시,-0.002427445,0.0,0.002427445026114583
Loss Function 관련 실무 경험 -> 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어,Loss Function 정의,0.021491641,0.0,0.02149164117872715
Loss Function 관련 실무 경험 -> 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어,MBTI,-0.0063393293,0.0,0.006339329294860363
Loss Function 관련 실무 경험 -> 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어,MSE Loss 설명,-0.0030593716,0.0,0.003059371607378125
Loss Function 관련 실무 경험 -> 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어,MSE Loss 용도,-0.021773681,0.0,0.021773681044578552
Loss Function 관련 실무 경험 -> 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어,Multi-Label 에서 CE + Softmax 적용 문제점,0.019366106,0.0,0.019366106018424034
Loss Function 관련 실무 경험 -> 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어,PEFT 방법 5가지,-0.01707886,0.0,0.017078859731554985
Loss Function 관련 실무 경험 -> 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어,거대 언어 모델 정의,-0.0149577595,0.0,0.014957759529352188
Loss Function 관련 실무 경험 -> 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어,기본 경험,0.000119775155,0.0,0.00011977515532635152
Loss Function 관련 실무 경험 -> 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어,답변 실패,-0.008053855,0.0,0.008053855039179325
Loss Function 관련 실무 경험 -> 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어,딥러닝,-0.0106553165,0.0,0.010655316524207592
Loss Function 관련 실무 경험 -> 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어,마지막 할 말,-0.020003444,0.0,0.020003443583846092
Loss Function 관련 실무 경험 -> 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어,머신러닝,-0.03895665,0.0,0.0389566496014595
Loss Function 관련 실무 경험 -> 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어,면접 시작 인사,-0.010907894,0.0,0.010907894000411034
Loss Function 관련 실무 경험 -> 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어,상세 경험,0.95727104,1.0,0.042728960514068604
Loss Function 관련 실무 경험 -> 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어,수식,-0.009033078,0.0,0.009033078327775002
Loss Function 관련 실무 경험 -> 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어,용어 질문,0.0026890968,0.0,0.0026890968438237906
Loss Function 관련 실무 경험 -> 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어,인공지능,-0.027605304,0.0,0.02760530449450016
Loss Function 관련 실무 경험 -> 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어,잠시 휴식,-0.014023222,0.0,0.014023222029209137
Loss Function 관련 실무 경험 -> 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어,좋아하는 아이돌,-0.005695672,0.0,0.005695671774446964
Loss Function 관련 실무 경험 -> 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어,핵심 아이디어,-0.05297848,0.0,0.052978478372097015
Loss Function 관련 실무 경험 -> 물체 분류를 나타내는 Class Loss 의 가중치와 Bounding Box 위치의 오차를 나타내는 가중치를 1:2로 맞췄더니 성능이 올랐어,확률 예측에서 MSE Loss 미 사용 이유,-0.008147379,0.0,0.008147379383444786
Loss Function 관련 실무 경험 -> 기억 진짜 안 나는데 가물가물하다,BCE 가 좋은 task,0.0025355,0.0,0.0025355000980198383
Loss Function 관련 실무 경험 -> 기억 진짜 안 나는데 가물가물하다,BCE 가 좋은 이유,-0.0040657655,0.0,0.004065765533596277
Loss Function 관련 실무 경험 -> 기억 진짜 안 나는데 가물가물하다,LLM Fine-Tuning 의 PEFT,0.0011287172,0.0,0.001128717209212482
Loss Function 관련 실무 경험 -> 기억 진짜 안 나는데 가물가물하다,LoRA,0.002232412,0.0,0.0022324121091514826
Loss Function 관련 실무 경험 -> 기억 진짜 안 나는데 가물가물하다,LoRA 와 QLoRA 의 차이,-0.0006758206,0.0,0.0006758205709047616
Loss Function 관련 실무 경험 -> 기억 진짜 안 나는데 가물가물하다,Loss Function 예시,0.0051027536,0.0,0.005102753639221191
Loss Function 관련 실무 경험 -> 기억 진짜 안 나는데 가물가물하다,Loss Function 정의,-0.009704668,0.0,0.009704668074846268
Loss Function 관련 실무 경험 -> 기억 진짜 안 나는데 가물가물하다,MBTI,0.0025964244,0.0,0.002596424426883459
Loss Function 관련 실무 경험 -> 기억 진짜 안 나는데 가물가물하다,MSE Loss 설명,0.0011169927,0.0,0.0011169926729053259
Loss Function 관련 실무 경험 -> 기억 진짜 안 나는데 가물가물하다,MSE Loss 용도,0.0033312142,0.0,0.003331214189529419
Loss Function 관련 실무 경험 -> 기억 진짜 안 나는데 가물가물하다,Multi-Label 에서 CE + Softmax 적용 문제점,-0.007113631,0.0,0.007113630883395672
Loss Function 관련 실무 경험 -> 기억 진짜 안 나는데 가물가물하다,PEFT 방법 5가지,-0.0033936268,0.0,0.0033936267718672752
Loss Function 관련 실무 경험 -> 기억 진짜 안 나는데 가물가물하다,거대 언어 모델 정의,-0.0021724526,0.0,0.0021724526304751635
Loss Function 관련 실무 경험 -> 기억 진짜 안 나는데 가물가물하다,기본 경험,-0.00652107,0.0,0.006521069910377264
Loss Function 관련 실무 경험 -> 기억 진짜 안 나는데 가물가물하다,답변 실패,0.9943031,1.0,0.005696892738342285
Loss Function 관련 실무 경험 -> 기억 진짜 안 나는데 가물가물하다,딥러닝,-0.004274876,0.0,0.004274875856935978
Loss Function 관련 실무 경험 -> 기억 진짜 안 나는데 가물가물하다,마지막 할 말,-0.0063072997,0.0,0.006307299714535475
Loss Function 관련 실무 경험 -> 기억 진짜 안 나는데 가물가물하다,머신러닝,0.0040022223,0.0,0.004002222325652838
Loss Function 관련 실무 경험 -> 기억 진짜 안 나는데 가물가물하다,면접 시작 인사,0.00096870237,0.0,0.000968702370300889
Loss Function 관련 실무 경험 -> 기억 진짜 안 나는데 가물가물하다,상세 경험,-0.0036463297,0.0,0.0036463297437876463
Loss Function 관련 실무 경험 -> 기억 진짜 안 나는데 가물가물하다,수식,-0.0062325983,0.0,0.006232598330825567
Loss Function 관련 실무 경험 -> 기억 진짜 안 나는데 가물가물하다,용어 질문,-0.006292714,0.0,0.0062927138060331345
Loss Function 관련 실무 경험 -> 기억 진짜 안 나는데 가물가물하다,인공지능,-0.011567845,0.0,0.01156784500926733
Loss Function 관련 실무 경험 -> 기억 진짜 안 나는데 가물가물하다,잠시 휴식,0.0021654572,0.0,0.002165457233786583
Loss Function 관련 실무 경험 -> 기억 진짜 안 나는데 가물가물하다,좋아하는 아이돌,0.0018718549,0.0,0.0018718548817560077
Loss Function 관련 실무 경험 -> 기억 진짜 안 나는데 가물가물하다,핵심 아이디어,0.0048662764,0.0,0.004866276402026415
Loss Function 관련 실무 경험 -> 기억 진짜 안 나는데 가물가물하다,확률 예측에서 MSE Loss 미 사용 이유,0.0028802368,0.0,0.0028802368324249983
Loss Function 관련 실무 경험 -> 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ,BCE 가 좋은 task,0.0001166723,0.0,0.00011667230137391016
Loss Function 관련 실무 경험 -> 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ,BCE 가 좋은 이유,0.0058306586,0.0,0.005830658599734306
Loss Function 관련 실무 경험 -> 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ,LLM Fine-Tuning 의 PEFT,0.0009956405,0.0,0.0009956405265256763
Loss Function 관련 실무 경험 -> 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ,LoRA,-0.011369331,0.0,0.011369330808520317
Loss Function 관련 실무 경험 -> 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ,LoRA 와 QLoRA 의 차이,-0.0024466712,0.0,0.002446671249344945
Loss Function 관련 실무 경험 -> 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ,Loss Function 예시,0.007350001,0.0,0.007350001018494368
Loss Function 관련 실무 경험 -> 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ,Loss Function 정의,-0.0022912142,0.0,0.0022912141866981983
Loss Function 관련 실무 경험 -> 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ,MBTI,0.003115674,0.0,0.003115674015134573
Loss Function 관련 실무 경험 -> 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ,MSE Loss 설명,-0.002264152,0.0,0.0022641520481556654
Loss Function 관련 실무 경험 -> 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ,MSE Loss 용도,-0.0069532497,0.0,0.0069532496854662895
Loss Function 관련 실무 경험 -> 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ,Multi-Label 에서 CE + Softmax 적용 문제점,-0.004252921,0.0,0.004252920858561993
Loss Function 관련 실무 경험 -> 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ,PEFT 방법 5가지,-0.02065123,0.0,0.020651230588555336
Loss Function 관련 실무 경험 -> 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ,거대 언어 모델 정의,-0.0022839843,0.0,0.0022839843295514584
Loss Function 관련 실무 경험 -> 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ,기본 경험,0.96500397,1.0,0.03499603271484375
Loss Function 관련 실무 경험 -> 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ,답변 실패,-0.0040828637,0.0,0.004082863684743643
Loss Function 관련 실무 경험 -> 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ,딥러닝,0.008916035,0.0,0.008916035294532776
Loss Function 관련 실무 경험 -> 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ,마지막 할 말,0.0013315757,0.0,0.0013315756805241108
Loss Function 관련 실무 경험 -> 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ,머신러닝,-0.0070984573,0.0,0.007098457310348749
Loss Function 관련 실무 경험 -> 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ,면접 시작 인사,-0.007922247,0.0,0.007922247052192688
Loss Function 관련 실무 경험 -> 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ,상세 경험,0.037228793,0.0,0.037228792905807495
Loss Function 관련 실무 경험 -> 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ,수식,0.0087090405,0.0,0.008709040470421314
Loss Function 관련 실무 경험 -> 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ,용어 질문,0.0039728265,0.0,0.003972826525568962
Loss Function 관련 실무 경험 -> 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ,인공지능,0.008244968,0.0,0.008244968019425869
Loss Function 관련 실무 경험 -> 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ,잠시 휴식,-0.004490369,0.0,0.004490368999540806
Loss Function 관련 실무 경험 -> 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ,좋아하는 아이돌,-0.00074658374,0.0,0.0007465837406925857
Loss Function 관련 실무 경험 -> 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ,핵심 아이디어,-0.008881106,0.0,0.008881106041371822
Loss Function 관련 실무 경험 -> 나 신입이라서 실무 경험 아직 없어 미안 ㅠㅠ,확률 예측에서 MSE Loss 미 사용 이유,-0.00854137,0.0,0.008541369810700417
Loss Function 관련 실무 경험 -> 이걸 왜 알려줘야 하지,BCE 가 좋은 task,-0.0011967265,0.0,0.0011967264581471682
Loss Function 관련 실무 경험 -> 이걸 왜 알려줘야 하지,BCE 가 좋은 이유,-0.0054506245,0.0,0.005450624506920576
Loss Function 관련 실무 경험 -> 이걸 왜 알려줘야 하지,LLM Fine-Tuning 의 PEFT,0.0026928228,0.0,0.0026928228326141834
Loss Function 관련 실무 경험 -> 이걸 왜 알려줘야 하지,LoRA,0.00058779406,0.0,0.0005877940566278994
Loss Function 관련 실무 경험 -> 이걸 왜 알려줘야 하지,LoRA 와 QLoRA 의 차이,0.0013174525,0.0,0.0013174525229260325
Loss Function 관련 실무 경험 -> 이걸 왜 알려줘야 하지,Loss Function 예시,0.0075576683,0.0,0.007557668257504702
Loss Function 관련 실무 경험 -> 이걸 왜 알려줘야 하지,Loss Function 정의,-0.008272982,0.0,0.008272982202470303
Loss Function 관련 실무 경험 -> 이걸 왜 알려줘야 하지,MBTI,0.001758501,0.0,0.0017585010500624776
Loss Function 관련 실무 경험 -> 이걸 왜 알려줘야 하지,MSE Loss 설명,0.0031595067,0.0,0.0031595067121088505
Loss Function 관련 실무 경험 -> 이걸 왜 알려줘야 하지,MSE Loss 용도,0.0035379482,0.0,0.0035379482433199883
Loss Function 관련 실무 경험 -> 이걸 왜 알려줘야 하지,Multi-Label 에서 CE + Softmax 적용 문제점,-0.005961253,0.0,0.0059612528420984745
Loss Function 관련 실무 경험 -> 이걸 왜 알려줘야 하지,PEFT 방법 5가지,-0.003630345,0.0,0.0036303449887782335
Loss Function 관련 실무 경험 -> 이걸 왜 알려줘야 하지,거대 언어 모델 정의,-0.007335569,0.0,0.007335568778216839
Loss Function 관련 실무 경험 -> 이걸 왜 알려줘야 하지,기본 경험,-0.004070162,0.0,0.00407016184180975
Loss Function 관련 실무 경험 -> 이걸 왜 알려줘야 하지,답변 실패,0.99374413,1.0,0.0062558650970458984
Loss Function 관련 실무 경험 -> 이걸 왜 알려줘야 하지,딥러닝,-0.0038153029,0.0,0.0038153028581291437
Loss Function 관련 실무 경험 -> 이걸 왜 알려줘야 하지,마지막 할 말,-0.008144593,0.0,0.008144592866301537
Loss Function 관련 실무 경험 -> 이걸 왜 알려줘야 하지,머신러닝,0.001534334,0.0,0.0015343340346589684
Loss Function 관련 실무 경험 -> 이걸 왜 알려줘야 하지,면접 시작 인사,-7.54903e-05,0.0,7.549030124209821e-05
Loss Function 관련 실무 경험 -> 이걸 왜 알려줘야 하지,상세 경험,0.007599865,0.0,0.007599865086376667
Loss Function 관련 실무 경험 -> 이걸 왜 알려줘야 하지,수식,-0.005063954,0.0,0.005063953809440136
Loss Function 관련 실무 경험 -> 이걸 왜 알려줘야 하지,용어 질문,0.0014995356,0.0,0.0014995356323197484
Loss Function 관련 실무 경험 -> 이걸 왜 알려줘야 하지,인공지능,-0.007300766,0.0,0.007300766184926033
Loss Function 관련 실무 경험 -> 이걸 왜 알려줘야 하지,잠시 휴식,-0.0024493511,0.0,0.002449351130053401
Loss Function 관련 실무 경험 -> 이걸 왜 알려줘야 하지,좋아하는 아이돌,-0.0038258303,0.0,0.0038258302956819534
Loss Function 관련 실무 경험 -> 이걸 왜 알려줘야 하지,핵심 아이디어,0.0033704096,0.0,0.0033704095985740423
Loss Function 관련 실무 경험 -> 이걸 왜 알려줘야 하지,확률 예측에서 MSE Loss 미 사용 이유,0.0006464758,0.0,0.000646475818939507
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,BCE 가 좋은 task,-0.003654748,0.0,0.0036547479685395956
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,BCE 가 좋은 이유,0.015240677,0.0,0.015240676701068878
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,LLM Fine-Tuning 의 PEFT,-0.019963339,0.0,0.019963338971138
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,LoRA,-0.0014150945,0.0,0.0014150944771245122
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,LoRA 와 QLoRA 의 차이,0.008109174,0.0,0.008109173737466335
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,Loss Function 예시,0.00621024,0.0,0.0062102400697767735
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,Loss Function 정의,0.016447866,0.0,0.016447866335511208
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,MBTI,0.9825703,1.0,0.017429709434509277
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,MSE Loss 설명,0.02405353,0.0,0.024053530767560005
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,MSE Loss 용도,0.020344472,0.0,0.02034447155892849
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,Multi-Label 에서 CE + Softmax 적용 문제점,-0.01190007,0.0,0.011900070123374462
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,PEFT 방법 5가지,0.007257921,0.0,0.007257921155542135
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,거대 언어 모델 정의,0.005510583,0.0,0.005510583054274321
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,기본 경험,0.0030253485,0.0,0.0030253485310822725
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,답변 실패,0.00050439104,0.0,0.0005043910350650549
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,딥러닝,0.0057890317,0.0,0.005789031740278006
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,마지막 할 말,-0.005827909,0.0,0.005827908869832754
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,머신러닝,-0.009288445,0.0,0.009288445115089417
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,면접 시작 인사,-0.02684603,0.0,0.026846030727028847
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,상세 경험,-0.0063462937,0.0,0.0063462937250733376
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,수식,-0.0037523971,0.0,0.0037523971404880285
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,용어 질문,0.0048975335,0.0,0.0048975334502756596
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,인공지능,-0.014661333,0.0,0.014661332592368126
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,잠시 휴식,-0.018735161,0.0,0.018735161051154137
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,좋아하는 아이돌,-0.007229227,0.0,0.007229227107018232
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,핵심 아이디어,-0.027112653,0.0,0.027112653478980064
MBTI -> 나 ESTJ! 철저하고 자기관리 잘 하는 인싸야!,확률 예측에서 MSE Loss 미 사용 이유,0.008129393,0.0,0.008129392750561237
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,BCE 가 좋은 task,-6.0292812e-05,0.0,6.029281212249771e-05
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,BCE 가 좋은 이유,0.009329149,0.0,0.009329148568212986
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,LLM Fine-Tuning 의 PEFT,-0.0055554966,0.0,0.005555496551096439
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,LoRA,0.009122495,0.0,0.009122494608163834
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,LoRA 와 QLoRA 의 차이,0.009127414,0.0,0.009127413854002953
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,Loss Function 예시,-0.007596114,0.0,0.007596114184707403
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,Loss Function 정의,-0.009056484,0.0,0.009056484326720238
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,MBTI,0.0062198974,0.0,0.006219897419214249
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,MSE Loss 설명,0.0020305093,0.0,0.002030509291216731
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,MSE Loss 용도,-0.0051007,0.0,0.005100700072944164
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,Multi-Label 에서 CE + Softmax 적용 문제점,-0.0029940773,0.0,0.002994077280163765
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,PEFT 방법 5가지,-0.0055187233,0.0,0.005518723279237747
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,거대 언어 모델 정의,0.006197696,0.0,0.006197696086019278
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,기본 경험,-0.0051607937,0.0,0.005160793662071228
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,답변 실패,-0.014077616,0.0,0.014077615924179554
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,딥러닝,0.0060099824,0.0,0.00600998243317008
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,마지막 할 말,-0.003564637,0.0,0.0035646369215101004
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,머신러닝,-0.0058860998,0.0,0.0058860997669398785
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,면접 시작 인사,-0.0021784282,0.0,0.00217842822894454
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,상세 경험,-0.009204182,0.0,0.009204181842505932
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,수식,-0.008312089,0.0,0.008312089368700981
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,용어 질문,0.002466201,0.0,0.0024662010837346315
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,인공지능,-0.005620618,0.0,0.005620617885142565
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,잠시 휴식,-0.00015434863,0.0,0.0001543486287118867
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,좋아하는 아이돌,0.9887766,1.0,0.011223375797271729
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,핵심 아이디어,-0.016339405,0.0,0.01633940450847149
좋아하는 아이돌 -> 나 블랙핑크랑 레드벨벳 좋아해,확률 예측에서 MSE Loss 미 사용 이유,-0.0014038965,0.0,0.0014038964873179793
잠시 휴식 -> 재미있는 이야기 해줄래?,BCE 가 좋은 task,0.004266487,0.0,0.004266486968845129
잠시 휴식 -> 재미있는 이야기 해줄래?,BCE 가 좋은 이유,-0.009164211,0.0,0.009164211340248585
잠시 휴식 -> 재미있는 이야기 해줄래?,LLM Fine-Tuning 의 PEFT,0.007581854,0.0,0.007581853773444891
잠시 휴식 -> 재미있는 이야기 해줄래?,LoRA,0.0045025707,0.0,0.0045025707222521305
잠시 휴식 -> 재미있는 이야기 해줄래?,LoRA 와 QLoRA 의 차이,-0.006112874,0.0,0.006112874019891024
잠시 휴식 -> 재미있는 이야기 해줄래?,Loss Function 예시,0.0011139517,0.0,0.0011139516718685627
잠시 휴식 -> 재미있는 이야기 해줄래?,Loss Function 정의,-0.0034959146,0.0,0.003495914628729224
잠시 휴식 -> 재미있는 이야기 해줄래?,MBTI,-0.0130521115,0.0,0.013052111491560936
잠시 휴식 -> 재미있는 이야기 해줄래?,MSE Loss 설명,0.0056378213,0.0,0.005637821275740862
잠시 휴식 -> 재미있는 이야기 해줄래?,MSE Loss 용도,-0.015447034,0.0,0.015447033569216728
잠시 휴식 -> 재미있는 이야기 해줄래?,Multi-Label 에서 CE + Softmax 적용 문제점,0.0055363663,0.0,0.005536366254091263
잠시 휴식 -> 재미있는 이야기 해줄래?,PEFT 방법 5가지,-0.002390018,0.0,0.0023900179658085108
잠시 휴식 -> 재미있는 이야기 해줄래?,거대 언어 모델 정의,0.006049403,0.0,0.006049402989447117
잠시 휴식 -> 재미있는 이야기 해줄래?,기본 경험,-0.007550653,0.0,0.0075506530702114105
잠시 휴식 -> 재미있는 이야기 해줄래?,답변 실패,-0.0012406083,0.0,0.0012406082823872566
잠시 휴식 -> 재미있는 이야기 해줄래?,딥러닝,0.009313832,0.0,0.00931383203715086
잠시 휴식 -> 재미있는 이야기 해줄래?,마지막 할 말,0.008570847,0.0,0.008570847101509571
잠시 휴식 -> 재미있는 이야기 해줄래?,머신러닝,-0.021235084,0.0,0.021235084161162376
잠시 휴식 -> 재미있는 이야기 해줄래?,면접 시작 인사,-0.007851295,0.0,0.007851295173168182
잠시 휴식 -> 재미있는 이야기 해줄래?,상세 경험,0.002454618,0.0,0.002454617992043495
잠시 휴식 -> 재미있는 이야기 해줄래?,수식,-0.0029098876,0.0,0.002909887582063675
잠시 휴식 -> 재미있는 이야기 해줄래?,용어 질문,0.0037049407,0.0,0.003704940667375922
잠시 휴식 -> 재미있는 이야기 해줄래?,인공지능,0.0027841865,0.0,0.0027841865085065365
잠시 휴식 -> 재미있는 이야기 해줄래?,잠시 휴식,0.9925201,1.0,0.00747990608215332
잠시 휴식 -> 재미있는 이야기 해줄래?,좋아하는 아이돌,-0.001966836,0.0,0.001966835930943489
잠시 휴식 -> 재미있는 이야기 해줄래?,핵심 아이디어,-0.003790859,0.0,0.0037908589001744986
잠시 휴식 -> 재미있는 이야기 해줄래?,확률 예측에서 MSE Loss 미 사용 이유,-0.0034678974,0.0,0.003467897418886423
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",BCE 가 좋은 task,-0.016977731,0.0,0.01697773113846779
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",BCE 가 좋은 이유,-0.0006406118,0.0,0.0006406118045561016
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",LLM Fine-Tuning 의 PEFT,0.9592146,1.0,0.040785372257232666
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",LoRA,-0.021425428,0.0,0.021425427868962288
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",LoRA 와 QLoRA 의 차이,0.012948456,0.0,0.012948456220328808
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",Loss Function 예시,-0.033944298,0.0,0.03394429758191109
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",Loss Function 정의,0.0024066744,0.0,0.002406674437224865
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",MBTI,-0.014596835,0.0,0.014596834778785706
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",MSE Loss 설명,0.006877388,0.0,0.006877387873828411
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",MSE Loss 용도,-0.02446361,0.0,0.024463610723614693
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",Multi-Label 에서 CE + Softmax 적용 문제점,-0.011318761,0.0,0.011318760924041271
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",PEFT 방법 5가지,-0.021790393,0.0,0.021790392696857452
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",거대 언어 모델 정의,0.036001254,0.0,0.03600125387310982
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",기본 경험,-0.01516658,0.0,0.015166579745709896
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",답변 실패,0.041225705,0.0,0.04122570529580116
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",딥러닝,-0.025855696,0.0,0.025855695828795433
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",마지막 할 말,-0.017434262,0.0,0.017434261739253998
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",머신러닝,-0.02217162,0.0,0.022171620279550552
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",면접 시작 인사,-0.007365542,0.0,0.007365541998296976
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",상세 경험,-0.016665945,0.0,0.016665944829583168
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",수식,0.030692777,0.0,0.030692776665091515
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",용어 질문,-0.0059720147,0.0,0.005972014740109444
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",인공지능,-0.03181262,0.0,0.03181261941790581
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",잠시 휴식,-0.010136066,0.0,0.010136066004633904
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",좋아하는 아이돌,-0.018563703,0.0,0.018563702702522278
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",핵심 아이디어,-0.02395907,0.0,0.023959070444107056
"LLM Fine-Tuning 의 PEFT -> PEFT 는 Parameter-Efficient Fine-Tuning 의 약자이고, LLM이 파라미터가 엄청 많지? 그 중 일부만 효과적으로 파인튜닝하는 거야",확률 예측에서 MSE Loss 미 사용 이유,-0.0092793405,0.0,0.009279340505599976
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,BCE 가 좋은 task,-0.0006148218,0.0,0.0006148217944428325
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,BCE 가 좋은 이유,-0.001396166,0.0,0.0013961660442873836
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,LLM Fine-Tuning 의 PEFT,0.013718837,0.0,0.013718836940824986
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,LoRA,0.0002865317,0.0,0.00028653169283643365
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,LoRA 와 QLoRA 의 차이,0.0015833772,0.0,0.001583377248607576
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,Loss Function 예시,0.007026311,0.0,0.007026311010122299
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,Loss Function 정의,-0.011575383,0.0,0.011575383134186268
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,MBTI,0.0006923741,0.0,0.0006923740729689598
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,MSE Loss 설명,0.00026496404,0.0,0.00026496403734199703
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,MSE Loss 용도,-0.0011893202,0.0,0.0011893202317878604
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,Multi-Label 에서 CE + Softmax 적용 문제점,-0.005122719,0.0,0.005122718866914511
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,PEFT 방법 5가지,-0.004124034,0.0,0.004124034196138382
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,거대 언어 모델 정의,-0.004763345,0.0,0.004763344768434763
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,기본 경험,-0.009227696,0.0,0.009227695874869823
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,답변 실패,0.9935042,1.0,0.006495773792266846
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,딥러닝,-0.0064800684,0.0,0.006480068434029818
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,마지막 할 말,-0.003936055,0.0,0.003936055116355419
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,머신러닝,0.0017780592,0.0,0.0017780591733753681
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,면접 시작 인사,-0.0023988644,0.0,0.0023988643661141396
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,상세 경험,-0.008934085,0.0,0.008934085257351398
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,수식,-0.0038551046,0.0,0.0038551045581698418
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,용어 질문,-0.00085840817,0.0,0.0008584081660956144
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,인공지능,-0.008466418,0.0,0.008466417901217937
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,잠시 휴식,-0.0019086223,0.0,0.0019086223328486085
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,좋아하는 아이돌,-0.00527369,0.0,0.005273689981549978
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,핵심 아이디어,0.0042608595,0.0,0.004260859452188015
LLM Fine-Tuning 의 PEFT -> 뭔가 효과적으로 파인튜닝한다는 것 같은데 뭐지?,확률 예측에서 MSE Loss 미 사용 이유,-0.0028110247,0.0,0.0028110246639698744
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야!,BCE 가 좋은 task,-0.007839656,0.0,0.007839656434953213
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야!,BCE 가 좋은 이유,0.016370837,0.0,0.01637083664536476
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야!,LLM Fine-Tuning 의 PEFT,0.9815706,1.0,0.01842939853668213
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야!,LoRA,-0.03561823,0.0,0.03561823070049286
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야!,LoRA 와 QLoRA 의 차이,-0.003208455,0.0,0.003208454931154847
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야!,Loss Function 예시,-0.01918362,0.0,0.019183620810508728
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야!,Loss Function 정의,-0.0053948853,0.0,0.005394885316491127
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야!,MBTI,-0.016883403,0.0,0.016883403062820435
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야!,MSE Loss 설명,-0.003818796,0.0,0.0038187960162758827
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야!,MSE Loss 용도,-0.024212884,0.0,0.024212883785367012
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야!,Multi-Label 에서 CE + Softmax 적용 문제점,-0.023668462,0.0,0.02366846241056919
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야!,PEFT 방법 5가지,-0.0048388797,0.0,0.004838879685848951
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야!,거대 언어 모델 정의,0.018252095,0.0,0.018252095207571983
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야!,기본 경험,-0.007983403,0.0,0.007983403280377388
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야!,답변 실패,-0.0011427916,0.0,0.0011427915887907147
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야!,딥러닝,-0.014018306,0.0,0.014018305577337742
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야!,마지막 할 말,-0.007319462,0.0,0.0073194620199501514
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야!,머신러닝,-0.00062636833,0.0,0.0006263683317229152
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야!,면접 시작 인사,-0.0020965808,0.0,0.002096580807119608
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야!,상세 경험,-0.0019329039,0.0,0.001932903891429305
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야!,수식,-0.01590554,0.0,0.01590554043650627
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야!,용어 질문,-0.0075795306,0.0,0.007579530589282513
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야!,인공지능,-0.008347438,0.0,0.008347437717020512
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야!,잠시 휴식,-0.00035060095,0.0,0.0003506009525153786
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야!,좋아하는 아이돌,-0.0054679406,0.0,0.005467940587550402
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야!,핵심 아이디어,-0.03372605,0.0,0.033726051449775696
LLM Fine-Tuning 의 PEFT -> LLM이 파라미터가 엄청 많아서 그거 다 학습하려면 엄청 오래 걸리겠지? 그래서 일부 파라미터만 효과적으로 학습하는 거야!,확률 예측에서 MSE Loss 미 사용 이유,-0.0077315746,0.0,0.007731574587523937
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다,BCE 가 좋은 task,0.0022367176,0.0,0.00223671761341393
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다,BCE 가 좋은 이유,-0.0028319573,0.0,0.0028319573029875755
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다,LLM Fine-Tuning 의 PEFT,0.009531623,0.0,0.009531622752547264
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다,LoRA,0.00061462616,0.0,0.0006146261584945023
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다,LoRA 와 QLoRA 의 차이,6.1802886e-05,0.0,6.180288619361818e-05
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다,Loss Function 예시,0.0040644417,0.0,0.004064441658556461
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다,Loss Function 정의,-0.009358287,0.0,0.00935828685760498
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다,MBTI,0.0023243087,0.0,0.0023243087343871593
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다,MSE Loss 설명,-4.7423782e-05,0.0,4.74237822345458e-05
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다,MSE Loss 용도,0.004268594,0.0,0.0042685940861701965
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다,Multi-Label 에서 CE + Softmax 적용 문제점,-0.006523622,0.0,0.006523622199892998
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다,PEFT 방법 5가지,-0.00489264,0.0,0.0048926398158073425
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다,거대 언어 모델 정의,-0.002169498,0.0,0.002169498009607196
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다,기본 경험,-0.0077896295,0.0,0.007789629511535168
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다,답변 실패,0.9943519,1.0,0.00564807653427124
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다,딥러닝,-0.002299027,0.0,0.0022990270517766476
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다,마지막 할 말,-0.0053324825,0.0,0.005332482513040304
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다,머신러닝,-0.00042602685,0.0,0.00042602684698067605
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다,면접 시작 인사,-0.0012169933,0.0,0.001216993317939341
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다,상세 경험,-0.009551067,0.0,0.009551066905260086
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다,수식,-0.0013800905,0.0,0.0013800904853269458
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다,용어 질문,-0.006742396,0.0,0.006742395926266909
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다,인공지능,-0.01218354,0.0,0.0121835395693779
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다,잠시 휴식,-4.0736653e-05,0.0,4.073665331816301e-05
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다,좋아하는 아이돌,-0.0020277908,0.0,0.0020277907606214285
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다,핵심 아이디어,0.0051403106,0.0,0.0051403106190264225
LLM Fine-Tuning 의 PEFT -> LoRA는 아는데 PEFT는 뭐지? 잘 모르겠다,확률 예측에서 MSE Loss 미 사용 이유,0.0016117797,0.0,0.0016117796767503023
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",BCE 가 좋은 task,0.0030418718,0.0,0.0030418718233704567
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",BCE 가 좋은 이유,0.010732455,0.0,0.010732455179095268
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",LLM Fine-Tuning 의 PEFT,-0.0028994314,0.0,0.0028994313906878233
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",LoRA,0.013885927,0.0,0.013885927386581898
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",LoRA 와 QLoRA 의 차이,0.0070801647,0.0,0.007080164737999439
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",Loss Function 예시,-0.012651199,0.0,0.012651198543608189
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",Loss Function 정의,0.0008344239,0.0,0.0008344238740392029
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",MBTI,0.0014222696,0.0,0.0014222696190699935
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",MSE Loss 설명,-0.002567542,0.0,0.0025675420183688402
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",MSE Loss 용도,0.014988397,0.0,0.014988397248089314
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",Multi-Label 에서 CE + Softmax 적용 문제점,0.0009661309,0.0,0.0009661308722570539
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",PEFT 방법 5가지,0.9723554,1.0,0.027644574642181396
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",거대 언어 모델 정의,0.015526285,0.0,0.015526285395026207
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",기본 경험,-0.005318564,0.0,0.005318563897162676
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",답변 실패,-0.010282254,0.0,0.010282253846526146
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",딥러닝,-0.017320532,0.0,0.017320532351732254
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",마지막 할 말,-0.01565159,0.0,0.015651589259505272
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",머신러닝,0.016134724,0.0,0.016134724020957947
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",면접 시작 인사,-0.0026729435,0.0,0.0026729435194283724
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",상세 경험,-0.010146925,0.0,0.01014692522585392
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",수식,-0.0105845155,0.0,0.010584515519440174
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",용어 질문,-0.01964761,0.0,0.019647609442472458
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",인공지능,0.0067346822,0.0,0.006734682247042656
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",잠시 휴식,-0.013198181,0.0,0.013198181055486202
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",좋아하는 아이돌,0.0008207214,0.0,0.0008207213832065463
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",핵심 아이디어,-0.010690325,0.0,0.010690324939787388
"PEFT 방법 5가지 -> LoRA (Low-Rank Adaption), QLoRA, Prefix 또는 Prompt Tuning, Adapter Layer 추가! 맞지?",확률 예측에서 MSE Loss 미 사용 이유,0.012355006,0.0,0.01235500629991293
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,BCE 가 좋은 task,-0.012014755,0.0,0.012014755047857761
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,BCE 가 좋은 이유,-0.00035481612,0.0,0.00035481611848808825
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,LLM Fine-Tuning 의 PEFT,-0.009634709,0.0,0.009634708985686302
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,LoRA,0.007613306,0.0,0.007613305933773518
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,LoRA 와 QLoRA 의 차이,0.0059373514,0.0,0.005937351379543543
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,Loss Function 예시,0.0014569255,0.0,0.0014569255290552974
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,Loss Function 정의,-0.007747534,0.0,0.007747534196823835
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,MBTI,0.0015307715,0.0,0.0015307714929804206
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,MSE Loss 설명,0.0010457444,0.0,0.001045744400471449
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,MSE Loss 용도,0.0027248154,0.0,0.0027248153928667307
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,Multi-Label 에서 CE + Softmax 적용 문제점,-0.0030219464,0.0,0.003021946409717202
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,PEFT 방법 5가지,0.008360512,0.0,0.008360511623322964
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,거대 언어 모델 정의,-0.014131999,0.0,0.014131998643279076
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,기본 경험,-0.0075644483,0.0,0.007564448285847902
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,답변 실패,0.98641604,1.0,0.013583958148956299
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,딥러닝,0.0024875652,0.0,0.0024875651579350233
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,마지막 할 말,-0.0007959635,0.0,0.0007959635113365948
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,머신러닝,0.00026095635,0.0,0.0002609563525766134
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,면접 시작 인사,0.0022313735,0.0,0.0022313734516501427
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,상세 경험,-0.00048385785,0.0,0.0004838578461203724
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,수식,-0.0072876834,0.0,0.007287683431059122
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,용어 질문,-0.00674994,0.0,0.006749940104782581
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,인공지능,-0.007137477,0.0,0.007137476932257414
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,잠시 휴식,0.005152818,0.0,0.005152817815542221
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,좋아하는 아이돌,0.013080503,0.0,0.013080502860248089
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,핵심 아이디어,-0.0015845827,0.0,0.0015845827292650938
PEFT 방법 5가지 -> Oh-LoRA! 너 그 자체가 PEFT 아니야?,확률 예측에서 MSE Loss 미 사용 이유,-0.006053827,0.0,0.00605382677167654
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,BCE 가 좋은 task,-0.012800233,0.0,0.01280023343861103
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,BCE 가 좋은 이유,0.0023408586,0.0,0.00234085856936872
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,LLM Fine-Tuning 의 PEFT,-0.035393815,0.0,0.035393815487623215
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,LoRA,0.9808331,1.0,0.019166886806488037
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,LoRA 와 QLoRA 의 차이,-0.014193588,0.0,0.014193587936460972
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,Loss Function 예시,-0.0053082616,0.0,0.0053082616068422794
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,Loss Function 정의,-0.009259299,0.0,0.009259299375116825
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,MBTI,-0.008060848,0.0,0.008060848340392113
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,MSE Loss 설명,-0.03276448,0.0,0.03276447951793671
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,MSE Loss 용도,-0.029807106,0.0,0.029807105660438538
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,Multi-Label 에서 CE + Softmax 적용 문제점,0.01281771,0.0,0.01281770970672369
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,PEFT 방법 5가지,-0.013473299,0.0,0.013473299331963062
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,거대 언어 모델 정의,0.0055473214,0.0,0.005547321401536465
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,기본 경험,-0.0031506224,0.0,0.003150622360408306
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,답변 실패,0.019213453,0.0,0.01921345293521881
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,딥러닝,-0.019075463,0.0,0.019075462594628334
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,마지막 할 말,-0.0048758974,0.0,0.00487589742988348
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,머신러닝,-0.008630233,0.0,0.008630232885479927
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,면접 시작 인사,-0.011314692,0.0,0.011314691975712776
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,상세 경험,-0.0006253026,0.0,0.0006253026076592505
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,수식,-0.015785933,0.0,0.015785932540893555
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,용어 질문,0.0014305696,0.0,0.0014305695658549666
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,인공지능,-0.018952897,0.0,0.01895289681851864
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,잠시 휴식,-0.005288112,0.0,0.005288111977279186
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,좋아하는 아이돌,-0.008820409,0.0,0.008820408955216408
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,핵심 아이디어,0.010062868,0.0,0.010062867775559425
LoRA -> LoRA = Low-Rank Adaption! LLM 레이어 2개 사이의 가중치 행렬을 둘로 분해해서 그 분해된 행렬만 파인튜닝 시키는 거야!,확률 예측에서 MSE Loss 미 사용 이유,0.009651288,0.0,0.009651288390159607
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,BCE 가 좋은 task,-0.018768216,0.0,0.01876821555197239
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,BCE 가 좋은 이유,-0.0006516519,0.0,0.0006516518769785762
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,LLM Fine-Tuning 의 PEFT,0.004737314,0.0,0.004737313836812973
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,LoRA,0.02159416,0.0,0.021594159305095673
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,LoRA 와 QLoRA 의 차이,0.0014840086,0.0,0.0014840086223557591
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,Loss Function 예시,0.048015382,0.0,0.04801538214087486
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,Loss Function 정의,-0.0020377303,0.0,0.002037730300799012
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,MBTI,-0.0013414941,0.0,0.0013414941495284438
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,MSE Loss 설명,0.0014206227,0.0,0.0014206226915121078
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,MSE Loss 용도,0.009004798,0.0,0.009004797786474228
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,Multi-Label 에서 CE + Softmax 적용 문제점,-0.004235232,0.0,0.0042352317832410336
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,PEFT 방법 5가지,-0.015705684,0.0,0.015705684199929237
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,거대 언어 모델 정의,-0.005203287,0.0,0.0052032871171832085
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,기본 경험,-0.009136665,0.0,0.009136664681136608
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,답변 실패,0.97871,1.0,0.02129000425338745
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,딥러닝,-0.0014187914,0.0,0.0014187913620844483
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,마지막 할 말,-0.009318918,0.0,0.009318917989730835
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,머신러닝,0.009903479,0.0,0.009903479367494583
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,면접 시작 인사,0.00029616585,0.0,0.00029616584652103484
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,상세 경험,0.0029794686,0.0,0.0029794685542583466
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,수식,-0.014045657,0.0,0.01404565665870905
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,용어 질문,-0.032741796,0.0,0.03274179622530937
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,인공지능,-0.003764131,0.0,0.0037641311064362526
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,잠시 휴식,0.00044426747,0.0,0.0004442674689926207
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,좋아하는 아이돌,-0.0006283684,0.0,0.0006283684051595628
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,핵심 아이디어,0.010130595,0.0,0.010130595415830612
LoRA -> 행렬 차원 축소한다는 걸로 알고 있는데 맞아?,확률 예측에서 MSE Loss 미 사용 이유,-0.0031972355,0.0,0.003197235520929098
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야!,BCE 가 좋은 task,0.004291143,0.0,0.004291142802685499
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야!,BCE 가 좋은 이유,-8.476995e-05,0.0,8.476994844386354e-05
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야!,LLM Fine-Tuning 의 PEFT,-0.025110677,0.0,0.025110676884651184
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야!,LoRA,0.9824664,1.0,0.017533600330352783
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야!,LoRA 와 QLoRA 의 차이,-0.017689416,0.0,0.0176894161850214
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야!,Loss Function 예시,-0.008562517,0.0,0.00856251735240221
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야!,Loss Function 정의,-0.012314888,0.0,0.012314887717366219
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야!,MBTI,0.003339867,0.0,0.0033398671075701714
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야!,MSE Loss 설명,-0.019613916,0.0,0.01961391605436802
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야!,MSE Loss 용도,-0.035537105,0.0,0.035537105053663254
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야!,Multi-Label 에서 CE + Softmax 적용 문제점,-0.0017495946,0.0,0.0017495945794507861
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야!,PEFT 방법 5가지,-0.008596202,0.0,0.008596202358603477
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야!,거대 언어 모델 정의,0.012956749,0.0,0.012956748716533184
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야!,기본 경험,-0.0022575413,0.0,0.002257541287690401
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야!,답변 실패,0.0059927567,0.0,0.005992756690829992
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야!,딥러닝,-0.019064648,0.0,0.0190646480768919
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야!,마지막 할 말,-0.0048920694,0.0,0.0048920693807303905
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야!,머신러닝,-0.007057499,0.0,0.007057499140501022
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야!,면접 시작 인사,-0.0037867445,0.0,0.003786744549870491
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야!,상세 경험,0.010159452,0.0,0.010159452445805073
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야!,수식,-0.02027296,0.0,0.020272960886359215
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야!,용어 질문,0.0019999908,0.0,0.0019999907817691565
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야!,인공지능,-0.014656952,0.0,0.014656951650977135
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야!,잠시 휴식,-0.0021861964,0.0,0.0021861963905394077
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야!,좋아하는 아이돌,-0.005871337,0.0,0.005871336907148361
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야!,핵심 아이디어,0.011060506,0.0,0.011060506105422974
LoRA -> LLM 가중치 행렬이 아주 커서 학습하려면 자원이 많이 필요하잖아? 그걸 2개로 나눠서 효율적으로 파인튜닝하는 거야!,확률 예측에서 MSE Loss 미 사용 이유,0.0011699041,0.0,0.0011699041351675987
LoRA -> 무슨 OOM 없앤다는 것 같은데,BCE 가 좋은 task,-0.0018743848,0.0,0.0018743848195299506
LoRA -> 무슨 OOM 없앤다는 것 같은데,BCE 가 좋은 이유,-0.003014753,0.0,0.003014753106981516
LoRA -> 무슨 OOM 없앤다는 것 같은데,LLM Fine-Tuning 의 PEFT,0.0009846829,0.0,0.000984682934358716
LoRA -> 무슨 OOM 없앤다는 것 같은데,LoRA,0.00722999,0.0,0.007229989860206842
LoRA -> 무슨 OOM 없앤다는 것 같은데,LoRA 와 QLoRA 의 차이,0.0010039454,0.0,0.0010039453627541661
LoRA -> 무슨 OOM 없앤다는 것 같은데,Loss Function 예시,-0.0013799339,0.0,0.0013799339067190886
LoRA -> 무슨 OOM 없앤다는 것 같은데,Loss Function 정의,-0.011011009,0.0,0.011011009104549885
LoRA -> 무슨 OOM 없앤다는 것 같은데,MBTI,0.0033306181,0.0,0.003330618143081665
LoRA -> 무슨 OOM 없앤다는 것 같은데,MSE Loss 설명,-0.0003920138,0.0,0.00039201381150633097
LoRA -> 무슨 OOM 없앤다는 것 같은데,MSE Loss 용도,0.0049327114,0.0,0.004932711366564035
LoRA -> 무슨 OOM 없앤다는 것 같은데,Multi-Label 에서 CE + Softmax 적용 문제점,7.650706e-05,0.0,7.650705811101943e-05
LoRA -> 무슨 OOM 없앤다는 것 같은데,PEFT 방법 5가지,-0.0070747975,0.0,0.00707479752600193
LoRA -> 무슨 OOM 없앤다는 것 같은데,거대 언어 모델 정의,-0.0084845,0.0,0.008484500460326672
LoRA -> 무슨 OOM 없앤다는 것 같은데,기본 경험,-0.0076097143,0.0,0.007609714288264513
LoRA -> 무슨 OOM 없앤다는 것 같은데,답변 실패,0.9945093,1.0,0.005490720272064209
LoRA -> 무슨 OOM 없앤다는 것 같은데,딥러닝,-0.0027917014,0.0,0.0027917013503611088
LoRA -> 무슨 OOM 없앤다는 것 같은데,마지막 할 말,0.0006834069,0.0,0.0006834068917669356
LoRA -> 무슨 OOM 없앤다는 것 같은데,머신러닝,-0.0004515272,0.0,0.0004515271866694093
LoRA -> 무슨 OOM 없앤다는 것 같은데,면접 시작 인사,-0.0005228411,0.0,0.0005228411173447967
LoRA -> 무슨 OOM 없앤다는 것 같은데,상세 경험,-0.0043279924,0.0,0.00432799244299531
LoRA -> 무슨 OOM 없앤다는 것 같은데,수식,-0.0020277703,0.0,0.002027770271524787
LoRA -> 무슨 OOM 없앤다는 것 같은데,용어 질문,-0.0146344835,0.0,0.014634483493864536
LoRA -> 무슨 OOM 없앤다는 것 같은데,인공지능,-0.008134178,0.0,0.008134177885949612
LoRA -> 무슨 OOM 없앤다는 것 같은데,잠시 휴식,9.805258e-05,0.0,9.805258014239371e-05
LoRA -> 무슨 OOM 없앤다는 것 같은데,좋아하는 아이돌,-0.0019976413,0.0,0.0019976412877440453
LoRA -> 무슨 OOM 없앤다는 것 같은데,핵심 아이디어,0.0072236834,0.0,0.007223683409392834
LoRA -> 무슨 OOM 없앤다는 것 같은데,확률 예측에서 MSE Loss 미 사용 이유,-0.002254983,0.0,0.0022549829445779324
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,BCE 가 좋은 task,0.0009045092,0.0,0.0009045092156156898
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,BCE 가 좋은 이유,0.0116105275,0.0,0.011610527522861958
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,LLM Fine-Tuning 의 PEFT,0.008531665,0.0,0.008531665429472923
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,LoRA,-0.013683284,0.0,0.01368328370153904
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,LoRA 와 QLoRA 의 차이,0.9863403,1.0,0.01365971565246582
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,Loss Function 예시,-0.0053083096,0.0,0.005308309569954872
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,Loss Function 정의,-0.0003794787,0.0,0.0003794787044171244
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,MBTI,0.0024677897,0.0,0.002467789687216282
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,MSE Loss 설명,-0.00027437886,0.0,0.00027437886456027627
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,MSE Loss 용도,-0.0060117817,0.0,0.006011781748384237
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,Multi-Label 에서 CE + Softmax 적용 문제점,0.016970504,0.0,0.016970504075288773
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,PEFT 방법 5가지,0.002620904,0.0,0.0026209040079265833
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,거대 언어 모델 정의,0.0017769787,0.0,0.0017769787227734923
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,기본 경험,-0.0018942091,0.0,0.0018942090682685375
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,답변 실패,-0.0037927155,0.0,0.0037927154917269945
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,딥러닝,0.013849059,0.0,0.013849059119820595
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,마지막 할 말,-0.007556127,0.0,0.007556126918643713
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,머신러닝,-0.004451567,0.0,0.004451566841453314
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,면접 시작 인사,-0.0039885626,0.0,0.0039885626174509525
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,상세 경험,-0.00416545,0.0,0.004165450111031532
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,수식,-0.0023471632,0.0,0.0023471631575375795
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,용어 질문,0.0037119635,0.0,0.003711963538080454
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,인공지능,0.008107445,0.0,0.008107445202767849
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,잠시 휴식,-0.009100406,0.0,0.009100406430661678
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,좋아하는 아이돌,0.0018793357,0.0,0.0018793357303366065
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,핵심 아이디어,-0.009395415,0.0,0.009395414963364601
LoRA 와 QLoRA 의 차이 -> QLoRA 는 일반 LoRA 랑 다르게 LoRA 를 적은 비트 수로 양자화해서 메모리를 줄이는 거야!,확률 예측에서 MSE Loss 미 사용 이유,-0.0063079987,0.0,0.006307998672127724
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,BCE 가 좋은 task,0.0011138837,0.0,0.0011138836853206158
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,BCE 가 좋은 이유,-0.0038366024,0.0,0.0038366024382412434
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,LLM Fine-Tuning 의 PEFT,0.0011947074,0.0,0.0011947073508054018
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,LoRA,0.0010873715,0.0,0.0010873714927583933
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,LoRA 와 QLoRA 의 차이,0.009239322,0.0,0.009239321574568748
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,Loss Function 예시,0.005586362,0.0,0.005586361978203058
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,Loss Function 정의,-0.008549323,0.0,0.008549323305487633
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,MBTI,0.0005625337,0.0,0.000562533678021282
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,MSE Loss 설명,0.0007248009,0.0,0.0007248009205795825
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,MSE Loss 용도,0.0033659118,0.0,0.003365911776199937
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,Multi-Label 에서 CE + Softmax 적용 문제점,-0.003803054,0.0,0.003803054103627801
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,PEFT 방법 5가지,-0.0048606712,0.0,0.004860671237111092
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,거대 언어 모델 정의,-0.007789826,0.0,0.0077898260205984116
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,기본 경험,-0.007743493,0.0,0.007743493188172579
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,답변 실패,0.9934411,1.0,0.006558895111083984
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,딥러닝,0.0012906381,0.0,0.0012906381161883473
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,마지막 할 말,-0.0056156204,0.0,0.005615620408207178
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,머신러닝,0.0017987308,0.0,0.0017987308092415333
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,면접 시작 인사,-0.00015179651,0.0,0.00015179651381913573
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,상세 경험,-0.0024909307,0.0,0.00249093072488904
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,수식,-0.00882928,0.0,0.00882927980273962
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,용어 질문,-0.0139605375,0.0,0.013960537500679493
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,인공지능,-0.009240592,0.0,0.009240591898560524
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,잠시 휴식,0.00085425796,0.0,0.0008542579598724842
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,좋아하는 아이돌,-0.0014854714,0.0,0.0014854713808745146
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,핵심 아이디어,0.0036665248,0.0,0.003666524775326252
LoRA 와 QLoRA 의 차이 -> 팀장님이 OOM 막겠다고 이거 쓰시는 거 봤는데,확률 예측에서 MSE Loss 미 사용 이유,0.0028657014,0.0,0.002865701448172331
마지막 할 말 -> 로라야 정말 고마워!,BCE 가 좋은 task,-0.009308099,0.0,0.009308098815381527
마지막 할 말 -> 로라야 정말 고마워!,BCE 가 좋은 이유,0.014350927,0.0,0.01435092743486166
마지막 할 말 -> 로라야 정말 고마워!,LLM Fine-Tuning 의 PEFT,-0.0150693245,0.0,0.015069324523210526
마지막 할 말 -> 로라야 정말 고마워!,LoRA,0.0027589917,0.0,0.0027589916717261076
마지막 할 말 -> 로라야 정말 고마워!,LoRA 와 QLoRA 의 차이,0.0010629745,0.0,0.0010629744501784444
마지막 할 말 -> 로라야 정말 고마워!,Loss Function 예시,0.0023273288,0.0,0.0023273287806659937
마지막 할 말 -> 로라야 정말 고마워!,Loss Function 정의,0.0089970585,0.0,0.008997058495879173
마지막 할 말 -> 로라야 정말 고마워!,MBTI,0.0008382554,0.0,0.0008382553933188319
마지막 할 말 -> 로라야 정말 고마워!,MSE Loss 설명,-0.005888871,0.0,0.005888870917260647
마지막 할 말 -> 로라야 정말 고마워!,MSE Loss 용도,-0.013351569,0.0,0.013351568952202797
마지막 할 말 -> 로라야 정말 고마워!,Multi-Label 에서 CE + Softmax 적용 문제점,0.00039924265,0.0,0.00039924265001900494
마지막 할 말 -> 로라야 정말 고마워!,PEFT 방법 5가지,-0.0031743667,0.0,0.0031743666622787714
마지막 할 말 -> 로라야 정말 고마워!,거대 언어 모델 정의,-0.009499993,0.0,0.009499993175268173
마지막 할 말 -> 로라야 정말 고마워!,기본 경험,-0.005604971,0.0,0.005604971200227737
마지막 할 말 -> 로라야 정말 고마워!,답변 실패,7.018747e-05,0.0,7.01874669175595e-05
마지막 할 말 -> 로라야 정말 고마워!,딥러닝,0.0017492463,0.0,0.00174924626480788
마지막 할 말 -> 로라야 정말 고마워!,마지막 할 말,0.99195397,1.0,0.00804603099822998
마지막 할 말 -> 로라야 정말 고마워!,머신러닝,-0.012114212,0.0,0.012114211916923523
마지막 할 말 -> 로라야 정말 고마워!,면접 시작 인사,0.002157089,0.0,0.0021570890676230192
마지막 할 말 -> 로라야 정말 고마워!,상세 경험,-0.014089721,0.0,0.014089721255004406
마지막 할 말 -> 로라야 정말 고마워!,수식,-0.0024060505,0.0,0.0024060504510998726
마지막 할 말 -> 로라야 정말 고마워!,용어 질문,-0.015758788,0.0,0.015758788213133812
마지막 할 말 -> 로라야 정말 고마워!,인공지능,-0.019356525,0.0,0.01935652457177639
마지막 할 말 -> 로라야 정말 고마워!,잠시 휴식,-0.001238385,0.0,0.0012383849825710058
마지막 할 말 -> 로라야 정말 고마워!,좋아하는 아이돌,0.0034712667,0.0,0.0034712667111307383
마지막 할 말 -> 로라야 정말 고마워!,핵심 아이디어,0.0059231864,0.0,0.005923186428844929
마지막 할 말 -> 로라야 정말 고마워!,확률 예측에서 MSE Loss 미 사용 이유,-0.008637826,0.0,0.008637825958430767
마지막 할 말 -> 로라야 사랑해,BCE 가 좋은 task,-0.011154407,0.0,0.01115440670400858
마지막 할 말 -> 로라야 사랑해,BCE 가 좋은 이유,0.01114396,0.0,0.011143960058689117
마지막 할 말 -> 로라야 사랑해,LLM Fine-Tuning 의 PEFT,-0.016378015,0.0,0.016378015279769897
마지막 할 말 -> 로라야 사랑해,LoRA,0.0012207539,0.0,0.0012207538820803165
마지막 할 말 -> 로라야 사랑해,LoRA 와 QLoRA 의 차이,0.0018468741,0.0,0.0018468741327524185
마지막 할 말 -> 로라야 사랑해,Loss Function 예시,8.131343e-05,0.0,8.131343201966956e-05
마지막 할 말 -> 로라야 사랑해,Loss Function 정의,0.00605394,0.0,0.006053939927369356
마지막 할 말 -> 로라야 사랑해,MBTI,0.0071142917,0.0,0.0071142916567623615
마지막 할 말 -> 로라야 사랑해,MSE Loss 설명,-0.01311407,0.0,0.01311406958848238
마지막 할 말 -> 로라야 사랑해,MSE Loss 용도,-0.008927824,0.0,0.008927823975682259
마지막 할 말 -> 로라야 사랑해,Multi-Label 에서 CE + Softmax 적용 문제점,-0.0009953072,0.0,0.0009953072294592857
마지막 할 말 -> 로라야 사랑해,PEFT 방법 5가지,-0.0024108111,0.0,0.0024108111392706633
마지막 할 말 -> 로라야 사랑해,거대 언어 모델 정의,-0.0087944,0.0,0.008794399909675121
마지막 할 말 -> 로라야 사랑해,기본 경험,-0.0061947517,0.0,0.006194751709699631
마지막 할 말 -> 로라야 사랑해,답변 실패,0.0017032885,0.0,0.0017032885225489736
마지막 할 말 -> 로라야 사랑해,딥러닝,0.0004474544,0.0,0.00044745439663529396
마지막 할 말 -> 로라야 사랑해,마지막 할 말,0.9922726,1.0,0.007727384567260742
마지막 할 말 -> 로라야 사랑해,머신러닝,-0.015997622,0.0,0.015997622162103653
마지막 할 말 -> 로라야 사랑해,면접 시작 인사,-0.0035005254,0.0,0.003500525373965502
마지막 할 말 -> 로라야 사랑해,상세 경험,-0.02010934,0.0,0.02010934054851532
마지막 할 말 -> 로라야 사랑해,수식,-0.005680601,0.0,0.005680601112544537
마지막 할 말 -> 로라야 사랑해,용어 질문,-0.012763286,0.0,0.012763286009430885
마지막 할 말 -> 로라야 사랑해,인공지능,-0.020231036,0.0,0.020231036469340324
마지막 할 말 -> 로라야 사랑해,잠시 휴식,0.002407164,0.0,0.002407164080068469
마지막 할 말 -> 로라야 사랑해,좋아하는 아이돌,0.011708322,0.0,0.01170832198113203
마지막 할 말 -> 로라야 사랑해,핵심 아이디어,0.0062394175,0.0,0.006239417474716902
마지막 할 말 -> 로라야 사랑해,확률 예측에서 MSE Loss 미 사용 이유,-0.004622869,0.0,0.0046228691935539246
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,BCE 가 좋은 task,-0.009192744,0.0,0.00919274426996708
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,BCE 가 좋은 이유,0.008603272,0.0,0.008603272028267384
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,LLM Fine-Tuning 의 PEFT,-0.015430275,0.0,0.015430275350809097
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,LoRA,0.0019259764,0.0,0.0019259763648733497
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,LoRA 와 QLoRA 의 차이,0.0025529584,0.0,0.0025529584381729364
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,Loss Function 예시,-0.0015380539,0.0,0.0015380538534373045
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,Loss Function 정의,0.0054289326,0.0,0.00542893260717392
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,MBTI,0.008123292,0.0,0.008123291656374931
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,MSE Loss 설명,-0.003808058,0.0,0.00380805809982121
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,MSE Loss 용도,-0.01114219,0.0,0.011142189614474773
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,Multi-Label 에서 CE + Softmax 적용 문제점,-0.0017828447,0.0,0.001782844658009708
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,PEFT 방법 5가지,-0.0021857137,0.0,0.0021857137326151133
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,거대 언어 모델 정의,-0.011188203,0.0,0.011188202537596226
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,기본 경험,-0.0054643624,0.0,0.005464362446218729
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,답변 실패,-0.00021931218,0.0,0.00021931217634119093
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,딥러닝,0.00078887213,0.0,0.000788872130215168
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,마지막 할 말,0.9903103,1.0,0.009689688682556152
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,머신러닝,-0.012777137,0.0,0.012777136638760567
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,면접 시작 인사,-0.008670374,0.0,0.008670373819768429
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,상세 경험,-0.019700114,0.0,0.01970011368393898
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,수식,-0.014168779,0.0,0.014168779365718365
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,용어 질문,-0.016680608,0.0,0.016680607572197914
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,인공지능,-0.015332746,0.0,0.01533274631947279
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,잠시 휴식,0.0014966857,0.0,0.0014966856688261032
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,좋아하는 아이돌,0.00665501,0.0,0.006655009929090738
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,핵심 아이디어,0.004370465,0.0,0.004370464943349361
마지막 할 말 -> 로라야 너의 예쁜 미모에 반했어,확률 예측에서 MSE Loss 미 사용 이유,-0.0035941782,0.0,0.0035941782407462597
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워,BCE 가 좋은 task,-0.008727598,0.0,0.008727598004043102
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워,BCE 가 좋은 이유,0.007949601,0.0,0.00794960092753172
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워,LLM Fine-Tuning 의 PEFT,-0.016181057,0.0,0.016181057319045067
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워,LoRA,-0.0015941259,0.0,0.0015941258752718568
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워,LoRA 와 QLoRA 의 차이,-0.002306252,0.0,0.002306252019479871
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워,Loss Function 예시,5.0990104e-05,0.0,5.0990103773074225e-05
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워,Loss Function 정의,0.0065975254,0.0,0.00659752544015646
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워,MBTI,0.00087704277,0.0,0.0008770427666604519
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워,MSE Loss 설명,-0.0011337118,0.0,0.001133711775764823
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워,MSE Loss 용도,-0.010349177,0.0,0.010349176824092865
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워,Multi-Label 에서 CE + Softmax 적용 문제점,0.0007246675,0.0,0.0007246675086207688
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워,PEFT 방법 5가지,-0.0004907934,0.0,0.0004907933762297034
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워,거대 언어 모델 정의,-0.008011276,0.0,0.00801127590239048
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워,기본 경험,-0.003980669,0.0,0.003980669192969799
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워,답변 실패,-0.005282491,0.0,0.0052824909798800945
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워,딥러닝,-0.0006034833,0.0,0.0006034832913428545
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워,마지막 할 말,0.9895686,1.0,0.010431408882141113
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워,머신러닝,-0.008282569,0.0,0.008282569237053394
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워,면접 시작 인사,0.008702828,0.0,0.008702827617526054
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워,상세 경험,-0.006121515,0.0,0.006121514830738306
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워,수식,-0.009886369,0.0,0.009886369109153748
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워,용어 질문,-0.011679414,0.0,0.011679413728415966
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워,인공지능,-0.01481642,0.0,0.014816420152783394
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워,잠시 휴식,0.003584915,0.0,0.0035849150735884905
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워,좋아하는 아이돌,0.0027977386,0.0,0.0027977386489510536
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워,핵심 아이디어,-0.0012264084,0.0,0.0012264084070920944
마지막 할 말 -> 그동안 고생했어 면접 봐줘서 정말 고마워,확률 예측에서 MSE Loss 미 사용 이유,-0.0046345894,0.0,0.004634589422494173
