import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, StoppingCriteriaList

import pandas as pd
import numpy as np

import time
import os
import sys
PROJECT_DIR_PATH = os.path.dirname(os.path.abspath(os.path.dirname(__file__)))
sys.path.append(PROJECT_DIR_PATH)

from stylegan_and_segmentation.stylegan_modified.stylegan_generator import StyleGANGeneratorForV3
from llm.memory_mechanism.train_sbert import load_pretrained_sbert_model
from llm.run_memory_mechanism import pick_best_memory_item
from llm.fine_tuning.inference import StopOnTokens, load_valid_user_prompts


global_path = os.path.dirname(os.path.abspath(os.path.dirname(os.path.abspath(os.path.dirname(__file__)))))
sys.path.append(global_path)

from global_common.visualize_tensor import save_tensor_png


PROPERTY_DIMS_Z = 7


# í•„ìš”í•œ ëª¨ë¸ ë¡œë”© : StyleGAN-FineTune-v3 Generator (Decoder), LLM (Polyglot-Ko 1.3B Fine-Tuned), S-BERT (RoBERTa-based)
# Create Date : 2025.04.23
# Last Update Date : -

# Arguments:
# - device (device) : ëª¨ë¸ë“¤ì„ mapping ì‹œí‚¬ device (GPU ë“±)

# Returns:
# - stylegan_generator   (nn.Module)    : StyleGAN-FineTune-v3 Generator (Decoder)
# - ohlora_llm           (LLM)          : LLM (Polyglot-Ko 1.3B Fine-Tuned)
# - ohlora_llm_tokenizer (tokenizer)    : LLM (Polyglot-Ko 1.3B Fine-Tuned) ì— ëŒ€í•œ tokenizer
# - sbert_model          (S-BERT Model) : S-BERT (RoBERTa-based)

def load_models(device):

    # load StyleGAN-FineTune-v3 generator model
    stylegan_generator = StyleGANGeneratorForV3(resolution=256)
    stylegan_modified_dir = f'{PROJECT_DIR_PATH}/stylegan_and_segmentation/stylegan_modified'
    generator_path = f'{stylegan_modified_dir}/stylegan_gen_fine_tuned_v3_ckpt_0005_gen.pth'

    generator_state_dict = torch.load(generator_path, map_location=device, weights_only=True)
    stylegan_generator.load_state_dict(generator_state_dict)
    stylegan_generator.to(device)

    # load Oh-LoRA final LLM and tokenizer
    ohlora_llm = AutoModelForCausalLM.from_pretrained(f'{PROJECT_DIR_PATH}/llm/models/polyglot_fine_tuned',
                                                      trust_remote_code=True,
                                                      torch_dtype=torch.bfloat16).cuda()

    ohlora_llm_tokenizer = AutoTokenizer.from_pretrained(f'{PROJECT_DIR_PATH}/llm/models/polyglot_fine_tuned')

    # Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
    ohlora_llm.generation_config.pad_token_id = ohlora_llm_tokenizer.pad_token_id

    # load S-BERT Model (RoBERTa-based)
    model_path = f'{PROJECT_DIR_PATH}/llm/models/memory_sbert/trained_sbert_model'
    sbert_model = load_pretrained_sbert_model(model_path)

    return stylegan_generator, ohlora_llm, ohlora_llm_tokenizer, sbert_model


# Oh-LoRA (ì˜¤ë¡œë¼) ì‹¤í–‰
# Create Date : 2025.04.23
# Last Update Date : -

# Arguments:
# - stylegan_generator   (nn.Module)    : StyleGAN-FineTune-v3 Generator (Decoder)
# - ohlora_llm           (LLM)          : LLM (Polyglot-Ko 1.3B Fine-Tuned)
# - ohlora_llm_tokenizer (tokenizer)    : LLM (Polyglot-Ko 1.3B Fine-Tuned) ì— ëŒ€í•œ tokenizer
# - sbert_model          (S-BERT Model) : S-BERT (RoBERTa-based)

# Running Mechanism:
# - Oh-LoRA LLM ë‹µë³€ ìƒì„± ì‹œë§ˆë‹¤ ì´ì— ê¸°ë°˜í•˜ì—¬ final_product/ohlora.png ê²½ë¡œì— ì˜¤ë¡œë¼ ì´ë¯¸ì§€ ìƒì„±
# - Oh-LoRA ë‹µë³€ì„ parsing í•˜ì—¬ llm/memory_mechanism/saved_memory/ohlora_memory.txt ê²½ë¡œì— ë©”ëª¨ë¦¬ ì €ì¥
# - S-BERT ëª¨ë¸ì„ ì´ìš©í•˜ì—¬, RAG ì™€ ìœ ì‚¬í•œ ë°©ì‹ìœ¼ë¡œ í•´ë‹¹ íŒŒì¼ì—ì„œ ì‚¬ìš©ì í”„ë¡¬í”„íŠ¸ì— ê°€ì¥ ì í•©í•œ ë©”ëª¨ë¦¬ ì •ë³´ë¥¼ ì°¾ì•„ì„œ ìµœì¢… LLM ì…ë ¥ì— ì¶”ê°€

def run_ohlora(stylegan_generator, ohlora_llm, ohlora_llm_tokenizer, sbert_model):

    while True:
        user_prompt = input('\nì˜¤ë¡œë¼ì—ê²Œ ë§í•˜ê¸° (Ctrl+C to finish) : ')
        best_memory_item = pick_best_memory_item(sbert_model,
                                                 user_prompt,
                                                 memory_file_name='ohlora_memory.txt',
                                                 verbose=False)

        if best_memory_item == '':
            final_ohlora_input = user_prompt
        else:
            final_ohlora_input = best_memory_item + ' ' + user_prompt

        # generate Oh-LoRA answer and post-process
        llm_answer = generate_llm_answer(ohlora_llm, ohlora_llm_tokenizer, final_ohlora_input)
        llm_answer_cleaned, memory_list = parse_memory(llm_answer)
        save_memory_list(memory_list)

        print(f'ğŸ‘±â€â™€ï¸ ì˜¤ë¡œë¼ : {llm_answer_cleaned}')

        # generate Oh-LoRA image
        eyes_score, mouth_score, pose_score = decide_property_scores(llm_answer_cleaned)
        generate_ohlora_image(stylegan_generator, eyes_score, mouth_score, pose_score)


# Oh-LoRA (ì˜¤ë¡œë¼) ì˜ ë‹µë³€ ìƒì„±
# Create Date : 2025.04.23
# Last Update Date : 2025.04.24
# - í­ ì—†ëŠ” ê³µë°± (zwsp), ì¤„ ë°”ê¿ˆ ì—†ëŠ” ê³µë°± (nbsp) ì œê±° ì²˜ë¦¬ ì¶”ê°€

# Arguments :
# - ohlora_llm           (LLM)       : LLM (Polyglot-Ko 1.3B Fine-Tuned)
# - ohlora_llm_tokenizer (tokenizer) : LLM (Polyglot-Ko 1.3B Fine-Tuned) ì— ëŒ€í•œ tokenizer
# - final_ohlora_input   (str)       : ì˜¤ë¡œë¼ğŸ‘±â€â™€ï¸ ì—ê²Œ ìµœì¢…ì ìœ¼ë¡œ ì…ë ¥ë˜ëŠ” ë©”ì‹œì§€ (ê²½ìš°ì— ë”°ë¼ memory text í¬í•¨)

# Returns :
# - ohlora_answer (str) : ì˜¤ë¡œë¼ğŸ‘±â€â™€ï¸ ê°€ ìƒì„±í•œ ë‹µë³€

def generate_llm_answer(ohlora_llm, ohlora_llm_tokenizer, final_ohlora_input):

    trial_count = 0
    max_trials = 30

    # tokenize final Oh-LoRA input
    final_ohlora_input_ = final_ohlora_input + ' (ë‹µë³€ ì‹œì‘)'

    inputs = ohlora_llm_tokenizer(final_ohlora_input_, return_tensors='pt')
    inputs = {'input_ids': inputs['input_ids'].to(ohlora_llm.device),
              'attention_mask': inputs['attention_mask'].to(ohlora_llm.device)}

    # for stopping criteria
    stop_token_ids = torch.tensor([1477, 1078, 4833, 12]).to(ohlora_llm.device)  # '(ë‹µë³€ ì¢…ë£Œ)'
    stopping_criteria = StoppingCriteriaList([StopOnTokens(stop_token_ids)])

    while trial_count < max_trials:
        outputs = ohlora_llm.generate(**inputs,
                                      max_length=80,
                                      do_sample=True,
                                      temperature=0.6,
                                      stopping_criteria=stopping_criteria)

        llm_answer = ohlora_llm_tokenizer.decode(outputs[0], skip_special_tokens=True)
        llm_answer = llm_answer[len(final_ohlora_input_):]
        llm_answer = llm_answer.replace('\u200b','').replace('\xa0', '')  # zwsp, nbsp (í­ ì—†ëŠ” ê³µë°±, ì¤„ ë°”ê¿ˆ ì—†ëŠ” ê³µë°±) ì œê±°
        trial_count += 1

        # check LLM answer and return or retry
        is_bracketed = llm_answer.startswith('[') and llm_answer.endswith(']')
        is_non_empty = (not is_bracketed) and llm_answer.replace('\n', '').replace('(ë‹µë³€ ì¢…ë£Œ)', '').replace(' ', '') != ''
        is_answer_end_mark = 'ë‹µë³€ ì¢…ë£Œ' in llm_answer.replace('(ë‹µë³€ ì¢…ë£Œ)', '') or 'ë‹µë³€ì¢…ë£Œ' in llm_answer.replace('(ë‹µë³€ ì¢…ë£Œ)', '')

        is_unnecessary_quote = '"' in llm_answer or 'â€' in llm_answer or 'â€œ' in llm_answer or 'â€™' in llm_answer
        is_unnecessary_mark = 'ï¿½' in llm_answer
        is_too_many_blanks = '     ' in llm_answer
        is_low_quality = is_unnecessary_quote or is_unnecessary_mark or is_too_many_blanks

        if is_non_empty and not is_answer_end_mark and 'http' not in llm_answer and not is_low_quality:
            if llm_answer.startswith(']'):
                return llm_answer[1:].replace('(ë‹µë³€ ì¢…ë£Œ)', '')
            return llm_answer.replace('(ë‹µë³€ ì¢…ë£Œ)', '')

    return '(ì½ì”¹)'


# Oh-LoRA (ì˜¤ë¡œë¼) ì˜ ìƒì„±ëœ ë‹µë³€ìœ¼ë¡œë¶€í„° memory ì •ë³´ë¥¼ parsing
# Create Date : 2025.04.23
# Last Update Date : -

# Arguments :
# - llm_answer (str) : ì˜¤ë¡œë¼ğŸ‘±â€â™€ï¸ ê°€ ìƒì„±í•œ ì›ë³¸ ë‹µë³€ (memory info í¬í•¨ ê°€ëŠ¥)

# Returns :
# - llm_answer_cleaned (str)       : ì˜¤ë¡œë¼ğŸ‘±â€â™€ï¸ ê°€ ìƒì„±í•œ ì›ë³¸ ë‹µë³€ì—ì„œ text clean ì„ ì‹¤ì‹œí•œ ì´í›„ì˜ ë‹µë³€
# - memory_list        (list(str)) : ì˜¤ë¡œë¼ğŸ‘±â€â™€ï¸ ê°€ ì €ì¥í•´ì•¼ í•  ë©”ëª¨ë¦¬ ëª©ë¡

def parse_memory(llm_answer):
    llm_answer_length = len(llm_answer)

    bracket_start_or_sentence_end_idx = llm_answer_length
    bracket_end_idx = None
    is_colon = False

    llm_answer_cleaned = ''
    memory_list = []

    for i in range(llm_answer_length-1, -1, -1):
        if llm_answer[i] == ']':
            bracket_end_idx = i

        if bracket_end_idx is not None and llm_answer[i] == ':':
            is_colon = True

        if llm_answer[i] == '[' and bracket_end_idx is not None:
            llm_answer_cleaned = llm_answer[bracket_end_idx+1:bracket_start_or_sentence_end_idx] + llm_answer_cleaned

            if is_colon:
                memory_list.append(llm_answer[i:bracket_end_idx+1])

            bracket_end_idx = None
            bracket_start_or_sentence_end_idx = i
            is_colon = False

    llm_answer_cleaned = llm_answer[:bracket_start_or_sentence_end_idx] + llm_answer_cleaned
    return llm_answer_cleaned, memory_list


# Oh-LoRA (ì˜¤ë¡œë¼) ì˜ ë©”ëª¨ë¦¬ ì •ë³´ë¥¼ llm/memory_mechanism/saved_memory/ohlora_memory.txt ì— ì €ì¥
# Create Date : 2025.04.23
# Last Update Date : -

# Arguments :
# - memory_list (list(str)) : ì˜¤ë¡œë¼ğŸ‘±â€â™€ï¸ ê°€ ì €ì¥í•´ì•¼ í•  ë©”ëª¨ë¦¬ ëª©ë¡

def save_memory_list(memory_list):
    memory_file_path = f'{PROJECT_DIR_PATH}/llm/memory_mechanism/saved_memory/ohlora_memory.txt'

    memory_file = open(memory_file_path, 'r', encoding='UTF8')
    memory_file_lines = memory_file.readlines()
    memory_file.close()

    memory_file_lines = [line.split('\n')[0] for line in memory_file_lines]

    for memory in memory_list:
        is_duplicated = False

        for line in memory_file_lines:
            if memory.replace(' ', '') == line.replace(' ', ''):
                is_duplicated = True
                break

        if not is_duplicated:
            memory_file_lines.append(memory)

    memory_file_lines_to_save = '\n'.join(memory_file_lines)

    memory_file = open(memory_file_path, 'w', encoding='UTF8')
    memory_file.write(memory_file_lines_to_save + '\n')
    memory_file.close()


# Oh-LoRA (ì˜¤ë¡œë¼) ì˜ ë‹µë³€ì— ë”°ë¼ ëˆˆì„ ëœ¬ ì •ë„ (eyes), ì…ì„ ë²Œë¦° ì •ë„ (mouth), ê³ ê°œ ëŒë¦¼ (pose) ì ìˆ˜ ì‚°ì¶œ
# Create Date : 2025.04.23
# Last Update Date : 2025.04.24
# - ëˆˆì„ í¬ê²Œ ëœ¨ê³  ì…ì„ ë²Œë¦¬ëŠ” ê°íƒ„ì‚¬ ì¡°ê±´ ì¼ë¶€ ìˆ˜ì •
# - ê³ ê°œ ëŒë¦¼ ì¡°ê±´ ë¡œì§ ì¼ë¶€ ìˆ˜ì •

# Arguments :
# - llm_answer_cleaned (str) : ì˜¤ë¡œë¼ğŸ‘±â€â™€ï¸ ê°€ ìƒì„±í•œ ì›ë³¸ ë‹µë³€ì—ì„œ text clean ì„ ì‹¤ì‹œí•œ ì´í›„ì˜ ë‹µë³€

# Returns :
# - eyes_score  (float) : ëˆˆì„ ëœ¬ ì •ë„ (eyes) ì˜ ì†ì„± ê°’ ì ìˆ˜
# - mouth_score (float) : ì…ì„ ë²Œë¦° ì •ë„ (mouth) ì˜ ì†ì„± ê°’ ì ìˆ˜
# - pose_score  (float) : ê³ ê°œ ëŒë¦¼ (pose) ì˜ ì†ì„± ê°’ ì ìˆ˜

def decide_property_scores(llm_answer_cleaned):

    if (('ì˜¤!' in llm_answer_cleaned) or ('ì™€!' in llm_answer_cleaned) or
        ('ì™€ìš°' in llm_answer_cleaned) or (llm_answer_cleaned.strip().startswith('ì˜¤ '))):

        eyes_score = 1.6
        mouth_score_bonus = 1.2

    elif ('ì•„!' in llm_answer_cleaned) or (llm_answer_cleaned.strip().startswith('ì•„ ')):
        eyes_score = 0.8
        mouth_score_bonus = 0.6

    else:
        eyes_score = -1.2
        mouth_score_bonus = 0.0

    if '!' in llm_answer_cleaned or '?' in llm_answer_cleaned:
        mouth_score = 0.3 + 0.3 * min(llm_answer_cleaned.count('!'), 3) + mouth_score_bonus / 3
    else:
        mouth_score = -1.2 + mouth_score_bonus

    if 'ë¯¸ì•ˆ' in llm_answer_cleaned and 'ì‹«ì–´' in llm_answer_cleaned:
        pose_score = 3.6
    elif 'ì˜¤!' not in llm_answer_cleaned and (' ëª» ' in llm_answer_cleaned or 'ì—†ì–´ ' in llm_answer_cleaned or 'ì—†ì–´.' in llm_answer_cleaned):
        pose_score = 3.0
    elif 'ë¯¸ì•ˆ' in llm_answer_cleaned or 'ì‹«ì–´' in llm_answer_cleaned or 'ë³„ë¡œ' in llm_answer_cleaned:
        pose_score = 2.4
    elif 'â€¦' in llm_answer_cleaned:
        pose_score = 0.6
    else:
        pose_score = -1.2

    return eyes_score, mouth_score, pose_score


# Oh-LoRA (ì˜¤ë¡œë¼) ì´ë¯¸ì§€ ìƒì„±
# Create Date : 2025.04.23
# Last Update Date : 2025.04.24
# - noise ê°•ë„ ìˆ˜ì • (0.3 -> 0.15)

# Arguments :
# - stylegan_generator (nn.Module) : StyleGAN-FineTune-v3 Generator (Decoder)
# - eyes_score         (float)     : ëˆˆì„ ëœ¬ ì •ë„ (eyes) ì˜ ì†ì„± ê°’ ì ìˆ˜
# - mouth_score        (float)     : ì…ì„ ë²Œë¦° ì •ë„ (mouth) ì˜ ì†ì„± ê°’ ì ìˆ˜
# - pose_score         (float)     : ê³ ê°œ ëŒë¦¼ (pose) ì˜ ì†ì„± ê°’ ì ìˆ˜

# Returns :
# - ì§ì ‘ ë°˜í™˜ë˜ëŠ” ê°’ ì—†ìŒ
# - final_product/ohlora.png ê²½ë¡œì— ì˜¤ë¡œë¼ ì´ë¯¸ì§€ ìƒì„±

def generate_ohlora_image(stylegan_generator, eyes_score, mouth_score, pose_score):

    # z vector ë¡œë”©
    z_vector_csv_path = f'{PROJECT_DIR_PATH}/final_product/final_ohlora_z_vector.csv'
    z_vector = pd.read_csv(z_vector_csv_path)
    z_vector_torch = torch.tensor(np.array(z_vector))

    # StyleGAN-FineTune-v3 ì— ì…ë ¥í•  ì†ì„± ê°’ì„ ë‚˜íƒ€ë‚´ëŠ” label ìƒì„±
    label = [eyes_score, 0.0, 0.0, mouth_score, pose_score, 0.0, 0.0]

    label_np = np.array([[label]])
    label_np = label_np.reshape((1, PROPERTY_DIMS_Z))
    label_torch = torch.tensor(label_np).to(torch.float32)

    with torch.no_grad():
        z = z_vector_torch.to(torch.float32)
        z_noised = z + 0.15 * torch.randn_like(z)

        generated_image = stylegan_generator(z=z_noised.cuda(), label=label_torch.cuda())['image']
        generated_image = generated_image.detach().cpu()

    save_tensor_png(generated_image[0],
                    image_save_path=f'{PROJECT_DIR_PATH}/final_product/ohlora.png')


# ë‹µë³€ ìƒì„± í…ŒìŠ¤íŠ¸ (í…ŒìŠ¤íŠ¸ìš© í•¨ìˆ˜, ì‹¤ì œ ì‚¬ìš© ì‹œì—ëŠ” í•´ë‹¹ í•¨ìˆ˜ ì‹¤í–‰ ë¶€ë¶„ ì£¼ì„ ì²˜ë¦¬)
# Create Date : 2025.04.24
# Last Update Date : -

# Arguments:
# - ohlora_llm           (LLM)       : LLM (Polyglot-Ko 1.3B Fine-Tuned)
# - ohlora_llm_tokenizer (tokenizer) : LLM (Polyglot-Ko 1.3B Fine-Tuned) ì— ëŒ€í•œ tokenizer

def test_generate(ohlora_llm, ohlora_llm_tokenizer):
    valid_user_prompts = load_valid_user_prompts()

    for user_prompt in valid_user_prompts:
        print(f'\nuser prompt : [{user_prompt}]')

        for _ in range(4):
            start_at = time.time()
            llm_answer = generate_llm_answer(ohlora_llm, ohlora_llm_tokenizer, user_prompt)
            elapsed_time = time.time() - start_at

            print(f'Oh-LoRA ğŸ‘±â€â™€ï¸ : [{llm_answer}] (ğŸ•š {elapsed_time:.2f}s)')


if __name__ == '__main__':

    # check device
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f'device : {device}')

    # load model
    stylegan_generator, ohlora_llm, ohlora_llm_tokenizer, sbert_model = load_models(device)
    print('ALL MODELS for Oh-LoRA (ì˜¤ë¡œë¼) load successful!! ğŸ‘±â€â™€ï¸')

    # test generating answer
#    test_generate(ohlora_llm, ohlora_llm_tokenizer)

    # run Oh-LoRA (ì˜¤ë¡œë¼)
    run_ohlora(stylegan_generator, ohlora_llm, ohlora_llm_tokenizer, sbert_model)
