## ëª©ì°¨

* [1. LLM Final Selection](#1-llm-final-selection)
  * [1-1. Polyglot-Ko 1.3B ì„ íƒ ì´ìœ ](#1-1-polyglot-ko-13b-ì„ íƒ-ì´ìœ ) 
  * [1-2. ì°¸ê³ : Gemma License](#1-2-ì°¸ê³ -gemma-license)
* [2. How to run Fine-Tuning](#2-how-to-run-fine-tuning)
* [3. LLM Memory (RAG-like concept)](#3-llm-memory-rag-like-concept)
  * [3-1. ë™ì‘ ì›ë¦¬](#3-1-ë™ì‘-ì›ë¦¬)
  * [3-2. í•™ìŠµ ë° í…ŒìŠ¤íŠ¸ ë°ì´í„° & í•™ìŠµ ì„¤ì •](#3-2-í•™ìŠµ-ë°-í…ŒìŠ¤íŠ¸-ë°ì´í„°--í•™ìŠµ-ì„¤ì •)
  * [3-3. í…ŒìŠ¤íŠ¸ ê²°ê³¼](#3-3-í…ŒìŠ¤íŠ¸-ê²°ê³¼)
* [4. Test / Run Model](#4-test--run-model)
  * [4-1. Prepare Model (Gemma-2 2B Based)](#4-1-prepare-model-gemma-2-2b-based)
  * [4-2. Prepare Model (Polyglot-Ko 1.3Bâœ… Based)](#4-2-prepare-model-polyglot-ko-13b-based)
  * [4-3. Prepare S-BERT Model](#4-3-prepare-s-bert-model)
  * [4-4. Run LLM Fine-Tuning](#4-4-run-llm-fine-tuning)
  * [4-5. Run Final Fine-Tuned Model](#4-5-run-final-fine-tuned-model)
  * [4-6. Run S-BERT Memory Model](#4-6-run-s-bert-memory-model)
* [5. Unsloth use test (âŒ FAILED)](#5-unsloth-use-test--failed)
* [6. í–¥í›„ í•˜ê³  ì‹¶ì€ ê²ƒ](#6-í–¥í›„-í•˜ê³ -ì‹¶ì€-ê²ƒ)

## 1. LLM Final Selection

* **Polyglot-Ko 1.3B (1.43 B params)**
  * [HuggingFace](https://huggingface.co/EleutherAI/polyglot-ko-1.3b)
* [LLM Selection Report](model_selection/README.md) ê¸°ì¤€
  * ìµœì¢… ëª¨ë¸ : **Gemma-2 2B**
  * ì˜ˆë¹„ ëª¨ë¸ : **Polyglot-Ko 1.3B (âœ… ìµœì¢… ì±„íƒ)**

### 1-1. Polyglot-Ko 1.3B ì„ íƒ ì´ìœ 

* **ì‹¤ì œ Fine-Tuning ëœ ëª¨ë¸** ì˜ ìƒì„± ë¬¸ì¥ ì¸¡ë©´ [(Gemma-2 2B í…ŒìŠ¤íŠ¸ ê²°ê³¼)](fine_tuning/fine_tuning_logs/2504221644%20(Inference,%2025042213%20dataset,%20temp=1.2).txt) [(Polyglot-Ko 1.3B í…ŒìŠ¤íŠ¸ ê²°ê³¼)](fine_tuning/fine_tuning_logs_polyglot/2504230855%20(Inference,%20epochs=60,%20rank=64,%20temp=0.6).txt)
  * Gemma-2 2B ê°€ Polyglot-Ko 1.3B ë³´ë‹¤ ìƒì„± ë¬¸ì¥ì˜ í’ˆì§ˆì´ ì „ë°˜ì ìœ¼ë¡œ ë–¨ì–´ì§
    * Gemma-2 2B ëŠ” ë™ì¼ ì§ˆë¬¸ì— ëŒ€í•´ **ìœ ì‚¬í•œ ë‹µë³€ì„ ìƒì„±** í•˜ëŠ” ë¹ˆë„ê°€ Polyglot-Ko 1.3B ë³´ë‹¤ í˜„ì €íˆ ë†’ìŒ
    * Gemma-2 2B ëŠ” **íŠ¹ì • ì§ˆë¬¸ì— ëŒ€í•´ empty answer ë¥¼ ìƒì„± (ì½ì”¹)** í•˜ëŠ” ë¹ˆë„ê°€ Polyglot-Ko 1.3B ë³´ë‹¤ í˜„ì €íˆ ë†’ìŒ
    * Gemma-2 2B ëŠ” **ì–´ìƒ‰í•œ ì™¸êµ­ì–´ ë¬¸ì¥** ì„ ìƒì„±í•˜ëŠ” ê²½ìš°ê°€ ë§ìŒ 
  * memory ì •ë³´ (ì˜ˆ: ```[ì˜¤ëŠ˜ ì¼ì •: ì‹ ê·œ ì•„ì´í…œ ë°œí‘œ]```) íŒŒì•… ë° íŒ¨ë“œë¦½ ëŒ€ì‘ (ê²½ê³  ì²˜ë¦¬) ëŠ¥ë ¥ì€ Gemma-2 2B ê°€ Polyglot-Ko 1.3B ë³´ë‹¤ ë†’ì€ í¸ì´ì§€ë§Œ, ì¹˜ëª…ì ì¸ ì´ìŠˆëŠ” ì•„ë‹˜
* ê¸°íƒ€
  * Gemma-2 2B ëŠ” Polyglot-Ko 1.3B ì™€ ë‹¬ë¦¬ [Totally Free License ê°€ ì•„ë‹˜](#1-2-ì°¸ê³ -gemma-license)
  * Polyglot-Ko 1.3B ëŠ” **íŒŒë¼ë¯¸í„° ê°œìˆ˜ê°€ Gemma-2 2B ì˜ ì ˆë°˜ ìˆ˜ì¤€ (2.61 B vs. 1.43 B)**
    * ì¦‰, í•™ìŠµ/ì¶”ë¡  ì‹œê°„ ë° ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ê´€ì ì—ì„œ ë¹„êµì  ê°€ë³ê³  ë¹ ë¦„

### 1-2. ì°¸ê³ : Gemma License

* Source : [Gemma Terms of Use > Use Restrictions](https://ai.google.dev/gemma/terms#3.2-use)
* Checked Date : Apr 21, 2025 (KST)

----

3.2 Use Restrictions

You must not use any of the Gemma Services:

* for the restricted uses set forth in the Gemma Prohibited Use Policy at [ai.google.dev/gemma/prohibited_use_policy](https://ai.google.dev/gemma/prohibited_use_policy) **("Prohibited Use Policy")**, which is hereby incorporated by reference into this Agreement; or
* in violation of applicable laws and regulations.

To the maximum extent permitted by law, Google reserves the right to restrict (remotely or otherwise) usage of any of the Gemma Services that Google reasonably believes are in violation of this Agreement.

----

## 2. How to run Fine-Tuning

**1. Fine-Tuning ë°©ë²• ë° ë°ì´í„°ì…‹**

* í•™ìŠµ ëª¨ë¸
  * **Polyglot-Ko 1.3B (1.43 B params) (âœ… ìµœì¢… ì±„íƒ)** [HuggingFace](https://huggingface.co/EleutherAI/polyglot-ko-1.3b) 
* í•™ìŠµ ë°©ë²• 
  * [SFT (Supervised Fine-Tuning)](https://github.com/WannaBeSuperteur/AI-study/blob/main/AI%20Basics/LLM%20Basics/LLM_%EA%B8%B0%EC%B4%88_Fine_Tuning_SFT.md)
  * [LoRA (Low-Rank Adaption)](https://github.com/WannaBeSuperteur/AI-study/blob/main/AI%20Basics/LLM%20Basics/LLM_%EA%B8%B0%EC%B4%88_Fine_Tuning_LoRA_QLoRA.md), LoRA Rank = 128
  * train for **80 epochs (= 3h 21m)** [(train report)](fine_tuning/fine_tuning_logs_polyglot/2504231956%20(epochs=80,%20rank=128,%20temp=0.6).txt)
* í•™ìŠµ ë°ì´í„°ì…‹
  * [Train & Valid Dataset](OhLoRA_fine_tuning_25042213.csv) (**360** Q & A pairs for training / **60** Q & A pairs for validation) 
* Fine-Tuning ë°©ë²• ì„ íƒ ê·¼ê±°
  * ë©”ëª¨ë¦¬ ë° ì—°ì‚°ëŸ‰ì„ ì ˆì•½ ê°€ëŠ¥í•œ, ìµœê·¼ ë§ì´ ì“°ì´ëŠ” LLM Fine-Tuning ë°©ë²• ì¤‘ í•˜ë‚˜
  * **Oh-LoRA (ì˜¤ë¡œë¼)** ë¼ëŠ” ì´ë¦„ì˜ ìƒì§•ì„±ì„ ê³ ë ¤
  * ë„ë¦¬ ì•Œë ¤ì§„ ë‹¤ë¥¸ ë°©ë²•ë“¤ì¸ [Prefix Tuning](https://github.com/WannaBeSuperteur/AI-study/blob/main/AI%20Basics/LLM%20Basics/LLM_%EA%B8%B0%EC%B4%88_Fine_Tuning_PEFT.md#2-3-prefix-tuning), [Prompt Tuning](https://github.com/WannaBeSuperteur/AI-study/blob/main/AI%20Basics/LLM%20Basics/LLM_%EA%B8%B0%EC%B4%88_Fine_Tuning_PEFT.md#2-4-prompt-tuning), [Adapter Layer ì¶”ê°€](https://github.com/WannaBeSuperteur/AI-study/blob/main/AI%20Basics/LLM%20Basics/LLM_%EA%B8%B0%EC%B4%88_Fine_Tuning_PEFT.md#2-5-adapter-layer-%EC%B6%94%EA%B0%80) ë“±ì€ Multi-task LLM ì— ë³´ë‹¤ ì í•©í•œë°, ë³¸ LLM ì€ **ë‹¨ìˆœ ëŒ€í™”í˜• LLM ì´ ëª©ì ì´ë¯€ë¡œ Multi-task ë¡œ ë³´ê¸° ë‹¤ì†Œ ì–´ë ¤ì›€**

**2. ëª¨ë¸ ë³„ í•™ìŠµ íŠ¹ì´ ì‚¬í•­**

* **Gemma-2 2B**
  * íŠ¹ì´ ì‚¬í•­ ì—†ìŒ
* **Polyglot-Ko 1.3B (âœ… ìµœì¢… ì±„íƒ)**
  * í•™ìŠµ ì‹œ ë¬¸ì œì  
    * Original Polyglot-Ko 1.3B LLM ì˜ tokenizer ì˜ end-of-sequence token ì¸ ```<|endoftext|>``` ê°€, **Fine-Tuning ëœ ëª¨ë¸ì—ì„œëŠ” ë§¤ìš° ë“œë¬¼ê²Œ ìƒì„±** ë¨
    * ì´ë¡œ ì¸í•´ **ê±°ì˜ ëŒ€ë¶€ë¶„ì˜ ìƒì„± ë¬¸ì¥ì´ max token length ì¸ 80 ì— ë„ë‹¬í•¨**
  * í•´ê²°ì±…ìœ¼ë¡œ, ë‹¤ìŒ ë°©ë²•ì„ ì´ìš©
    * ëª¨ë“  í•™ìŠµ ë°ì´í„°ì˜ ë‹µë³€ ë¶€ë¶„ì˜ ëì— ```(ë‹µë³€ ì¢…ë£Œ)``` ë¬¸êµ¬ë¥¼ ì¶”ê°€ í›„,
    * ```stopping_criteria``` ë¥¼ ì´ìš©í•˜ì—¬ ```(ë‹µë³€ ì¢…ë£Œ)``` ì— í•´ë‹¹í•˜ëŠ” token ì´ ì¶œë ¥ë  ì‹œ ë¬¸ì¥ ìƒì„± ì¤‘ì§€

## 3. LLM Memory (RAG-like concept)

* LLM ì—ëŠ” ê¸°ë³¸ì ìœ¼ë¡œ ë©”ëª¨ë¦¬ê°€ ì—†ì–´ì„œ **ë°©ê¸ˆ í•œ ë§ì¡°ì°¨ ê¸°ì–µí•˜ì§€ ëª»í•˜ê³ **, ì´ëŠ” [í™˜ê° í˜„ìƒ](https://github.com/WannaBeSuperteur/AI-study/blob/main/AI%20Basics/LLM%20Basics/LLM_%EA%B8%B0%EC%B4%88_%ED%99%98%EA%B0%81_%ED%98%84%EC%83%81.md) ì˜ ì›ì¸ ì¤‘ í•˜ë‚˜ì„
* ë³¸ Oh-LoRA í”„ë¡œì íŠ¸ì—ì„œëŠ” [RAG (Retrieval Augmented Generation)](https://github.com/WannaBeSuperteur/AI-study/blob/main/AI%20Basics/LLM%20Basics/LLM_%EA%B8%B0%EC%B4%88_RAG.md) ê³¼ ìœ ì‚¬í•œ ë°©ë²•ìœ¼ë¡œ LLM ì˜ ë©”ëª¨ë¦¬ êµ¬í˜„

### 3-1. ë™ì‘ ì›ë¦¬

* ì‚¬ìš©ì ì…ë ¥ì— ëŒ€í•´ **ê°€ì¥ ê´€ë ¨ ìˆëŠ” memory item** ì„ [S-BERT (Sentence BERT)](https://github.com/WannaBeSuperteur/AI-study/blob/main/Natural%20Language%20Processing/Basics_BERT%2C%20SBERT%20%EB%AA%A8%EB%8D%B8.md#sbert-%EB%AA%A8%EB%8D%B8) ëª¨ë¸ì„ ì´ìš©í•˜ì—¬ íƒìƒ‰
* ì°¾ì€ memory item (ë‹¨, **cosine similarity $\ge$ 0.6** ì´ì–´ì•¼ í•¨) ì„ ì‚¬ìš©ì ì…ë ¥ì˜ ë§¨ ì•ì— ì¶”ê°€
* ì˜¤ë¡œë¼ğŸ‘±â€â™€ï¸ ì—ê²Œ **memory item ë‚´ìš©ì´ ì•ì— ì¶”ê°€ëœ í”„ë¡¬í”„íŠ¸ë¥¼ ìµœì¢… ì „ë‹¬**
* êµ¬í˜„ ì½”ë“œ
  * [S-BERT Training](memory_mechanism/train_sbert.py)
  * [S-BERT Inference](memory_mechanism/inference_sbert.py)
  * [Entry & Best Memory Item Choice](run_memory_mechanism.py)

![image](../../images/250408_28.PNG)

### 3-2. í•™ìŠµ ë° í…ŒìŠ¤íŠ¸ ë°ì´í„° & í•™ìŠµ ì„¤ì •

* í•™ìŠµ ë° í…ŒìŠ¤íŠ¸ ë°ì´í„°
  * **ì‹¤ì œ ë°ì´í„°** ëŠ” **ë°ì´í„° ìƒì„±ìš© ì¡°í•©** ì˜ ê° line ì˜ **memory** (ì˜ˆ: ```[ì˜¤ëŠ˜ ì¼ì •: ì¹œêµ¬ë‘ ì¹´í˜ ë°©ë¬¸]```) ì™€ **message** (ë‚˜ë¨¸ì§€ ë¶€ë¶„) ì„ SQL ì˜ cartesian product ì™€ ìœ ì‚¬í•œ ë°©ë²•ìœ¼ë¡œ combination (?) í•˜ì—¬ ìƒì„±
  * [ë°ì´í„° ìƒì„± êµ¬í˜„ ì½”ë“œ](memory_mechanism/generate_dataset.py)

| ë°ì´í„°        | ë°ì´í„° ìƒì„±ìš© ì¡°í•©                                                                    | ì‹¤ì œ ë°ì´í„°<br>(í•™ìŠµ ëŒ€ìƒ column : ```memory_0``` ```user_prompt_1``` ```similarity_score```) |
|------------|-------------------------------------------------------------------------------|--------------------------------------------------------------------------------------|
| í•™ìŠµ ë° valid | [train_dataset_combs.txt](memory_mechanism/train_dataset_combs.txt) (40 rows) | [train_dataset.csv](memory_mechanism/train_dataset.csv) (1,600 rows)                 |
| í…ŒìŠ¤íŠ¸        | [test_dataset_combs.txt](memory_mechanism/test_dataset_combs.txt) (20 rows)   | [test_dataset.csv](memory_mechanism/test_dataset.csv) (400 rows)                     |

* Cosine Similarity ì˜ Ground Truth ê°’
  * memory text ì˜ key (ì˜ˆ: ```[ì˜¤ëŠ˜ ì¼ì •: ì¹œêµ¬ë‘ ì¹´í˜ ë°©ë¬¸]``` â†’ ```ì˜¤ëŠ˜ ì¼ì •```) ì— ëŒ€í•´,
  * ì´ key ë¥¼ **ê³µë°±ìœ¼ë¡œ êµ¬ë¶„í•œ ê° ë‹¨ì–´ (ì˜ˆ: ```ì˜¤ëŠ˜``` ```ì¼ì •```) ì— ëŒ€í•œ [IoU Score](https://github.com/WannaBeSuperteur/AI-study/blob/main/AI%20Basics/Data%20Science%20Basics/%EB%8D%B0%EC%9D%B4%ED%84%B0_%EC%82%AC%EC%9D%B4%EC%96%B8%EC%8A%A4_%EA%B8%B0%EC%B4%88_Metrics.md#2-1-iou)** ë¥¼ Ground Truth ë¡œ í•¨
  * ë‹¨, ```ì¢‹ì•„í•˜ëŠ” ì•„ì´ëŒ``` ê³¼ ```ì¢‹ì•„í•˜ëŠ” ê°€ìˆ˜``` ë¼ëŠ” key ëŠ” ë™ì¼í•œ key ë¡œ ê°„ì£¼ 
* í•™ìŠµ ì„¤ì •
  * Base Model : ```klue/roberta-base``` [(HuggingFace Link)](https://huggingface.co/klue/roberta-base)
  * Pooling ì„¤ì • : Mean Pooling ì ìš©
  * 10 epochs
* [ì°¸ê³ í•œ ë¸”ë¡œê·¸ í¬ìŠ¤íŒ…](https://velog.io/@jaehyeong/Basic-NLP-sentence-transformers-%EB%9D%BC%EC%9D%B4%EB%B8%8C%EB%9F%AC%EB%A6%AC%EB%A5%BC-%ED%99%9C%EC%9A%A9%ED%95%9C-SBERT-%ED%95%99%EC%8A%B5-%EB%B0%A9%EB%B2%95)

### 3-3. í…ŒìŠ¤íŠ¸ ê²°ê³¼

* Predicted vs. True Cosine Similarity ë¹„êµ (í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹)

![image](../../images/250408_27.PNG)

* MSE, MAE & Corr-coef (í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹)

| MSE    | MAE    | Corr-coef |
|--------|--------|-----------|
| 0.0880 | 0.1681 | 0.6259    |

## 4. Test / Run Model

### 4-1. Prepare Model (Gemma-2 2B Based)

**1. Gemma-2-2b Original Unsloth Model (by Google & Unsloth)**

* ```2025_04_08_OhLoRA/llm/models/original``` ì—, [gemma-2-2b-it Hugging-Face](https://huggingface.co/unsloth/gemma-2-2b-it/tree/main) ì—ì„œ ë‹¤ìš´ë¡œë“œë°›ì€ ëª¨ë¸ ë° ê´€ë ¨ íŒŒì¼ ì €ì¥
  * ì´ 9 ê°œ íŒŒì¼ (ê°ì¢… ì •ë³´ í¬í•¨)
  * ì´ë•Œ ```models/original``` ë””ë ‰í† ë¦¬ëŠ” Clone ë°›ì€ repo. ì— ì›ë˜ ì—†ìœ¼ë¯€ë¡œ, ìƒˆë¡œ ìƒì„±  

* **ğŸš¨ VERY IMPORTANT ğŸš¨**
  * **ì¶”ê°€ ê°œë°œ ë˜ëŠ” ì‹¤ì œ ì‚¬ìš© ì‹œ, [Gemma License](#1-2-ì°¸ê³ -gemma-license) ë¥¼ ì¤€ìˆ˜í•´ì•¼ í•¨**

```
2025_04_08_OhLoRA
- llm
  - fine_tuning
  - model_selection
  - models                        (í•„ìš” ì‹œ ë””ë ‰í† ë¦¬ ìƒˆë¡œ ìƒì„±)
    - original                    (ë””ë ‰í† ë¦¬ ìƒˆë¡œ ìƒì„±)
      - .gitarrtibutes            (ë‹¤ìš´ë¡œë“œ ë°›ì€ íŒŒì¼)
      - README.md                 (ë‹¤ìš´ë¡œë“œ ë°›ì€ íŒŒì¼)
      - config.json               (ë‹¤ìš´ë¡œë“œ ë°›ì€ íŒŒì¼)
      - generation_config.json    (ë‹¤ìš´ë¡œë“œ ë°›ì€ íŒŒì¼)
      - model.safetensors         (ë‹¤ìš´ë¡œë“œ ë°›ì€ íŒŒì¼)
      - special_tokens_map.json   (ë‹¤ìš´ë¡œë“œ ë°›ì€ íŒŒì¼)
      - tokenizer.json            (ë‹¤ìš´ë¡œë“œ ë°›ì€ íŒŒì¼)
      - tokenizer.model           (ë‹¤ìš´ë¡œë“œ ë°›ì€ íŒŒì¼)
      - tokenizer_config.json     (ë‹¤ìš´ë¡œë“œ ë°›ì€ íŒŒì¼)
    - ...
  - unsloth_test
  - ...  
```

**2. Final Fine-Tuned Model**

* [LLM Fine Tuning ë°©ë²•](#4-4-run-llm-fine-tuning) ì˜ **"1. Gemma2 2B Fine Tuning"** ëŒ€ë¡œ ì‹¤í–‰í•˜ë©´ ```2025_04_08_OhLoRA/llm/models/fine_tuned``` ì— Fine-Tuning ëœ Gemma2-2B ëª¨ë¸ ì €ì¥ë¨

### 4-2. Prepare Model (Polyglot-Ko 1.3Bâœ… Based)

**1. Polyglot-Ko Original Model (by EleutherAI, âœ… ìµœì¢… ì±„íƒ)**

* ```2025_04_08_OhLoRA/llm/models/polyglot_original``` ì—, [Polyglot-Ko 1.3B Hugging-Face](https://huggingface.co/EleutherAI/polyglot-ko-1.3b) ì—ì„œ ë‹¤ìš´ë¡œë“œë°›ì€ ëª¨ë¸ ë° ê´€ë ¨ íŒŒì¼ ì €ì¥
  * ì´ 12 ê°œ íŒŒì¼ (ê°ì¢… ì •ë³´ í¬í•¨)
  * ì´ë•Œ ```models/polyglot_original``` ë””ë ‰í† ë¦¬ëŠ” Clone ë°›ì€ repo. ì— ì›ë˜ ì—†ìœ¼ë¯€ë¡œ, ìƒˆë¡œ ìƒì„±

```
2025_04_08_OhLoRA
- llm
  - fine_tuning
  - model_selection
  - models                                 (í•„ìš” ì‹œ ë””ë ‰í† ë¦¬ ìƒˆë¡œ ìƒì„±)
    - polyglot_original                    (ë””ë ‰í† ë¦¬ ìƒˆë¡œ ìƒì„±)
      - .gitarrtibutes                     (ë‹¤ìš´ë¡œë“œ ë°›ì€ íŒŒì¼)
      - README.md                          (ë‹¤ìš´ë¡œë“œ ë°›ì€ íŒŒì¼)
      - config.json                        (ë‹¤ìš´ë¡œë“œ ë°›ì€ íŒŒì¼)
      - generation_config.json             (ë‹¤ìš´ë¡œë“œ ë°›ì€ íŒŒì¼)
      - model-00001-of-00003.safetensors   (ë‹¤ìš´ë¡œë“œ ë°›ì€ íŒŒì¼)
      - model-00002-of-00003.safetensors   (ë‹¤ìš´ë¡œë“œ ë°›ì€ íŒŒì¼)
      - model-00003-of-00003.safetensors   (ë‹¤ìš´ë¡œë“œ ë°›ì€ íŒŒì¼)
      - model.safetensors.index.json       (ë‹¤ìš´ë¡œë“œ ë°›ì€ íŒŒì¼)
      - pytorch_model.bin                  (ë‹¤ìš´ë¡œë“œ ë°›ì€ íŒŒì¼)
      - special_tokens_map.json            (ë‹¤ìš´ë¡œë“œ ë°›ì€ íŒŒì¼)
      - tokenizer.json                     (ë‹¤ìš´ë¡œë“œ ë°›ì€ íŒŒì¼)
      - tokenizer_config.json              (ë‹¤ìš´ë¡œë“œ ë°›ì€ íŒŒì¼)
    - ...
  - unsloth_test
  - ...  
```

**2. Final Fine-Tuned Model**

* ```2025_04_08_OhLoRA/llm/models/polyglot_fine_tuned``` ì—, [HuggingFace Link](https://huggingface.co/daebakgazua/250408_OhLoRA_LLM/tree/main) ë¡œë¶€í„° ë‹¤ìš´ë¡œë“œ ë°›ì€ ëª¨ë¸ ì €ì¥
  * ì´ 8 ê°œ íŒŒì¼ (ê°ì¢… ì •ë³´ í¬í•¨)
  * ì´ë•Œ ```models/polyglot_fine_tuned``` ë””ë ‰í† ë¦¬ëŠ” Clone ë°›ì€ repo. ì— ì›ë˜ ì—†ìœ¼ë¯€ë¡œ, ìƒˆë¡œ ìƒì„±

```
2025_04_08_OhLoRA
- llm
  - fine_tuning
  - model_selection
  - models                                 (í•„ìš” ì‹œ ë””ë ‰í† ë¦¬ ìƒˆë¡œ ìƒì„±)
    - polyglot_fine_tuned                  (ë””ë ‰í† ë¦¬ ìƒˆë¡œ ìƒì„±)
      - .gitarrtibutes                     (ë‹¤ìš´ë¡œë“œ ë°›ì€ íŒŒì¼)
      - adapter_config.json                (ë‹¤ìš´ë¡œë“œ ë°›ì€ íŒŒì¼)
      - adapter_model.safetensors          (ë‹¤ìš´ë¡œë“œ ë°›ì€ íŒŒì¼)
      - README.md                          (ë‹¤ìš´ë¡œë“œ ë°›ì€ íŒŒì¼)
      - special_tokens_map.json            (ë‹¤ìš´ë¡œë“œ ë°›ì€ íŒŒì¼)
      - tokenizer.json                     (ë‹¤ìš´ë¡œë“œ ë°›ì€ íŒŒì¼)
      - tokenizer_config.json              (ë‹¤ìš´ë¡œë“œ ë°›ì€ íŒŒì¼)
    - ...
  - unsloth_test
  - ...  
```

### 4-3. Prepare S-BERT Model

**1. Final Fine-Tuned S-BERT Model**

* ```2025_04_08_OhLoRA/llm/models/memory_sbert/trained_sbert_model``` ì—, [HuggingFace Link](https://huggingface.co/daebakgazua/250408_OhLoRA_LLM_SBERT/tree/main) ë¡œë¶€í„° ë‹¤ìš´ë¡œë“œ ë°›ì€ ëª¨ë¸ ì €ì¥
  * ì´ 12 ê°œ íŒŒì¼ (ê°ì¢… ì •ë³´ í¬í•¨)
  * ì´ë•Œ ```models/memory_sbert/trained_sbert_model``` ë””ë ‰í† ë¦¬ëŠ” Clone ë°›ì€ repo. ì— ì›ë˜ ì—†ìœ¼ë¯€ë¡œ, ìƒˆë¡œ ìƒì„±

```
2025_04_08_OhLoRA
- llm
  - fine_tuning
  - model_selection
  - models                                                     (í•„ìš” ì‹œ ë””ë ‰í† ë¦¬ ìƒˆë¡œ ìƒì„±)
    - memory_sbert                                             (ë””ë ‰í† ë¦¬ ìƒˆë¡œ ìƒì„±)
      - trained_sbert_model                                    (ë””ë ‰í† ë¦¬ ìƒˆë¡œ ìƒì„±)
        - 1_Pooling                                            (ë””ë ‰í† ë¦¬ ìƒˆë¡œ ìƒì„±)
          - config.json                                        (ë‹¤ìš´ë¡œë“œ ë°›ì€ íŒŒì¼)
        - eval                                                 (ë””ë ‰í† ë¦¬ ìƒˆë¡œ ìƒì„±)
          - similarity_evaluation_valid_evaluator_results.csv  (ë‹¤ìš´ë¡œë“œ ë°›ì€ íŒŒì¼)
        - config.json                                          (ë‹¤ìš´ë¡œë“œ ë°›ì€ íŒŒì¼)
        - config_sentence_transformers.json                    (ë‹¤ìš´ë¡œë“œ ë°›ì€ íŒŒì¼)
        - model.safetensors                                    (ë‹¤ìš´ë¡œë“œ ë°›ì€ íŒŒì¼)
        - modules.json                                         (ë‹¤ìš´ë¡œë“œ ë°›ì€ íŒŒì¼)
        - README.md                                            (ë‹¤ìš´ë¡œë“œ ë°›ì€ íŒŒì¼)
        - sentence_bert_config.json                            (ë‹¤ìš´ë¡œë“œ ë°›ì€ íŒŒì¼)
        - special_tokens_map.json                              (ë‹¤ìš´ë¡œë“œ ë°›ì€ íŒŒì¼)
        - tokenizer.json                                       (ë‹¤ìš´ë¡œë“œ ë°›ì€ íŒŒì¼)
        - tokenizer_config.json                                (ë‹¤ìš´ë¡œë“œ ë°›ì€ íŒŒì¼)
        - vocab.txt                                            (ë‹¤ìš´ë¡œë“œ ë°›ì€ íŒŒì¼)
    - ...
  - unsloth_test
  - ...  
```

### 4-4. Run LLM Fine-Tuning

**1. Gemma2 2B Fine Tuning**

* ë¨¼ì €, [ì´ ë¬¸ë‹¨](#4-1-prepare-model-gemma-2-2b-based) > **"1. Gemma-2-2b Original Unsloth Model (by Google & Unsloth)"** ì— ë”°ë¼ Original Gemma2 2B ëª¨ë¸ ì¤€ë¹„
* í”„ë¡œì íŠ¸ ë©”ì¸ ë””ë ‰í† ë¦¬ (```2025_04_08_OhLoRA```) ì—ì„œ ```python llm/run_fine_tuning.py``` ì‹¤í–‰
* **Gemma2 2B ëª¨ë¸ ì‚¬ìš© ì‹œ, ë¼ì´ì„ ìŠ¤ ì•ˆë‚´ í•„ë… (ì™„ì „ ììœ  ë¼ì´ì„ ìŠ¤ê°€ ì•„ë‹˜)** 

**2. Polyglot-Ko 1.3B (âœ… ìµœì¢… ì±„íƒ) Fine Tuning**

* ë¨¼ì €, [ì´ ë¬¸ë‹¨](#4-2-prepare-model-polyglot-ko-13b-based) > **"1. Polyglot-Ko Original Model (by EleutherAI, âœ… ìµœì¢… ì±„íƒ)"** ì— ë”°ë¼ Original Polyglot-Ko 1.3B ëª¨ë¸ ì¤€ë¹„
* í”„ë¡œì íŠ¸ ë©”ì¸ ë””ë ‰í† ë¦¬ (```2025_04_08_OhLoRA```) ì—ì„œ ```python llm/run_fine_tuning_polyglot.py``` ì‹¤í–‰

### 4-5. Run Final Fine-Tuned Model

**ì§„í–‰ ìˆœì„œ**

* ë¨¼ì €, [í•´ë‹¹ ë¬¸ë‹¨](#4-2-prepare-model-polyglot-ko-13b-based) > **"2. Final Fine-Tuned Model"** ì„ ì°¸ê³ í•˜ì—¬ **ìµœì¢… Oh-LoRA LLM ëª¨ë¸ (Polyglot-Ko 1.3B Fine-Tuned LLM)** ì¤€ë¹„
* í”„ë¡œì íŠ¸ ë©”ì¸ ë””ë ‰í† ë¦¬ (```2025_04_08_OhLoRA```) ì—ì„œ ```python llm/run_fine_tuning_polyglot.py``` ì‹¤í–‰
  * ì´ë•Œ, Final Fine-Tuned ëª¨ë¸ ```2025_04_08_OhLoRA/llm/models/polyglot_fine_tuned``` ì´ **ì´ë¯¸ ì¡´ì¬** í•˜ë¯€ë¡œ, ëª¨ë¸ì´ ìƒˆë¡œ í•™ìŠµë˜ì§€ ì•Šê³  **Final Fine-Tuned Model ì— ëŒ€í•œ Inference ê°€ ì§„í–‰** ë¨

### 4-6. Run S-BERT Memory Model

**ì§„í–‰ ìˆœì„œ**

* ë¨¼ì €, [í•´ë‹¹ ë¬¸ë‹¨](#4-3-prepare-s-bert-model) > **"1. Final Fine-Tuned S-BERT Model"** ì„ ì°¸ê³ í•˜ì—¬ **ìµœì¢… Oh-LoRA S-BERT ëª¨ë¸** ì¤€ë¹„
* í”„ë¡œì íŠ¸ ë©”ì¸ ë””ë ‰í† ë¦¬ (```2025_04_08_OhLoRA```) ì—ì„œ ```python llm/run_memory_mechanism.py``` ì‹¤í–‰

## 5. Unsloth use test (âŒ FAILED)

**1. ì‹¤í—˜ ëª©ì **

* 2024ë…„ì— ê³µê°œëœ [Unsloth](https://unsloth.ai/) ë¼ëŠ” íˆ´ì„ ì´ìš©í•˜ë©´ **LLM ì˜ í•™ìŠµ ì‹œê°„ ë° ë©”ëª¨ë¦¬ ë“± ìì›ì„ ì ˆì•½** í•  ìˆ˜ ìˆë‹¤.
* ë³¸ task (with Quadro M6000 12GB GPU) ì—ì„œëŠ” ì–´ëŠ ì •ë„ì˜ ì†ë„ í–¥ìƒ ë° ë©”ëª¨ë¦¬ ì ˆê°ì´ ìˆëŠ”ì§€ íŒŒì•…í•˜ì—¬ í–¥í›„ ì°¸ê³ í•œë‹¤.

**2. ì‹¤í—˜ ê²°ê³¼**

* ê²°ë¡ 
  * **âŒ Quadro M6000 (12 GB) ì—ì„œ Unsloth í•™ìŠµ ë¶ˆê°€ëŠ¥** [(ì°¸ê³ )](https://github.com/unslothai/unsloth/issues/1998)

```
RuntimeError: Found Quadro M6000 which is too old to be supported by the triton GPU compiler, which is used as the backend. Triton only supports devices of CUDA Capability >= 7.0, but your device is of CUDA capability 5.2       
```

| í…ŒìŠ¤íŠ¸              | Inference ë©”ëª¨ë¦¬<br>(nvidia-smi ì¸¡ì •ê°’) | Inference ì‹œê°„ | Fine-Tuning ë©”ëª¨ë¦¬<br>(nvidia-smi ì¸¡ì •ê°’) | Fine-Tuning ì‹œê°„ |
|------------------|-----------------------------------|--------------|-------------------------------------|----------------|
| **with** Unsloth | 2,169 MB (3,829 MB) âŒ             | âŒ            | âŒ                                   | âŒ              |
| **w/o** Unsloth  | 5,013 MB (5,924 MB)               | 158.2 s      | 5,994 MB (9,503 MB)                 | 19.7 s         |
| ì ˆê°               | 56.7 % (35.4 %)                   | -            | -                                   | -              |

**3. ì‹¤í—˜ ì„¤ì •**

* Inference
  * run inference on 25 data (Q & A pairs)
* Fine-Tuning
  * **60 (data * epochs) = 20 data * 3 epochs**
    * ì‹¤ì œ í•™ìŠµ ë°ì´í„°ì˜ **16.7% = 0.167 epochs** ë¶„ëŸ‰
  * epoch & batch size
    * 3 epochs
    * train batch size = 4
    * valid batch size = 1
  * data
    * total 20 data (Q & A pairs) for training
    * 5 data (Q & A pairs) for validation

**3. ì½”ë“œ ì‹¤í–‰ ë°©ë²•**

* ë¨¼ì €, [Prepare Model](#4-1-prepare-model-gemma-2-2b-based) ì— ë‚˜ì˜¨ ëŒ€ë¡œ ëª¨ë¸ ì¤€ë¹„
* ```2025_04_08_OhLoRA``` ë©”ì¸ ë””ë ‰í† ë¦¬ì—ì„œ ì‹¤í–‰

| í…ŒìŠ¤íŠ¸              | Python ëª…ë ¹ì–´                                            |
|------------------|-------------------------------------------------------|
| **with** Unsloth | ```python llm/unsloth_test/test_with_unsloth.py```    |
| **w/o** Unsloth  | ```python llm/unsloth_test/test_without_unsloth.py``` |

## 6. í–¥í›„ í•˜ê³  ì‹¶ì€ ê²ƒ

Oh-LoRA ì°¨ê¸° ë²„ì „ ê°œë°œ ì‹œ ì§„í–‰í•˜ê³  ì‹¶ì€ ê²ƒë“¤:

* **1. í•™ìŠµ ë°ì´í„° ì¦ëŸ‰**
  * í•™ìŠµ ë°ì´í„°ì…‹ ê¸°ì¡´ 360ê°œ â†’ 1,000ê°œ ì •ë„ë¡œ ì¦ëŸ‰ (ë‹¤ì–‘í•œ ìƒí™©)
  * í•™ìŠµ ë°ì´í„°ì˜ LLM output ì— ë©”ì‹œì§€, ë©”ëª¨ë¦¬ ì™¸ì— **í‘œì •ì„ ë°”ê¾¸ëŠ” ì§€ì‹œë¬¸** ë„ ì¶”ê°€
    * ì´ ê²½ìš°, ë©”ì‹œì§€, ë©”ëª¨ë¦¬, í‘œì • ì§€ì‹œë¬¸ì„ ê°ê° ê°œë³„ LLM (ì´ 3ê°œ) ìœ¼ë¡œ ì²˜ë¦¬í•  ìˆ˜ ìˆìŒ
    * 1ê°œì˜ LLM ìœ¼ë¡œ ì´ 3ê°€ì§€ ë°ì´í„°ë¥¼ ëª¨ë‘ ì²˜ë¦¬í•˜ëŠ” ê²½ìš°, max token count 80 â†’ 96 ì •ë„ë¡œ ìƒí–¥ ê°€ëŠ¥

* **2. ë” ë§¤ë ¥ì ì¸ ë‹µë³€ì„ í•˜ë„ë¡ ì„±ëŠ¥ í–¥ìƒ**
  * LoRA Configuration ì˜ Target Module ë²”ìœ„ ì¡°ì •
  * LoRA Rank ìƒí–¥

* **3. RAG-like Memory ë©”ì»¤ë‹ˆì¦˜ ê³ ë„í™”**
  * S-BERT í•™ìŠµ ë°ì´í„° ì¶”ê°€ í™•ë³´
  * Ground-truth Cosine Similarity Score ì‚°ì¶œ ì•Œê³ ë¦¬ì¦˜ ê³ ë„í™”
  * ê¸°íƒ€ ì¶”ê°€ ì—°êµ¬ (S-BERT ë¥¼ ìœ„í•œ ì ì ˆí•œ Embedding ëª¨ë¸ ì„ íƒ ë“±)