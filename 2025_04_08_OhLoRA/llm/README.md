## ëª©ì°¨

* [1. LLM Final Selection](#1-llm-final-selection)
  * [1-1. Polyglot-Ko 1.3B ì„ íƒ ì´ìœ ](#1-1-polyglot-ko-13b-ì„ íƒ-ì´ìœ ) 
  * [1-2. ì°¸ê³ : Gemma License](#1-2-ì°¸ê³ -gemma-license)
* [2. How to run Fine-Tuning](#2-how-to-run-fine-tuning)
* [3. LLM Memory (RAG-like concept)](#3-llm-memory-rag-like-concept)
* [4. Test / Run Model](#4-test--run-model)
  * [4-1. Prepare Model (Gemma-2 2B Based)](#4-1-prepare-model-gemma-2-2b-based)
  * [4-2. Prepare Model (Polyglot-Ko 1.3B Based)](#4-2-prepare-model-polyglot-ko-13b-based)
  * [4-3. Unsloth use test](#4-3-unsloth-use-test)
  * [4-4. Run LLM Fine-Tuning](#4-4-run-llm-fine-tuning)
  * [4-5. Run Final Fine-Tuned Model](#4-5-run-final-fine-tuned-model)

## 1. LLM Final Selection

* **Polyglot-Ko 1.3B (1.43 B params)**
  * [HuggingFace](https://huggingface.co/EleutherAI/polyglot-ko-1.3b)
* [LLM Selection Report](model_selection/README.md) ê¸°ì¤€
  * ìµœì¢… ëª¨ë¸ : **Gemma-2 2B**
  * ì˜ˆë¹„ ëª¨ë¸ : **Polyglot-Ko 1.3B (âœ… ìµœì¢… ì±„íƒ)**

### 1-1. Polyglot-Ko 1.3B ì„ íƒ ì´ìœ 

* **ì‹¤ì œ Fine-Tuning ëœ ëª¨ë¸** ì˜ ìƒì„± ë¬¸ì¥ ì¸¡ë©´ [(Gemma-2 2B í…ŒìŠ¤íŠ¸ ê²°ê³¼)](fine_tuning/fine_tuning_logs/2504221644%20(Inference,%2025042213%20dataset,%20temp=1.2).txt) [(Polyglot-Ko 1.3B í…ŒìŠ¤íŠ¸ ê²°ê³¼)](fine_tuning/fine_tuning_logs_polyglot/2504230855%20(Inference,%20epochs=60,%20rank=64,%20temp=0.6).txt)
  * Gemma-2 2B ê°€ Polyglot-Ko 1.3B ë³´ë‹¤ ìƒì„± ë¬¸ì¥ì˜ í’ˆì§ˆì´ ì „ë°˜ì ìœ¼ë¡œ ë–¨ì–´ì§
    * Gemma-2 2B ëŠ” ë™ì¼ ì§ˆë¬¸ì— ëŒ€í•´ **ìœ ì‚¬í•œ ë‹µë³€ì„ ìƒì„±** í•˜ëŠ” ë¹ˆë„ê°€ Polyglot-Ko 1.3B ë³´ë‹¤ í˜„ì €íˆ ë†’ìŒ
    * Gemma-2 2B ëŠ” **íŠ¹ì • ì§ˆë¬¸ì— ëŒ€í•´ empty answer ë¥¼ ìƒì„± (ì½ì”¹)** í•˜ëŠ” ë¹ˆë„ê°€ Polyglot-Ko 1.3B ë³´ë‹¤ í˜„ì €íˆ ë†’ìŒ
    * Gemma-2 2B ëŠ” **ì–´ìƒ‰í•œ ì™¸êµ­ì–´ ë¬¸ì¥** ì„ ìƒì„±í•˜ëŠ” ê²½ìš°ê°€ ë§ìŒ 
  * memory ì •ë³´ (ì˜ˆ: ```[ì˜¤ëŠ˜ ì¼ì •: ì‹ ê·œ ì•„ì´í…œ ë°œí‘œ]```) íŒŒì•… ë° íŒ¨ë“œë¦½ ëŒ€ì‘ (ê²½ê³  ì²˜ë¦¬) ëŠ¥ë ¥ì€ Gemma-2 2B ê°€ Polyglot-Ko 1.3B ë³´ë‹¤ ë†’ì€ í¸ì´ì§€ë§Œ, ì¹˜ëª…ì ì¸ ì´ìŠˆëŠ” ì•„ë‹˜
* ê¸°íƒ€
  * Gemma-2 2B ëŠ” Polyglot-Ko 1.3B ì™€ ë‹¬ë¦¬ [Totally Free License ê°€ ì•„ë‹˜](#1-2-ì°¸ê³ -gemma-license)
  * Polyglot-Ko 1.3B ëŠ” **íŒŒë¼ë¯¸í„° ê°œìˆ˜ê°€ Gemma-2 2B ì˜ ì ˆë°˜ ìˆ˜ì¤€ (2.61 B vs. 1.43 B)**
    * ì¦‰, í•™ìŠµ/ì¶”ë¡  ì‹œê°„ ë° ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ê´€ì ì—ì„œ ë¹„êµì  ê°€ë³ê³  ë¹ ë¦„

### 1-2. ì°¸ê³ : Gemma License

* Source : [Gemma Terms of Use > Use Restrictions](https://ai.google.dev/gemma/terms#3.2-use)
* Checked Date : Apr 21, 2025 (KST)

----

3.2 Use Restrictions

You must not use any of the Gemma Services:

* for the restricted uses set forth in the Gemma Prohibited Use Policy at [ai.google.dev/gemma/prohibited_use_policy](https://ai.google.dev/gemma/prohibited_use_policy) **("Prohibited Use Policy")**, which is hereby incorporated by reference into this Agreement; or
* in violation of applicable laws and regulations.

To the maximum extent permitted by law, Google reserves the right to restrict (remotely or otherwise) usage of any of the Gemma Services that Google reasonably believes are in violation of this Agreement.

----

## 2. How to run Fine-Tuning

**1. Fine-Tuning ë°©ë²• ë° ë°ì´í„°ì…‹**

* í•™ìŠµ ëª¨ë¸
  * **Polyglot-Ko 1.3B (1.43 B params)** [HuggingFace](https://huggingface.co/EleutherAI/polyglot-ko-1.3b) 
* í•™ìŠµ ë°©ë²• 
  * [SFT (Supervised Fine-Tuning)](https://github.com/WannaBeSuperteur/AI-study/blob/main/AI%20Basics/LLM%20Basics/LLM_%EA%B8%B0%EC%B4%88_Fine_Tuning_SFT.md)
  * [LoRA (Low-Rank Adaption)](https://github.com/WannaBeSuperteur/AI-study/blob/main/AI%20Basics/LLM%20Basics/LLM_%EA%B8%B0%EC%B4%88_Fine_Tuning_LoRA_QLoRA.md), LoRA Rank = 64
  * train for **60 epochs (= 2h 21m)** [(train report)](fine_tuning/fine_tuning_logs_polyglot/2504230200%20(stop%20crit,%2060%20epochs,%20rank=64).txt)
* í•™ìŠµ ë°ì´í„°ì…‹
  * [Train & Valid Dataset](OhLoRA_fine_tuning_25042213.csv) (**360** Q & A pairs for training / **60** Q & A pairs for validation) 
* Fine-Tuning ë°©ë²• ì„ íƒ ê·¼ê±°
  * ë©”ëª¨ë¦¬ ë° ì—°ì‚°ëŸ‰ì„ ì ˆì•½ ê°€ëŠ¥í•œ, ìµœê·¼ ë§ì´ ì“°ì´ëŠ” LLM Fine-Tuning ë°©ë²• ì¤‘ í•˜ë‚˜
  * **Oh-LoRA (ì˜¤ë¡œë¼)** ë¼ëŠ” ì´ë¦„ì˜ ìƒì§•ì„±ì„ ê³ ë ¤
  * ë„ë¦¬ ì•Œë ¤ì§„ ë‹¤ë¥¸ ë°©ë²•ë“¤ì¸ [Prefix Tuning](https://github.com/WannaBeSuperteur/AI-study/blob/main/AI%20Basics/LLM%20Basics/LLM_%EA%B8%B0%EC%B4%88_Fine_Tuning_PEFT.md#2-3-prefix-tuning), [Prompt Tuning](https://github.com/WannaBeSuperteur/AI-study/blob/main/AI%20Basics/LLM%20Basics/LLM_%EA%B8%B0%EC%B4%88_Fine_Tuning_PEFT.md#2-4-prompt-tuning), [Adapter Layer ì¶”ê°€](https://github.com/WannaBeSuperteur/AI-study/blob/main/AI%20Basics/LLM%20Basics/LLM_%EA%B8%B0%EC%B4%88_Fine_Tuning_PEFT.md#2-5-adapter-layer-%EC%B6%94%EA%B0%80) ë“±ì€ Multi-task LLM ì— ë³´ë‹¤ ì í•©í•œë°, ë³¸ LLM ì€ **ë‹¨ìˆœ ëŒ€í™”í˜• LLM ì´ ëª©ì ì´ë¯€ë¡œ Multi-task ë¡œ ë³´ê¸° ë‹¤ì†Œ ì–´ë ¤ì›€**

**2. ëª¨ë¸ ë³„ í•™ìŠµ íŠ¹ì´ ì‚¬í•­**

* **Polyglot-Ko 1.3B**
  * í•™ìŠµ ì‹œ ë¬¸ì œì  
    * Original Polyglot-Ko 1.3B LLM ì˜ tokenizer ì˜ end-of-sequence token ì¸ ```<|endoftext|>``` ê°€, **Fine-Tuning ëœ ëª¨ë¸ì—ì„œëŠ” ë§¤ìš° ë“œë¬¼ê²Œ ìƒì„±** ë¨
    * ì´ë¡œ ì¸í•´ **ê±°ì˜ ëŒ€ë¶€ë¶„ì˜ ìƒì„± ë¬¸ì¥ì´ max token length ì¸ 80 ì— ë„ë‹¬í•¨**
  * í•´ê²°ì±…ìœ¼ë¡œ, ë‹¤ìŒ ë°©ë²•ì„ ì´ìš©
    * ëª¨ë“  í•™ìŠµ ë°ì´í„°ì˜ ë‹µë³€ ë¶€ë¶„ì˜ ëì— ```(ë‹µë³€ ì¢…ë£Œ)``` ë¬¸êµ¬ë¥¼ ì¶”ê°€ í›„,
    * ```stopping_criteria``` ë¥¼ ì´ìš©í•˜ì—¬ ```(ë‹µë³€ ì¢…ë£Œ)``` ì— í•´ë‹¹í•˜ëŠ” token ì´ ì¶œë ¥ë  ì‹œ ë¬¸ì¥ ìƒì„± ì¤‘ì§€

## 3. LLM Memory (RAG-like concept)

* TBU

## 4. Test / Run Model

### 4-1. Prepare Model (Gemma-2 2B Based)

**1. Gemma-2-2b Original Unsloth Model (by Google & Unsloth)**

* ```2025_04_08_OhLoRA/llm/models/original``` ì—, [gemma-2-2b-it Hugging-Face](https://huggingface.co/unsloth/gemma-2-2b-it/tree/main) ì—ì„œ ë‹¤ìš´ë¡œë“œë°›ì€ ëª¨ë¸ ë° ê´€ë ¨ íŒŒì¼ ì €ì¥
  * ì´ 9 ê°œ íŒŒì¼ (ê°ì¢… ì •ë³´ í¬í•¨)
  * ì´ë•Œ ```models/original``` ë””ë ‰í† ë¦¬ëŠ” Clone ë°›ì€ repo. ì— ì›ë˜ ì—†ìœ¼ë¯€ë¡œ, ìƒˆë¡œ ìƒì„±  

* **ğŸš¨ VERY IMPORTANT ğŸš¨**
  * **ì¶”ê°€ ê°œë°œ ë˜ëŠ” ì‹¤ì œ ì‚¬ìš© ì‹œ, [Gemma License](#1-2-ì°¸ê³ -gemma-license) ë¥¼ ì¤€ìˆ˜í•´ì•¼ í•¨**

```
2025_04_08_OhLoRA
- llm
  - fine_tuning
  - model_selection
  - models                        (í•„ìš” ì‹œ ë””ë ‰í† ë¦¬ ìƒˆë¡œ ìƒì„±)
    - original                    (ë””ë ‰í† ë¦¬ ìƒˆë¡œ ìƒì„±)
      - .gitarrtibutes            (ë‹¤ìš´ë¡œë“œ ë°›ì€ íŒŒì¼)
      - README.md                 (ë‹¤ìš´ë¡œë“œ ë°›ì€ íŒŒì¼)
      - config.json               (ë‹¤ìš´ë¡œë“œ ë°›ì€ íŒŒì¼)
      - generation_config.json    (ë‹¤ìš´ë¡œë“œ ë°›ì€ íŒŒì¼)
      - model.safetensors         (ë‹¤ìš´ë¡œë“œ ë°›ì€ íŒŒì¼)
      - special_tokens_map.json   (ë‹¤ìš´ë¡œë“œ ë°›ì€ íŒŒì¼)
      - tokenizer.json            (ë‹¤ìš´ë¡œë“œ ë°›ì€ íŒŒì¼)
      - tokenizer.model           (ë‹¤ìš´ë¡œë“œ ë°›ì€ íŒŒì¼)
      - tokenizer_config.json     (ë‹¤ìš´ë¡œë“œ ë°›ì€ íŒŒì¼)
    - ...
  - unsloth_test
  - ...  
```

**2. Final Fine-Tuned Model**

* ```2025_04_08_OhLoRA/llm/models/fine_tuned``` ì— ëª¨ë¸ ì €ì¥
* TBU (ê¸°ì¡´ ëª¨ë¸ ì¤€ë¹„ ë°©ë²•)

### 4-2. Prepare Model (Polyglot-Ko 1.3B Based)

**1. Polyglot-Ko Original Model (by EleutherAI)**

* ```2025_04_08_OhLoRA/llm/models/polyglot_original``` ì—, [Polyglot-Ko 1.3B Hugging-Face](https://huggingface.co/EleutherAI/polyglot-ko-1.3b) ì—ì„œ ë‹¤ìš´ë¡œë“œë°›ì€ ëª¨ë¸ ë° ê´€ë ¨ íŒŒì¼ ì €ì¥
  * ì´ 12 ê°œ íŒŒì¼ (ê°ì¢… ì •ë³´ í¬í•¨)
  * ì´ë•Œ ```models/polyglot_original``` ë””ë ‰í† ë¦¬ëŠ” Clone ë°›ì€ repo. ì— ì›ë˜ ì—†ìœ¼ë¯€ë¡œ, ìƒˆë¡œ ìƒì„±

```
2025_04_08_OhLoRA
- llm
  - fine_tuning
  - model_selection
  - models                                 (í•„ìš” ì‹œ ë””ë ‰í† ë¦¬ ìƒˆë¡œ ìƒì„±)
    - polyglot_original                    (ë””ë ‰í† ë¦¬ ìƒˆë¡œ ìƒì„±)
      - .gitarrtibutes                     (ë‹¤ìš´ë¡œë“œ ë°›ì€ íŒŒì¼)
      - README.md                          (ë‹¤ìš´ë¡œë“œ ë°›ì€ íŒŒì¼)
      - config.json                        (ë‹¤ìš´ë¡œë“œ ë°›ì€ íŒŒì¼)
      - generation_config.json             (ë‹¤ìš´ë¡œë“œ ë°›ì€ íŒŒì¼)
      - model-00001-of-00003.safetensors   (ë‹¤ìš´ë¡œë“œ ë°›ì€ íŒŒì¼)
      - model-00002-of-00003.safetensors   (ë‹¤ìš´ë¡œë“œ ë°›ì€ íŒŒì¼)
      - model-00003-of-00003.safetensors   (ë‹¤ìš´ë¡œë“œ ë°›ì€ íŒŒì¼)
      - model.safetensors.index.json       (ë‹¤ìš´ë¡œë“œ ë°›ì€ íŒŒì¼)
      - pytorch_model.bin                  (ë‹¤ìš´ë¡œë“œ ë°›ì€ íŒŒì¼)
      - special_tokens_map.json            (ë‹¤ìš´ë¡œë“œ ë°›ì€ íŒŒì¼)
      - tokenizer.json                     (ë‹¤ìš´ë¡œë“œ ë°›ì€ íŒŒì¼)
      - tokenizer_config.json              (ë‹¤ìš´ë¡œë“œ ë°›ì€ íŒŒì¼)
    - ...
  - unsloth_test
  - ...  
```

**2. Final Fine-Tuned Model**

* ```2025_04_08_OhLoRA/llm/models/polyglot_fine_tuned``` ì— ëª¨ë¸ ì €ì¥
* TBU (ê¸°ì¡´ ëª¨ë¸ ì¤€ë¹„ ë°©ë²•)

### 4-3. Unsloth use test

**1. ì‹¤í—˜ ëª©ì **

* 2024ë…„ì— ê³µê°œëœ [Unsloth](https://unsloth.ai/) ë¼ëŠ” íˆ´ì„ ì´ìš©í•˜ë©´ **LLM ì˜ í•™ìŠµ ì‹œê°„ ë° ë©”ëª¨ë¦¬ ë“± ìì›ì„ ì ˆì•½** í•  ìˆ˜ ìˆë‹¤.
* ë³¸ task (with Quadro 6000 12GB GPU) ì—ì„œëŠ” ì–´ëŠ ì •ë„ì˜ ì†ë„ í–¥ìƒ ë° ë©”ëª¨ë¦¬ ì ˆê°ì´ ìˆëŠ”ì§€ íŒŒì•…í•˜ì—¬ í–¥í›„ ì°¸ê³ í•œë‹¤.

**2. ì‹¤í—˜ ê²°ê³¼**

* ê²°ë¡ 
  * **âŒ Quadro M6000 (12 GB) ì—ì„œ Unsloth í•™ìŠµ ë¶ˆê°€ëŠ¥** [(ì°¸ê³ )](https://github.com/unslothai/unsloth/issues/1998)

```
RuntimeError: Found Quadro M6000 which is too old to be supported by the triton GPU compiler, which is used as the backend. Triton only supports devices of CUDA Capability >= 7.0, but your device is of CUDA capability 5.2       
```

| í…ŒìŠ¤íŠ¸              | Inference ë©”ëª¨ë¦¬<br>(nvidia-smi ì¸¡ì •ê°’) | Inference ì‹œê°„ | Fine-Tuning ë©”ëª¨ë¦¬<br>(nvidia-smi ì¸¡ì •ê°’) | Fine-Tuning ì‹œê°„ |
|------------------|-----------------------------------|--------------|-------------------------------------|----------------|
| **with** Unsloth | 2,169 MB (3,829 MB) âŒ             | âŒ            | âŒ                                   | âŒ              |
| **w/o** Unsloth  | 5,013 MB (5,924 MB)               | 158.2 s      | 5,994 MB (9,503 MB)                 | 19.7 s         |
| ì ˆê°               | 56.7 % (35.4 %)                   | -            | -                                   | -              |

**3. ì‹¤í—˜ ì„¤ì •**

* Inference
  * run inference on 25 data (Q & A pairs)
* Fine-Tuning
  * **60 (data * epochs) = 20 data * 3 epochs**
    * ì‹¤ì œ í•™ìŠµ ë°ì´í„°ì˜ **16.7% = 0.167 epochs** ë¶„ëŸ‰
  * epoch & batch size
    * 3 epochs
    * train batch size = 4
    * valid batch size = 1
  * data
    * total 20 data (Q & A pairs) for training
    * 5 data (Q & A pairs) for validation

**3. ì½”ë“œ ì‹¤í–‰ ë°©ë²•**

* ë¨¼ì €, [Prepare Model](#4-1-prepare-model) ì— ë‚˜ì˜¨ ëŒ€ë¡œ ëª¨ë¸ ì¤€ë¹„
* ```2025_04_08_OhLoRA``` ë©”ì¸ ë””ë ‰í† ë¦¬ì—ì„œ ì‹¤í–‰

| í…ŒìŠ¤íŠ¸              | Python ëª…ë ¹ì–´                                            |
|------------------|-------------------------------------------------------|
| **with** Unsloth | ```python llm/unsloth_test/test_with_unsloth.py```    |
| **w/o** Unsloth  | ```python llm/unsloth_test/test_without_unsloth.py``` |

### 4-4. Run LLM Fine-Tuning

* Gemma2 2B Fine Tuning
  * í”„ë¡œì íŠ¸ ë©”ì¸ ë””ë ‰í† ë¦¬ (```2025_04_08_OhLoRA```) ì—ì„œ ```python llm/run_fine_tuning.py``` ì‹¤í–‰
* **Polyglot-Ko 1.3B (âœ… ìµœì¢… ì±„íƒ)** Fine Tuning
  * í”„ë¡œì íŠ¸ ë©”ì¸ ë””ë ‰í† ë¦¬ (```2025_04_08_OhLoRA```) ì—ì„œ ```python llm/run_fine_tuning_polyglot.py``` ì‹¤í–‰

### 4-5. Run Final Fine-Tuned Model

* ë¨¼ì € (TBU) ë¥¼ ì°¸ê³ í•˜ì—¬ **ëª¨ë¸ (Polyglot-Ko 1.3B Fine-Tuned LLM)** ì¤€ë¹„
* TBU